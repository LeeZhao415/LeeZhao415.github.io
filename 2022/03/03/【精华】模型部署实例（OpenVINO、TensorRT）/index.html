<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>【精华】模型部署实例（OpenVINO、TensorRT） | 且听风吟，御剑于心！</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="LeeZhao,LeeZhao's Blog" />
  
  <meta name="description" content="文章目录   （1）OpenVINO 部署 NanoDet 模型  1&gt; nanodet 简介 2&gt; 环境配置 3&gt; NanoDet 模型训练和转换 ONNX 4&gt; NanoDet 模型部署 5&gt; 核心代码一览 6&gt; 推理时间展示及预测结果展示   （2）TensorRT 部署深度学习模型  1&gt; 背景 2&gt; 相关技术 3&gt; tensorf">
<meta property="og:type" content="article">
<meta property="og:title" content="【精华】模型部署实例（OpenVINO、TensorRT）">
<meta property="og:url" content="https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/index.html">
<meta property="og:site_name" content="且听风吟，御剑于心！">
<meta property="og:description" content="文章目录   （1）OpenVINO 部署 NanoDet 模型  1&gt; nanodet 简介 2&gt; 环境配置 3&gt; NanoDet 模型训练和转换 ONNX 4&gt; NanoDet 模型部署 5&gt; 核心代码一览 6&gt; 推理时间展示及预测结果展示   （2）TensorRT 部署深度学习模型  1&gt; 背景 2&gt; 相关技术 3&gt; tensorf">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210130130455826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDU0MTUx,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/c961b6b959233494f133799097940297.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/9e81d7152cbe9e4dbd45f14eb340d704.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/b7a75ea584e9dd89ef2867e4f2259648.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/641c21accf80edb33a2254ef22cac8c8.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/89445b8586b760aa23f31453e1137ad6.png">
<meta property="article:published_time" content="2022-03-03T14:56:11.000Z">
<meta property="article:modified_time" content="2022-03-03T15:18:59.118Z">
<meta property="article:author" content="LeeZhao">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="模型部署">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20210130130455826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDU0MTUx,size_16,color_FFFFFF,t_70">
  
  
    <link rel="icon" href="/images/hatRSS blk.ico">
  
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'true', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?true";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>

  
  <div style="display: none;">
    <script src="//s22.cnzz.com/z_stat.php?id=true&web_id=true" language="JavaScript"></script>
  </div>


<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
      <header id="header">
    <div id="banner"></div>
    <div id="header-outer">
        <div id="header-menu" class="header-menu-pos animated">
            <div class="header-menu-container">
                <a href="/" class="left">
                    <span class="site-title">LeeZhao&#39;s Blog</span>
                </a>
                <nav id="header-menu-nav" class="right">
                    
                    <a  href="/">
                        <i class="fa fa-home"></i>
                        <span>Home</span>
                    </a>
                    
                    <a  href="/archives">
                        <i class="fa fa-archive"></i>
                        <span>Archives</span>
                    </a>
                    
                    <a  href="/about">
                        <i class="fa fa-user"></i>
                        <span>About</span>
                    </a>
                    
                </nav>
                <a class="mobile-header-menu-button">
                    <i class="fa fa-bars"></i>
                </a>
            </div>
        </div>
        <div id="header-row">
            <div id="logo">
                <a href="/">
                    <img src="/images/logo.jpg" alt="logo">
                </a>
            </div>
            <div class="header-info">
                <div id="header-title">
                    
                    <h2>
                        LeeZhao&#39;s Blog
                    </h2>
                    
                </div>
                <div id="header-description">
                    
                    <h3>
                        且听风吟，御剑于心！
                    </h3>
                    
                </div>
            </div>
            <nav class="header-nav">
                <div class="social">
                    
                        <a title="CSDN" target="_blank" href="//blog.csdn.net/qq_36722887">
                            <i class="fa fa-home fa-2x"></i></a>
                    
                        <a title="Github" target="_blank" href="//github.com/leezhao415">
                            <i class="fa fa-github fa-2x"></i></a>
                    
                        <a title="Weibo" target="_blank" href="//weibo.com/u/5120617296/home?topnav=1&wvr=6">
                            <i class="fa fa-weibo fa-2x"></i></a>
                    
                        <a title="CodeSearch" target="_blank" href="//codesearch.aixcoder.com">
                            <i class="fa fa-twitter fa-2x"></i></a>
                    
                </div>
            </nav>
        </div>
    </div>
</header>
      <div class="outer">
        <section id="main" class="body-wrap"><article id="post-【精华】模型部署实例（OpenVINO、TensorRT）" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="post-title" itemprop="name">
      【精华】模型部署实例（OpenVINO、TensorRT）
    </h1>
    <div class="post-title-bar">
      <ul>
          
              <li>
                  <i class="fa fa-book"></i>
                  
                      <a href="/categories/Hot/">Hot</a>
                  
              </li>
          
        <li>
          <i class="fa fa-calendar"></i>  2022-03-03
        </li>
        <li>
          <i class="fa fa-eye"></i>
          <span id="busuanzi_value_page_pv"></span>
        </li>
      </ul>
    </div>
  

          
      </header>
    
    <div class="article-entry post-content" itemprop="articleBody">
      
            
            <meta name="referrer" content="no-referrer">
<hr>
<p><strong>文章目录</strong></p>
<!-- toc -->
<ul>
<li><a href="#1openvino%E9%83%A8%E7%BD%B2nanodet%E6%A8%A1%E5%9E%8B">（1）OpenVINO 部署 NanoDet 模型</a>
<ul>
<li><a href="#1-nanodet%E7%AE%80%E4%BB%8B">1&gt; nanodet 简介</a></li>
<li><a href="#2-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">2&gt; 环境配置</a></li>
<li><a href="#3-nanodet%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E8%BD%AC%E6%8D%A2onnx">3&gt; NanoDet 模型训练和转换 ONNX</a></li>
<li><a href="#4-nanodet%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2">4&gt; NanoDet 模型部署</a></li>
<li><a href="#5-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81%E4%B8%80%E8%A7%88">5&gt; 核心代码一览</a></li>
<li><a href="#6-%E6%8E%A8%E7%90%86%E6%97%B6%E9%97%B4%E5%B1%95%E7%A4%BA%E5%8F%8A%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA">6&gt; 推理时间展示及预测结果展示</a></li>
</ul>
</li>
<li><a href="#2tensorrt%E9%83%A8%E7%BD%B2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B">（2）TensorRT 部署深度学习模型</a>
<ul>
<li><a href="#1-%E8%83%8C%E6%99%AF">1&gt; 背景</a></li>
<li><a href="#2-%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF">2&gt; 相关技术</a></li>
<li><a href="#3-tensorflow%E6%A8%A1%E5%9E%8Btensorrt%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B">3&gt; tensorflow 模型 tensorRT 部署教程</a></li>
<li><a href="#4-caffe%E6%A8%A1%E5%9E%8Btensorrt%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B">4&gt; Caffe 模型 tensorRT 部署教程</a></li>
<li><a href="#5-%E4%B8%BAtensorrt%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82">5&gt; 为 tensorRT 添加自定义层</a></li>
<li><a href="#6-%E4%B8%BAcaffeparser%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82%E6%94%AF%E6%8C%81">6&gt; 为 CaffeParser 添加自定义层支持</a></li>
<li><a href="#7-%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95">7&gt; 心得体会（踩坑记录）</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->
<hr>
<h4><span id="1openvino-部署-nanodet-模型"> （1）OpenVINO 部署 NanoDet 模型</span></h4>
<h5><span id="1gt-nanodet-简介"> 1&gt; nanodet 简介</span></h5>
<p>NanoDet （<a target="_blank" rel="noopener" href="https://github.com/RangiLyu/nanodet%EF%BC%89%E6%98%AF%E4%B8%80%E4%B8%AA%E9%80%9F%E5%BA%A6%E8%B6%85%E5%BF%AB%E5%92%8C%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%9A%84Anchor-free">https://github.com/RangiLyu/nanodet）是一个速度超快和轻量级的 Anchor-free</a> 目标检测模型。想了解算法本身的可以去搜一搜之前机器之心的介绍。</p>
<h5><span id="2gt-环境配置"> 2&gt; 环境配置</span></h5>
<ul>
<li><a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=Ubuntu&amp;spm=1001.2101.3001.7020">Ubuntu</a>：18.04</li>
<li><a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=OpenVINO&amp;spm=1001.2101.3001.7020">OpenVINO</a>：2020.4</li>
<li><a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=OpenCV&amp;spm=1001.2101.3001.7020">OpenCV</a>：3.4.2</li>
<li>OpenVINO 和 OpenCV 安装包（编译好了，也可以自己从官网下载自己编译）可以从链接: <a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1zxtPKm-Q48Is5mzKbjGHeg">https://pan.baidu.com/s/1zxtPKm-Q48Is5mzKbjGHeg</a> 密码: gw5c 下载</li>
<li>OpenVINO 安装</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tar -xvzf l_openvino_toolkit_p_2020.4.287.tgz</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> l_openvino_toolkit_p_2020.4.287</span><br><span class="line">sudo ./install_GUI.sh 一路next安装</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /opt/intel/openvino/install_dependencies</span><br><span class="line">sudo ./install_openvino_dependencies.sh</span><br><span class="line"></span><br><span class="line">vi ~/.bashrc</span><br></pre></td></tr></table></figure>
<ul>
<li>把如下两行放置到 bashrc 文件尾</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /opt/intel/openvino/bin/setupvars.sh</span><br><span class="line"><span class="built_in">source</span> /opt/intel/openvino/opencv/setupvars.sh</span><br></pre></td></tr></table></figure>
<ul>
<li>source ~/.bashrc 激活环境</li>
<li>模型优化配置步骤</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites</span><br><span class="line">sudo ./install_prerequisites_onnx.sh（模型是从onnx转为IR文件，只需配置onnx依赖）</span><br></pre></td></tr></table></figure>
<ul>
<li>OpenCV 配置</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xvzf opencv-3.4.2.zip <span class="comment"># 解压OpenCV到用户根目录即可，以便后续调用。（这是我编译好的版本，有需要可以自己编译）</span></span><br></pre></td></tr></table></figure>
<h5><span id="3gt-nanodet-模型训练和转换-onnx"> 3&gt; NanoDet 模型训练和转换 ONNX</span></h5>
<ul>
<li>git clone <a target="_blank" rel="noopener" href="https://github.com/Wulingtian/nanodet.git">https://github.com/Wulingtian/nanodet.git</a></li>
<li>cd nanodet</li>
<li>cd config 配置模型文件，训练模型</li>
<li>定位到 nanodet 目录，进入 tools 目录，打开 export.py 文件，配置 cfg_path model_path out_path 三个参数</li>
<li>定位到 nanodet 目录，运行 python tools/export.py 得到转换后的 onnx 模型</li>
<li>python /opt/intel/openvino/deployment_tools/model_optimizer/mo_onnx.py --input_model onnx 模型 --output_dir 期望模型输出的路径。得到 IR 文件</li>
</ul>
<h5><span id="4gt-nanodet-模型部署"> 4&gt; NanoDet 模型部署</span></h5>
<ul>
<li>sudo apt install cmake 安装 cmake</li>
<li>git clone <a target="_blank" rel="noopener" href="https://github.com/Wulingtian/nanodet_openvino.git">https://github.com/Wulingtian/nanodet_openvino.git</a> （求 star！）</li>
<li>cd nanodet_openvino 打开 CMakeLists.txt 文件，修改 OpenCV_INCLUDE_DIRS 和 OpenCV_LIBS_DIR，之前已经把 OpenCV 解压到根目录了，所以按照你自己的路径指定</li>
<li>定位到 nanodet_openvino，cd models 把之前生成的 IR 模型（包括 bin 和 xml 文件）文件放到该目录下</li>
<li>定位到 nanodet_openvino， cd test_imgs 把需要测试的图片放到该目录下</li>
<li>定位到 nanodet_openvino，编辑 main.cpp，xml_path 参数修改为 &quot;…/models/ 你的模型名称.xml&quot;</li>
<li>编辑 num_class 设置类别数，例如：我训练的模型是安全帽检测，只有 1 类，那么设置为 1</li>
<li>编辑 src 设置测试图片路径，src 参数修改为 &quot;…/test_imgs/ 你的测试图片&quot;</li>
<li>定位到 nanodet_openvino</li>
<li>mkdir build; cd build; cmake … ;make</li>
<li>./detect_test 输出平均推理时间，以及保存预测图片到当前目录下，至此，部署完成！</li>
</ul>
<h5><span id="5gt-核心代码一览"> 5&gt; 核心代码一览</span></h5>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//主要对图片进行预处理，包括resize和归一化</span></span><br><span class="line">std::vector&lt;<span class="keyword">float</span>&gt; Detector::prepareImage(cv::Mat &amp;src_img)&#123;</span><br><span class="line"> </span><br><span class="line">    std::vector&lt;<span class="keyword">float</span>&gt; result(INPUT_W * INPUT_H * <span class="number">3</span>);</span><br><span class="line">    <span class="keyword">float</span> *data = result.data();</span><br><span class="line">    <span class="keyword">float</span> ratio = <span class="keyword">float</span>(INPUT_W) / <span class="keyword">float</span>(src_img.cols) &lt; <span class="keyword">float</span>(INPUT_H) / <span class="keyword">float</span>(src_img.rows) ? <span class="keyword">float</span>(INPUT_W) / <span class="keyword">float</span>(src_img.cols) : <span class="keyword">float</span>(INPUT_H) / <span class="keyword">float</span>(src_img.rows);</span><br><span class="line">    cv::Mat flt_img = cv::Mat::zeros(cv::Size(INPUT_W, INPUT_H), CV_8UC3);</span><br><span class="line">    cv::Mat rsz_img = cv::Mat::zeros(cv::Size(src_img.cols*ratio, src_img.rows*ratio), CV_8UC3);</span><br><span class="line">    cv::resize(src_img, rsz_img, cv::Size(), ratio, ratio);</span><br><span class="line"> </span><br><span class="line">    rsz_img.copyTo(flt_img(cv::Rect(<span class="number">0</span>, <span class="number">0</span>, rsz_img.cols, rsz_img.rows)));</span><br><span class="line">    flt_img.convertTo(flt_img, CV_32FC3);</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> channelLength = INPUT_W * INPUT_H;</span><br><span class="line">    std::vector&lt;cv::Mat&gt; split_img = &#123;</span><br><span class="line">            cv::Mat(INPUT_W, INPUT_H, CV_32FC1, data + channelLength * <span class="number">2</span>),</span><br><span class="line">            cv::Mat(INPUT_W, INPUT_H, CV_32FC1, data + channelLength),</span><br><span class="line">            cv::Mat(INPUT_W, INPUT_H, CV_32FC1, data)</span><br><span class="line">    &#125;;</span><br><span class="line"> </span><br><span class="line">    cv::split(flt_img, split_img);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++) &#123;</span><br><span class="line"> </span><br><span class="line">        split_img[i] = (split_img[i] - img_mean[i]) / img_std[i];</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">//加载IR模型，初始化网络</span></span><br><span class="line"><span class="keyword">bool</span> Detector::init(<span class="keyword">string</span> xml_path,<span class="keyword">double</span> cof_threshold,<span class="keyword">double</span> nms_area_threshold,<span class="keyword">int</span> input_w, <span class="keyword">int</span> input_h, <span class="keyword">int</span> num_class, <span class="keyword">int</span> r_rows, <span class="keyword">int</span> r_cols, std::vector&lt;<span class="keyword">int</span>&gt; s, std::vector&lt;<span class="keyword">float</span>&gt; i_mean,std::vector&lt;<span class="keyword">float</span>&gt; i_std)&#123;</span><br><span class="line">    _xml_path = xml_path;</span><br><span class="line">    _cof_threshold = cof_threshold;</span><br><span class="line">    _nms_area_threshold = nms_area_threshold;</span><br><span class="line">    INPUT_W = input_w;</span><br><span class="line">    INPUT_H = input_h;</span><br><span class="line">    NUM_CLASS = num_class;</span><br><span class="line">    refer_rows = r_rows;</span><br><span class="line">    refer_cols = r_cols;</span><br><span class="line">    strides = s;</span><br><span class="line">    img_mean = i_mean;</span><br><span class="line">    img_std = i_std;</span><br><span class="line">    Core ie;</span><br><span class="line">    auto cnnNetwork = ie.ReadNetwork(_xml_path); </span><br><span class="line"> </span><br><span class="line">    InputsDataMap inputInfo(cnnNetwork.getInputsInfo());</span><br><span class="line">    InputInfo::Ptr&amp; input = inputInfo.begin()-&gt;second;</span><br><span class="line">    _input_name = inputInfo.begin()-&gt;first;</span><br><span class="line">    input-&gt;setPrecision(Precision::FP32);</span><br><span class="line">    input-&gt;getInputData()-&gt;setLayout(Layout::NCHW);</span><br><span class="line">    ICNNNetwork::InputShapes inputShapes = cnnNetwork.getInputShapes();</span><br><span class="line">    SizeVector&amp; inSizeVector = inputShapes.begin()-&gt;second;</span><br><span class="line">    cnnNetwork.reshape(inputShapes);</span><br><span class="line">    _outputinfo = OutputsDataMap(cnnNetwork.getOutputsInfo());</span><br><span class="line">    <span class="keyword">for</span> (auto &amp;output : _outputinfo) &#123;</span><br><span class="line">        output.second-&gt;setPrecision(Precision::FP32);</span><br><span class="line">    &#125;</span><br><span class="line">    _network =  ie.LoadNetwork(cnnNetwork, <span class="string">&quot;CPU&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//模型推理及获取输出结果</span></span><br><span class="line">vector&lt;Detector::Bbox&gt; Detector::process_frame(Mat&amp; inframe)&#123;</span><br><span class="line"> </span><br><span class="line">    cv::Mat showImage = inframe.<span class="keyword">clone</span>();</span><br><span class="line">    std::vector&lt;<span class="keyword">float</span>&gt; pr_img = prepareImage(inframe);</span><br><span class="line">    InferRequest::Ptr infer_request = _network.CreateInferRequestPtr();</span><br><span class="line">    Blob::Ptr frameBlob = infer_request-&gt;GetBlob(_input_name);</span><br><span class="line">    InferenceEngine::LockedMemory&lt;<span class="keyword">void</span>&gt; blobMapped = InferenceEngine::as&lt;InferenceEngine::MemoryBlob&gt;(frameBlob)-&gt;wmap();</span><br><span class="line">    <span class="keyword">float</span>* blob_data = blobMapped.<span class="keyword">as</span>&lt;<span class="keyword">float</span>*&gt;();</span><br><span class="line"> </span><br><span class="line">    memcpy(blob_data, pr_img.data(), <span class="number">3</span> * INPUT_H * INPUT_W * sizeof(<span class="keyword">float</span>));</span><br><span class="line"> </span><br><span class="line">    infer_request-&gt;Infer();</span><br><span class="line">    vector&lt;Rect&gt; origin_rect;</span><br><span class="line">    vector&lt;<span class="keyword">float</span>&gt; origin_rect_cof;</span><br><span class="line">    <span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">    vector&lt;Bbox&gt; bboxes;</span><br><span class="line">    <span class="keyword">for</span> (auto &amp;output : _outputinfo) &#123;</span><br><span class="line">        auto output_name = output.first;</span><br><span class="line">        Blob::Ptr blob = infer_request-&gt;GetBlob(output_name);</span><br><span class="line">        LockedMemory&lt;<span class="keyword">const</span> <span class="keyword">void</span>&gt; blobMapped = <span class="keyword">as</span>&lt;MemoryBlob&gt;(blob)-&gt;rmap();</span><br><span class="line">        <span class="keyword">float</span> *output_blob = blobMapped.<span class="keyword">as</span>&lt;<span class="keyword">float</span> *&gt;();</span><br><span class="line">        bboxes = postProcess(showImage,output_blob);</span><br><span class="line">        ++i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> bboxes;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//对模型输出结果进行解码及nms</span></span><br><span class="line">std::vector&lt;Detector::Bbox&gt; Detector::postProcess(<span class="keyword">const</span> cv::Mat &amp;src_img,</span><br><span class="line">                              <span class="keyword">float</span> *output) &#123;</span><br><span class="line">    GenerateReferMatrix();</span><br><span class="line">    std::vector&lt;Detector::Bbox&gt; result;</span><br><span class="line">    <span class="keyword">float</span> *out = output;</span><br><span class="line">    <span class="keyword">float</span> ratio = std::max(<span class="keyword">float</span>(src_img.cols) / <span class="keyword">float</span>(INPUT_W), <span class="keyword">float</span>(src_img.rows) / <span class="keyword">float</span>(INPUT_H));</span><br><span class="line">    cv::Mat result_matrix = cv::Mat(refer_rows, NUM_CLASS + <span class="number">4</span>, CV_32FC1, out);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> row_num = <span class="number">0</span>; row_num &lt; refer_rows; row_num++) &#123;</span><br><span class="line">        Detector::Bbox box;</span><br><span class="line">        auto *row = result_matrix.ptr&lt;<span class="keyword">float</span>&gt;(row_num);</span><br><span class="line">        auto max_pos = std::max_element(row + <span class="number">4</span>, row + NUM_CLASS + <span class="number">4</span>);</span><br><span class="line">        box.prob = row[max_pos - row];</span><br><span class="line">        <span class="keyword">if</span> (box.prob &lt; _cof_threshold)</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        box.classes = max_pos - row - <span class="number">4</span>;</span><br><span class="line">        auto *anchor = refer_matrix.ptr&lt;<span class="keyword">float</span>&gt;(row_num);</span><br><span class="line">        box.x = (anchor[<span class="number">0</span>] - row[<span class="number">0</span>] * anchor[<span class="number">2</span>] + anchor[<span class="number">0</span>] + row[<span class="number">2</span>] * anchor[<span class="number">2</span>]) / <span class="number">2</span> * ratio;</span><br><span class="line">        box.y = (anchor[<span class="number">1</span>] - row[<span class="number">1</span>] * anchor[<span class="number">2</span>] + anchor[<span class="number">1</span>] + row[<span class="number">3</span>] * anchor[<span class="number">2</span>]) / <span class="number">2</span> * ratio;</span><br><span class="line">        box.w = (row[<span class="number">2</span>] + row[<span class="number">0</span>]) * anchor[<span class="number">2</span>] * ratio;</span><br><span class="line">        box.h = (row[<span class="number">3</span>] + row[<span class="number">1</span>]) * anchor[<span class="number">2</span>] * ratio;</span><br><span class="line">        result.push_back(box);</span><br><span class="line">    &#125;</span><br><span class="line">    NmsDetect(result);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5><span id="6gt-推理时间展示及预测结果展示"> 6&gt; 推理时间展示及预测结果展示</span></h5>
<center><img src="https://img-blog.csdnimg.cn/20210130130455826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDU0MTUx,size_16,color_FFFFFF,t_70" alt="img" style="zoom:60%;"></center>
<p>我的老笔记本平均推理时间 15ms 左右，CPU 下实时推理</p>
<p>安全帽检测结果</p>
<p>至此完成了 NanoDet 在 X86 CPU 上的部署，希望有帮助到大家。</p>
<h4><span id="2tensorrt-部署深度学习模型"> （2）TensorRT 部署深度学习模型</span></h4>
<p>原帖：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/84125533">https://zhuanlan.zhihu.com/p/84125533</a></p>
<h5><span id="1gt-背景"> 1&gt; 背景</span></h5>
<p>目前主流的<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020">深度学习</a>框架（caffe，mxnet，tensorflow，pytorch 等）进行模型推断的速度都并不优秀，在实际工程中用上述的框架进行模型部署往往是比较低效的。而通过 Nvidia 推出的 tensorRT 工具来部署主流框架上训练的模型能够极大的提高模型推断的速度，往往相比与原本的框架能够有至少 1 倍以上的速度提升，同时占用的设备内存也会更加的少。因此对是所有需要部署模型的同志来说，掌握用 tensorRT 来部署深度学习模型的方法是非常有用的。</p>
<h5><span id="2gt-相关技术"> 2&gt; 相关技术</span></h5>
<center><img src="https://img-blog.csdnimg.cn/img_convert/c961b6b959233494f133799097940297.png" alt="img" style="zoom:80%;"></center>
<p>上面的图片取自 TensorRT 的官网，里面列出了 tensorRT 使用的一些技术。可以看到比较成熟的深度学习落地技术：模型量化、动态内存优化、层的融合等技术均已经在 tensorRT 中集成了，这也是它能够极大提高模型推断速度的原因。总体来说 tensorRT 将训练好的模型通过一系列的优化技术转化为了能够在特定平台（GPU）上以高性能运行的代码，也就是最后图中生成的 Inference engine。目前也有一些其他的工具能够实现类似 tensorRT 的功能，例如<a href="https://link.zhihu.com/?target=https%3A//github.com/dmlc/tvm"> TVM</a>，<a href="https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/TensorComprehensions">TensorComprehensions</a> 也能有效的提高模型在特定平台上的推断速度，但是由于目前企业主流使用的都是 Nvidia 生产的计算设备，在这些设备上 nvidia 推出的 tensorRT 性能相比其他工具会更有优势一些。而且 tensorRT 依赖的代码库仅仅包括 C++ 和 cuda，相对与其他工具要更为精简一些。</p>
<h5><span id="3gt-tensorflow-模型-tensorrt-部署教程"> 3&gt; tensorflow 模型 tensorRT 部署教程</span></h5>
<p>实际工程部署中多采用 c<ins> 进行部署，因此在本教程中也使用的是 tensorRT 的 C</ins>API，tensorRT 版本为 5.1.5。具体 tensorRT 安装可参考教程 [<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64053177">深度学习] TensorRT 安装</a>，以及官网的安装说明。</p>
<h6><span id="1模型持久化"> （1）模型持久化</span></h6>
<p>部署 tensorflow 模型的第一步是模型持久化，将模型结构和权重保存到一个.pb 文件当中。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pb_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), [v.op.name <span class="keyword">for</span> v <span class="keyword">in</span> outputs])</span><br><span class="line"><span class="keyword">with</span> tf.gfile.FastGFile(<span class="string">&#x27;./pbmodel_name.pb&#x27;</span>, mode=<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(pb_graph.SerializeToString())</span><br></pre></td></tr></table></figure>
<p>具体只需在模型定义和权重读取之后执行以上代码，调用 tf.graph_util.convert_variables_to_constants 函数将权重转为常量，其中 outputs 是需要作为输出的<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=tensor&amp;spm=1001.2101.3001.7020"> tensor</a> 的列表，最后用 pb_graph.SerializeToString () 将 graph 序列化并写入到 pb 文件当中，这样就生成了 pb 模型。</p>
<h6><span id="2生成-uff-模型"> （2）生成 uff 模型</span></h6>
<p>有了 pb 模型，需要将其转换为 tensorRT 可用的 uff 模型，只需调用 uff 包自带的 convert 脚本即可</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python <span class="regexp">/usr/</span>lib<span class="regexp">/python2.7/</span>site-packages<span class="regexp">/uff/</span>bin/convert_to_uff.py   pbmodel_name.pb</span><br></pre></td></tr></table></figure>
<p>如转换成功会输出如下信息，包含图中总结点的个数以及推断出的输入输出节点的信息</p>
<center><img src="https://img-blog.csdnimg.cn/img_convert/9e81d7152cbe9e4dbd45f14eb340d704.png" alt="img" style="zoom:80%;"></center>
<h6><span id="3tensorrt-c-api-部署模型"> （3）tensorRT c++ API 部署模型</span></h6>
<p>使用 tensorRT 部署生成好的 uff 模型需要先讲 uff 中保存的模型权值以及网络结构导入进来，然后执行优化算法生成对应的 inference engine。具体代码如下，首先需要定义一个 IBuilder* builder，一个用来解析 uff 文件的 parser 以及 builder 创建的 network，parser 会将 uff 文件中的模型参数和网络结构解析出来存到 network，解析前要预先告诉 parser 网络输入输出输出的节点。解析后 builder 就能根据 network 中定义的网络结构创建 engine。在创建 engine 前会需要指定最大的 batchsize 大小，之后使用 engine 时输入的 batchsize 不能超过这个数值否则就会出错。推断时如果 batchsize 和设定最大值一样时效率最高。举个例子，如果设定最大 batchsize 为 10，实际推理输入一个 batch 10 张图的时候平均每张推断时间是 4ms 的话，输入一个 batch 少于 10 张图的时候平均每张图推断时间会高于 4ms。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">IBuilder* builder = createInferBuilder(gLogger.getTRTLogger());</span><br><span class="line">auto parser = createUffParser();</span><br><span class="line">parser-&gt;registerInput(inputtensor_name, Dims3(INPUT_C, INPUT_H, INPUT_W), UffInputOrder::kNCHW);</span><br><span class="line">parser-&gt;registerOutput(outputtensor_name);</span><br><span class="line">    INetworkDefinition* network = builder-&gt;createNetwork();</span><br><span class="line">    <span class="keyword">if</span> (!parser-&gt;parse(uffFile, *network, nvinfer1::DataType::kFLOAT))</span><br><span class="line">    &#123;</span><br><span class="line">        gLogError &lt;&lt; <span class="string">&quot;Failure while parsing UFF file&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">return</span> nullptr;</span><br><span class="line">    &#125;  </span><br><span class="line">    builder-&gt;setMaxBatchSize(maxBatchSize);</span><br><span class="line">    builder-&gt;setMaxWorkspaceSize(MAX_WORKSPACE);</span><br><span class="line">    ICudaEngine* engine = builder-&gt;buildCudaEngine(*network);</span><br><span class="line">    <span class="keyword">if</span> (!engine)</span><br><span class="line">    &#123;</span><br><span class="line">        gLogError &lt;&lt; <span class="string">&quot;Unable to create engine&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">return</span> nullptr;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>生成 engine 之后就可以进行推断了，执行推断时需要有一个上下文执行上下文 IExecutionContext* context，可以通过 engine-&gt;createExecutionContext () 获得。执行推断的核心代码是</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context-&gt;execute(batchSize, &amp;buffers<span class="selector-attr">[0]</span>);  </span><br></pre></td></tr></table></figure>
<p>其中 buffer 是一个 void * 数组对应的是模型输入输出 tensor 的设备地址，通过 cudaMalloc 开辟输入输出所需要的设备空间（显存）将对应指针存到 buffer 数组中，在执行 execute 操作前通过 cudaMemcpy 把输入数据（输入图像）拷贝到对应输入的设备空间，执行 execute 之后还是通过 cudaMemcpy 把输出的结果从设备上拷贝出来。</p>
<p>更为详细的例程可以参考 TensorRT 官方的 samples 中的<a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT/tree/master/samples/opensource/sampleUffMNIST"> sampleUffMNIST 代码</a></p>
<h6><span id="4加速比情况"> （4）加速比情况</span></h6>
<p>实际工程中我在 Tesla M40 上用 tensorRT 来加速过 Resnet-50，Inception-resnet-v2，谷歌图像检索模型 Delf（DEep Local Features），加速前后单张图推断用时比较如下图（单位 ms）</p>
<center><img src="https://img-blog.csdnimg.cn/img_convert/b7a75ea584e9dd89ef2867e4f2259648.png" alt="img"></center>
<h5><span id="4gt-caffe-模型-tensorrt-部署教程"> 4&gt; Caffe 模型 tensorRT 部署教程</span></h5>
<p>相比与 tensorflow 模型 caffe 模型的转换更加简单，不需要有 tensorflow 模型转 uff 模型这类的操作，tensorRT 能够直接解析 prototxt 和 caffemodel 文件获取模型的网络结构和权重。具体解析流程和上文描述的一致，不同的是 caffe 模型的 parser 不需要预先指定输入层，这是因为 prototxt 已经进行了输入层的定义，parser 能够自动解析出输入，另外 caffeparser 解析网络后返回一个 IBlobNameToTensor *blobNameToTensor 记录了网络中 tensor 和 pototxt 中名字的对应关系，在解析之后就需要通过这个对应关系，按照输出 tensor 的名字列表 outputs 依次找到对应的 tensor 并通过 network-&gt;markOutput 函数将其标记为输出，之后就可以生成 engine 了。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">IBuilder* builder = createInferBuilder(gLogger);</span><br><span class="line">    INetworkDefinition* network = builder-&gt;createNetwork();</span><br><span class="line">    ICaffeParser* parser = createCaffeParser();</span><br><span class="line">    DataType modelDataType = DataType::kFLOAT;</span><br><span class="line">    <span class="keyword">const</span> IBlobNameToTensor *blobNameToTensor =	parser-&gt;parse(deployFile.c_str(),</span><br><span class="line">                                                              modelFile.c_str(),</span><br><span class="line">                                                              *network,</span><br><span class="line">                                                              modelDataType);</span><br><span class="line">    assert(blobNameToTensor != nullptr);</span><br><span class="line">    <span class="keyword">for</span> (auto&amp; s : outputs) network-&gt;markOutput(*blobNameToTensor-&gt;find(s.c_str()));</span><br><span class="line"> </span><br><span class="line">    builder-&gt;setMaxBatchSize(maxBatchSize);</span><br><span class="line">    builder-&gt;setMaxWorkspaceSize(<span class="number">1</span> &lt;&lt; <span class="number">30</span>);</span><br><span class="line">    engine = builder-&gt;buildCudaEngine(*network);</span><br></pre></td></tr></table></figure>
<p>生成 engine 后执行的方式和上一节描述的一致，详细的例程可以参考<a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT/tree/master/samples/opensource/sampleMNIST"> SampleMNIST</a></p>
<h6><span id="1加速比情况"> （1）加速比情况</span></h6>
<p>实际工程中我在 Tesla M40 上用 tensorRT 加速过 caffe 的 VGG19，SSD 速度变为 1.6 倍，ResNet50，MobileNetV2 加速前后单张图推断用时比较如下图（单位 ms）</p>
<center><img src="https://img-blog.csdnimg.cn/img_convert/641c21accf80edb33a2254ef22cac8c8.png" alt="img"></center>
<h5><span id="5gt-为-tensorrt-添加自定义层"> 5&gt; 为 tensorRT 添加自定义层</span></h5>
<p>tensorRT 目前只支持一些非常常见的操作，有很多操作它并不支持比如上采样 Upsample 操作，这时候就需要我们自行将其编写为 tensorRT 的插件层，从而使得这些不能支持的操作能在 tensorRT 中使用。以定义 Upsample 层为例，我们首先要定义一个继承自 tensorRT 插件基类的 Upsample 类</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Upsample</span>: <span class="type">public IPluginExt</span></span></span><br></pre></td></tr></table></figure>
<p>然后要实现该类的一些必要方法，首先是 2 个构造函数，一个是传参数构建，另一个是从序列化后的比特流构建。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> <span class="built_in">Upsample</span>(<span class="keyword">int</span> scale = <span class="number">2</span>) : <span class="built_in">mScale</span>(scale) &#123;</span><br><span class="line">        <span class="built_in">assert</span>(mScale &gt; <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//定义上采样倍数</span></span><br><span class="line"> <span class="built_in">Upsmaple</span>(<span class="keyword">const</span> <span class="keyword">void</span> *data, <span class="keyword">size_t</span> length) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">char</span> *d = <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">const</span> <span class="keyword">char</span> *&gt;(data), *a = d;</span><br><span class="line">        mScale = read&lt;<span class="keyword">int</span>&gt;(d);</span><br><span class="line">        mDtype = read&lt;DataType&gt;(d);</span><br><span class="line">        mCHW = read&lt;DimsCHW&gt;(d);</span><br><span class="line">        <span class="built_in">assert</span>(mScale &gt; <span class="number">0</span>);</span><br><span class="line">        <span class="built_in">assert</span>(d == a + length);</span><br><span class="line">    &#125;</span><br><span class="line">~<span class="built_in">Upsample</span>()</span><br><span class="line">    &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>一些定义层输出信息的方法</p>
<figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">   <span class="function"><span class="built_in">int</span> <span class="title">getNbOutputs</span>(<span class="params"></span>) <span class="keyword">const</span> <span class="keyword">override</span></span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//模型的输出个数</span></span><br><span class="line">    <span class="function">Dims <span class="title">getOutputDimensions</span>(<span class="params"><span class="built_in">int</span> index, <span class="keyword">const</span> Dims *inputs, <span class="built_in">int</span> nbInputDims</span>) <span class="keyword">override</span></span> &#123;</span><br><span class="line">       <span class="comment">// std::cout &lt;&lt; &quot;Get ouputdims!!!&quot; &lt;&lt; std::endl;</span></span><br><span class="line">        assert(nbInputDims == <span class="number">1</span>);</span><br><span class="line">        assert(inputs[<span class="number">0</span>].nbDims == <span class="number">3</span>);</span><br><span class="line">        <span class="keyword">return</span> DimsCHW(inputs[<span class="number">0</span>].d[<span class="number">0</span>], inputs[<span class="number">0</span>].d[<span class="number">1</span>] * mScale, inputs[<span class="number">0</span>].d[<span class="number">2</span>] * mScale);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//获取模型输出的形状</span></span><br></pre></td></tr></table></figure>
<p>根据输入的形状个数以及采用的数据类型检查合法性以及配置层参数的方法</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">    bool supportsFormat(DataType type, PluginFormat format) const override &#123;</span><br><span class="line">        return (type == DataType::kFLOAT || type == DataType::kHALF || type == DataType::kINT8)</span><br><span class="line">               &amp;&amp; format == PluginFormat::kNCHW;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//检查层是否支持当前的数据类型和格式</span></span><br><span class="line">    void configureWithFormat(<span class="keyword">const</span> Dims *inputDims, int nbInputs, <span class="keyword">const</span> Dims *outputDims, int nbOutputs,</span><br><span class="line">                             DataType type, PluginFormat format, int maxBatchSize) override</span><br><span class="line">       &#123;</span><br><span class="line">         mDtype = <span class="class"><span class="keyword">type</span>;</span></span><br><span class="line">         mCHW.c() = inputDims[<span class="number">0</span>].d[<span class="number">0</span>];</span><br><span class="line">         mCHW.h() = inputDims[<span class="number">0</span>].d[<span class="number">1</span>];</span><br><span class="line">         mCHW.w() = inputDims[<span class="number">0</span>].d[<span class="number">2</span>];</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">//配置层的参数</span></span><br></pre></td></tr></table></figure>
<p>层的序列化方法</p>
<figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function">size_t <span class="title">getSerializationSize</span>(<span class="params"></span>) <span class="keyword">override</span></span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">sizeof</span>(mScale) + <span class="keyword">sizeof</span>(mDtype) + <span class="keyword">sizeof</span>(mCHW);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//输出序列化层所需的长度</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">serialize</span>(<span class="params"><span class="keyword">void</span> *buffer</span>) <span class="keyword">override</span></span> &#123;</span><br><span class="line">        <span class="built_in">char</span> *d = reinterpret_cast&lt;<span class="built_in">char</span> *&gt;(buffer), *a = d;</span><br><span class="line">        write(d, mScale);</span><br><span class="line">        write(d, mDtype);</span><br><span class="line">        write(d, mCHW);</span><br><span class="line">        assert(d == a + getSerializationSize());</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//将层参数序列化为比特流</span></span><br></pre></td></tr></table></figure>
<p>层的运算方法</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">size_t</span> <span class="title">getWorkspaceSize</span><span class="params">(<span class="keyword">int</span> maxBatchSize)</span> <span class="keyword">const</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//层运算需要的临时工作空间大小</span></span><br><span class="line"> <span class="function"><span class="keyword">int</span> <span class="title">enqueue</span><span class="params">(<span class="keyword">int</span> batchSize, <span class="keyword">const</span> <span class="keyword">void</span> *<span class="keyword">const</span> *inputs, <span class="keyword">void</span> **outputs, <span class="keyword">void</span> *workspace,</span></span></span><br><span class="line"><span class="function"><span class="params">                cudaStream_t stream)</span> <span class="keyword">override</span></span>;</span><br><span class="line"><span class="comment">//层执行计算的具体操作</span></span><br></pre></td></tr></table></figure>
<p>在 enqueue 中我们调用编写好的 cuda kenerl 来进行 Upsample 的计算</p>
<p>完成了 Upsample 类的定义，我们就可以直接在网络中添加我们编写的插件了，通过如下语句我们就定义一个上采样 2 倍的上采样层。addPluginExt 的第一个输入是 ITensor** 类别，这是为了支持多输出的情况，第二个参数就是输入个数，第三个参数就是需要创建的插件类对象。</p>
<figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Upsample <span class="title">up</span>(<span class="params"><span class="number">2</span></span>)；</span></span><br><span class="line"><span class="function">auto upsamplelayer</span>=network-&gt;addPluginExt(inputtensot,<span class="number">1</span>,up)</span><br></pre></td></tr></table></figure>
<h5><span id="6gt-为-caffeparser-添加自定义层支持"> 6&gt; 为 CaffeParser 添加自定义层支持</span></h5>
<p>对于我们自定义的层如果写到了 caffe prototxt 中，在部署模型时调用 caffeparser 来解析就会报错。</p>
<p>还是以 Upsample 为例，如果在 prototxt 中有下面这段来添加了一个 upsample 的层</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">&quot;upsample0&quot;</span></span><br><span class="line">  type: &quot;Upsample&quot;</span><br><span class="line">  bottom: <span class="string">&quot;ReLU11&quot;</span></span><br><span class="line">  top: <span class="string">&quot;Upsample1&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这时再调用</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> IBlobNameToTensor *blobNameToTensor =	parser-&gt;parse(deployFile.c_str(),</span><br><span class="line">                                                              modelFile.c_str(),</span><br><span class="line">                                                              *network,</span><br><span class="line">                                                              modelDataType);</span><br></pre></td></tr></table></figure>
<p>就会出现错误</p>
<center><img src="https://img-blog.csdnimg.cn/img_convert/89445b8586b760aa23f31453e1137ad6.png" alt="img" style="zoom:80%;"></center>
<p>之前我们已经编写了 Upsample 的插件，怎么让 tensorRT 的 caffe parser 识别出 prototxt 中的 upsample 层自动构建我们自己编写的插件呢？这时我们就需要定义一个插件工程类继承基类 nvinfer1::IPluginFactory, nvcaffeparser1::IPluginFactoryExt。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PluginFactory</span> :</span> <span class="keyword">public</span> nvinfer1::IPluginFactory, <span class="keyword">public</span> nvcaffeparser1::IPluginFactoryExt</span><br></pre></td></tr></table></figure>
<p>其中必须要的实现的方法有判断一个层是否是 plugin 的方法，输入的参数就是 prototxt 中 layer 的 name，通过 name 来判断一个层是否注册为插件</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPlugin</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *name)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">isPluginExt</span>(name);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPluginExt</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *name)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">char</span> *aa = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">6</span>];</span><br><span class="line">        <span class="built_in">memcpy</span>(aa, name, <span class="number">5</span>);</span><br><span class="line">        aa[<span class="number">5</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> res = !<span class="built_in">strcmp</span>(aa, <span class="string">&quot;upsam&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//判断层名字是否是upsample层的名字</span></span><br></pre></td></tr></table></figure>
<p>根据名字创建插件的方法，有两中方式一个是由权重构建，另一个是由序列化后的比特流创建，对应了插件的两种构造函数，Upsample 没有权重，对于其他有权重的插件就能够用传入的 weights 初始化层。mplugin 是一个 vector 用来存储所有创建的插件层。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">IPlugin *<span class="title">createPlugin</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *layerName, <span class="keyword">const</span> nvinfer1::Weights *weights, <span class="keyword">int</span> nbWeights)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="built_in">assert</span>(<span class="built_in">isPlugin</span>(layerName));</span><br><span class="line">        mPlugin.<span class="built_in">push_back</span>(std::unique_ptr&lt;Upsample&gt;(<span class="keyword">new</span> <span class="built_in">Upsample</span>(<span class="number">2</span>)));</span><br><span class="line">        <span class="keyword">return</span> mPlugin[mPlugin.<span class="built_in">size</span>() - <span class="number">1</span>].<span class="built_in">get</span>();</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function">IPlugin *<span class="title">createPlugin</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *layerName, <span class="keyword">const</span> <span class="keyword">void</span> *serialData, <span class="keyword">size_t</span> serialLength)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">        <span class="built_in">assert</span>(<span class="built_in">isPlugin</span>(layerName));</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="built_in">Upsample</span>(serialData, serialLength);</span><br><span class="line">    &#125;</span><br><span class="line"> std::vector &lt;std::unique_ptr&lt;Upsample&gt;&gt; mPlugin;</span><br></pre></td></tr></table></figure>
<p>最后需要定义一个 destroy 方法来释放所有创建的插件层。</p>
<figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">void</span> <span class="title">destroyPlugin</span>(<span class="params"></span>)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (unsigned <span class="built_in">int</span> i = <span class="number">0</span>; i &lt; mPlugin.size(); i++) &#123;</span><br><span class="line">            mPlugin[i].reset();</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于 prototxt 存在多个多种插件的情况，可以在 isPlugin，createPlugin 方法中添加新的条件分支，根据层的名字创建对应的插件层。</p>
<p>实现了 PluginFactory 之后在调用 caffeparser 的时候需要设置使用它，在调用 parser-&gt;parser 之前加入如下代码</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">PluginFactory</span> pluginFactory;</span><br><span class="line"><span class="title">parser</span>-&gt;setPluginFactoryExt(&amp;pluginFactory);</span><br></pre></td></tr></table></figure>
<p>就可以设置 parser 按照 pluginFactory 里面定义的规则来创建插件层，这样之前出现的不能解析 Upsample 层的错误就不会再出现了。</p>
<p>官方添加插件层的样例<a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT/tree/master/samples/opensource/samplePlugin"> samplePlugin</a> 可以作为参考</p>
<h5><span id="7gt-心得体会踩坑记录"> 7&gt; 心得体会（踩坑记录）</span></h5>
<ol>
<li>
<p>转 tensorflow 模型时，生成 pb 模型、转换 uff 模型以及调用 uffparser 时 register Input，output，这三个过程中输入输出节点的名字一定要注意保持一致，否则最终在 parser 进行解析时会出现错误，找不到输入输出节点。</p>
</li>
<li>
<p>除了本文中列举的 pluginExt，tensorRT 中插件基类还有 IPlugin，IPluginV2，继承这些基类所需要实现的类方法有细微区别，具体情况可自行查看 tensorRT 安装文件夹下的 include/NvInfer.h 文件。同时添加自己写的层到网络时的函数有 addPlugin，addPluginExt，addPluginV2 这几种和 IPlugin，IPluginExt，IPluginV2 一一对应，不能够混用，否则有些默认调用的类方法不会调用的，比如用 addPlugin 添加的 PluginExt 层是不会调用 configureWithFormat 方法的，因为 IPlugin 类没有该方法。同样的在还有 caffeparser 的 setPluginFactory 和 setPluginFactoryExt 也是不能混用的。</p>
</li>
<li>
<p>运行程序出现 cuda failure 一般情况下是由于将内存数据拷贝到磁盘时出现了非法内存访问，注意检查 buffer 开辟的空间大小和拷贝过去数据的大小是否一致.</p>
</li>
<li>
<p>有一些操作在 tensorRT 中不支持但是可以通过一些支持的操作进行组合替代，比如 ，这样可以省去一些编写自定义层的时间。</p>
</li>
<li>
<p>tensorflow 中的 flatten 操作默认时 keepdims=False 的，但是在转化 uff 文时会默认按照 keepdims=True 转换，因此在 tensorflow 中对 flatten 后的向量进行 transpose、expanddims 等等操作，在转换到 uff 后用 tensorRT 解析时容易出现错误，比如 “Order size is not matching the number dimensions of TensorRT” 。最好设置 tensorflow 的 reduce，flatten 操作的 keepdims=True，保持层的输出始终为 4 维形式，能够有效避免转到 tensorRT 时出现各种奇怪的错误。</p>
</li>
<li>
<p>tensorRT 中的 slice 层存在一定问题，我用 network-&gt;addSlice 给网络添加 slice 层后，在执行 buildengine 这一步时就会出错 nvinfer1::builder::checkSanity (const nvinfer1::builder::Graph&amp;): Assertion `tensors.size () == g.tensors.size ()’ failed.，构建网络时最好避开使用 slice 层，或者自己实现自定层来执行 slice 操作。</p>
</li>
<li>
<p>tensorRT 的 github 中有着部分的开源代码以及丰富的示例代码，多多学习能够帮助更快的掌握 tensorRT 的使用</p>
</li>
</ol>

            <div class="post-copyright">
    <div class="content">
        <p>最后更新： 2022年03月03日 23:18</p>
        <p>原始链接： <a class="post-url" href="/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/" title="【精华】模型部署实例（OpenVINO、TensorRT）">https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/</a></p>
        <footer>
            <a href="https://leezhao415.github.io">
                <img src="/images/logo.jpg" alt="LeeZhao">
                LeeZhao
            </a>
        </footer>
    </div>
</div>

      
        
            
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;">赏</a>
</div>

<div id="reward" class="post-modal reward-lay">
    <a class="close" href="javascript:;" id="reward-close">×</a>
    <span class="reward-title">
        <i class="icon icon-quote-left"></i>
        请我吃糖~
        <i class="icon icon-quote-right"></i>
    </span>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/images/wechat_code.jpg" alt="打赏二维码">
        </div>
        <div class="reward-select">
            
            <label class="reward-select-item checked" data-id="wechat" data-wechat="/images/wechat_code.jpg">
                <img class="reward-select-item-wechat" src="/images/wechat.png" alt="微信">
            </label>
            
            
            <label class="reward-select-item" data-id="alipay" data-alipay="/images/alipay_code.jpg">
                <img class="reward-select-item-alipay" src="/images/alipay.png" alt="支付宝">
            </label>
            
        </div>
    </div>
</div>


        
    </div>
    <footer class="article-footer">
        
        
<div class="post-share">
    <a href="javascript:;" id="share-sub" class="post-share-fab">
        <i class="fa fa-share-alt"></i>
    </a>
    <div class="post-share-list" id="share-list">
        <ul class="share-icons">
          <li>
            <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/&title=《【精华】模型部署实例（OpenVINO、TensorRT）》 — 且听风吟，御剑于心！&pic=images/OpenVINO部署NanoDet模型.jpg" data-title="微博">
              <i class="fa fa-weibo"></i>
            </a>
          </li>
          <li>
            <a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信">
              <i class="fa fa-weixin"></i>
            </a>
          </li>
          <li>
            <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/&title=《【精华】模型部署实例（OpenVINO、TensorRT）》 — 且听风吟，御剑于心！&source=" data-title="QQ">
              <i class="fa fa-qq"></i>
            </a>
          </li>
          <li>
            <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/" data-title="Facebook">
              <i class="fa fa-facebook"></i>
            </a>
          </li>
          <li>
            <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《【精华】模型部署实例（OpenVINO、TensorRT）》 — 且听风吟，御剑于心！&url=https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/&via=https://leezhao415.github.io" data-title="Twitter">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
          <li>
            <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/" data-title="Google+">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        </ul>
     </div>
</div>
<div class="post-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;" id="wxShare-close">×</a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/" alt="微信分享二维码">
</div>

<div class="mask"></div>

        
        <ul class="article-footer-menu">
            
            
  <li class="article-footer-tags">
    <i class="fa fa-tags"></i>
      
    <a href="/tags/人工智能/" class="color5">人工智能</a>
      
    <a href="/tags/模型部署/" class="color5">模型部署</a>
      
  </li>

        </ul>
        
    </footer>
  </div>
</article>


    <aside class="post-toc-pos post-toc-top" id="post-toc">
        <nav class="post-toc-wrap">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> （1）OpenVINO 部署 NanoDet 模型</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 1&gt; nanodet 简介</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 2&gt; 环境配置</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 3&gt; NanoDet 模型训练和转换 ONNX</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 4&gt; NanoDet 模型部署</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 5&gt; 核心代码一览</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 6&gt; 推理时间展示及预测结果展示</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> （2）TensorRT 部署深度学习模型</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 1&gt; 背景</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 2&gt; 相关技术</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 3&gt; tensorflow 模型 tensorRT 部署教程</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-6"><a class="post-toc-link"><span class="post-toc-text"> （1）模型持久化</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link"><span class="post-toc-text"> （2）生成 uff 模型</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link"><span class="post-toc-text"> （3）tensorRT c++ API 部署模型</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link"><span class="post-toc-text"> （4）加速比情况</span></a></li></ol></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 4&gt; Caffe 模型 tensorRT 部署教程</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-6"><a class="post-toc-link"><span class="post-toc-text"> （1）加速比情况</span></a></li></ol></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 5&gt; 为 tensorRT 添加自定义层</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 6&gt; 为 CaffeParser 添加自定义层支持</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link"><span class="post-toc-text"> 7&gt; 心得体会（踩坑记录）</span></a></li></ol></li></ol>
        </nav>
    </aside>
    

<nav id="article-nav">
  
    <a href="/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/" id="article-nav-newer" class="article-nav-link-wrap">

      <span class="article-nav-title">
        <i class="fa fa-hand-o-left" aria-hidden="true"></i>
        
          【精华】视频理解研究进展
        
      </span>
    </a>
  
  
    <a href="/2022/03/03/ForgeryNet-%E7%9B%AE%E5%89%8D%E5%85%AC%E5%BC%80%E6%9C%80%E5%A4%A7%E7%9A%84%E6%B7%B1%E5%BA%A6%E4%BA%BA%E8%84%B8%E4%BC%AA%E9%80%A0%E6%95%B0%E6%8D%AE%E9%9B%86CVPR2021/" id="article-nav-older" class="article-nav-link-wrap">
      <span class="article-nav-title">ForgeryNet_目前公开最大的深度人脸伪造数据集CVPR2021</span>
      <i class="fa fa-hand-o-right" aria-hidden="true"></i>
    </a>
  
</nav>



    
        <div id="SOHUCS" sid="【精华】模型部署实例（OpenVINO、TensorRT）" ></div>
<script type="text/javascript">
    (function(){
        var appid = 'true';
        var conf = 'true';
        var width = window.innerWidth || document.documentElement.clientWidth;
        if (width < 960) {
            window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script>
    
</section>
        
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


      <p>
        Powered by  <a href="http://hexo.io/" target="_blank">Hexo</a>
        Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a>
      &copy; 2024 LeeZhao<br>
      </p>
    </div>
  </div>
</footer>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script>
  var mihoConfig = {
      root: "https://leezhao415.github.io",
      animate: true,
      isHome: false,
      share: true,
      reward: 1
  }
</script>
<div class="sidebar">
    <div id="sidebar-search" title="Search">
        <i class="fa fa-search"></i>
    </div>
    <div id="sidebar-category" title="Categories">
        <i class="fa fa-book"></i>
    </div>
    <div id="sidebar-tag" title="Tags">
        <i class="fa fa-tags"></i>
    </div>
    <div id="sidebar-top">
        <span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span>
    </div>
</div>
<div class="sidebar-menu-box" id="sidebar-menu-box">
    <div class="sidebar-menu-box-container">
        <div id="sidebar-menu-box-categories">
            <a class="category-link" href="/categories/Hot/">Hot</a>
        </div>
        <div id="sidebar-menu-box-tags">
            <a href="/tags/AIGC%E5%89%8D%E6%B2%BF/" style="font-size: 10px;">AIGC前沿</a> <a href="/tags/CV-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E7%AE%B1/" style="font-size: 10px;">CV/目标检测工具箱</a> <a href="/tags/CV%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 10px;">CV数据集</a> <a href="/tags/CV%E6%9C%AA%E6%9D%A5/" style="font-size: 10px;">CV未来</a> <a href="/tags/CV%E7%AE%97%E6%B3%95/" style="font-size: 10px;">CV算法</a> <a href="/tags/IOU/" style="font-size: 10px;">IOU</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/MOT/" style="font-size: 10px;">MOT</a> <a href="/tags/NCNN%E9%83%A8%E7%BD%B2/" style="font-size: 10px;">NCNN部署</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/NLP-BERT/" style="font-size: 10px;">NLP-BERT</a> <a href="/tags/NLP-%E5%8F%91%E5%B1%95%E5%8F%B2/" style="font-size: 10px;">NLP-发展史</a> <a href="/tags/NLP-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" style="font-size: 10px;">NLP-模型优化</a> <a href="/tags/NLP-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7/" style="font-size: 10px;">NLP/数据增强工具</a> <a href="/tags/NLP-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/" style="font-size: 10px;">NLP/评估指标</a> <a href="/tags/OpenCV%E4%B9%8BDNN%E6%A8%A1%E5%9D%97/" style="font-size: 10px;">OpenCV之DNN模块</a> <a href="/tags/PaddlePaddle/" style="font-size: 10px;">PaddlePaddle</a> <a href="/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 10px;">Python数据分析</a> <a href="/tags/ReID/" style="font-size: 10px;">ReID</a> <a href="/tags/Transformer-DETR-CV/" style="font-size: 10px;">Transformer/DETR(CV)</a> <a href="/tags/VSLAM/" style="font-size: 11.67px;">VSLAM</a> <a href="/tags/YOLOX/" style="font-size: 10px;">YOLOX</a> <a href="/tags/YOLOX%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 11.67px;">YOLOX目标检测</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">三维建模</a> <a href="/tags/%E4%B8%94%E8%AF%BB%E6%96%87%E6%91%98/" style="font-size: 13.33px;">且读文摘</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 20px;">人工智能</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-CV/" style="font-size: 10px;">人工智能/CV</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 10px;">人脸识别</a> <a href="/tags/%E5%90%8D%E4%BA%BA%E5%90%8D%E8%A8%80/" style="font-size: 10px;">名人名言</a> <a href="/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">多任务学习模型</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 11.67px;">多模态</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/" style="font-size: 10px;">大数据框架</a> <a href="/tags/%E5%AF%92%E7%AA%91%E8%B5%8B/" style="font-size: 10px;">寒窑赋</a> <a href="/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">度量学习</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 10px;">操作系统</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/" style="font-size: 10px;">数据库原理</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/" style="font-size: 11.67px;">数据结构与算法</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 11.67px;">数据集</a> <a href="/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/" style="font-size: 10px;">智能家居</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 10px;">机器学习/损失函数</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0/" style="font-size: 10px;">梯度更新</a> <a href="/tags/%E6%A6%82%E8%BF%B0/" style="font-size: 10px;">概述</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" style="font-size: 10px;">模型优化</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/" style="font-size: 10px;">模型性能指标</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" style="font-size: 16.67px;">模型部署</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">深度学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" style="font-size: 10px;">深度学习环境配置</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">深度模型</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%89/" style="font-size: 10px;">深度模型（目标检测）</a> <a href="/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" style="font-size: 10px;">激活函数</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%EF%BC%89/" style="font-size: 10px;">目标检测（人脸检测）</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/" style="font-size: 10px;">目标跟踪</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">知识蒸馏</a> <a href="/tags/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE%E6%88%90%E6%9E%9C/" style="font-size: 10px;">科研项目成果</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 11.67px;">算法</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/" style="font-size: 18.33px;">编程工具</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" style="font-size: 10px;">编程语言</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/" style="font-size: 10px;">网络编程</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/" style="font-size: 10px;">网络通信</a> <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP/" style="font-size: 10px;">自然语言处理NLP</a> <a href="/tags/%E8%A1%A8%E9%9D%A2%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B/" style="font-size: 10px;">表面缺陷检测</a> <a href="/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" style="font-size: 10px;">视频理解</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 10px;">计算机视觉</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV/" style="font-size: 15px;">计算机视觉CV</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%BA%93/" style="font-size: 10px;">计算机视觉库</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%A1%B6%E4%BC%9A/" style="font-size: 10px;">计算机顶会</a>
        </div>
    </div>
    <a href="javascript:;" class="sidebar-menu-box-close">&times;</a>
</div>
<div class="mobile-header-menu-nav" id="mobile-header-menu-nav">
    <div class="mobile-header-menu-container">
        <span class="title">Menus</span>
        <ul class="mobile-header-menu-navbar">
            
            <li>
                <a  href="/">
                    <i class="fa fa-home"></i><span>Home</span>
                </a>
            </li>
            
            <li>
                <a  href="/archives">
                    <i class="fa fa-archive"></i><span>Archives</span>
                </a>
            </li>
            
            <li>
                <a  href="/about">
                    <i class="fa fa-user"></i><span>About</span>
                </a>
            </li>
            
        </ul>
    </div>
    <div class="mobile-header-tag-container">
        <span class="title">Tags</span>
        <div id="mobile-header-container-tags">
            <a href="/tags/AIGC%E5%89%8D%E6%B2%BF/" style="font-size: 10px;">AIGC前沿</a> <a href="/tags/CV-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E7%AE%B1/" style="font-size: 10px;">CV/目标检测工具箱</a> <a href="/tags/CV%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 10px;">CV数据集</a> <a href="/tags/CV%E6%9C%AA%E6%9D%A5/" style="font-size: 10px;">CV未来</a> <a href="/tags/CV%E7%AE%97%E6%B3%95/" style="font-size: 10px;">CV算法</a> <a href="/tags/IOU/" style="font-size: 10px;">IOU</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/MOT/" style="font-size: 10px;">MOT</a> <a href="/tags/NCNN%E9%83%A8%E7%BD%B2/" style="font-size: 10px;">NCNN部署</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/NLP-BERT/" style="font-size: 10px;">NLP-BERT</a> <a href="/tags/NLP-%E5%8F%91%E5%B1%95%E5%8F%B2/" style="font-size: 10px;">NLP-发展史</a> <a href="/tags/NLP-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" style="font-size: 10px;">NLP-模型优化</a> <a href="/tags/NLP-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7/" style="font-size: 10px;">NLP/数据增强工具</a> <a href="/tags/NLP-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/" style="font-size: 10px;">NLP/评估指标</a> <a href="/tags/OpenCV%E4%B9%8BDNN%E6%A8%A1%E5%9D%97/" style="font-size: 10px;">OpenCV之DNN模块</a> <a href="/tags/PaddlePaddle/" style="font-size: 10px;">PaddlePaddle</a> <a href="/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 10px;">Python数据分析</a> <a href="/tags/ReID/" style="font-size: 10px;">ReID</a> <a href="/tags/Transformer-DETR-CV/" style="font-size: 10px;">Transformer/DETR(CV)</a> <a href="/tags/VSLAM/" style="font-size: 11.67px;">VSLAM</a> <a href="/tags/YOLOX/" style="font-size: 10px;">YOLOX</a> <a href="/tags/YOLOX%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 11.67px;">YOLOX目标检测</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">三维建模</a> <a href="/tags/%E4%B8%94%E8%AF%BB%E6%96%87%E6%91%98/" style="font-size: 13.33px;">且读文摘</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 20px;">人工智能</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-CV/" style="font-size: 10px;">人工智能/CV</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 10px;">人脸识别</a> <a href="/tags/%E5%90%8D%E4%BA%BA%E5%90%8D%E8%A8%80/" style="font-size: 10px;">名人名言</a> <a href="/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">多任务学习模型</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 11.67px;">多模态</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/" style="font-size: 10px;">大数据框架</a> <a href="/tags/%E5%AF%92%E7%AA%91%E8%B5%8B/" style="font-size: 10px;">寒窑赋</a> <a href="/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">度量学习</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 10px;">操作系统</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/" style="font-size: 10px;">数据库原理</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/" style="font-size: 11.67px;">数据结构与算法</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 11.67px;">数据集</a> <a href="/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/" style="font-size: 10px;">智能家居</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 10px;">机器学习/损失函数</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0/" style="font-size: 10px;">梯度更新</a> <a href="/tags/%E6%A6%82%E8%BF%B0/" style="font-size: 10px;">概述</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" style="font-size: 10px;">模型优化</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/" style="font-size: 10px;">模型性能指标</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" style="font-size: 16.67px;">模型部署</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">深度学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" style="font-size: 10px;">深度学习环境配置</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">深度模型</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%89/" style="font-size: 10px;">深度模型（目标检测）</a> <a href="/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" style="font-size: 10px;">激活函数</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%EF%BC%89/" style="font-size: 10px;">目标检测（人脸检测）</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/" style="font-size: 10px;">目标跟踪</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">知识蒸馏</a> <a href="/tags/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE%E6%88%90%E6%9E%9C/" style="font-size: 10px;">科研项目成果</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 11.67px;">算法</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/" style="font-size: 18.33px;">编程工具</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" style="font-size: 10px;">编程语言</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/" style="font-size: 10px;">网络编程</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/" style="font-size: 10px;">网络通信</a> <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP/" style="font-size: 10px;">自然语言处理NLP</a> <a href="/tags/%E8%A1%A8%E9%9D%A2%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B/" style="font-size: 10px;">表面缺陷检测</a> <a href="/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" style="font-size: 10px;">视频理解</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 10px;">计算机视觉</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV/" style="font-size: 15px;">计算机视觉CV</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%BA%93/" style="font-size: 10px;">计算机视觉库</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%A1%B6%E4%BC%9A/" style="font-size: 10px;">计算机顶会</a>
        </div>
    </div>
</div>
<div class="search-wrap">
    <span class="search-close">&times;</span>
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
            <i class="icon icon-lg icon-chevron-left"></i>
        </a>
        <input class="search-field" placeholder="Search..." id="keywords">
        <a id="search-submit" href="javascript:;">
            <i class="fa fa-search"></i>
        </a>
    <div class="search-container" id="search-container">
        <ul class="search-result" id="search-result">
        </ul>
    </div>
</div>

<div id="search-tpl">
    <li class="search-result-item">
        <a href="{url}" class="search-item-li">
            <span class="search-item-li-title" title="{title}">{title}</span>
        </a>
    </li>
</div>

<script src="/js/search.js"></script>


<script src="/js/main.js"></script>



  <script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script>
  <div id="particles"></div>
  
<script src="/js/particles.js"></script>








  
<link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">

  <script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script>
  
<script src="/js/animate.js"></script>



  
<script src="/js/pop-img.js"></script>

  <script>
     $(".article-entry p img").popImg();
  </script>

  </div>
</body>
</html>