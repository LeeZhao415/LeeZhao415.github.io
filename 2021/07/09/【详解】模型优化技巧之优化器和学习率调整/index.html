<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>【详解】模型优化技巧之优化器和学习率调整 | 且听风吟，御剑于心！</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="LeeZhao,LeeZhao's Blog" />
  
  <meta name="description" content="文章目录   PyTorch 十大优化器  1 torch.optim.SGD 2 torch.optim.ASGD 3 torch.optim.Rprop 4 torch.optim.Adagrad 5 torch.optim.Adadelta 6 torch.optim.RMSprop 7 torch.optim.Adam(AMSGrad) 8 torch.optim.Adamax 9 t">
<meta property="og:type" content="article">
<meta property="og:title" content="【详解】模型优化技巧之优化器和学习率调整">
<meta property="og:url" content="https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/index.html">
<meta property="og:site_name" content="且听风吟，御剑于心！">
<meta property="og:description" content="文章目录   PyTorch 十大优化器  1 torch.optim.SGD 2 torch.optim.ASGD 3 torch.optim.Rprop 4 torch.optim.Adagrad 5 torch.optim.Adadelta 6 torch.optim.RMSprop 7 torch.optim.Adam(AMSGrad) 8 torch.optim.Adamax 9 t">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-07-09T11:31:37.000Z">
<meta property="article:modified_time" content="2021-07-09T12:25:35.488Z">
<meta property="article:author" content="LeeZhao">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="NLP-模型优化">
<meta name="twitter:card" content="summary">
  
  
    <link rel="icon" href="/images/hatRSS blk.ico">
  
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'true', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?true";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>

  
  <div style="display: none;">
    <script src="//s22.cnzz.com/z_stat.php?id=true&web_id=true" language="JavaScript"></script>
  </div>


<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
      <header id="header">
    <div id="banner"></div>
    <div id="header-outer">
        <div id="header-menu" class="header-menu-pos animated">
            <div class="header-menu-container">
                <a href="/" class="left">
                    <span class="site-title">LeeZhao&#39;s Blog</span>
                </a>
                <nav id="header-menu-nav" class="right">
                    
                    <a  href="/">
                        <i class="fa fa-home"></i>
                        <span>Home</span>
                    </a>
                    
                    <a  href="/archives">
                        <i class="fa fa-archive"></i>
                        <span>Archives</span>
                    </a>
                    
                    <a  href="/about">
                        <i class="fa fa-user"></i>
                        <span>About</span>
                    </a>
                    
                </nav>
                <a class="mobile-header-menu-button">
                    <i class="fa fa-bars"></i>
                </a>
            </div>
        </div>
        <div id="header-row">
            <div id="logo">
                <a href="/">
                    <img src="/images/logo.jpg" alt="logo">
                </a>
            </div>
            <div class="header-info">
                <div id="header-title">
                    
                    <h2>
                        LeeZhao&#39;s Blog
                    </h2>
                    
                </div>
                <div id="header-description">
                    
                    <h3>
                        且听风吟，御剑于心！
                    </h3>
                    
                </div>
            </div>
            <nav class="header-nav">
                <div class="social">
                    
                        <a title="CSDN" target="_blank" href="//blog.csdn.net/qq_36722887">
                            <i class="fa fa-home fa-2x"></i></a>
                    
                        <a title="Github" target="_blank" href="//github.com/leezhao415">
                            <i class="fa fa-github fa-2x"></i></a>
                    
                        <a title="Weibo" target="_blank" href="//weibo.com/u/5120617296/home?topnav=1&wvr=6">
                            <i class="fa fa-weibo fa-2x"></i></a>
                    
                        <a title="CodeSearch" target="_blank" href="//codesearch.aixcoder.com">
                            <i class="fa fa-twitter fa-2x"></i></a>
                    
                </div>
            </nav>
        </div>
    </div>
</header>
      <div class="outer">
        <section id="main" class="body-wrap"><article id="post-【详解】模型优化技巧之优化器和学习率调整" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="post-title" itemprop="name">
      【详解】模型优化技巧之优化器和学习率调整
    </h1>
    <div class="post-title-bar">
      <ul>
          
              <li>
                  <i class="fa fa-book"></i>
                  
                      <a href="/categories/Hot/">Hot</a>
                  
              </li>
          
        <li>
          <i class="fa fa-calendar"></i>  2021-07-09
        </li>
        <li>
          <i class="fa fa-eye"></i>
          <span id="busuanzi_value_page_pv"></span>
        </li>
      </ul>
    </div>
  

          
      </header>
    
    <div class="article-entry post-content" itemprop="articleBody">
      
            
            <meta name="referrer" content="no-referrer">
<hr>
<p><strong>文章目录</strong></p>
<!-- toc -->
<ul>
<li><a href="#pytorch%E5%8D%81%E5%A4%A7%E4%BC%98%E5%8C%96%E5%99%A8">PyTorch 十大优化器</a>
<ul>
<li><a href="#1-torchoptimsgd">1 torch.optim.SGD</a></li>
<li><a href="#2-torchoptimasgd">2 torch.optim.ASGD</a></li>
<li><a href="#3-torchoptimrprop">3 torch.optim.Rprop</a></li>
<li><a href="#4-torchoptimadagrad">4 torch.optim.Adagrad</a></li>
<li><a href="#5-torchoptimadadelta">5 torch.optim.Adadelta</a></li>
<li><a href="#6-torchoptimrmsprop">6 torch.optim.RMSprop</a></li>
<li><a href="#7-torchoptimadamamsgrad">7 torch.optim.Adam(AMSGrad)</a></li>
<li><a href="#8-torchoptimadamax">8 torch.optim.Adamax</a></li>
<li><a href="#9-torchoptimsparseadam">9 torch.optim.SparseAdam</a></li>
<li><a href="#10-torchoptimlbfgs">10 torch.optim.LBFGS</a></li>
</ul>
</li>
<li><a href="#pytorch-%E5%85%AD%E5%A4%A7%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95">PyTorch 六大学习率调整方法</a>
<ul>
<li><a href="#1-lr_schedulersteplr">1 lr_scheduler.StepLR</a></li>
<li><a href="#2-lr_schedulermultisteplr">2 lr_scheduler.MultiStepLR</a></li>
<li><a href="#3-lr_schedulerexponentiallr">3 lr_scheduler.ExponentialLR</a></li>
<li><a href="#4-lr_schedulercosineannealinglr">4 lr_scheduler.CosineAnnealingLR</a></li>
<li><a href="#5-lr_schedulerreducelronplateau">5 lr_scheduler.ReduceLROnPlateau</a></li>
<li><a href="#6-lr_schedulerlambdalr">6 lr_scheduler.LambdaLR</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->
<hr>
<h3><span id="pytorch-十大优化器"> PyTorch 十大优化器</span></h3>
<h4><span id="1-torchoptimsgd"> 1 torch.optim.SGD</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">SGD</span>(<span class="params">params, lr=, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>, weight_decay=<span class="number">0</span>, nesterov=<span class="literal">False</span></span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
可实现 SGD 优化算法，带动量 SGD 优化算法，带 NAG (Nesterov accelerated gradient) 动量 SGD 优化算法，并且均可拥有 weight_decay 项。</p>
<p><strong>参数：</strong><br>
<strong>params(iterable)</strong>- 参数组 (参数组的概念请查看 3.2 优化器基类：Optimizer)，优化器要管理的那部分参数。<br>
<strong>lr(float)</strong>- 初始学习率，可按需随着训练过程不断调整学习率。<br>
<strong>momentum(float)</strong>- 动量，通常设置为 0.9，0.8<br>
<strong>dampening(float)</strong>- dampening for momentum ，暂时不了其功能，在源码中是这样用的：buf.mul_(momentum).add_(1 - dampening, d_p)，值得注意的是，若采用 nesterov，dampening 必须为 0.<br>
<strong>weight_decay(float)</strong>- 权值衰减系数，也就是 L2 正则项的系数<br>
<strong> nesterov (bool)</strong>- bool 选项，是否使用 NAG (Nesterov accelerated gradient)</p>
<p><strong>注意事项：</strong><br>
pytroch 中使用 SGD 十分需要注意的是，更新公式与其他框架略有不同！<br>
pytorch 中是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v=ρ∗v+g</span><br><span class="line">p=p−lr∗v = p - lr∗ρ∗v - lr∗g</span><br><span class="line"><span class="number">12</span></span><br></pre></td></tr></table></figure>
<p>其他框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v=ρ∗v+lr∗g</span><br><span class="line">p=p−v = p - ρ∗v - lr∗g</span><br><span class="line"><span class="number">12</span></span><br></pre></td></tr></table></figure>
<p>ρ 是动量，v 是速率，g 是梯度，p 是参数，其实差别就是在 ρ∗v 这一项，pytorch 中将此项也乘了一个学习率。</p>
<h4><span id="2-torchoptimasgd"> 2 torch.optim.ASGD</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">ASGD</span>(<span class="params">params, lr=<span class="number">0.01</span>, lambd=<span class="number">0.0001</span>, alpha=<span class="number">0.75</span>, t0=<span class="number">1000000.0</span>, weight_decay=<span class="number">0</span></span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
ASGD 也成为 SAG，均表示随机平均梯度下降 (Averaged Stochastic Gradient Descent)，简单地说 ASGD 就是用空间换时间的一种 SGD，详细可参看论文：<a target="_blank" rel="noopener" href="http://riejohnson.com/rie/stograd_nips.pdf">http://riejohnson.com/rie/stograd_nips.pdf</a></p>
<p><strong>参数：</strong><br>
<strong>params(iterable)</strong> - 参数组 (参数组的概念请查看 3.1 优化器基类：Optimizer)，优化器要优化的那些参数。<br>
<strong>lr(float)</strong> - 初始学习率，可按需随着训练过程不断调整学习率。<br>
<strong>lambd(float)</strong> - 衰减项，默认值 1e-4。<br>
<strong>alpha(float)</strong> - power for eta update ，默认值 0.75。<br>
<strong>t0(float)</strong> - point at which to start averaging，默认值 1e6。<br>
<strong>weight_decay(float)</strong> - 权值衰减系数，也就是 L2 正则项的系数。</p>
<h4><span id="3-torchoptimrprop"> 3 torch.optim.Rprop</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">Rprop</span>(<span class="params">params, lr=<span class="number">0.01</span>, etas=(<span class="params"><span class="number">0.5</span>, <span class="number">1.2</span></span>), step_sizes=(<span class="params"><span class="number">1e-06</span>, <span class="number">50</span></span>)</span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
实现 Rprop 优化方法 (弹性反向传播)，优化方法原文《Martin Riedmiller und Heinrich Braun: Rprop - A Fast Adaptive Learning Algorithm. Proceedings of the International Symposium on Computer and Information Science VII, 1992》<br>
该优化方法适用于 full-batch，不适用于 mini-batch，因而在 min-batch 大行其道的时代里，很少见到。</p>
<h4><span id="4-torchoptimadagrad"> 4 torch.optim.Adagrad</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">Adagrad</span>(<span class="params">params, lr=<span class="number">0.01</span>, lr_decay=<span class="number">0</span>, weight_decay=<span class="number">0</span>, initial_accumulator_value=<span class="number">0</span></span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
实现 Adagrad 优化方法 (Adaptive Gradient)，Adagrad 是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。<br>
详细公式请阅读：Adaptive Subgradient Methods for Online Learning and Stochastic Optimization<br>
John Duchi, Elad Hazan, Yoram Singer; 12(Jul):2121−2159, 2011.(<a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a>)</p>
<h4><span id="5-torchoptimadadelta"> 5 torch.optim.Adadelta</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">Adadelta</span>(<span class="params">params, lr=<span class="number">1.0</span>, rho=<span class="number">0.9</span>, eps=<span class="number">1e-06</span>, weight_decay=<span class="number">0</span></span>)</span></span><br></pre></td></tr></table></figure>
<p>功能：<br>
实现 Adadelta 优化方法。Adadelta 是 Adagrad 的改进。Adadelta 分母中采用距离当前时间点比较近的累计项，这可以避免在训练后期，学习率过小。<br>
详细公式请阅读:<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1212.5701.pdf">https://arxiv.org/pdf/1212.5701.pdf</a></p>
<h4><span id="6-torchoptimrmsprop"> 6 torch.optim.RMSprop</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">RMSprop</span>(<span class="params">params, lr=<span class="number">0.01</span>, alpha=<span class="number">0.99</span>, eps=<span class="number">1e-08</span>, weight_decay=<span class="number">0</span>, momentum=<span class="number">0</span>, centered=<span class="literal">False</span></span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
实现 RMSprop 优化方法（Hinton 提出），RMS 是均方根（root meam square）的意思。RMSprop 和 Adadelta 一样，也是对 Adagrad 的一种改进。RMSprop 采用均方根作为分母，可缓解 Adagrad 学习率下降较快的问题。并且引入均方根，可以减少摆动，详细了解可读：<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a></p>
<h4><span id="7-torchoptimadamamsgrad"> 7 torch.optim.Adam(AMSGrad)</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">Adam</span>(<span class="params">params, lr=<span class="number">0.001</span>, betas=(<span class="params"><span class="number">0.9</span>, <span class="number">0.999</span></span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">0</span>, amsgrad=<span class="literal">False</span></span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
实现 Adam (Adaptive Moment Estimation)) 优化方法。Adam 是一种自适应学习率的优化方法，Adam 利用梯度的一阶矩估计和二阶矩估计动态的调整学习率。吴老师课上说过，Adam 是结合了 Momentum 和 RMSprop，并进行了偏差修正。<br>
<strong>参数：</strong><br>
amsgrad- 是否采用 AMSGrad 优化方法，asmgrad 优化方法是针对 Adam 的改进，通过添加额外的约束，使学习率始终为正值。(AMSGrad，ICLR-2018 Best-Pper 之一，《On the convergence of Adam and Beyond》)。<br>
详细了解 Adam 可阅读，Adam: A Method for Stochastic Optimization (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>)。</p>
<h4><span id="8-torchoptimadamax"> 8 torch.optim.Adamax</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">Adamax</span>(<span class="params">params, lr=<span class="number">0.002</span>, betas=(<span class="params"><span class="number">0.9</span>, <span class="number">0.999</span></span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">0</span></span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
实现 Adamax 优化方法。Adamax 是对 Adam 增加了一个学习率上限的概念，所以也称之为 Adamax。<br>
详细了解可阅读，Adam: A Method for Stochastic Optimization (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>)(没错，就是 Adam 论文中提出了 Adamax)。</p>
<h4><span id="9-torchoptimsparseadam"> 9 torch.optim.SparseAdam</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">SparseAdam</span>(<span class="params">params, lr=<span class="number">0.001</span>, betas=(<span class="params"><span class="number">0.9</span>, <span class="number">0.999</span></span>), eps=<span class="number">1e-08</span></span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
针对稀疏张量的一种 “阉割版” Adam 优化方法。<br>
only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters</p>
<h4><span id="10-torchoptimlbfgs"> 10 torch.optim.LBFGS</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">LBFGS</span>(<span class="params">params, lr=<span class="number">1</span>, max_iter=<span class="number">20</span>, max_eval=<span class="literal">None</span>, tolerance_grad=<span class="number">1e-05</span>, tolerance_change=<span class="number">1e-09</span>, history_size=<span class="number">100</span>, line_search_fn=<span class="literal">None</span></span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
实现 L-BFGS（Limited-memory Broyden–Fletcher–Goldfarb–Shanno）优化方法。L-BFGS 属于拟牛顿算法。L-BFGS 是对 BFGS 的改进，特点就是节省内存。</p>
<h3><span id="pytorch-六大学习率调整方法"> PyTorch 六大学习率调整方法</span></h3>
<p>优化器中最重要的一个参数就是学习率，合理的学习率可以使优化器快速收敛。一般在训练初期给予较大的学习率，随着训练的进行，学习率逐渐减小。学习率什么时候减小，减小多少，这就涉及到学习率调整方法。<br>
PyTorch 中提供了六种方法供大家使用，下面将一一介绍，最后对学习率调整方法进行总结。</p>
<h4><span id="1-lr_schedulersteplr"> 1 lr_scheduler.StepLR</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">lr_scheduler</span>.<span class="title">StepLR</span> (<span class="params"> optimizer , step_size , gamma=<span class="number">0.1</span> , last_epoch=-<span class="number">1</span> </span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
等间隔调整学习率，调整倍数为 gamma 倍，调整间隔为 step_size。间隔单位是 step。需要注意的是，step 通常是指 epoch，不要弄成 iteration 了。<br>
参数：<br>
 <code>step_size(int)</code> - 学习率下降间隔数，若为 30，则会在 30、60、90… 个 step 时，将学习率调整为 lr*gamma。<br>
 <code>gamma(float)</code> - 学习率调整倍数，默认为 0.1 倍，即下降 10 倍。<br>
 <code>last_epoch(int)</code> - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始 值。</p>
<h4><span id="2-lr_schedulermultisteplr"> 2 lr_scheduler.MultiStepLR</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">lr_scheduler</span>.<span class="title">MultiStepLR</span> (<span class="params"> optimizer , milestones , gamma=<span class="number">0.1</span> , last_epoch=-<span class="number">1</span> </span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
按设定的间隔调整学习率。这个方法适合后期调试使用，观察 loss 曲线，为每个实验定制学习率调整时机。<br>
参数：<br>
 <code>milestones(list)</code> - 一个 list，每一个元素代表何时调整学习率，list 元素必须是递增的。如 milestones=[30,80,120]<br>
 <code>gamma(float)</code> - 学习率调整倍数，默认为 0.1 倍，即下降 10 倍。<br>
 <code>last_epoch(int)</code> - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始值。</p>
<h4><span id="3-lr_schedulerexponentiallr"> 3 lr_scheduler.ExponentialLR</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">lr_scheduler</span>.<span class="title">ExponentialLR</span> (<span class="params"> optimizer , gamma , last_epoch=-<span class="number">1</span> </span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
按指数衰减调整学习率，调整公式: lr = lr * gammaepoch<br>
 参数：<br>
 <code>gamma</code> - 学习率调整倍数的底，指数为 epoch，即 gammaepoch<br>
 <code>last_epoch(int)</code> - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始值。</p>
<h4><span id="4-lr_schedulercosineannealinglr"> 4 lr_scheduler.CosineAnnealingLR</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">lr_scheduler</span>.<span class="title">CosineAnnealingLR</span> (<span class="params"> optimizer , T_max , eta_min=<span class="number">0</span> , last_epoch=-<span class="number">1</span> </span>)</span></span><br></pre></td></tr></table></figure>
<p>以余弦函数为周期，并在每个周期最大值时重新设置学习率。具体如下图所示</p>
<p>详细请阅读论文《 SGDR: Stochastic Gradient Descent with Warm Restarts》(ICLR-2017)： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.03983">https://arxiv.org/abs/1608.03983</a><br>
 参数：<br>
 <code>T_max(int)</code> - 一次学习率周期的迭代次数，即 T_max 个 epoch 之后重新设置学习率。<br>
 <code>eta_min(float)</code> - 最小学习率，即在一个周期中，学习率最小会下降到 eta_min，默认值为 0。<br>
学习率调整公式为：</p>
<p>可以看出是以初始学习率为最大学习率，以 2*Tmax 为周期，在一个周期内先下降，后上升。</p>
<h4><span id="5-lr_schedulerreducelronplateau"> 5 lr_scheduler.ReduceLROnPlateau</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">lr_scheduler</span>.<span class="title">ReduceLROnPlateau</span> (<span class="params"> optimizer , mode=<span class="string">&#x27;min&#x27;</span> ,factor=<span class="number">0.1</span> , patience=<span class="number">10</span> , verbose=<span class="literal">False</span> , threshold=<span class="number">0.0001</span> , threshold_mode=<span class="string">&#x27;rel&#x27;</span> , cooldown=<span class="number">0</span> , min_lr=<span class="number">0</span> , eps=<span class="number">1e-08</span> </span>)</span></span><br></pre></td></tr></table></figure>
<p><strong>功能：</strong><br>
当某指标不再变化（下降或升高），调整学习率，这是非常实用的学习率调整策略。<br>
例如，当验证集的 loss 不再下降时，进行学习率调整；或者监测验证集的 accuracy，当 accuracy 不再上升时，则调整学习率。<br>
参数：<br>
 <code>mode(str)</code> - 模式选择，有 min 和 max 两种模式，min 表示当指标不再降低 (如监测 loss)，max 表示当指标不再升高 (如监测 accuracy)。<br>
 <code>factor(float)</code> - 学习率调整倍数 (等同于其它方法的 gamma)，即学习率更新为 lr = lr *factor<br>
 <code>patience(int)</code> - 直译 ——“耐心”，即忍受该指标多少个 step 不变化，当忍无可忍时，调整学习率。<br>
 <code>verbose(bool)</code> - 是否打印学习率信息， print (‘Epoch {:5d}: reducing learning rate’ ’ of group {} to {:.4e}.’.format (epoch, i, new_lr))<br>
 <code>threshold(float)</code> - Threshold for measuring the new optimum ，配合 threshold_mode 使用。<br>
 <code>threshold_mode(str)</code> - 选择判断指标是否达最优的模式，有两种模式，rel 和 abs。<br>
当 threshold_mode==rel，并且 mode==max 时， dynamic_threshold = best * (1 +threshold) ；<br>
当 threshold_mode==rel，并且 mode==min 时， dynamic_threshold = best * (1 -threshold) ；<br>
当 threshold_mode==abs，并且 mode==max 时， dynamic_threshold = best + threshold ；<br>
当 threshold_mode==abs，并且 mode==min 时， dynamic_threshold = best - threshold<br>
cooldown (int)- “ 冷却时间 “ ，当调整学习率之后，让学习率调整策略冷静一下，让模型再训练一段时间，再重启监测模式。<br>
min_lr (float or list)- 学习率下限，可为 float ，或者 list ，当有多个参数组时，可用 list 进行设置。<br>
eps (float)- 学习率衰减的最小值，当学习率变化小于 eps 时，则不调整学习率。</p>
<h4><span id="6-lr_schedulerlambdalr"> 6 lr_scheduler.LambdaLR</span></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">optim</span>.<span class="title">lr_scheduler</span>.<span class="title">LambdaLR</span> (<span class="params"> optimizer , lr_lambda , last_epoch=-<span class="number">1</span> </span>)</span></span><br></pre></td></tr></table></figure>
<p>功能：<br>
为不同参数组设定不同学习率调整策略。调整规则为，lr = base_lr *lmbda (self.last_epoch) 。<br>
参数：<br>
 <code>lr_lambda(function or list)</code> - 一个计算学习率调整倍数的函数，输入通常为 step，当有多个参数组时，设为 list。<br>
 <code>last_epoch(int)</code> - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">例如： </span><br><span class="line">ignored_params = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">id</span>, net.fc3.parameters())) </span><br><span class="line">base_params = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: <span class="built_in">id</span>(p) <span class="keyword">not</span> <span class="keyword">in</span> ignored_params, net.parameters()) </span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">&#123;<span class="string">&#x27;params&#x27;</span>: base_params&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;params&#x27;</span>: net.fc3.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.001</span>*<span class="number">100</span>&#125;], <span class="number">0.001</span>, momentum=<span class="number">0.9</span>,weight_decay=<span class="number">1e-4</span>)</span><br><span class="line">lambda1 = <span class="keyword">lambda</span> epoch: epoch // <span class="number">3</span></span><br><span class="line">lambda2 = <span class="keyword">lambda</span> epoch: <span class="number">0.95</span> ** epoch</span><br><span class="line"></span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">scheduler.step()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span>, i, <span class="string">&#x27;lr: &#x27;</span>, scheduler.get_lr())</span><br><span class="line">train(...)</span><br><span class="line">validate(...)</span><br><span class="line">输出： </span><br><span class="line">epoch: <span class="number">0</span> lr: [<span class="number">0.0</span>, <span class="number">0.1</span>]</span><br><span class="line">epoch: <span class="number">1</span> lr: [<span class="number">0.0</span>, <span class="number">0.095</span>]</span><br><span class="line">epoch: <span class="number">2</span> lr: [<span class="number">0.0</span>, <span class="number">0.09025</span>]</span><br><span class="line">epoch: <span class="number">3</span> lr: [<span class="number">0.001</span>, <span class="number">0.0857375</span>]</span><br><span class="line">epoch: <span class="number">4</span> lr: [<span class="number">0.001</span>, <span class="number">0.081450625</span>]</span><br><span class="line">epoch: <span class="number">5</span> lr: [<span class="number">0.001</span>, <span class="number">0.07737809374999999</span>]</span><br><span class="line">epoch: <span class="number">6</span> lr: [<span class="number">0.002</span>, <span class="number">0.07350918906249998</span>]</span><br><span class="line">epoch: <span class="number">7</span> lr: [<span class="number">0.002</span>, <span class="number">0.06983372960937498</span>]</span><br><span class="line">epoch: <span class="number">8</span> lr: [<span class="number">0.002</span>, <span class="number">0.06634204312890622</span>]</span><br><span class="line">epoch: <span class="number">9</span> lr: [<span class="number">0.003</span>, <span class="number">0.0630249409724609</span>]</span><br></pre></td></tr></table></figure>
<p>为什么第一个参数组的学习率会是 0 呢？ 来看看学习率是如何计算的。<br>
第一个参数组的初始学习率设置为 0.001, lambda1 = lambda epoch: epoch // 3,<br>
 第 1 个 epoch 时，由 lr = base_lr * lmbda (self.last_epoch)，可知道 lr = 0.001 *<br>
(0//3) ，又因为 1//3 等于 0，所以导致学习率为 0。<br>
第二个参数组的学习率变化，就很容易看啦，初始为 0.1，lr = 0.1 * 0.95^epoch ，当 epoch 为 0 时，lr=0.1 ，epoch 为 1 时，lr=0.1*0.95。</p>
<p><strong>学习率调整小结</strong><br>
 PyTorch 提供了六种学习率调整方法，可分为三大类，分别是</p>
<ol>
<li>有序调整；</li>
<li>自适应调整；</li>
<li>自定义调整。<br>
第一类，依一定规律有序进行调整，这一类是最常用的，分别是等间隔下降 (Step)，按需设定下降间隔 (MultiStep)，指数下降 (Exponential) 和 CosineAnnealing。这四种方法的调整时机都是人为可控的，也是训练时常用到的。<br>
第二类，依训练状况伺机调整，这就是 ReduceLROnPlateau 方法。该法通过监测某一指标的变化情况，当该指标不再怎么变化的时候，就是调整学习率的时机，因而属于自适应的调整。<br>
第三类，自定义调整，Lambda。Lambda 方法提供的调整策略十分灵活，我们可以为不同的层设定不同的学习率调整方法，这在 fine-tune 中十分有用，我们不仅可为不同的层设定不同的学习率，还可以为其设定不同的学习率调整策略，简直不能更棒！</li>
</ol>

            <div class="post-copyright">
    <div class="content">
        <p>最后更新： 2021年07月09日 20:25</p>
        <p>原始链接： <a class="post-url" href="/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/" title="【详解】模型优化技巧之优化器和学习率调整">https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/</a></p>
        <footer>
            <a href="https://leezhao415.github.io">
                <img src="/images/logo.jpg" alt="LeeZhao">
                LeeZhao
            </a>
        </footer>
    </div>
</div>

      
        
            
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;">赏</a>
</div>

<div id="reward" class="post-modal reward-lay">
    <a class="close" href="javascript:;" id="reward-close">×</a>
    <span class="reward-title">
        <i class="icon icon-quote-left"></i>
        请我吃糖~
        <i class="icon icon-quote-right"></i>
    </span>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/images/wechat_code.jpg" alt="打赏二维码">
        </div>
        <div class="reward-select">
            
            <label class="reward-select-item checked" data-id="wechat" data-wechat="/images/wechat_code.jpg">
                <img class="reward-select-item-wechat" src="/images/wechat.png" alt="微信">
            </label>
            
            
            <label class="reward-select-item" data-id="alipay" data-alipay="/images/alipay_code.jpg">
                <img class="reward-select-item-alipay" src="/images/alipay.png" alt="支付宝">
            </label>
            
        </div>
    </div>
</div>


        
    </div>
    <footer class="article-footer">
        
        
<div class="post-share">
    <a href="javascript:;" id="share-sub" class="post-share-fab">
        <i class="fa fa-share-alt"></i>
    </a>
    <div class="post-share-list" id="share-list">
        <ul class="share-icons">
          <li>
            <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/&title=《【详解】模型优化技巧之优化器和学习率调整》 — 且听风吟，御剑于心！&pic=images/模型优化.jpg" data-title="微博">
              <i class="fa fa-weibo"></i>
            </a>
          </li>
          <li>
            <a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信">
              <i class="fa fa-weixin"></i>
            </a>
          </li>
          <li>
            <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/&title=《【详解】模型优化技巧之优化器和学习率调整》 — 且听风吟，御剑于心！&source=" data-title="QQ">
              <i class="fa fa-qq"></i>
            </a>
          </li>
          <li>
            <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/" data-title="Facebook">
              <i class="fa fa-facebook"></i>
            </a>
          </li>
          <li>
            <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《【详解】模型优化技巧之优化器和学习率调整》 — 且听风吟，御剑于心！&url=https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/&via=https://leezhao415.github.io" data-title="Twitter">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
          <li>
            <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/" data-title="Google+">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        </ul>
     </div>
</div>
<div class="post-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;" id="wxShare-close">×</a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/" alt="微信分享二维码">
</div>

<div class="mask"></div>

        
        <ul class="article-footer-menu">
            
            
  <li class="article-footer-tags">
    <i class="fa fa-tags"></i>
      
    <a href="/tags/人工智能/" class="color5">人工智能</a>
      
    <a href="/tags/NLP-模型优化/" class="color4">NLP-模型优化</a>
      
  </li>

        </ul>
        
    </footer>
  </div>
</article>


    <aside class="post-toc-pos post-toc-top" id="post-toc">
        <nav class="post-toc-wrap">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link"><span class="post-toc-text"> PyTorch 十大优化器</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 1 torch.optim.SGD</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 2 torch.optim.ASGD</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 3 torch.optim.Rprop</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 4 torch.optim.Adagrad</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 5 torch.optim.Adadelta</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 6 torch.optim.RMSprop</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 7 torch.optim.Adam(AMSGrad)</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 8 torch.optim.Adamax</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 9 torch.optim.SparseAdam</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 10 torch.optim.LBFGS</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link"><span class="post-toc-text"> PyTorch 六大学习率调整方法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 1 lr_scheduler.StepLR</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 2 lr_scheduler.MultiStepLR</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 3 lr_scheduler.ExponentialLR</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 4 lr_scheduler.CosineAnnealingLR</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 5 lr_scheduler.ReduceLROnPlateau</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link"><span class="post-toc-text"> 6 lr_scheduler.LambdaLR</span></a></li></ol></li></ol>
        </nav>
    </aside>
    

<nav id="article-nav">
  
    <a href="/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91BERT%E7%9A%843%E4%B8%AAEmbedding%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/" id="article-nav-newer" class="article-nav-link-wrap">

      <span class="article-nav-title">
        <i class="fa fa-hand-o-left" aria-hidden="true"></i>
        
          【详解】BERT的3个Embedding的实现原理
        
      </span>
    </a>
  
  
    <a href="/2021/07/09/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2%E2%80%94Word-Embedding%E5%88%B0Bert%E6%A8%A1%E5%9E%8B/" id="article-nav-older" class="article-nav-link-wrap">
      <span class="article-nav-title">自然语言处理中的预训练技术发展史—Word Embedding到Bert模型</span>
      <i class="fa fa-hand-o-right" aria-hidden="true"></i>
    </a>
  
</nav>



    
        <div id="SOHUCS" sid="【详解】模型优化技巧之优化器和学习率调整" ></div>
<script type="text/javascript">
    (function(){
        var appid = 'true';
        var conf = 'true';
        var width = window.innerWidth || document.documentElement.clientWidth;
        if (width < 960) {
            window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script>
    
</section>
        
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


      <p>
        Powered by  <a href="http://hexo.io/" target="_blank">Hexo</a>
        Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a>
      &copy; 2024 LeeZhao<br>
      </p>
    </div>
  </div>
</footer>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script>
  var mihoConfig = {
      root: "https://leezhao415.github.io",
      animate: true,
      isHome: false,
      share: true,
      reward: 1
  }
</script>
<div class="sidebar">
    <div id="sidebar-search" title="Search">
        <i class="fa fa-search"></i>
    </div>
    <div id="sidebar-category" title="Categories">
        <i class="fa fa-book"></i>
    </div>
    <div id="sidebar-tag" title="Tags">
        <i class="fa fa-tags"></i>
    </div>
    <div id="sidebar-top">
        <span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span>
    </div>
</div>
<div class="sidebar-menu-box" id="sidebar-menu-box">
    <div class="sidebar-menu-box-container">
        <div id="sidebar-menu-box-categories">
            <a class="category-link" href="/categories/Hot/">Hot</a>
        </div>
        <div id="sidebar-menu-box-tags">
            <a href="/tags/AIGC%E5%89%8D%E6%B2%BF/" style="font-size: 10px;">AIGC前沿</a> <a href="/tags/CV-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E7%AE%B1/" style="font-size: 10px;">CV/目标检测工具箱</a> <a href="/tags/CV%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 10px;">CV数据集</a> <a href="/tags/CV%E6%9C%AA%E6%9D%A5/" style="font-size: 10px;">CV未来</a> <a href="/tags/CV%E7%AE%97%E6%B3%95/" style="font-size: 10px;">CV算法</a> <a href="/tags/IOU/" style="font-size: 10px;">IOU</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/MOT/" style="font-size: 10px;">MOT</a> <a href="/tags/NCNN%E9%83%A8%E7%BD%B2/" style="font-size: 10px;">NCNN部署</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/NLP-BERT/" style="font-size: 10px;">NLP-BERT</a> <a href="/tags/NLP-%E5%8F%91%E5%B1%95%E5%8F%B2/" style="font-size: 10px;">NLP-发展史</a> <a href="/tags/NLP-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" style="font-size: 10px;">NLP-模型优化</a> <a href="/tags/NLP-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7/" style="font-size: 10px;">NLP/数据增强工具</a> <a href="/tags/NLP-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/" style="font-size: 10px;">NLP/评估指标</a> <a href="/tags/OpenCV%E4%B9%8BDNN%E6%A8%A1%E5%9D%97/" style="font-size: 10px;">OpenCV之DNN模块</a> <a href="/tags/PaddlePaddle/" style="font-size: 10px;">PaddlePaddle</a> <a href="/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 10px;">Python数据分析</a> <a href="/tags/ReID/" style="font-size: 10px;">ReID</a> <a href="/tags/Transformer-DETR-CV/" style="font-size: 10px;">Transformer/DETR(CV)</a> <a href="/tags/VSLAM/" style="font-size: 11.67px;">VSLAM</a> <a href="/tags/YOLOX/" style="font-size: 10px;">YOLOX</a> <a href="/tags/YOLOX%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 11.67px;">YOLOX目标检测</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">三维建模</a> <a href="/tags/%E4%B8%94%E8%AF%BB%E6%96%87%E6%91%98/" style="font-size: 13.33px;">且读文摘</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 20px;">人工智能</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-CV/" style="font-size: 10px;">人工智能/CV</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 10px;">人脸识别</a> <a href="/tags/%E5%90%8D%E4%BA%BA%E5%90%8D%E8%A8%80/" style="font-size: 10px;">名人名言</a> <a href="/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">多任务学习模型</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 11.67px;">多模态</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/" style="font-size: 10px;">大数据框架</a> <a href="/tags/%E5%AF%92%E7%AA%91%E8%B5%8B/" style="font-size: 10px;">寒窑赋</a> <a href="/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">度量学习</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 10px;">操作系统</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/" style="font-size: 10px;">数据库原理</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/" style="font-size: 11.67px;">数据结构与算法</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 11.67px;">数据集</a> <a href="/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/" style="font-size: 10px;">智能家居</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 10px;">机器学习/损失函数</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0/" style="font-size: 10px;">梯度更新</a> <a href="/tags/%E6%A6%82%E8%BF%B0/" style="font-size: 10px;">概述</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" style="font-size: 10px;">模型优化</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/" style="font-size: 10px;">模型性能指标</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" style="font-size: 16.67px;">模型部署</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">深度学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" style="font-size: 10px;">深度学习环境配置</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">深度模型</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%89/" style="font-size: 10px;">深度模型（目标检测）</a> <a href="/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" style="font-size: 10px;">激活函数</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%EF%BC%89/" style="font-size: 10px;">目标检测（人脸检测）</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/" style="font-size: 10px;">目标跟踪</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">知识蒸馏</a> <a href="/tags/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE%E6%88%90%E6%9E%9C/" style="font-size: 10px;">科研项目成果</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 11.67px;">算法</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/" style="font-size: 18.33px;">编程工具</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" style="font-size: 10px;">编程语言</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/" style="font-size: 10px;">网络编程</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/" style="font-size: 10px;">网络通信</a> <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP/" style="font-size: 10px;">自然语言处理NLP</a> <a href="/tags/%E8%A1%A8%E9%9D%A2%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B/" style="font-size: 10px;">表面缺陷检测</a> <a href="/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" style="font-size: 10px;">视频理解</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 10px;">计算机视觉</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV/" style="font-size: 15px;">计算机视觉CV</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%BA%93/" style="font-size: 10px;">计算机视觉库</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%A1%B6%E4%BC%9A/" style="font-size: 10px;">计算机顶会</a>
        </div>
    </div>
    <a href="javascript:;" class="sidebar-menu-box-close">&times;</a>
</div>
<div class="mobile-header-menu-nav" id="mobile-header-menu-nav">
    <div class="mobile-header-menu-container">
        <span class="title">Menus</span>
        <ul class="mobile-header-menu-navbar">
            
            <li>
                <a  href="/">
                    <i class="fa fa-home"></i><span>Home</span>
                </a>
            </li>
            
            <li>
                <a  href="/archives">
                    <i class="fa fa-archive"></i><span>Archives</span>
                </a>
            </li>
            
            <li>
                <a  href="/about">
                    <i class="fa fa-user"></i><span>About</span>
                </a>
            </li>
            
        </ul>
    </div>
    <div class="mobile-header-tag-container">
        <span class="title">Tags</span>
        <div id="mobile-header-container-tags">
            <a href="/tags/AIGC%E5%89%8D%E6%B2%BF/" style="font-size: 10px;">AIGC前沿</a> <a href="/tags/CV-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E7%AE%B1/" style="font-size: 10px;">CV/目标检测工具箱</a> <a href="/tags/CV%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 10px;">CV数据集</a> <a href="/tags/CV%E6%9C%AA%E6%9D%A5/" style="font-size: 10px;">CV未来</a> <a href="/tags/CV%E7%AE%97%E6%B3%95/" style="font-size: 10px;">CV算法</a> <a href="/tags/IOU/" style="font-size: 10px;">IOU</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/MOT/" style="font-size: 10px;">MOT</a> <a href="/tags/NCNN%E9%83%A8%E7%BD%B2/" style="font-size: 10px;">NCNN部署</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/NLP-BERT/" style="font-size: 10px;">NLP-BERT</a> <a href="/tags/NLP-%E5%8F%91%E5%B1%95%E5%8F%B2/" style="font-size: 10px;">NLP-发展史</a> <a href="/tags/NLP-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" style="font-size: 10px;">NLP-模型优化</a> <a href="/tags/NLP-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7/" style="font-size: 10px;">NLP/数据增强工具</a> <a href="/tags/NLP-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/" style="font-size: 10px;">NLP/评估指标</a> <a href="/tags/OpenCV%E4%B9%8BDNN%E6%A8%A1%E5%9D%97/" style="font-size: 10px;">OpenCV之DNN模块</a> <a href="/tags/PaddlePaddle/" style="font-size: 10px;">PaddlePaddle</a> <a href="/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 10px;">Python数据分析</a> <a href="/tags/ReID/" style="font-size: 10px;">ReID</a> <a href="/tags/Transformer-DETR-CV/" style="font-size: 10px;">Transformer/DETR(CV)</a> <a href="/tags/VSLAM/" style="font-size: 11.67px;">VSLAM</a> <a href="/tags/YOLOX/" style="font-size: 10px;">YOLOX</a> <a href="/tags/YOLOX%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 11.67px;">YOLOX目标检测</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E5%BB%BA%E6%A8%A1/" style="font-size: 10px;">三维建模</a> <a href="/tags/%E4%B8%94%E8%AF%BB%E6%96%87%E6%91%98/" style="font-size: 13.33px;">且读文摘</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 20px;">人工智能</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-CV/" style="font-size: 10px;">人工智能/CV</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/" style="font-size: 10px;">人脸识别</a> <a href="/tags/%E5%90%8D%E4%BA%BA%E5%90%8D%E8%A8%80/" style="font-size: 10px;">名人名言</a> <a href="/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">多任务学习模型</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" style="font-size: 11.67px;">多模态</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/" style="font-size: 10px;">大数据框架</a> <a href="/tags/%E5%AF%92%E7%AA%91%E8%B5%8B/" style="font-size: 10px;">寒窑赋</a> <a href="/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">度量学习</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 10px;">操作系统</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/" style="font-size: 10px;">数据库原理</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/" style="font-size: 11.67px;">数据结构与算法</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 11.67px;">数据集</a> <a href="/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/" style="font-size: 10px;">智能家居</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" style="font-size: 10px;">机器学习/损失函数</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0/" style="font-size: 10px;">梯度更新</a> <a href="/tags/%E6%A6%82%E8%BF%B0/" style="font-size: 10px;">概述</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" style="font-size: 10px;">模型优化</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/" style="font-size: 10px;">模型性能指标</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" style="font-size: 16.67px;">模型部署</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">深度学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" style="font-size: 10px;">深度学习环境配置</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">深度模型</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%89/" style="font-size: 10px;">深度模型（目标检测）</a> <a href="/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" style="font-size: 10px;">激活函数</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%EF%BC%89/" style="font-size: 10px;">目标检测（人脸检测）</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/" style="font-size: 10px;">目标跟踪</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 10px;">知识蒸馏</a> <a href="/tags/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE%E6%88%90%E6%9E%9C/" style="font-size: 10px;">科研项目成果</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 11.67px;">算法</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/" style="font-size: 18.33px;">编程工具</a> <a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" style="font-size: 10px;">编程语言</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/" style="font-size: 10px;">网络编程</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/" style="font-size: 10px;">网络通信</a> <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP/" style="font-size: 10px;">自然语言处理NLP</a> <a href="/tags/%E8%A1%A8%E9%9D%A2%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B/" style="font-size: 10px;">表面缺陷检测</a> <a href="/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" style="font-size: 10px;">视频理解</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 10px;">计算机视觉</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV/" style="font-size: 15px;">计算机视觉CV</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%BA%93/" style="font-size: 10px;">计算机视觉库</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%A1%B6%E4%BC%9A/" style="font-size: 10px;">计算机顶会</a>
        </div>
    </div>
</div>
<div class="search-wrap">
    <span class="search-close">&times;</span>
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
            <i class="icon icon-lg icon-chevron-left"></i>
        </a>
        <input class="search-field" placeholder="Search..." id="keywords">
        <a id="search-submit" href="javascript:;">
            <i class="fa fa-search"></i>
        </a>
    <div class="search-container" id="search-container">
        <ul class="search-result" id="search-result">
        </ul>
    </div>
</div>

<div id="search-tpl">
    <li class="search-result-item">
        <a href="{url}" class="search-item-li">
            <span class="search-item-li-title" title="{title}">{title}</span>
        </a>
    </li>
</div>

<script src="/js/search.js"></script>


<script src="/js/main.js"></script>



  <script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script>
  <div id="particles"></div>
  
<script src="/js/particles.js"></script>








  
<link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">

  <script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script>
  
<script src="/js/animate.js"></script>



  
<script src="/js/pop-img.js"></script>

  <script>
     $(".article-entry p img").popImg();
  </script>

  </div>
</body>
</html>