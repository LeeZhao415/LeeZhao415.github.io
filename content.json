{"meta":{"title":"且听风吟，御剑于心！","subtitle":"","description":"","author":"LeeZhao","url":"https://leezhao415.github.io","root":"/"},"pages":[{"title":"","date":"2021-11-21T14:48:04.668Z","updated":"2021-11-21T14:48:04.668Z","comments":true,"path":"about/index.html","permalink":"https://leezhao415.github.io/about/index.html","excerpt":"","text":"关于本站 站名： 且听风吟，御剑于心！ 站长： LeeZhao 地址： https://leezhao415.github.io/ 标志： 简介： AI 程序猿 &amp; AI 工程狮 关于我 从事 AI 人工智能研究，主要从事计算机视觉算法开发，主要开发语言 Python，熟悉深度学习框架 Tensorflow、PyTorch，PaddlePaddle、Keras 等，有 GPU 计算、多核编程、线性 / 非线性优化算法项目经验；有一定的代码洁癖。 热爱开源项目、热爱新技术、热爱新事物。 关于工作 城市：北京 关于学习 正在往终身学习者前进… 学习方向：Python (人工智能) 关于座右铭 Heavy is the head who wears the crown 欲戴王冠，必承其重！ 关于爱好 热爱运动，尤其喜爱篮球、羽毛球、阅读、旅行。 联系我 Home: LeeZhao_CSDN Email: 18155166734@163.com GitHub: LeeZhao"},{"title":"MiHo-主题安装和配置详情","date":"2017-07-31T16:00:00.000Z","updated":"2021-04-18T12:41:34.773Z","comments":true,"path":"blog/installation-configuration.html","permalink":"https://leezhao415.github.io/blog/installation-configuration.html","excerpt":"","text":"一。主题简介 MiHo 是一款单栏响应式的 Hexo 主题；基于 Hexo 3.0+ 制作，兼容移动端浏览；主题的代码托管在 GitHub 上， 欢迎 Star 和 Fork；如遇到问题或发表建议，可以提 Issues，也可以在博客中留言给我，另外，喜欢的话不妨给个 Star。 二。安装 2.1 安装主题 1$ git clone https://github.com/WongMinHo/hexo-theme-miho.git themes/miho MiHo 主题需要 Hexo 3.0 或以上版本，请先升级。 2.2 更新 12cd themes/mihogit pull 2.3 依赖安装 如下依赖如果已经安装，请看配置介绍。 Json-content 生成站点文章静态数据，用于站内搜索。 1npm install hexo-generator-json-content --save 三。站点配置 站点配置文件 _config.yml 在 hexo 根目录下。 3.1 启用主题 1`theme: miho` 3.2 网站基本配置 以下配置是站点的全局配置，更多配置，请查看 123456title: MinHow&#x27;s Blogsubtitle: 网站副标题description: 专注 WEB 开发的技术博客author: MinHowlanguage: 网站使用的语言timezone: 网站时区 3.3 jsonContent 配置 详细的配置请查看 hexo-generator-json-content 123456789101112131415161718jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: false 四。主题配置 编辑主题配置文件， themes/miho/_config.yml 。 4.1 属性 下面将介绍几个比较重要的配置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122# hexo-theme-miho# https://github.com/wongminho/hexo-theme-miho# Favicon of your site | 网站iconfavicon: /favicon.ico# Header# Keywords of your site | 网站关键字keywords: MinHow,MinHow&#x27;s Blog# Head headline | 头部标题header_title: MinHow&#x27;s Blog# Head description | 头部描述header_description: 一个专注 WEB 开发的技术博客# Link to your logo | logo地址logo: images/logo.png# Link to your banner_img | 首页banner图地址banner_img: images/banner.jpg# Menu setting | 菜单设置# name: Font Awesome icon | Font Awesome 图标# title: Home Title | 标题# url: //minhow.com Url, absolute or relative path | 链接，绝对或相对路径# target: true Whether to jump out | 是否跳出menu: home: title: Home url: / target: false archive: title: Archives url: /archives target: false user: title: About url: /about target: false# Social setting, use to display social information | 社交设置，用来展示社交信息# name: Font Awesome icon | Font Awesome 图标# title: Home Icon title | 图标标题# url: //minhow.com Url, absolute or relative path | 链接，绝对或相对路径# target: true Whether to jump out | 是否跳出social: home: title: MinHow url: //minhow.com target: true github: title: Github url: //github.com/wongminho target: true weibo: title: Weibo url: //weibo.com/WongMinHo target: true twitter: title: Twitter url: //twitter.com/huangminhow target: true #qq: #weixin: #snapchat: #telegram: #mail: #facebook: #google: #linkedin:# Content# Excerpt length | 摘录长度excerpt_length: 190# Excerpt link | 摘录链接excerpt_link: more&gt;&gt;# New window open link | 新窗口打开文章open_new_link: false# Article default cover picture，size：350*150 | 文章默认封面图，尺寸：350*150cover_picture: images/banner.jpg# Open background particles | 开启背景粒子open_bg_particle: true# Open animation in homepage and head | 开启主页及头部动画open_animation: true# Article# Open toc | 是否开启toctoc: true# Open share | 是否开启分享share: true# Style customization | 样式定制style: # Main color tone | 主色调 main_color: &#x27;#0cc&#x27;# Comments | 评论# 畅言，输入appid和appkeychangyan_appid: falsechangyan_appkey: false# 友言，输入idyouyan_id: false# disqusdisqus: false# Analytics | 分析# 站长分析，输入站点idcnzz_analytics: false# 百度分析，输入key值baidu_analytics: false# google analytics | google分析google_analytics: false# Footer# Access statistics | “不蒜子”访问量统计access_counter: on: true site_uv: 总访客数： site_pv: 总访问量：# Copyright Information | 版权信息copyright: 2017 MinHow 4.2 文章封面图 文章默认封面图，尺寸：350*150，当文章基本配置没有 cover_picture 时才显示。 1cover_picture: images/banner.jpg 4.3 开启背景粒子 是否开启背景粒子。 1open_bg_particle: true 4.4 开启主页及头部动画 是否开启主页及头部动画。 1open_animation: true 4.5 评论 支持畅言、disqus。 1234567# 畅言，输入appid和appkeychangyan_appid: falsechangyan_appkey: false# 友言，输入idyouyan_id: false# disqusdisqus: false 4.6 数据统计 支持站长、百度、google 三种数据统计，正确填写配置信息即可。 123456# 站长分析，输入站点idcnzz_analytics: false# 百度分析，输入key值baidu_analytics: false# google分析google_analytics: false 4.7 文章基本配置 123456789101112---title: Hello Worlddate: 2017-06-18categories: Firstauthor: MinHowtags: - First - Secondcover_picture: /images/banner.jpg--- MinHow-This is a summary&lt;!-- more --&gt; 说明： 需要注意 tags 和摘要的写法，不然首页不能正确显示标签和摘要； cover_picture 文章封面图，不填默认显示 _config.yml 配置的图片。","author":"MinHow"}],"posts":[{"title":"【精华】AIGC启元2024","slug":"【精华】AIGC启元2024","date":"2024-03-15T08:16:00.000Z","updated":"2024-03-18T03:50:01.657Z","comments":true,"path":"2024/03/15/【精华】AIGC启元2024/","link":"","permalink":"https://leezhao415.github.io/2024/03/15/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91AIGC%E5%90%AF%E5%85%832024/","excerpt":"","text":"文章目录 AIGC 前沿 (1) Gemini 1.5 Pro（谷歌新一代多模态大模型） (2) Sora（文本生成视频大模型） (3) EMO（阿里生成式 AI 模型） (4) Playground v2.5（文生图大模型） (5) VSP-LLM（唇语识别） (6) Ideogram ai （文生图大模型） (7) LTX studio（生成式 AI 电影制作平台） (8) Claude3（LLM） (9) Open Sora（文生视频大模型） (10) Yi-9B（LLM） (11) CARES Copilot1.0（多模态手术大模型） (12) Figure 01 通用机器人（Figure AI + OpenAI） (13) Devin（AI 软件工程师助手） (14) BEHAVIOR-1K（李飞飞团队 — 具身智能基准） (15) MM1 大模型（苹果公司多模态大模型） (16) AesopAgent（达摩院 — 智能体驱动的进化系统） (17) CogView3（文生图大模型） (18) AutoDev（微软团队全自动 AI 驱动软件开发框架） (19) VLOGGER（Google 图生音频驱动视频方法） (20) TextMonkey（Monkey 多模态大模型在文档领域的应用） (21) Open-Sora 1.0（文生视频大模型） (22) Grok-1（马斯克开源大语言模型） AIGC 前沿 (1) Gemini 1.5 Pro（谷歌新一代多模态大模型） 2024.02.16 谷歌新一代多模态大模型 Gemini 1.5 Pro，在性能上超越 OpenAI 的 GPT-4 Turbo，堪称业界最强大模型。 推荐文章： “打假” Sora，谷歌 Gemini 1.5 Pro 第一波评测出炉｜甲子光年 官网链接： https://openai.com/sora (2) Sora（文本生成视频大模型） 2024.02.16 Sora 文本生成视频的大模型。它所展现出来的能力几乎可以 “碾压” 目前全球能实现文本生成视频的大模型 包 括 Runway、Pika、Stable Video Diffusion 等 20 多个产品。 用户仅需输入简短一句话，Sora 就可生成一段长达 60 秒的视频，远远超过市面上同类型级别的 AI 视频生成时长。在此之前，AI 视频模型生成时长几乎在 10 秒以内，而 “明星模型” Runway 和 Pika 等也仅有 3 到 4 秒。 推荐文章： Sora 到底有多强？ | 微软最新 Sora 综述 官网链接： Gemma Open Models (3) EMO（阿里生成式 AI 模型） 2024.02.28 生成式 AI 模型 EMO（Emote Portrait Alive）。EMO 仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容 “张嘴” 唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然。 推荐文章： 阿里 EMO 模型，一张照片就能造谣 官网链接： https://humanaigc.github.io/emote-portrait-alive/ (4) Playground v2.5（文生图大模型） 2024.02.28 Playground 在去年发布 Playground v2.0 之后再次开源新的文生图模型 Playground v2.5。相比上一个版本，Playground v2.5 在美学质量，颜色和对比度，多尺度生成以及以人为中心的细节处理有比较大的提升。 推荐文章： 超过 Midjourney v5.2 的开源文生图大模型 Playground v2.5 来了 官网链接： https://playground.com/ (5) VSP-LLM（唇语识别） 2024.02.28 一种通过观察视频中人的嘴型来理解和翻译说话内容的技术，也就是识别唇语。该技术能够将视频中的唇动转化为文本（视觉语音识别），并将这些唇动直接翻译成目标语言的文本 (视觉语音翻译)。不仅如此，VSP-LLM 还能智能识别和去除视频中不必要的重复信息，使处理过程更加快速和准确。 推荐文章： VSP-LLM：可通过观察视频中人的嘴型来识别唇语 官网链接： https://github.com/sally-sh/vsp-llm (6) Ideogram ai （文生图大模型） 2024.02.29 Ideogram 发布了最新的 Ideogram1.0 图像生成模型，该模型具有强大的文字生成能力和提示词理解能力。Ideogram1.0 在文本渲染准确性方面实现了飞跃。 推荐文章：Ideogram 1.0 图像生成模型发布 文字生成能力更强大了 官网链接：https://top.aibase.com/tool/ideogram-ai (7) LTX studio（生成式 AI 电影制作平台） 2024.02.29 生成式 AI 电影制作平台 —LTX Studio，用户只需要输入文本就能生成超 25 秒的微电影视频，同时可对镜头切换、角色、场景一致性、摄像机、灯光等进行可视化精准控制。 推荐文章： 效果比 Sora 惊艳，著名 AI 平台大动作！文本生成超 25 秒视频，带背景音乐、转场等效果 官网链接： https://ltx.studio (8) Claude3（LLM） 2024.03.04 Claude3 是由 Anthropic 发布的最新的 AI 大模型系列，同时，Claude3 是多模态大模型 ，具有强大的 “视觉能力”。Claude3 Opus 已经在部分行业行为准则中的表现优于 OpenAI 的 GPT-4 和谷歌的 Gemini Ultra，如本科生水平知识（MMLU）、研究生级别专家推理（GPQA）和基础数学（GSM8K）。 推荐文章： OpenAI 劲敌出现！Claude3 正式发布，超越 GTP-4? 官网链接： https://www.anthropic.com/claude (9) Open Sora（文生视频大模型） 2024.03.01 北大团队联合兔展发起了一项 Sora 复现计划 ——Open Sora 推荐文章： 北大与兔展智能发起复现 Sora，框架已开源 官网链接： https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html https://github.com/PKU-YuanGroup/Open-Sora-Plan (10) Yi-9B（LLM） 2024.03.06 李开复旗下 AI 公司零一万物的最新力作 ——Yi-9B 大模型正式对外开源发布。这款具有 90 亿参数的大模型，在代码和数学能力上达到了前所未有的高度，同时保持了对消费级显卡的良好兼容性，为广大开发者和研究人员提供了前所未有的便利性和强大功能。 Yi-9B 作为 Yi 系列中的新成员，被誉为 “理科状元”，特别加强了在代码和数学方面的学习能力。相较于市场上其他类似规模的开源模型，如 Mistral-7B、SOLAR-10.7B、Gemma-7B 等，Yi-9B 展现出了最佳的性能表现。特别值得一提的是，Yi-9B 既提供了浮点数版本（BF 16），也提供了整数版本（Int8），使其能够轻松部署在包括 RTX 4090 和 RTX 3090 在内的消费级显卡上，大大降低了使用门槛和成本。 推荐文章： 零一万物开源 Yi-9B 大模型，消费级显卡可用，代码数学历史最强 官网链接： https://github.com/01-ai/Yi (11) CARES Copilot1.0（多模态手术大模型） 2024.03.11 CARES Copilot 是由中国科学院香港创新院 AI 中心研发的一个可信赖、可解释、面向医疗垂直领域并能与智能医疗设备高度集成的大模型系统。CARES Copilot 1.0 实现了图像、文本、语音、视频、MRI、CT、超声等多模态的手术数据理解。支持超过 100K 上下文的长窗口理解和高效分析，能理解超过 3000 页的复杂手术教材，对于年轻医生的培训和教学具有极高的实用价值。此外，该系统能通过深度检索功能，快速精确地提取手术教材、专家指南、医学论文等专业文档的信息，确保其提供的答案具有高度的可信度和可追溯性。经测试，系统能在一秒钟内完成百万级数据的快速检索，同时保持 95% 的准确率。该系统已在多家医院的不同科室进行了内部测试和迭代优化。 推荐文章： CARES Copilot 1.0 多模态手术大模型发布，可实现轻量化部署 官网链接： / (12) Figure 01 通用机器人（Figure AI + OpenAI） 2024.03.13 Figure 01 通用机器人由 Figure AI 和 OpenAI 合作完成。展示视频中，Figure AI 人形机器人具有视觉能力并能表述所见画面，它伸手拿起桌上的苹果，并解释了这么做的原因，人类的提问后，这台人形机器人 “思索” 2~3 秒后便能顺畅作答，手部动作速度则接近人类。据视频介绍，机器人采用了端到端神经网络。 该人形机器人由 OpenAI 提供了视觉推理和语言理解，Figure AI 的神经网络则提供快速、灵巧的机器人动作。人形机器人将摄像机的图像输入和麦克风接收的语音文字输入 OpenAI 提供的视觉语言大模型（VLM）中，该模型可以理解图像和文字。Figure 机载相机以 10hz 的频率拍摄画面，随后神经网络以 200hz 的频率输出 24 个自由度动作。画面中的人形机器人不依赖远程操作，行为都是学习而得的。 推荐文章： 与 OpenAI 合作 13 天后，Figure 人形机器人展示与人类对话能力 官网链接： / (13) Devin（AI 软件工程师助手） 2024.03.13 一家成立不到两个月但拥有十名天才工程师的初创公司 Cognition 推出了一款名为 Devin 的人工智能（AI）助手，可以协助人类软件工程师完成诸多开发任务。Devin 不同于现有其他 AI 编码者，它可以从零构建网站、自行部署应用、修复漏洞、学习新技术等，人类只需扮演一个下指令和监督的角色。 这是第一个真正意义上完全自主的 AI 软件工程师，一亮相即掀起轩然大波，因为人们担心：人类程序员是不是真要失业了？ 推荐文章： 人类程序员真要失业？首位 “AI 软件工程师” 亮相引爆科技圈 官网链接： / (14) BEHAVIOR-1K（李飞飞团队 — 具身智能基准） 2024.02.27 来自斯坦福、得克萨斯大学奥斯汀分校等大学的研究团队推出了一项以人为本的机器人技术综合模拟基准 ——BEHAVIOR-1K。 BEHAVIOR-1K 包括两个部分，由 “您希望机器人为您做什么？” 这一问题的广泛调查结果指导和推动。第一部分是对 1000 种日常活动的定义，以 50 个场景（房屋、花园、餐厅、办公室等）为基础，其中有 9000 多个标注了丰富物理和语义属性的物体。其次是 OMNIGIBSON，这是一个模拟环境，通过对刚体、可变形体和液体进行逼真的物理模拟和渲染来支持这些活动。 实验表明，BEHAVIOR-1K 中的活动是长视距的，并且依赖于复杂的操作技能，这两点对于最先进的机器人学习解决方案来说仍然是一个挑战。为了校准 BEHAVIOR-1K 的模拟与现实之间的差距，研究团队进行了一项初步研究，将在模拟公寓中使用移动机械手学习到的解决方案转移到现实世界中。 研究团队希望 BEHAVIOR-1K 以人为本的特性、多样性和现实性能使其在具身智能和机器人学习研究中发挥重要作用。 推荐文章： stanford Behavior-1k—— 包含一千种日常任务的具身智能 benchmark 官网链接： / (15) MM1 大模型（苹果公司多模态大模型） 2024.03.15 苹果公司最新发布了一款名为 MM1 的大型多模态基础模型，拥有 300 亿参数，采用了 MoE 架构，并且超过一半的作者是华人。 该模型采用了 MoE 变体，并且在预训练指标和多项多模态基准测试上表现出了领先水平。研究者通过多项消融试验，探讨了模型架构、预训练数据选择以及训练程序等方面的重要性。他们发现，图像分辨率、视觉编码器损失和预训练数据在建模设计中都起着关键作用。 MM1 的发布标志着苹果在多模态领域的重要进展，也为未来苹果可能推出的相关产品奠定了技术基础。该研究的成果对于推动生成式人工智能领域的发展具有重要意义，值得业界密切关注。 推荐文章： 苹果大模型 MM1 入场：参数达到 300 亿 超半数作者是华人 论文地址： https://arxiv.org/pdf/2403.09611.pdf (16) AesopAgent（达摩院 — 智能体驱动的进化系统） 2024.03.15 阿里达摩院提出了一个关于故事到视频制作的智能体驱动进化系统 ——AesopAgent，它是智能体技术在多模态内容生成方面的实际应用。 该系统在一个统一的框架内集成了多种生成功能，因此个人用户可以轻松利用这些模块。这一创新系统可将用户故事提案转化为脚本、图像和音频，然后将这些多模态内容整合到视频中。此外，动画单元（如 Gen-2 和 Sora）可以使视频更具感染力。 推荐文章： 阿里达摩院提出 AesopAgent：从故事到视频制作，智能体驱动的进化系统 论文地址： https://arxiv.org/pdf/2403.07952.pdf (17) CogView3（文生图大模型） 2024.03.10 文生图系统的最新进展主要是由扩散模型推动的。然而，单级文本到图像扩散模型在计算效率和图像细节细化方面仍面临挑战。为了解决这个问题，来自清华大学和智谱 AI 的研究团队提出了 CogView3—— 一个能提高文本到图像扩散性能的创新级联框架。 据介绍，CogView3 是第一个在文本到图像生成领域实现 relay diffusion 的模型，它通过首先创建低分辨率图像，然后应用基于中继（relay-based）的超分辨率来执行任务。这种方法不仅能产生有竞争力的文本到图像输出，还能大大降低训练和推理成本。 实验结果表明，在人类评估中，CogView3 比目前最先进的开源文本到图像扩散模型 SDXL 高出 77.0%，而所需的推理时间仅为后者的 1/2。经过提炼（distilled）的 CogView3 变体性能与 SDXL 相当，而推理时间仅为后者的 1/10。 推荐文章： CogView3：更精细、更快速的文生图 论文地址： https://arxiv.org/pdf/2403.05121.pdf (18) AutoDev（微软团队全自动 AI 驱动软件开发框架） 2024.03.10 微软团队推出了全自动 AI 驱动软件开发框架 AutoDev，该框架专为自主规划和执行复杂的软件工程任务而设计。AutoDev 使用户能够定义复杂的软件工程目标，并将其分配给 AutoDev 的自主 AI 智能体来实现。这些 AI 智能体可以对代码库执行各种操作，包括文件编辑、检索、构建过程、执行、测试和 git 操作。它们还能访问文件、编译器输出、构建和测试日志、静态分析工具等。这使得 AI 智能体能够以完全自动化的方式执行任务并全面了解所需的上下文信息。 此外，AutoDev 还将所有操作限制在 Docker 容器内，建立了一个安全的开发环境。该框架结合了防护栏以确保用户隐私和文件安全，允许用户在 AutoDev 中定义特定的允许或限制命令和操作。 研究团队在 HumanEval 数据集上对 AutoDev 进行了测试，在代码生成和测试生成方面分别取得了 91.5% 和 87.8% 的 Pass@1 好成绩，证明了它在自动执行软件工程任务的同时维护安全和用户控制的开发环境方面的有效性。 推荐文章： AutoDev 1.5.3：精准的自动化测试生成、本地模型强化与流程自动化优化 论文地址： / (19) VLOGGER（Google 图生音频驱动视频方法） 2024.03.14 Google Research 提出了一种从单张人物输入图像生成音频驱动人类视频的方法 ——VLOGGER，它建立在最近成功的生成扩散模型基础之上。 VLOGGER 由两部分组成，一是随机人体到三维运动扩散模型，二是一种基于扩散的新型架构，它通过空间和时间控制来增强文本到图像模型。这有助于生成长度可变的高质量视频，并可通过人脸和身体的高级表示轻松控制。 与之前的工作相比，这一方法不需要对每个人进行训练，不依赖于人脸检测和裁剪，能生成完整的图像（不仅仅是人脸或嘴唇），并能考虑广泛的情况（如可见躯干或不同的主体身份），这对于正确合成交流的人类至关重要。研究团队还提出了一个包含三维姿势和表情注释的全新多样化数据集 MENTOR，它比以前的数据集大一个数量级（800000 identities），并且包含动态手势。研究团队在其上训练并简化了他们的主要技术贡献。 VLOGGER 在三个公共基准测试中的表现达到了 SOTA，考虑到图像质量、身份保留和时间一致性，同时还能生成上半身手势。VLOGGER 在多个多样性指标方面的表现都表明其架构选择和 MENTOR 的使用有利于大规模训练一个公平、无偏见的模型。最后，研究团队还展示了在视频编辑和个性化方面的应用。 推荐文章： VLOGGER：基于多模态扩散的具身虚拟形象合成 论文地址： https://arxiv.org/pdf/2403.08764.pdf (20) TextMonkey（Monkey 多模态大模型在文档领域的应用） 2024.03.15 TextMonkey 是 Monkey 在文档领域的重要升级，突破了通用文档理解能力的边界，在场景文字识别、办公文档摘要生成、数学问题问答、文档版式分析，表格理解，图表问答，电子文档关键信息抽取等 12 项等文档权威数据集以及在国际上规模最全的文档图像智能数据集 OCRBench 上取得了显著突破，通用文档理解性能大幅超越现有方法。 TextMonkey 能帮助我们结构化图表、表格以及文档数据，通过将图像内容转化为轻量级的数据交换格式，方便记录和提取。TextMonkey 也能作为智能手机代理，无需接触后端，仅需语音输入及屏幕截图，即能够模仿人类的点击手势，能够在手机上执行各种任务，自主操控手机应用程序。 推荐文章： 华科大研发多模态大模型 “猴子” 升级 [全网首发中文版] TextMonkey: An OCRFree Large Multimodal Model for Understanding Document GitHub 仓库地址： https://github.com/Yuliang-Liu/Monkey 论文地址： https://arxiv.org/pdf/2311.06607.pdf (21) Open-Sora 1.0（文生视频大模型） 2024.03.17 Colossal-AI 团队全面开源全球首个类 Sora 架构视频生成模型 「Open-Sora 1.0」，涵盖了整个训练流程，包括数据处理、所有训练细节和模型权重，携手全球 AI 热爱者共同推进视频创作的新纪元。 Colossal-AI 团队深入解读 Sora 复现方案的多个关键维度，包括模型架构设计、训练复现方案、数据预处理、模型生成效果展示以及高效训练优化策略。 推荐文章： 没等来 OpenAI，等来了 Open-Sora 全面开源 GitHub 仓库地址： https://github.com/hpcaitech/Open-Sora (22) Grok-1（马斯克开源大语言模型） 2024.03.17 马斯克宣布开源 Grok-1，这使得 Grok-1 成为当前参数量最大的开源大语言模型，拥有 3140 亿参数，远超 OpenAI GPT-3.5 的 1750 亿。有意思的是，Grok-1 宣布开源的封面图为 Midjourney 生成，可谓 “AI helps AI”。 Grok-1 是一个规模较大（314B 参数）的模型，需要有足够 GPU 内存的机器才能使用示例代码测试模型。网友表示这可能需要一台拥有 628 GB GPU 内存的机器。此外，该存储库中 MoE 层的实现效率并不高，之所以选择该实现是为了避免需要自定义内核来验证模型的正确性。 目前已开源的热门大模型包括 Meta 的 Llama2、法国的 Mistral 等。通常来说，发布开源模型有助于社区展开大规模的测试和反馈，意味着模型本身的迭代速度也能加快。 推荐文章： 马斯克用行动反击 开源自家顶级大模型 压力给到 OpenAI GitHub 仓库地址： https://github.com/xai-org/grok-1 官方博客： https://x.ai/blog/grok-os 模型磁力链接： https://academictorrents.com/details/5f96d43576e3d386c9ba65b883210a393b68210e","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"AIGC前沿","slug":"AIGC前沿","permalink":"https://leezhao415.github.io/tags/AIGC%E5%89%8D%E6%B2%BF/"}]},{"title":"【精华】计算机视觉研究方向综述","slug":"【精华】计算机视觉研究方向综述","date":"2022-07-26T15:53:32.000Z","updated":"2022-07-26T16:34:07.971Z","comments":true,"path":"2022/07/26/【精华】计算机视觉研究方向综述/","link":"","permalink":"https://leezhao415.github.io/2022/07/26/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E7%BB%BC%E8%BF%B0/","excerpt":"","text":"文章目录 1 目标检测 1 PP-YOLOE 2 PP-PicoDet 增强版 (PicoDet-XS) 3 Transformer 检测系列模型 4 YOLOv5 5 YOLOX 6 YOLOv6 7 YOLOv7 7 旋转框检测 S2ANet 2 实例分割 1 Mask RCNN 2 SOLOv2 3 多目标跟踪 1 PP-Tracking 2 DeepSORT 3 JDE 4 FairMOT 5 ByteTrack 4 关键点检测 1 PP-TinyPose 2 HigherHRNet 3 HRNet 5 语义分割 1 PP-HumanSeg v2 2 PP-LiteSeg 3 OCRNet 6 交互式分割 1 EISeg 2 RITM 3 EdgeFlow 7 Matting 1 PP-Matting 2 DIM 3 MODNet 4 PP-HumanMatting 8 全景分割 1 Panoptic-DeepLab 9 OCR 1 PP-OCRv3 2 PP-OCRv2 3 PP-Structure 10 图像分类 11 应用案例 1 PaddleDetection 2 PaddleSeg 3 PaddleOCR 4 PaddleClas 1 目标检测 1 PP-YOLOE 高精度云边一体 SOTA 目标检测模型 PP-YOLOE，分 s/m/l/x 版本，l 版本 COCO test2017 数据集精度 51.6%，V100 预测速度 78.1 FPS，支持混合精度训练，训练较 PP-YOLOv2 加速 33%，全系列多尺度模型，满足不同硬件算力需求，可适配服务器、边缘端 GPU 及其他服务器端 AI 加速卡。 优化过程 PP-YOLO( 45.9% /V100 72.9FPS ) ----&gt; PP-YOLOv2( 49.5% /V100 68.9FPS ) ----&gt; PP-YOLOE( 51.6% /V100 78.1FPS ) 2 PP-PicoDet 增强版 (PicoDet-XS) 边缘端和 CPU 端超轻量 SOTA 目标检测模型 PP-PicoDet 增强版， 精度提升2% 左右，CPU 预测速度提升63% ，新增 参数量0.7M 的 PicoDet-XS 模型，提供模型稀疏化和量化功能，便于模型加速，各类硬件无需单独开发后处理模块，降低部署门槛。 3 Transformer 检测系列模型 1 DETR DETR 2 Deformable DETR Deformable DETR 3 Sparse RCNN Sparse RCNN 4 YOLOv5 5 YOLOX YOLOX 目标检测模型，支持 nano/tiny/s/m/l/x 版本，x 版本 COCO val2017 数据集精度 51.8%。 6 YOLOv6 7 YOLOv7 7 旋转框检测 S2ANet S2ANet 2 实例分割 1 Mask RCNN 2 SOLOv2 3 多目标跟踪 1 PP-Tracking 实时跟踪系统 PP-Tracking，覆盖单、多镜头下行人、车辆、多类别跟踪，对小目标、密集型特殊优化，提供人、车流量技术解决方案。 2 DeepSORT DeepSORT 3 JDE JDE 4 FairMOT FairMOT 5 ByteTrack ByteTrack 4 关键点检测 1 PP-TinyPose PP-TinyPose 轻量级关键点特色模型 PP-TinyPose，单人场景 FP16 推理可达 122FPS、51.8AP，具有精度高速度快、检测人数无限制、微小目标效果好的优势。 2 HigherHRNet 3 HRNet 5 语义分割 1 PP-HumanSeg v2 实时人像分割模型 PP-HumanSeg v2，推理速度提升 45.5%，移动端达到 64.26 FPS，分割精度更高、通用型更强、零成本开箱即用。 2 PP-LiteSeg 超轻量级语义分割模型 PP-LiteSeg 3 OCRNet OCRNet 6 交互式分割 1 EISeg EISeg 2 RITM 3 EdgeFlow 7 Matting 1 PP-Matting 高精度抠图模型 PP-Matting 2 DIM DIM 3 MODNet MODNet 4 PP-HumanMatting PP-HumanMatting 8 全景分割 1 Panoptic-DeepLab Panoptic-DeepLab 9 OCR 1 PP-OCRv3 速度可比情况下，PP-OCRv3 中文场景效果相比于 PP-OCRv2 再提升 5%，英文场景提升 11%，80 语种多语言模型平均识别准确率提升 5% 以上； 2 PP-OCRv2 PP-OCRv2 在 CPU 推理速度相比于 PP-OCR server 提升 220%；效果相比于 PP-OCR mobile 提升 7%。 3 PP-Structure 文档结构分析 PP-Structure 工具包，支持版面分析与表格识别（含 Excel 导出） 10 图像分类 11 应用案例 1 PaddleDetection 1 行人分析工具 PP-Human v2 四大产业特色功能：高性能易扩展的五大复杂行为识别、闪电级人体属性识别、一行代码即可实现的人流检测与轨迹留存以及高精度跨镜跟踪 底层核心算法性能强劲：覆盖行人检测、跟踪、属性三类核心算法能力，对目标人数、光线、背景均无限制 极低使用门槛：提供保姆级全流程开发及模型优化策略、一行命令完成推理、兼容各类数据输入格式 2 基于 PP-PicoDet 增强版的路面垃圾检测 基于 PP-PicoDet 增强版的路面垃圾检测 3 基于 PP-PicoDet 的通信塔识别及 Android 端部署 基于 PP-PicoDet 的通信塔识别及 Android 端部署 4 基于 Faster-RCNN 的瓷砖表面瑕疵检测 基于 Faster-RCNN 的瓷砖表面瑕疵检测 5 基于 PaddleDetection 的 PCB 瑕疵检测 基于 PaddleDetection 的 PCB 瑕疵检测 6 基于 FairMOT 实现人流量统计 基于 FairMOT 实现人流量统计 7 基于 YOLOv3 实现跌倒检测 基于 YOLOv3 实现跌倒检测 8 基于 PP-PicoDetv2 的路面垃圾检测 基于 PP-PicoDetv2 的路面垃圾检测 9 基于人体关键点检测的合规检测 基于人体关键点检测的合规检测 2 PaddleSeg 1 10 分钟上手 PaddleSeg 10 分钟上手 PaddleSeg 2 PaddleSeg 实战之人像分割 PaddleSeg 实战之人像分割 3 PaddleSeg 实战之小数据集 3D 椎骨分割 PaddleSeg 实战之小数据集 3D 椎骨分割 4 PaddleSeg 实战之车道线图像分割 PaddleSeg 实战之车道线图像分割 5 PaddleSeg 动态图 API 使用教程 PaddleSeg 动态图 API 使用教程 3 PaddleOCR 行业 类别 亮点 文档说明 模型下载 制造 数码管识别 数码管数据合成、漏识别调优 光功率计数码管字符识别 下载链接 金融 通用表单识别 多模态通用表单结构化提取 多模态表单识别 下载链接 交通 车牌识别 多角度图像处理、轻量模型、端侧部署 轻量级车牌识别 下载链接 4 PaddleClas PULC 超轻量图像分类方案快速体验：点击这里 PP-ShiTu 图像识别快速体验：点击这里","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"CV未来","slug":"CV未来","permalink":"https://leezhao415.github.io/tags/CV%E6%9C%AA%E6%9D%A5/"}]},{"title":"【精华】多目标跟踪MOT综述","slug":"【精华】多目标跟踪MOT综述","date":"2022-07-26T15:53:05.000Z","updated":"2022-07-26T16:27:15.326Z","comments":true,"path":"2022/07/26/【精华】多目标跟踪MOT综述/","link":"","permalink":"https://leezhao415.github.io/2022/07/26/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AAMOT%E7%BB%BC%E8%BF%B0/","excerpt":"","text":"文章目录 多目标跟踪 MOT 1 目标跟踪分类 2 常用数据集及下载管理 3 Github 4 参考博客 多目标跟踪 MOT 多目标跟踪技术（MOT），作为计算机视觉领域中基础的、重要的研究方向之一，可广泛应用在交通管理、安防监控、自动驾驶、机器人、体育赛事转播等领域，其已成为一大研究热点。 1 目标跟踪分类 ● 根据跟踪的目标数量，目标跟踪任务可分为单目标跟踪（SOT）和多目标跟踪（MOT）； ● 根据背景状态，可分为静态背景下的目标跟踪和动态背景下的目标跟踪； ● 根据摄像头数量，可分为单摄像头跟踪和多摄像头跟踪； ● 根据任务计算类型，可分为在线跟踪、离线跟踪； 2 常用数据集及下载管理 目前多目标跟踪领域的重要基准是 MOTChallenge，作为上传并公布多目标跟踪方法研究成果的公共平台，其拥有最大的公开行人跟踪数据集。 其提供的数据集包括：MOT 15、MOT 16、 MOT 17、MOT 20，这些数据集都提供了训练集的标注，训练集与测试集的检测，以及数据集的目标检测结果，主要侧重于密集场景下行人跟踪任务。 下载地址 MOT15 MOT16 MOT17 MOT20 TAO 数据集快速下载管理 首先按照以下命令下载 image_lists.zip 并解压放在 PaddleDetection/dataset/mot 目录下： 1wget https:&#x2F;&#x2F;dataset.bj.bcebos.com&#x2F;mot&#x2F;image_lists.zip 然后按照以下命令可以快速下载 MIX 数据集的各个子数据集，并解压放在 PaddleDetection/dataset/mot 目录下： 1234567wget https:&#x2F;&#x2F;dataset.bj.bcebos.com&#x2F;mot&#x2F;MOT17.zipwget https:&#x2F;&#x2F;dataset.bj.bcebos.com&#x2F;mot&#x2F;Caltech.zipwget https:&#x2F;&#x2F;dataset.bj.bcebos.com&#x2F;mot&#x2F;CUHKSYSU.zipwget https:&#x2F;&#x2F;dataset.bj.bcebos.com&#x2F;mot&#x2F;PRW.zipwget https:&#x2F;&#x2F;dataset.bj.bcebos.com&#x2F;mot&#x2F;Cityscapes.zipwget https:&#x2F;&#x2F;dataset.bj.bcebos.com&#x2F;mot&#x2F;ETHZ.zipwget https:&#x2F;&#x2F;dataset.bj.bcebos.com&#x2F;mot&#x2F;MOT16.zip 最终目录为： 12345678910111213141516171819202122dataset&#x2F;mot |——————image_lists |——————caltech.10k.val |——————caltech.all |——————caltech.train |——————caltech.val |——————citypersons.train |——————citypersons.val |——————cuhksysu.train |——————cuhksysu.val |——————eth.train |——————mot16.train |——————mot17.train |——————prw.train |——————prw.val |——————Caltech |——————Cityscapes |——————CUHKSYSU |——————ETHZ |——————MOT16 |——————MOT17 |——————PRW 3 Github mmtracking PaddleDetection 4 参考博客 多目标跟踪（MOT）数据集 多目标跟踪数据集 ：MOT16、MOT17 数据集介绍以及多目标跟踪指标评测 多目标跟踪之 OC-SORT 目标跟踪之 MOT 经典算法：ByteTrack 算法原理以及多类别跟踪 自动驾驶论文阅读笔记 —— 精读 QDTrack 多目标跟踪 MOT 未来研究方向讨论 ECCV 2022 Oral | 大连理工 / 字节 / 港大提出 Unicorn：目标跟踪任务的大统一模型","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"目标跟踪","slug":"目标跟踪","permalink":"https://leezhao415.github.io/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"}]},{"title":"【精华】多模态研究学习","slug":"【精华】多模态研究学习","date":"2022-07-26T15:52:08.000Z","updated":"2022-07-26T16:28:41.636Z","comments":true,"path":"2022/07/26/【精华】多模态研究学习/","link":"","permalink":"https://leezhao415.github.io/2022/07/26/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E5%A4%9A%E6%A8%A1%E6%80%81%E7%A0%94%E7%A9%B6%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"文章目录 多模态研究学习 1 多模态综述 2 X-VLM 3 ERNIE-VILG 4 FLAVA 5 OFA 6 STA 多模态研究学习 2021 年诺贝尔生理学、医学奖揭晓，获奖者是戴维・朱利叶斯（DavidJulius）和阿代姆・帕塔博蒂安（Ardem Patapoutian），表彰他们在 “发现温度和触觉感受器” 方面作出的贡献。那作为算法从业者，我们该思考些什么呢？人在感知这个世界的时候，主要的方式就是视觉，嗅觉，听觉等等。其中语音，文字和图像是最重要的传播载体，这三个领域的研究也都在这几年得到了快速的发展，今天我们就来看看其交叉的领域即文字 + 图像的图文多模态，其实多模态涉及的领域很多，目前主流的是文字 + 图像这一分支。从 2018 年 Bert 横空出世以后，以预训练模型为基石的各个领域百花齐放，下面梳理的多模态预训练模型也是在这样一个背景下诞生的，具体大概是从 2019 年开始涌现的。主要包括 VILBERT 、 B2T2 、 LXMERT 、 VisualBERT 、 Unicoder-VL 、 VL-BERT 、 UNITER 、 Pixel-BERT 、 ERNIE-ViL 、 UNIMO 、 CLIP 、 FLAVA 、 ERNIE-VILG 、 X-VLM 、 OFA 、 STA 等。目前布局在这一赛道的公司包括：腾讯、百度、谷歌、微软、Facebook、UCLA、京东、阿里等等。 1 多模态综述 多模态综述 2 X-VLM 字节 AI Lab 提出多模态模型：X-VLM，学习视觉和语言多粒度对齐 论文: https://arxiv.org/pdf/2111.08276.pdf Github: https://github.com/zengyan-97/X-VLM 3 ERNIE-VILG 多模态生成模型 ERNIE-VILG 论文: https://arxiv.org/pdf/2112.15283.pdf 体验接口: https://wenxin.baidu.com/younger/apiDetail?id=20008 4 FLAVA 最新图文大一统多模态模型：FLAVA 论文: https://arxiv.org/pdf/2112.04482.pdf Github: https://github.com/Mryangkaitonggithub.com 5 OFA ICML 2022｜达摩院多模态模型 OFA，实现模态、任务和架构三个统一 论文: https://arxiv.org/pdf/2202.03052.pdf Github: https://github.com/OFA-Sys/OFA 体验接口: https://huggingface.co/OFA-Sys 6 STA 电子科大（申恒涛团队）&amp; 京东 AI（梅涛团队）提出用于视频问答的结构化双流注意网络，性能 SOTA！优于基于双视频表示的方法！ 论文: https://arxiv.org/pdf/2206.01017.pdf","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"多模态","slug":"多模态","permalink":"https://leezhao415.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"}]},{"title":"ONNX模型构造与代码检查","slug":"ONNX模型构造与代码检查","date":"2022-07-26T15:51:31.000Z","updated":"2022-07-26T15:55:48.525Z","comments":true,"path":"2022/07/26/ONNX模型构造与代码检查/","link":"","permalink":"https://leezhao415.github.io/2022/07/26/ONNX%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0%E4%B8%8E%E4%BB%A3%E7%A0%81%E6%A3%80%E6%9F%A5/","excerpt":"","text":"文章目录 ONNX 模型构造与代码检查 1 构造描述张量信息的对象 ValueInfoProto 2 构造算子节点信息 NodeProto 3 构造计算图 GraphProto 4 封装计算图 5 检查代码 ONNX Python API 构造模型完整代码 ONNX 模型构造与代码检查 参考博客:https://zhuanlan.zhihu.com/p/516920606 1 构造描述张量信息的对象 ValueInfoProto 12345678import onnx from onnx import helper from onnx import TensorProto a = helper.make_tensor_value_info(&#x27;a&#x27;, TensorProto.FLOAT, [10, 10]) x = helper.make_tensor_value_info(&#x27;x&#x27;, TensorProto.FLOAT, [10, 10]) b = helper.make_tensor_value_info(&#x27;b&#x27;, TensorProto.FLOAT, [10, 10]) output = helper.make_tensor_value_info(&#x27;output&#x27;, TensorProto.FLOAT, [10, 10]) 2 构造算子节点信息 NodeProto 12mul = helper.make_node(&#x27;Mul&#x27;, [&#x27;a&#x27;, &#x27;x&#x27;], [&#x27;c&#x27;]) add = helper.make_node(&#x27;Add&#x27;, [&#x27;c&#x27;, &#x27;b&#x27;], [&#x27;output&#x27;]) 3 构造计算图 GraphProto 1graph = helper.make_graph([mul, add], &#x27;linear_func&#x27;, [a, x, b], [output]) 4 封装计算图 用 helper.make_model 把计算图 GraphProto 封装进模型 ModelProto 1model = helper.make_model(graph) 5 检查代码 123onnx.checker.check_model(model) print(model) onnx.save(model, &#x27;linear_func.onnx&#x27;) ONNX Python API 构造模型完整代码 123456789101112131415161718192021222324import onnx from onnx import helper from onnx import TensorProto # input and output a = helper.make_tensor_value_info(&#x27;a&#x27;, TensorProto.FLOAT, [10, 10]) x = helper.make_tensor_value_info(&#x27;x&#x27;, TensorProto.FLOAT, [10, 10]) b = helper.make_tensor_value_info(&#x27;b&#x27;, TensorProto.FLOAT, [10, 10]) output = helper.make_tensor_value_info(&#x27;output&#x27;, TensorProto.FLOAT, [10, 10]) # Mul mul = helper.make_node(&#x27;Mul&#x27;, [&#x27;a&#x27;, &#x27;x&#x27;], [&#x27;c&#x27;]) # Add add = helper.make_node(&#x27;Add&#x27;, [&#x27;c&#x27;, &#x27;b&#x27;], [&#x27;output&#x27;]) # graph and model graph = helper.make_graph([mul, add], &#x27;linear_func&#x27;, [a, x, b], [output]) model = helper.make_model(graph) # save model onnx.checker.check_model(model) print(model) onnx.save(model, &#x27;linear_func.onnx&#x27;)","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"模型部署","slug":"模型部署","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"}]},{"title":"【精华】YOLOv6训练自己的数据集","slug":"【精华】YOLOv6训练自己的数据集","date":"2022-07-26T15:51:05.000Z","updated":"2022-07-26T15:54:44.166Z","comments":true,"path":"2022/07/26/【精华】YOLOv6训练自己的数据集/","link":"","permalink":"https://leezhao415.github.io/2022/07/26/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91YOLOv6%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86/","excerpt":"","text":"文章目录 资源下载 (1) YOLOv6 (2) 参考博客 1 前言 2 数据集获取 3 数据集转化 4 工程文件配置 (1) 配置模型文件 (2) 配置数据集文件 (3) 其他文件修改 5 模型训练及推理 (1) 训练环境搭建 (2) 模型训练 (3) 模型验证 (4) 模型推理 资源下载 (1) YOLOv6 ​ Github ​ [美团技术团队] YOLOv6：又快又准的目标检测框架开源啦 ​ 美团 AI 团队博客 (2) 参考博客 ​ YOLO 系列梳理（九）初尝新鲜出炉的 YOLOv6 ​ SIoU Loss: More Powerful Learning for Bounding Box Regression ​ YOLOX: Exceeding YOLO Series in 2021 ​ RepVGG: Making VGG-style ConvNets Great Again 1 前言 ​ 本文主要记录使用 YOLOv6 训练自己数据集的过程，数据集以 Objects365 数据集为例. 2 数据集获取 链接：https://pan.baidu.com/s/1QiWm8hCJus3LstZkz6Mzdw 提取码：wmrx 3 数据集转化 Objects365 数据集为 COCO 格式数据，数据集文件格式如下: 1234567891011Objects365--Images --train --obj365_train_**.jpg --val --obj365_val_**.jpg--Annotations --train --train.json --val --val.json YOLOv6 默认使用 YOLO 格式数据集，其中使用位置坐标格式为中心点坐标，数据集文件格式如下: 1234567891011121314151617Objects365_yolov6--images --train2017 --obj365_train_**.jpg --val2017 --obj365_val_**.jpg--labels --train2017 --train2017.txt --classes.txt --obj365_train_**.txt --obj365_train_**.txt --val2017 --val2017.txt --classes.txt --obj365_val_**.txt --obj365_val_**.txt 通过以下脚本实现 COCO 格式的数据集转化为 YOLO 格式的数据集: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#COCO 格式的数据集转化为 YOLO 格式的数据集# YOLO格式数据集文件结构&#x27;&#x27;&#x27;--images --train2017 --1_train.jpg --val2017 --2_val.jpg--labels --train2017 --train2017.txt --classes.txt --1_train.txt --val2017 --val2017.txt --classes.txt --2_val.txt&#x27;&#x27;&#x27;import osimport jsonfrom tqdm import tqdmdef convert(size, box): dw = 1. / (size[0]) dh = 1. / (size[1]) x = box[0] + box[2] / 2.0 y = box[1] + box[3] / 2.0 w = box[2] h = box[3]#round函数确定(xmin, ymin, xmax, ymax)的小数位数 x = round(x * dw, 6) w = round(w * dw, 6) y = round(y * dh, 6) h = round(h * dh, 6) return (x, y, w, h)if __name__ == &#x27;__main__&#x27;: # --------------------------------------------------------------------------------------------------------- # json_file = &quot;your/to/path/Objects365/Annotations/train/train.json&quot; # Objects365 json_path ana_txt_save_path = &quot;your/to/path/Objects365_yolov6/labels/train2017&quot; # anno_txt_save_path list_file = open(os.path.join(ana_txt_save_path, &#x27;train2017.txt&#x27;), &#x27;w&#x27;) txt_images_path = &#x27;your/to/path/Objects365_yolov6/images/train2017&#x27; # --------------------------------------------------------------------------------------------------------- # data = json.load(open(json_file, &#x27;r&#x27;)) if not os.path.exists(ana_txt_save_path): os.makedirs(ana_txt_save_path) id_map = &#123;&#125; # 数据集的id不连续！重新映射一下再输出！ with open(os.path.join(ana_txt_save_path, &#x27;classes.txt&#x27;), &#x27;w&#x27;) as f: # 写入classes.txt for i, category in enumerate(data[&#x27;categories&#x27;]): f.write(f&quot;&#123;category[&#x27;name&#x27;]&#125;\\n&quot;) id_map[category[&#x27;id&#x27;]] = i # print(id_map) #这里需要根据自己的需要，更改写入图像相对路径的文件位置。 for img in tqdm(data[&#x27;images&#x27;]): filename = img[&quot;file_name&quot;] img_width = img[&quot;width&quot;] img_height = img[&quot;height&quot;] img_id = img[&quot;id&quot;] head, tail = os.path.splitext(filename) ana_txt_name = head + &quot;.txt&quot; # 对应的txt名字，与jpg一致 f_txt = open(os.path.join(ana_txt_save_path, ana_txt_name), &#x27;w&#x27;) for ann in data[&#x27;annotations&#x27;]: if ann[&#x27;image_id&#x27;] == img_id: box = convert((img_width, img_height), ann[&quot;bbox&quot;]) f_txt.write(&quot;%s %s %s %s %s\\n&quot; % (id_map[ann[&quot;category_id&quot;]], box[0], box[1], box[2], box[3])) f_txt.close() #将图片的相对路径写入train2017或val2017的路径 list_file.write(txt_images_path + &#x27;/%s.jpg\\n&#x27; %(head)) list_file.close() 4 工程文件配置 (1) 配置模型文件 模型文件路径: config/yolov6n_objects365.py (新建) 以 YOLOv6n 为例: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# YOLOv6n modelmodel = dict( type=&#x27;YOLOv6n&#x27;, pretrained=&#x27;./weights/yolov6n.pt&#x27;, depth_multiple=0.33, width_multiple=0.25, backbone=dict( type=&#x27;EfficientRep&#x27;, num_repeats=[1, 6, 12, 18, 6], out_channels=[64, 128, 256, 512, 1024], ), neck=dict( type=&#x27;RepPAN&#x27;, num_repeats=[12, 12, 12, 12], out_channels=[256, 128, 128, 256, 256, 512], ), head=dict( type=&#x27;EffiDeHead&#x27;, in_channels=[128, 256, 512], num_layers=3, begin_indices=24, anchors=1, out_indices=[17, 20, 23], strides=[8, 16, 32], iou_type=&#x27;ciou&#x27; ))solver = dict( optim=&#x27;SGD&#x27;, lr_scheduler=&#x27;Cosine&#x27;, lr0=0.00258, lrf=0.17, momentum=0.779, weight_decay=0.00058, warmup_epochs=1.33, warmup_momentum=0.86, warmup_bias_lr=0.0711)data_aug = dict( hsv_h=0.0188, hsv_s=0.704, hsv_v=0.36, degrees=0.373, translate=0.0902, scale=0.491, shear=0.602, flipud=0.00856, fliplr=0.5, mosaic=1.0, mixup=0.243) (2) 配置数据集文件 数据集配置文件路径: data/objects365.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344train: your/to/path/Objects365_yolov6/images/train2017val: your/to/path/Objects365_yolov6/images/val2017test: your/to/path/Objects365_yolov6/images/val2017#anno_path: your/to/path/Objects365_yolov6/annotations/instances_val2017.json # 该标签为程序自动生成，不能指定为原COCO数据集的标签文件，路径只需修改根目录即可# number of classesnc: 365# class namesnames: [ &quot;human&quot;,&quot;sneakers&quot;,&quot;chair&quot;,&quot;hat&quot;,&quot;lamp&quot;,&quot;bottle&quot;,&quot;cabinet/shelf&quot;,&quot;cup&quot;,&quot;car&quot;,&quot;glasses&quot;,&quot;picture/frame&quot;,&quot;desk&quot;,&quot;handbag&quot;, &quot;street lights&quot;,&quot;book&quot;,&quot;plate&quot;,&quot;helmet&quot;,&quot;leather shoes&quot;,&quot;pillow&quot;,&quot;glove&quot;,&quot;potted plant&quot;,&quot;bracelet&quot;,&quot;flower&quot;,&quot;monitor&quot;, &quot;storage box&quot;,&quot;plants pot/vase&quot;,&quot;bench&quot;,&quot;wine glass&quot;,&quot;boots&quot;,&quot;dining table&quot;,&quot;umbrella&quot;,&quot;boat&quot;,&quot;flag&quot;,&quot;speaker&quot;, &quot;trash bin/can&quot;,&quot;stool&quot;,&quot;backpack&quot;,&quot;sofa&quot;,&quot;belt&quot;,&quot;carpet&quot;,&quot;basket&quot;,&quot;towel/napkin&quot;,&quot;slippers&quot;,&quot;bowl&quot;,&quot;barrel/bucket&quot;, &quot;coffee table&quot;,&quot;suv&quot;,&quot;toy&quot;,&quot;tie&quot;,&quot;bed&quot;,&quot;traffic light&quot;,&quot;pen/pencil&quot;,&quot;microphone&quot;,&quot;sandals&quot;,&quot;canned&quot;,&quot;necklace&quot;, &quot;mirror&quot;,&quot;faucet&quot;,&quot;bicycle&quot;,&quot;bread&quot;,&quot;high heels&quot;,&quot;ring&quot;,&quot;van&quot;,&quot;watch&quot;,&quot;combine with bowl&quot;,&quot;sink&quot;,&quot;horse&quot;,&quot;fish&quot;, &quot;apple&quot;,&quot;traffic sign&quot;,&quot;camera&quot;,&quot;candle&quot;,&quot;stuffed animal&quot;,&quot;cake&quot;,&quot;motorbike/motorcycle&quot;,&quot;wild bird&quot;,&quot;laptop&quot;, &quot;knife&quot;,&quot;cellphone&quot;,&quot;paddle&quot;,&quot;truck&quot;,&quot;cow&quot;,&quot;power outlet&quot;,&quot;clock&quot;,&quot;drum&quot;,&quot;fork&quot;,&quot;bus&quot;,&quot;hanger&quot;,&quot;nightstand&quot;, &quot;pot/pan&quot;,&quot;sheep&quot;,&quot;guitar&quot;,&quot;traffic cone&quot;,&quot;tea pot&quot;,&quot;keyboard&quot;,&quot;tripod&quot;,&quot;hockey stick&quot;,&quot;fan&quot;,&quot;dog&quot;,&quot;spoon&quot;, &quot;blackboard/whiteboard&quot;,&quot;balloon&quot;,&quot;air conditioner&quot;,&quot;cymbal&quot;,&quot;mouse&quot;,&quot;telephone&quot;,&quot;pickup truck&quot;,&quot;orange&quot;,&quot;banana&quot;, &quot;airplane&quot;,&quot;luggage&quot;,&quot;skis&quot;,&quot;soccer&quot;,&quot;trolley&quot;,&quot;oven&quot;,&quot;remote&quot;,&quot;combine with glove&quot;,&quot;paper towel&quot;,&quot;refrigerator&quot;, &quot;train&quot;,&quot;tomato&quot;,&quot;machinery vehicle&quot;,&quot;tent&quot;,&quot;shampoo/shower gel&quot;,&quot;head phone&quot;,&quot;lantern&quot;,&quot;donut&quot;,&quot;cleaning products&quot;, &quot;sailboat&quot;,&quot;tangerine&quot;,&quot;pizza&quot;,&quot;kite&quot;,&quot;computer box&quot;,&quot;elephant&quot;,&quot;toiletries&quot;,&quot;gas stove&quot;,&quot;broccoli&quot;,&quot;toilet&quot;,&quot;stroller&quot;, &quot;shovel&quot;,&quot;baseball bat&quot;,&quot;microwave&quot;,&quot;skateboard&quot;,&quot;surfboard&quot;,&quot;surveillance camera&quot;,&quot;gun&quot;,&quot;Life saver&quot;,&quot;cat&quot;,&quot;lemon&quot;, &quot;liquid soap&quot;,&quot;zebra&quot;,&quot;duck&quot;,&quot;sports car&quot;,&quot;giraffe&quot;,&quot;pumpkin&quot;,&quot;Accordion/keyboard/piano&quot;,&quot;radiator&quot;,&quot;converter&quot;, &quot;tissue&quot;,&quot;carrot&quot;,&quot;washing machine&quot;,&quot;vent&quot;,&quot;cookies&quot;,&quot;cutting/chopping board&quot;,&quot;tennis racket&quot;,&quot;candy&quot;, &quot;skating and skiing shoes&quot;,&quot;scissors&quot;,&quot;folder&quot;,&quot;baseball&quot;,&quot;strawberry&quot;,&quot;bow tie&quot;,&quot;pigeon&quot;,&quot;pepper&quot;,&quot;coffee machine&quot;, &quot;bathtub&quot;,&quot;snowboard&quot;,&quot;suitcase&quot;,&quot;grapes&quot;,&quot;ladder&quot;,&quot;pear&quot;,&quot;american football&quot;,&quot;basketball&quot;,&quot;potato&quot;,&quot;paint brush&quot;, &quot;printer&quot;,&quot;billiards&quot;,&quot;fire hydrant&quot;,&quot;goose&quot;,&quot;projector&quot;,&quot;sausage&quot;,&quot;fire extinguisher&quot;,&quot;extension cord&quot;,&quot;facial mask&quot;, &quot;tennis ball&quot;,&quot;chopsticks&quot;,&quot;Electronic stove and gas stove&quot;,&quot;pie&quot;,&quot;frisbee&quot;,&quot;kettle&quot;,&quot;hamburger&quot;,&quot;golf club&quot;,&quot;cucumber&quot;, &quot;clutch&quot;,&quot;blender&quot;,&quot;tong&quot;,&quot;slide&quot;,&quot;hot dog&quot;,&quot;toothbrush&quot;,&quot;facial cleanser&quot;,&quot;mango&quot;,&quot;deer&quot;,&quot;egg&quot;,&quot;violin&quot;,&quot;marker&quot;, &quot;ship&quot;,&quot;chicken&quot;,&quot;onion&quot;,&quot;ice cream&quot;,&quot;tape&quot;,&quot;wheelchair&quot;,&quot;plum&quot;,&quot;bar soap&quot;,&quot;scale&quot;,&quot;watermelon&quot;,&quot;cabbage&quot;,&quot;router/modem&quot;, &quot;golf ball&quot;,&quot;pine apple&quot;,&quot;crane&quot;,&quot;fire truck&quot;,&quot;peach&quot;,&quot;cello&quot;,&quot;notepaper&quot;,&quot;tricycle&quot;,&quot;toaster&quot;,&quot;helicopter&quot;,&quot;green beans&quot;, &quot;brush&quot;,&quot;carriage&quot;,&quot;cigar&quot;,&quot;earphone&quot;,&quot;penguin&quot;,&quot;hurdle&quot;,&quot;swing&quot;,&quot;radio&quot;,&quot;CD&quot;,&quot;parking meter&quot;,&quot;swan&quot;,&quot;garlic&quot;,&quot;french fries&quot;, &quot;horn&quot;,&quot;avocado&quot;,&quot;saxophone&quot;,&quot;trumpet&quot;,&quot;sandwich&quot;,&quot;cue&quot;,&quot;kiwi fruit&quot;,&quot;bear&quot;,&quot;fishing rod&quot;,&quot;cherry&quot;,&quot;tablet&quot;,&quot;green vegetables&quot;, &quot;nuts&quot;,&quot;corn&quot;,&quot;key&quot;,&quot;screwdriver&quot;,&quot;globe&quot;,&quot;broom&quot;,&quot;pliers&quot;,&quot;hammer&quot;,&quot;volleyball&quot;,&quot;eggplant&quot;,&quot;trophy&quot;,&quot;board eraser&quot;,&quot;dates&quot;, &quot;rice&quot;,&quot;tape measure/ruler&quot;,&quot;dumbbell&quot;,&quot;hamimelon&quot;,&quot;stapler&quot;,&quot;camel&quot;,&quot;lettuce&quot;,&quot;goldfish&quot;,&quot;meat balls&quot;,&quot;medal&quot;,&quot;toothpaste&quot;, &quot;antelope&quot;,&quot;shrimp&quot;,&quot;rickshaw&quot;,&quot;trombone&quot;,&quot;pomegranate&quot;,&quot;coconut&quot;,&quot;jellyfish&quot;,&quot;mushroom&quot;,&quot;calculator&quot;,&quot;treadmill&quot;,&quot;butterfly&quot;, &quot;egg tart&quot;,&quot;cheese&quot;,&quot;pomelo&quot;,&quot;pig&quot;,&quot;race car&quot;,&quot;rice cooker&quot;,&quot;tuba&quot;,&quot;crosswalk sign&quot;,&quot;papaya&quot;,&quot;hair dryer&quot;,&quot;green onion&quot;,&quot;chips&quot;, &quot;dolphin&quot;,&quot;sushi&quot;,&quot;urinal&quot;,&quot;donkey&quot;,&quot;electric drill&quot;,&quot;spring rolls&quot;,&quot;tortoise/turtle&quot;,&quot;parrot&quot;,&quot;flute&quot;,&quot;measuring cup&quot;,&quot;shark&quot;, &quot;steak&quot;,&quot;poker card&quot;,&quot;binoculars&quot;,&quot;llama&quot;,&quot;radish&quot;,&quot;noodles&quot;,&quot;mop&quot;,&quot;yak&quot;,&quot;crab&quot;,&quot;microscope&quot;,&quot;barbell&quot;,&quot;Bread/bun&quot;,&quot;baozi&quot;, &quot;lion&quot;,&quot;red cabbage&quot;,&quot;polar bear&quot;,&quot;lighter&quot;,&quot;mangosteen&quot;,&quot;seal&quot;,&quot;comb&quot;,&quot;eraser&quot;,&quot;pitaya&quot;,&quot;scallop&quot;,&quot;pencil case&quot;,&quot;saw&quot;, &quot;table tennis paddle&quot;,&quot;okra&quot;,&quot;starfish&quot;,&quot;monkey&quot;,&quot;eagle&quot;,&quot;durian&quot;,&quot;rabbit&quot;,&quot;game board&quot;,&quot;french horn&quot;,&quot;ambulance&quot;,&quot;asparagus&quot;, &quot;hoverboard&quot;,&quot;pasta&quot;,&quot;target&quot;,&quot;hotair balloon&quot;,&quot;chainsaw&quot;,&quot;lobster&quot;,&quot;iron&quot;,&quot;flashlight&quot;] (3) 其他文件修改 针对 Objects365 此类类别个数超过超过 2 位数的数据集需修改以下文件: 文件路径: YOLOv6/yolov6/data/datasets.py 123456789101112131415161718192021222324252627282930313233343536373839404142 @staticmethod def check_label_files(args): img_path, lb_path = args nm, nf, ne, nc, msg = 0, 0, 0, 0, &quot;&quot; # number (missing, found, empty, message try: if osp.exists(lb_path): nf = 1 # label found with open(lb_path, &quot;r&quot;) as f: labels = [ x.split() for x in f.read().strip().splitlines() if len(x) ] labels = np.array(labels, dtype=np.float32) if len(labels): assert all( len(l) == 5 for l in labels ), f&quot;&#123;lb_path&#125;: wrong label format.&quot; assert ( labels &gt;= 0 ).all(), f&quot;&#123;lb_path&#125;: Label values error: all values in label file must &gt; 0&quot;# --------------------------------------注释掉本断言-------------------------------------------------------------------- # # assert ( # labels[:, 1:] &lt;= 1 # ).all(), f&quot;&#123;lb_path&#125;: Label values error: all coordinates must be normalized&quot;# -------------------------------------------------------------------------------------------------------------------- # _, indices = np.unique(labels, axis=0, return_index=True) if len(indices) &lt; len(labels): # duplicate row check labels = labels[indices] # remove duplicates msg += f&quot;WARNING: &#123;lb_path&#125;: &#123;len(labels) - len(indices)&#125; duplicate labels removed&quot; labels = labels.tolist() else: ne = 1 # label empty labels = [] else: nm = 1 # label missing labels = [] return img_path, labels, nc, nm, nf, ne, msg except Exception as e: nc = 1 msg = f&quot;WARNING: &#123;lb_path&#125;: ignoring invalid labels: &#123;e&#125;&quot; return None, None, nc, nm, nf, ne, msg 5 模型训练及推理 (1) 训练环境搭建 使用 conda 或者 Docker 虚拟环境皆可，此处请自行搭建 (2) 模型训练 训练参数配置 12345678910111213141516171819202122# 文件位置: tools/train.pydef get_args_parser(add_help=True): parser = argparse.ArgumentParser(description=&#x27;YOLOv6 PyTorch Training&#x27;, add_help=add_help) parser.add_argument(&#x27;--data-path&#x27;, default=&#x27;./data/coco.yaml&#x27;, type=str, help=&#x27;path of dataset&#x27;) parser.add_argument(&#x27;--conf-file&#x27;, default=&#x27;./configs/yolov6s.py&#x27;, type=str, help=&#x27;experiments description file&#x27;) parser.add_argument(&#x27;--img-size&#x27;, type=int, default=640, help=&#x27;train, val image size (pixels)&#x27;) parser.add_argument(&#x27;--batch-size&#x27;, default=32, type=int, help=&#x27;total batch size for all GPUs&#x27;) parser.add_argument(&#x27;--epochs&#x27;, default=400, type=int, help=&#x27;number of total epochs to run&#x27;) parser.add_argument(&#x27;--workers&#x27;, default=4, type=int, help=&#x27;number of data loading workers (default: 8)&#x27;) parser.add_argument(&#x27;--device&#x27;, default=&#x27;0&#x27;, type=str, help=&#x27;cuda device, i.e. 0 or 0,1,2,3 or cpu&#x27;) parser.add_argument(&#x27;--eval-interval&#x27;, type=int, default=20, help=&#x27;evaluate at every interval epochs&#x27;) parser.add_argument(&#x27;--eval-final-only&#x27;, action=&#x27;store_true&#x27;, help=&#x27;only evaluate at the final epoch&#x27;) parser.add_argument(&#x27;--heavy-eval-range&#x27;, default=50, help=&#x27;evaluating every epoch for last such epochs (can be jointly used with --eval-interval)&#x27;) parser.add_argument(&#x27;--check-images&#x27;, action=&#x27;store_true&#x27;, help=&#x27;check images when initializing datasets&#x27;) parser.add_argument(&#x27;--check-labels&#x27;, action=&#x27;store_true&#x27;, help=&#x27;check label files when initializing datasets&#x27;) parser.add_argument(&#x27;--output-dir&#x27;, default=&#x27;./runs/train&#x27;, type=str, help=&#x27;path to save outputs&#x27;) parser.add_argument(&#x27;--name&#x27;, default=&#x27;exp&#x27;, type=str, help=&#x27;experiment name, saved to output_dir/name&#x27;) parser.add_argument(&#x27;--dist_url&#x27;, type=str, default=&quot;default url: tcp://127.0.0.1:8888&quot;) parser.add_argument(&#x27;--gpu_count&#x27;, type=int, default=0) parser.add_argument(&#x27;--local_rank&#x27;, type=int, default=-1, help=&#x27;DDP parameter&#x27;) parser.add_argument(&#x27;--resume&#x27;, type=str, default=None, help=&#x27;resume the corresponding ckpt&#x27;) 模型训练 1python tools/train.py --batch-size 32 --conf-file configs/yolov6n_objects365.py --data-path data/objects365.yaml --device 0 (3) 模型验证 验证参数配置 1234567891011121314151617# 文件位置: tools/eval.pydef get_args_parser(add_help=True): parser = argparse.ArgumentParser(description=&#x27;YOLOv6 PyTorch Evalating&#x27;, add_help=add_help) parser.add_argument(&#x27;--data&#x27;, type=str, default=&#x27;./data/coco.yaml&#x27;, help=&#x27;dataset.yaml path&#x27;) parser.add_argument(&#x27;--weights&#x27;, type=str, default=&#x27;./weights/yolov6s.pt&#x27;, help=&#x27;model.pt path(s)&#x27;) parser.add_argument(&#x27;--batch-size&#x27;, type=int, default=32, help=&#x27;batch size&#x27;) parser.add_argument(&#x27;--img-size&#x27;, type=int, default=640, help=&#x27;inference size (pixels)&#x27;) parser.add_argument(&#x27;--conf-thres&#x27;, type=float, default=0.001, help=&#x27;confidence threshold&#x27;) parser.add_argument(&#x27;--iou-thres&#x27;, type=float, default=0.65, help=&#x27;NMS IoU threshold&#x27;) parser.add_argument(&#x27;--task&#x27;, default=&#x27;val&#x27;, help=&#x27;val, or speed&#x27;) parser.add_argument(&#x27;--device&#x27;, default=&#x27;0&#x27;, help=&#x27;cuda device, i.e. 0 or 0,1,2,3 or cpu&#x27;) parser.add_argument(&#x27;--half&#x27;, default=False, action=&#x27;store_true&#x27;, help=&#x27;whether to use fp16 infer&#x27;) parser.add_argument(&#x27;--save_dir&#x27;, type=str, default=&#x27;runs/val/&#x27;, help=&#x27;evaluation save dir&#x27;) parser.add_argument(&#x27;--name&#x27;, type=str, default=&#x27;exp&#x27;, help=&#x27;save evaluation results to save_dir/name&#x27;) args = parser.parse_args() LOGGER.info(args) return args 模型验证 1python tools/eval.py --data data/objects365.yaml --batch-size 32 --weights weights/yolov6n.pt --task val (4) 模型推理 推理参数配置 123456789101112131415161718192021222324# 文件位置: tools/infer.pydef get_args_parser(add_help=True): parser = argparse.ArgumentParser(description=&#x27;YOLOv6 PyTorch Inference.&#x27;, add_help=add_help) parser.add_argument(&#x27;--weights&#x27;, type=str, default=&#x27;weights/yolov6s.pt&#x27;, help=&#x27;model path(s) for inference.&#x27;) parser.add_argument(&#x27;--source&#x27;, type=str, default=&#x27;data/images&#x27;, help=&#x27;the source path, e.g. image-file/dir.&#x27;) parser.add_argument(&#x27;--yaml&#x27;, type=str, default=&#x27;data/coco.yaml&#x27;, help=&#x27;data yaml file.&#x27;) parser.add_argument(&#x27;--img-size&#x27;, type=int, default=640, help=&#x27;the image-size(h,w) in inference size.&#x27;) parser.add_argument(&#x27;--conf-thres&#x27;, type=float, default=0.25, help=&#x27;confidence threshold for inference.&#x27;) parser.add_argument(&#x27;--iou-thres&#x27;, type=float, default=0.45, help=&#x27;NMS IoU threshold for inference.&#x27;) parser.add_argument(&#x27;--max-det&#x27;, type=int, default=1000, help=&#x27;maximal inferences per image.&#x27;) parser.add_argument(&#x27;--device&#x27;, default=&#x27;0&#x27;, help=&#x27;device to run our model i.e. 0 or 0,1,2,3 or cpu.&#x27;) parser.add_argument(&#x27;--save-txt&#x27;, action=&#x27;store_true&#x27;, help=&#x27;save results to *.txt.&#x27;) parser.add_argument(&#x27;--save-img&#x27;, action=&#x27;store_false&#x27;, help=&#x27;save visuallized inference results.&#x27;) parser.add_argument(&#x27;--classes&#x27;, nargs=&#x27;+&#x27;, type=int, help=&#x27;filter by classes, e.g. --classes 0, or --classes 0 2 3.&#x27;) parser.adkd_argument(&#x27;--agnostic-nms&#x27;, action=&#x27;store_true&#x27;, help=&#x27;class-agnostic NMS.&#x27;) parser.add_argument(&#x27;--project&#x27;, default=&#x27;runs/inference&#x27;, help=&#x27;save inference results to project/name.&#x27;) parser.add_argument(&#x27;--name&#x27;, default=&#x27;exp&#x27;, help=&#x27;save inference results to project/name.&#x27;) parser.add_argument(&#x27;--hide-labels&#x27;, default=False, action=&#x27;store_true&#x27;, help=&#x27;hide labels.&#x27;) parser.add_argument(&#x27;--hide-conf&#x27;, default=False, action=&#x27;store_true&#x27;, help=&#x27;hide confidences.&#x27;) parser.add_argument(&#x27;--half&#x27;, action=&#x27;store_true&#x27;, help=&#x27;whether to use FP16 half-precision inference.&#x27;) args = parser.parse_args() LOGGER.info(args) return args 模型推理 1python tools/infer.py --weights weights/yolov6n.pt --yaml data/objects365.yaml --source your/to/images.jpg","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}]},{"title":"【精华】从YOLOX详解模型优化思路及涨点技巧","slug":"【精华】从YOLOX详解模型优化思路及涨点技巧","date":"2022-03-11T15:37:12.000Z","updated":"2022-03-11T15:39:13.747Z","comments":true,"path":"2022/03/11/【精华】从YOLOX详解模型优化思路及涨点技巧/","link":"","permalink":"https://leezhao415.github.io/2022/03/11/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E4%BB%8EYOLOX%E8%AF%A6%E8%A7%A3%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%B6%A8%E7%82%B9%E6%8A%80%E5%B7%A7/","excerpt":"","text":"文章目录 从 YOLOX 详解模型优化思路及涨点技巧 一、简介 二、YOLOX 三、与 SOTA 的比较 四、流媒体感知挑战赛 五、总结 从 YOLOX 详解模型优化思路及涨点技巧 源码：https://github.com/Megvii-BaseDetection/YOLOX 一、简介 在这份报告中，我们对 YOLO 系列进行了一些有经验的改进，形成了一个新的高性能检测器–YOLOX。我们将 YOLO 检测器切换到无锚的方式，并进行其他先进的检测技术，即解耦头和领先的标签分配策略 SimOTA，在大规模的模型范围内获得最先进的结果。对于只有 0.91M 参数和 1.08G FLOPs 的 YOLO Nano，我们在 COCO 上得到 25.3% 的 AP，超过 NanoDet 1.8% 的 AP；对于 YOLOv3，工业界最广泛使用的检测器之一，我们将其提升到 47. 3%，对于 YOLOX-L，其参数量与 YOLOv4-CSP、YOLOv5-L 大致相同，我们在 Tesla V100 上以 68.9 FPS 的速度在 COCO 上实现了 50.0% 的 AP，比 YOLOv5-L 超出 1.8% 的 AP。此外，我们使用单个 YOLOX-L 模型赢得了 stream 感知挑战赛的第一名。我们希望这份报告能够为开发者和研究人员在实际场景中提供经验，我们还提供了支持 ONNX、TensorRT、NCNN 和 Openvino 的部署版本。源代码在 https://github.com/Megvii-BaseDetection/YOLOX。 随着目标检测的发展，YOLO 系列 [23, 24, 25, 1, 7] 始终追求实时应用的最佳速度和精度折衷。他们提取了当时最先进的检测技术（例如，YOLOv2 [24] 的锚点 [26]，YOLOv3 [25] 的残差网 [9]），并优化了最佳实践的实施。目前，YOLOv5 [7] 拥有最佳的权衡性能，在 13.7 毫秒的 COCO 上有 48.2% 的 AP。 尽管如此，在过去两年中，目标检测学术界的主要进展集中在无锚检测器 [29, 40, 14]、高级标签分配策略 [37, 36, 12, 41, 22, 4] 和端到端（无 NMS）检测器 [2, 32, 39]。这些还没有被整合到 YOLO 系列中，因为 YOLOv4 和 YOLOv5 仍然是基于锚的检测器，使用人工筛选的分配规则进行训练。 这就是我们在这里的原因，通过经验丰富的优化为 YOLO 系列提供这些最新的进展。考虑到 YOLOv4 和 YOLOv5 对于基于锚的 pipeline 可能有点过度优化，我们选择 YOLOv3 [25] 作为我们的起点（我们将 YOLOv3-SPP 设置为默认的 YOLOv3）。事实上，由于在各种实际应用中计算资源有限，软件支持不足，YOLOv3 仍然是业界使用最广泛的检测器之一。 如图 1 所示，通过上述技术的经验更新，我们将 YOLOv3 在 640×640 分辨率的 COCO 上的 AP 提升到 47.3%（YOLOX-DarkNet53），大大超过了 YOLOv3 目前的最佳实践（44.3% AP，ultralytics version2）。此外，当切换到采用先进的 CSPNet [31] 主干网和额外的 PAN [19] 头的高级 YOLOv5 架构时，YOLOX-L 在 640×640 分辨率的 COCO 上实现了 50.0% 的 AP，比对应的 YOLOv5-L 的 AP 高出 1.8%。我们还在小尺寸的模型上测试了我们的设计策略。YOLOX-Tiny 和 YOLOX-Nano（只有 0.91M 的参数和 1.08G 的 FLOPs）分别比对应的 YOLOv4-Tiny 和 NanoDet3 的性能好 10% AP 和 1.8% AP。 ​ 图 1: YOLOX 和其他最先进的目标检测的精确模型的速度 - 精度权衡（顶部）和移动设备上的精简模型的尺寸 - 精度曲线（底部)。 我们已经在 https://github. com/Megvii-BaseDetection/YOLOX 发布了我们的代码，并支持 ONNX、TensorRT、NCNN 和 Openvino。还有一件事值得一提，我们用一个 YOLOX-L 模型赢得了 stream 感知挑战赛的第一名。 二、YOLOX 2.1. YOLOX-DarkNet53 我们选择 YOLOv3 [25] 与 Darknet53 作为我们的基线。在下面的部分，我们将一步一步地走过 YOLOX 的整个系统设计。 实施细节: 我们的训练设置从基线到最终模型基本一致。我们在 COCO train 2017 [17] 上总共训练了 300 个 epochs，并进行了 5 个 epochs 的预热。我们使用随机梯度下降法（SGD）进行训练。我们使用 lr×BatchSize/64 的学习率（线性缩放 [8]），初始 lr=0.01，余弦 lr 计划。权重衰减为 0.0005，SGD 动量为 0.9。批次大小为 128，默认为典型的 8-GPU 设备。其他批次大小包括单 GPU 训练也很好。输入大小从 448 到 832 均匀地抽取，有 32 个步长。本报告中的 FPS 和延迟都是在单个 Tesla V100 上用 FP16 精度和 batch=1 测量的。 YOLOv3 基线 我们的基线采用了 DarkNet53 主干网和 SPP 层的架构，在一些论文中被称为 YOLOv3-SPP [1, 7]。与原始实现 [25] 相比，我们稍微改变了一些训练策略，增加了 EMA 权重更新、余弦 lr 计划、IoU 损失和 IoU 感知的分支。我们使用 BCE 损失来训练 cls 和 obj 分支，使用 IoU 损失来训练 reg 分支。这些一般的训练技巧与 YOLOX 的关键改进是不相关的，因此我们把它们放在基线上。此外，我们只进行了 RandomHorizontalFlip、ColorJitter 和 multi-scale 的数据增强，而放弃了 RandomResizedCrop 策略，因为我们发现 RandomResizedCrop 与计划中的马赛克增强有一定的重叠性。通过这些改进，我们的基线在 COCO 值上取得了 38.5% 的 AP，如表 2 所示。 ​ 表 2: YOLOX-Darknet53 在 COCO val 上的 AP（%) 路线图。所有的模型都是在 640×640 分辨率下测试的， ​ 在 Tesla V100 上采用 FP16 - 精度和 batch=1。本表中的延迟和 FPS 是在没有后期处理的情况下测量的。 解耦头 在目标检测中，分类和回归任务之间的冲突是一个著名的问题 [27, 34]。因此，用于分类和定位的解耦头被广泛用于大多数 one-stage 和 two-stage 检测器中 [16, 29, 35, 34]。然而，由于 YOLO 系列的主干和特征金字塔（如 FPN [13]，PAN [20]。）不断发展，它们的检测头仍然是耦合的，如图 2 所示。 图 2：说明了 YOLOv3 头和提议的解耦头之间的区别。对于每一级 FPN 特征，我们首先采用 1×1 的卷积层，将特征通道减少到 256 个，然后增加两个平行分支，每个分支有两个 3×320 的卷积层，分别用于分类和回归任务。IoU 分支被添加到回归分支上。 我们的两个分析实验表明，耦合的检测头可能会损害性能。如图 3 所示，用解耦头取代 YOLO 的头，大大改善了收敛速度。解耦头对 YOLO 的端到端版本至关重要（接下来将介绍）。从 Tab. 1 可以看出，端到端性能在耦合头的情况下减少了 4.2% 的 AP，而在解耦合头的情况下，减少的 AP 为 0.8%。因此，我们用图 2 中的轻型解耦头取代 YOLO 检测头。具体来说，它包含一个 1×1 的卷积层以减少通道尺寸，然后是两个平行的分支，分别有两个 3×3 的卷积层。我们在表 2 中报告了在 V100 上使用 batch=1 的推理时间，以及轻型解耦头的推理时间，轻型解耦头带来了额外的 1.1 毫秒（11.6 毫秒对 10.5 毫秒）。 ​ 表 1: 解耦头对端到端 YOLO 在 AP（%) 方面对 COCO 的影响。 强大的数据增强 我们在增强策略中加入了 Mosaic 和 MixUp，以提高 YOLOX 的性能。Mosaic 是由 ultralytics-YOLOv32 提出的一种高效的增强策略。它随后被广泛用于 YOLOv4 [1]、YOLOv5 [7] 和其他检测器 [3] 中。MixUp [10] 最初是为图像分类任务设计的，但后来在 BoF [38] 中被修改为目标检测训练。我们在模型中采用了 MixUp 和 Mosaic 的实现，并在最后 15 个 epoch 中关闭了它，在 Tab.2 中实现了 42.0% 的 AP。2. 在使用强大的数据增强后，我们发现 ImageNet 的预训练没有更多好处，因此我们从头开始训练以下所有模型。 无锚点 YOLOv4 [1] 和 YOLOv5 [7] 都遵循 YOLOv3 [25] 的原始基于锚点的 pipeline。然而，锚点机制有许多已知的问题。首先，为了达到最佳的检测性能，需要在训练前进行聚类分析，以确定一组最佳锚点。那些聚类的锚是特定领域的，通用性较差。其次，锚点机制增加了检测头的复杂性，以及每张图像的预测数量。在一些边人工智能系统中，在设备之间移动如此大量的预测（例如，从 NPU 到 CPU）可能成为整体延迟的潜在瓶颈。 无锚检测器 [29, 40, 14] 在过去两年中发展迅速。这些工作表明，无锚检测器的性能可以与基于锚的检测器相媲美。无锚机制大大减少了需要启发式调整的设计参数的数量和涉及的许多技巧（例如，锚聚类 [24]，网格敏感 [11]。）以获得良好的性能，使检测器，特别是其训练和解码阶段，大大简化 [29]。 将 YOLO 切换到无锚的方式是非常简单的。我们将每个位置的预测值从 3 减少到 1，并使其直接预测四个值，即网格左上角的两个偏移量，以及预测的 box 的高度和宽度。我们将每个目标的中心位置指定为正样本，并像 [29] 中那样预先定义一个比例范围，以指定每个目标的 FPN 水平。这样的修改减少了检测器的参数和 GFLOPs，使其更快，但获得了更好的性能 42.9% 的 AP，如表所示。 多正例 为了与 YOLOv3 的分配规则保持一致，上述无锚版本只为每个目标选择一个正例样本（中心位置），同时忽略其他高质量的预测。然而，优化这些高质量的预测也可能带来有益的梯度，这可能会缓解训练期间正 / 负采样的极端不平衡。我们简单地将中心的 3×3 区域指定为 正例区域，在 FCOS [29] 中也被称为 “中心采样”。如表 2 所示，检测器的性能提高到 45.0% 的 AP，已经超过了目前超分析的最佳实践 - YOLOv3（44.3% AP2）。 SimOTA 高级标签分配是近年来目标检测的另一个重要进展。基于我们自己的研究 OTA [4]，我们总结出高级标签分配的四个关键观点。 1). 损失 / 质量意识 2). 中心先验 3). 每个 ground truth 的动态正锚数（简写为动态 top-k） 4). 全局 view。 OTA 符合上述所有四个规则，因此我们选择它作为候选标签分配策略。 具体来说，OTA [4] 从全局角度分析了标签分配，并将分配程序表述为最优传输（OT）问题，在目前的分配策略中产生了 SOTA 的性能 [12, 41, 36, 22, 37]。然而，在实践中，我们发现通过 Sinkhorn-Knopp 算法解决 OT 问题会带来 25% 的额外训练时间，这对于训练 300 个 epochs 来说是相当昂贵的。因此，我们将其简化为动态 top-k 策略，命名为 SimOTA，以获得一个近似的解决方案。 我们在此简单介绍一下 SimOTA。SimOTA 首先计算配对程度，用开销 [4, 5, 12, 2] 或质量 [33] 来表示每个预测 - gt 对。例如，在 SimOTA 中，gt gi 和预测 pj 之间的开销被计算为: 其中 λ 是一个平衡系数。Lcls ij 和 Lreg ij 是 gt gi 和预测 pj 之间的分类损失和回归损失。然后，对于 gt gi，我们选择固定中心区域内开销最小的前 k 个预测作为其正样本。最后，这些正例预测的相应网格被指定为正例，而其余的网格为负例。请注意，不同的 ground truth 下，k 值是不同的。更多细节请参考 OTA [4] 中的动态 k 估计策略。 SimOTA 不仅减少了训练时间，而且还避免了 SinkhornKnopp 算法中额外的求解器超参数。如表 2 所示。2，SimOTA 将检测器从 45.0% 的 AP 提高到 47.3% 的 AP，比 SOTA 超分析 - YOLOv3 高 3.0% 的 AP，显示了高级分配策略的力量。 端到端 YOLO 我们遵循 [39]，增加了两个额外的卷积层，一对一的标签分配和停止梯度。这些使检测器能够以端到端的方式进行检测，但性能和推理速度略有下降，如表 2 中所列。 因此，我们把它作为一个可选的模块，不参与我们的最终模型。 2.2. 其他主干 除了 DarkNet53，我们还在其他不同规模的主干上测试了 YOLOX，在这些主干上，YOLOX 取得了与所有相应的一致的改进。 YOLOv5 中的修正 CSPNet 为了进行公平的比较，我们采用了 YOLOv5 的主干，包括修正的 CSPNet [31]、SiLU 激活和 PAN [19] 头。我们还遵循它的缩放规则来制作 YOLOXS、YOLOX-M、YOLOX-L 和 YOLOX-X 模型。与表 3 中的 YOLOv5 相比，我们的模型得到了一致的改进。从 3.0% 到 1.0% 的 AP，只有边际的时间增加（来自解耦头）。 ​ 表 3: YOLOX 和 YOLOv5 在 COCO 上的 AP（%) 方面的比较。所有的模型都是在 640×640 的分辨率下测试的，在 Tesla V100 上用 FP16 - 精度和 batch=1。 Tiny 和 Nano 检测器 我们将我们的模型进一步缩小为 YOLOX-Tiny，以便与 YOLOv4-Tiny [30] 进行比较。对于移动设备，我们采用深度卷积来构建 YOLOX-Nano 模型，它只有 0.91M 的参数和 1.08G 的 FLOPs。如表 4 所示。YOLOX 在比同类模型更小的情况下表现良好。 ​ 表 4: YOLOX-Tiny 和 YOLOX-Nano 与同类产品在 COCO 阀上的 AP（%) 的比较。所有的模型都是在 416×416 的分辨率下测试的。 模型大小和数据增强 在我们的实验中，所有的模型都保持着几乎相同的学习进度和优化参数，如 2.1 中所描述的。然而，我们发现，合适的增强策略在不同规模的模型中是不同的。正如 Tab. 5 显示，虽然对 YOLOX-L 应用 MixUp 可以使 AP 提高 0.9%，但对于像 YOLOX-Nano 这样的小模型来说，削弱增强效果更好。具体来说，在训练小模型，即 YOLOX-S、YOLOX-Tiny 和 YOLOX-Nano 时，我们取消了 Mix up 增强，并削弱了马赛克（将比例范围从 [0.1, 2.0] 减少到 [0.5, 1.5]）。这样的修改将 YOLOX-Nano 的 AP 从 24.0% 提高到 25.3%。 ​ 表 5: 不同模型尺寸下的数据增量效果。Scale Jit. 代表了马赛克图像的比例抖动范围。 ​ 采用 Copypaste 时，使用 COCO trainval 的实例 mask 标注。 对于大型模型，我们还发现，更强的增强功能更有帮助。事实上，我们的 MixUp 实现比 [38] 中的原始版本要重一部分。受 Copypaste [6] 的启发，我们在混合它们之前用一个随机采样的比例因子抖动了两个图像。为了了解 Mixup 在比例抖动方面的能力，我们在 YOLOX-L 上将其与 Copypaste 进行比较。注意到 Copypaste 需要额外的实例 mask 标注，而 MixUp 不需要。但是，如表 5 所示，这两种方法在性能上具有竞争力。所示，这两种方法取得了有竞争力的性能，表明在没有实例 mask 标注的情况下，规模抖动的 MixUp 是 Copypaste 的合格替代品。 三、与 SOTA 的比较 有一个传统是显示 SOTA 的比较表，如表 6。然而，请记住，该表中模型的推理速度往往是不可控的，因为速度随软件和硬件而变化。因此，我们在图 1 中对所有的 YOLO 系列使用相同的硬件和代码基础，绘制出有所控制的速度 / 准确度曲线。 ​ 表 6: 在 COCO 2017 test-dev 上比较不同目标检测的速度和准确性。我们选择所有在 300 个 epoch 上训练的模型进行公平比较。 我们注意到，有一些高性能的 YOLO 系列具有更大的模型尺寸，如 Scale-YOLOv4 [30] 和 YOLOv5-P6 [7]。而目前基于 transformer 的探测器 [21] 将精度 SOTA 推至∼60AP。由于时间和资源的限制，我们没有在本报告中探讨这些重要的特征。然而，它们已经在我们的范围内了。 四、流媒体感知挑战赛 WAD 2021 上的流式感知挑战赛是通过最近提出的一个指标：流式准确度 [15] 对准确度和延迟进行联合评估。这个指标背后的关键见解是在每个时间瞬间联合评估整个感知堆栈的输出，迫使堆栈考虑在计算发生时应该忽略的流数据量 [15]。我们发现，在 30FPS 数据流上，该指标的最佳权衡点是推理时间≤33ms 的强大模型。所以我们采用了 YOLOX-L 模型和 TensorRT 来制作我们的最终模型，以赢得挑战赛的第一名。更多细节请参考挑战赛网站 5。 五、总结 在这份报告中，我们介绍了 YOLO 系列的一些有经验的更新，这形成了一个高性能的无锚检测器，称为 YOLOX。YOLOX 配备了一些最新的先进检测技术，即解耦头、无锚和先进的标签分配策略，在所有模型大小上，YOLOX 在速度和精度之间实现了比其他同行更好的权衡。值得注意的是，我们将 YOLOv3 的架构提升到了 47.3% 的 AP，超过了目前最佳实践的 3.0% AP，而 YOLOv3 由于其广泛的兼容性，仍然是业界最广泛使用的检测器之一。我们希望这份报告能够帮助开发者和研究人员在实际场景中获得更好的体验。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"模型优化","slug":"模型优化","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"}]},{"title":"智能家居系统方案","slug":"智能家居系统方案","date":"2022-03-10T13:36:30.000Z","updated":"2022-03-10T13:40:06.773Z","comments":true,"path":"2022/03/10/智能家居系统方案/","link":"","permalink":"https://leezhao415.github.io/2022/03/10/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85%E7%B3%BB%E7%BB%9F%E6%96%B9%E6%A1%88/","excerpt":"","text":"文章目录 智能家居系统方案 智能家居系统方案 智能家居体验馆七大体验： 玄关 ：智能门锁、入侵报警、一键场景、照明联动、背景音。 厨房 ：环境监测、用水安全、一键场景、用电安全、用气安全、新风空调。 餐厅 ：智能调光、背景音乐、暖通空调、一键场景。 客厅 ：环境监测、智能灯光、一键场景、家庭影院、暖通空调、智能电器、安防报警。 书房 ：环境监测、智能灯光、一键场景、背景音乐、暖通空调。 卧室 ：环境监测、智能灯光、一键场景、背景音乐、暖通空调、智能电器、睡眠监测。 卫生间 ：暖通空调、智能电器、一键场景、背景音乐、人体感应。 可以根据个人需求尽情体验智能家居带来的舒适生活！","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"智能家居","slug":"智能家居","permalink":"https://leezhao415.github.io/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"}]},{"title":"【精华】深度学习中从基础综述、论文笔记到工程经验、训练技巧","slug":"【精华】深度学习中从基础综述、论文笔记到工程经验、训练技巧","date":"2022-03-10T13:36:09.000Z","updated":"2022-03-10T13:39:09.569Z","comments":true,"path":"2022/03/10/【精华】深度学习中从基础综述、论文笔记到工程经验、训练技巧/","link":"","permalink":"https://leezhao415.github.io/2022/03/10/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E4%BB%8E%E5%9F%BA%E7%A1%80%E7%BB%BC%E8%BF%B0%E3%80%81%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%88%B0%E5%B7%A5%E7%A8%8B%E7%BB%8F%E9%AA%8C%E3%80%81%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/","excerpt":"","text":"文章目录 经验 | 深度学习中从基础综述、论文笔记到工程经验、训练技巧 前言 一、基础 二、综述 三、Anchor Free 四、神经架构搜索 NAS 五、从头训练 六、展望未来 七、研究心得 八、工程经验 九、比赛技巧 十、论文笔记 经验 | 深度学习中从基础综述、论文笔记到工程经验、训练技巧 前言 依稀记得两年前刚开始接触深度学习和目标检测的时候，惊喜地发现知乎上有很多优质的目标检测相关文章，其中不少甚至是论文作者自己写的，极具食用价值。通过阅读这些文章，不仅加深了我对现有目标检测领域的理解，还让我深深体会到了同行们对科研的热爱，一句话概括：受益匪浅。因此我打算将自认为不错的知乎文章分享出来，希望能让对目标检测感兴趣的同学们更快地了解目标检测，认识在这个领域辛勤耕耘的大神们。开放的社区也一定会更加促进目标检测领域的研究！ 需要说明的是本文并没有包揽所有目标检测文章，只有那些我读过并且觉得受益匪浅的文章。 （以下排名不分先后，单纯按照我的收藏夹顺序…） 一、基础 【1】作为目标检测领域最经典的模型，Faster RCNN 的大名无人不知无人不晓。而全网把 Faster RCNN 讲的最透彻的，非此文莫属。仅凭一篇文章就让无数同学记住了白裳这个名字。强烈安利，新人必看。 白裳：一文读懂 Faster RCNN https://zhuanlan.zhihu.com/p/31426458 【2】作者为旷视科技南京研究院研究员，本文针对 mmAP 这一经典的目标检测评价指标详细解析其定义初衷和具体计算方式；下半篇深入分析 mmAP 的特点，并介绍针对这些特点现有方法如何 “hack” mmAP，最后将提出几个 mmAP 未考虑到的评测要素。 zzzz1：浅析经典目标检测评价指标–mmAP（一） https://zhuanlan.zhihu.com/p/55575423 zzzz1：浅析经典目标检测评价指标–mmAP（二） https://zhuanlan.zhihu.com/p/56899189 【3】作者为清华 CS 博士，本文从 Normalization 的背景讲起，用一个公式概括 Normalization 的基本思想与通用框架，将各大主流方法一一对号入座进行深入的对比分析，并从参数和数据的伸缩不变性的角度探讨 Normalization 有效的深层原因。 Juliuszh：详解深度学习中的 Normalization，BN/LN/WN https://zhuanlan.zhihu.com/p/33173246 【4】作者为百度 CV 工程师，本文详细阐述了目前分类损失函数为何多用交叉熵，而不是 KL 散度。 KevinCK：交叉熵、相对熵（KL 散度）、JS 散度和 Wasserstein 距离（推土机距离） https://zhuanlan.zhihu.com/p/74075915 【5】本文节选自王峰博士的毕业论文。Softmax 交叉熵损失函数应该是目前最常用的分类损失函数了，本文从最优化的角度来推导出 Softmax 交叉熵损失函数，极具启发性。 王峰：从最优化的角度看待 Softmax 损失函数 https://zhuanlan.zhihu.com/p/45014864 【6】本文指出了学术论文与工业界需求偏差，分析了为什么有些模型 FLOPs 很低，但其推理速度却很慢。 Phoenix Li：FLOPs 与模型推理速度 https://zhuanlan.zhihu.com/p/122943688 【7】空洞卷积 (Dilated/Atrous Convolution)，广泛应用于语义分割与目标检测等任务中。本文介绍了空洞卷积的作用，感受野的计算，以及其 gridding 问题。 点点点：总结 - 空洞卷积 (Dilated/Atrous Convolution) https://zhuanlan.zhihu.com/p/50369448 【8】本文介绍了两大类处理目标多尺度的方法：图像金字塔和特征金字塔。 点点点：总结 - CNN 中的目标多尺度处理 zhuanlan.zhihu.com https://zhuanlan.zhihu.com/p/70523190 【9】本文指出对于 CNN 而言，深度之外，感受野以及该感受野上的通道数，真正决定了网络的性能。CNN 是一种利用卷积实现二维泛函空间到二维泛函空间映射的神经网络。 akkaze - 郑安坤：CNN 真的需要下采样（上采样）吗？(原创) https://zhuanlan.zhihu.com/p/94477174 【10】教你如何计算神经网络的复杂度 Michael Yuan：卷积神经网络的复杂度分析 https://zhuanlan.zhihu.com/p/31575074 【11】本文提炼了一个核心观点：目标检测中存在多种多样的不平衡，这些不平衡会影响最终的检测精度，而现有的许多研究可以归结为解决这些不平衡方法。将不平衡分为四类：类别不平衡，尺度不平衡；空间不平衡，多任务损失优化之间的不平衡。 ChenJoya：Imbalance Problems in Object Detection: A Review https://zhuanlan.zhihu.com/p/82371629 【12】能否对卷积神经网络工作原理做一个直观的解释？ https://www.zhihu.com/question/39022858/answer/224446917 【13】stone：令人拍案称奇的 Mask RCNN https://zhuanlan.zhihu.com/p/37998710 【14】视频中的目标检测与图像中的目标检测具体有什么区别？ https://www.zhihu.com/question/52185576/answer/155679253 【15】中国移不动：5 分钟理解 Focal Loss 与 GHM—— 解决样本不平衡利器 https://zhuanlan.zhihu.com/p/80594704 【16】燕小花：目标检测小 tricks–样本不均衡处理 https://zhuanlan.zhihu.com/p/60612064 【17】YaqiLYU：目标检测中的 Anchor https://zhuanlan.zhihu.com/p/55824651 【18】小小将：目标检测 | SSD 原理与实现 https://zhuanlan.zhihu.com/p/33544892 【19】小小将：目标检测 | YOLO 原理与实现 https://zhuanlan.zhihu.com/p/32525231 二、综述 【1】当时带领我入坑的目标检测综述，记录了 2017.12.31 之前的所有目标检测算法。让当时的我了解到原来目前目标检测领域的深度学习方法主要分为两类：two stage 和 one stage。 Ronald：综述：深度学习时代的目标检测算法 zhuanlan.zhihu.com https://zhuanlan.zhihu.com/p/33277354 【2】俞刚博士在目标检测领域也是很出名的大佬，之前在旷视科技待了 5 年，任 Detection 组组长，19 年年底跳槽到腾讯 PCG 光影研究室。下面这篇文章是他在旷视的时候做的分享，讲述了什么是目标检测，其技术现状是什么；目前目标检测领域的五个挑战点及旷视分别给出的技术解决方案；最后展望了目标检测的未来，the devil is in the detail。 旷视科技：R Talk | 旷视科技目标检测概述：Beyond RetinaNet and Mask R-CNN https://zhuanlan.zhihu.com/p/38154492 【3】一篇关于旋转目标检测论文的综述。在目前已知开源的数据集中，只有遥感和文字类数据集才涉及到旋转框的目标检测。（还有少量未开源的 logo / 商品数据集） qianlinjun：旋转目标 (遥感 / 文字) 检测方法整理（2017-19 年） https://zhuanlan.zhihu.com/p/98703562 【4】文本作者为国科大 CS 博士，目前就职于微软亚研院。知乎专栏致力于计算机视觉迁移学习相关研究。 王晋东不在家：《迁移学习简明手册》发布啦！ https://zhuanlan.zhihu.com/p/35352154 【5】本文作者为旷视研究员。人脸检测是人脸识别的第一站，旷视的努力主要体现在紧紧围绕人脸检测领域顽固而核心的问题展开，攻坚克难，功夫花在刀刃上，比如人脸尺度的变动及遮挡等，实现速度与精度的双重涨点。 王剑锋：人脸检测江湖的那些事儿 —— 从旷视说起 https://zhuanlan.zhihu.com/p/56619497 三、Anchor Free 【1】这篇文章主要讲一下有代表性的 Anchor-Free 模型 (包括 DenseBox、YOLO、CornerNet、ExtremeNet、FSAF、FCOS、FoveaBox)，分成 3 个部分来介绍 (早期探索、基于关键点、密集预测)。最后作者认为设计合适的 gt，是提升目标检测速度和精度的关键。 陀飞轮：目标检测：Anchor-Free 时代 https://zhuanlan.zhihu.com/p/62103812 【2】陈恺博士是大名鼎鼎的 mmdetection 的作者，他毕业于港中文，目前就职于商汤科技。本文揭示了几篇 anchor free 论文设计的异曲同工之妙。历史的车轮滚滚向前， 目标检测研究似乎也在不断轮回。 陈恺：物体检测的轮回: anchor-based 与 anchor-free https://zhuanlan.zhihu.com/p/62372897 【3】本文作者王乃岩博士为图森首席科学家。 Naiyan Wang：聊聊 Anchor 的 &quot;前世今生&quot;（上） https://zhuanlan.zhihu.com/p/63273342 Naiyan Wang：聊聊 Anchor 的 &quot;前世今生&quot;（下） https://zhuanlan.zhihu.com/p/68291859 四、神经架构搜索 NAS CLAY：神经网络架构搜索 (NAS) 中的 milestones https://zhuanlan.zhihu.com/p/94252445 如何评价 Google Brain 团队最新检测论文 SpineNet？ https://www.zhihu.com/question/360562458/answer/959198888 五、从头训练 【1】朱睿是 ScratchDet 的作者，目前在港中文读研，他开门见山地指出关于从 0 训练检测器的个人观点：1）需要能够稳定梯度的优化手段（比如 clip_gradient、BN、GN、SN、等等）；2）训练足够多的 epoch 与合适的学习率；3）对于小数据集，在训练时需要一定的数据增广。 如何评价何恺明等 arxiv 新作 Rethinking ImageNet Pre-training？ https://www.zhihu.com/question/303234604/answer/537395863 【2】沈志强是 DSOD 的作者，他在这篇文章中指出：起初从头训练之所以没有使用预训练模型效果好，更本质的一点是 batch size 太小（通常每张卡上 1 或 2）导致 BN 收敛不好。而最近出现的 GN，SN 缓解了这个问题。 沈志强：Object Detection from Scratch - A Brief Review https://zhuanlan.zhihu.com/p/137410354 六、展望未来 【1】作者为北大数院助理教授，本文提出了未来深度学习领域值得 follow 的几个大方向：1）深入反思和理解深度学习的行为；2）新模型的构建；3）新的数据场景或数据结构中的深度学习模型；4）Learning-enhanced algorithms；5）对应用友好的模型和算法；6）和硬件结合的方向。 时至今日，深度学习领域有哪些值得追踪的前沿研究？ https://www.zhihu.com/question/385326992/answer/1164005349 【2】作者为旷视科技实习生，本文认为：2017 年是顶峰，2018 年呈现饱和趋势，2019 年基本宣告了这个时代的落幕。如果真的选择做纯目标检测，必须去啃硬骨头了。 想知道目标检测领域中还有哪些方向能做？ https://www.zhihu.com/question/351254577/answer/861582672 七、研究心得 zibuyu9：好的研究想法从哪里来 https://zhuanlan.zhihu.com/p/93765082 陈天奇：机器学习科研的十年 https://zhuanlan.zhihu.com/p/74249758 Naiyan Wang：闲聊几句科研三观 https://zhuanlan.zhihu.com/p/37042197 田渊栋：一些感悟 https://zhuanlan.zhihu.com/p/26178137 八、工程经验 ps. 因为我自己主要用 pytorch，主要都是关于 pytorch 的～ 巽二：简单两步加速 PyTorch 里的 Dataloader https://zhuanlan.zhihu.com/p/68191407 张皓：PyTorch Cookbook（常用代码段整理合集） https://zhuanlan.zhihu.com/p/59205847 Pytorch 有什么节省显存的小技巧？ https://www.zhihu.com/question/274635237/answer/755102181 lbin：pytorch + apex 生活变得更美好 https://zhuanlan.zhihu.com/p/57958993 Jack Stark：[深度学习框架] PyTorch 常用代码段 https://zhuanlan.zhihu.com/p/104019160 商汤科技 SenseTime：模型量化了解一下？ https://zhuanlan.zhihu.com/p/132561405 如何评价商汤开源的 mm-detection 检测库？ https://www.zhihu.com/question/294578141/answer/509367634 九、比赛技巧 Caleb Ge：Crowdhuman 人体检测比赛第一名经验总结 https://zhuanlan.zhihu.com/p/68677880 AI 科技大本营：Hinton 等人最新研究：大幅提升模型准确率，标签平滑技术到底怎么用？ https://zhuanlan.zhihu.com/p/72685158 商汤科技 SenseTime：CVPR 2020 丨商汤 TSD 目标检测算法获得 Open Images 冠军 https://zhuanlan.zhihu.com/p/131576433 十、论文笔记 2020 Amusi：大神接棒，YOLOv4 来了！ https://zhuanlan.zhihu.com/p/135909702 Amusi：ResNeSt 实现有误？ https://zhuanlan.zhihu.com/p/135220104 Hassassin：一行代码提升迁移性能 | CVPR2020 Oral https://zhuanlan.zhihu.com/p/121507249 mileistone：也谈阿里达摩院的频域学习论文 https://zhuanlan.zhihu.com/p/115584408) 2019 陈恺：Guided Anchoring: 物体检测器也能自己学 Anchor https://zhuanlan.zhihu.com/p/55854246) pprp：打通多个视觉任务的全能 Backbone:HRNet https://zhuanlan.zhihu.com/p/134253318 陀飞轮：GCNet：当 Non-local 遇见 SENet https://zhuanlan.zhihu.com/p/64988633) OLDPAN：扔掉 anchor！真正的 CenterNet——Objects as Points 论文解读 https://zhuanlan.zhihu.com/p/66048276 TeddyZhang：目标检测：FCOS（2019） https://zhuanlan.zhihu.com/p/62869137 如何评价 zhangshifeng 最新的讨论 anchor based/free 的论文？ https://www.zhihu.com/question/359595879/answer/927861326 如何看待 CVPR2019 论文 Libra R-CNN（一个全面平衡的目标检测器）？ https://www.zhihu.com/question/319458937/answer/647082241 Naiyan Wang：TridentNet：处理目标检测中尺度变化新思路 https://zhuanlan.zhihu.com/p/54334986) 2018 Naiyan Wang：CVPR18 Detection 文章选介（上） https://zhuanlan.zhihu.com/p/35882192 Naiyan Wang：CVPR18 Detection 文章选介（下） https://zhuanlan.zhihu.com/p/36431183 商汤科技 SenseTime：商汤科技 44 篇论文入选 CVPR 2018 https://zhuanlan.zhihu.com/p/36688720)","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"深度学习","slug":"深度学习","permalink":"https://leezhao415.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"【精华】VSLAM技术综述","slug":"【精华】VSLAM技术综述","date":"2022-03-10T13:35:51.000Z","updated":"2022-03-10T13:38:13.379Z","comments":true,"path":"2022/03/10/【精华】VSLAM技术综述/","link":"","permalink":"https://leezhao415.github.io/2022/03/10/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91VSLAM%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/","excerpt":"","text":"文章目录 1 什么是视觉 SLAM 2 VSLAM 算法分类 （1）间接法及其典型系统 （2）直接法及其典型系统 3 总结 随着 ORB-SLAM 的崛起，视觉 SLAM 技术掀起了一波热潮，今天小白带着小伙伴一起学习视觉 SLAM 经典的解决方案。 1 什么是视觉 SLAM SLAM 是 “Simultaneous Localization And Mapping” 的缩写，可译为同步定位与建图。概率 SLAM 问题 (the probabilistic SLAM problem) 起源于 1986 年的 IEEE Robotics and Automation Conference 大会上，研究人员希望能将估计理论方法 (estimation-theoretic methods) 应用在构图和定位问题中。 SLAM 最早被应用在机器人领域，其目标是在没有任何先验知识的情况下，根据传感器数据实时构建周围环境地图，同时根据这个地图推测自身的定位。 假设机器人携带传感器 (相机) 在未知环境中运动，为方便起见，把一段连续时间的运动变成离散时刻 t=1，…k，而在这些时刻，用 x 表示机器人的自身位置，则各时刻的位置就记为 x1，x2…xk，它构成了机器人的轨迹。地图方面，假设地图由许多个路标组成，而每个时刻，传感器会测量到一部分路标点，得到它们的观测数据。设路标点共有 N 个，用 y1，y2…yn 表示。通过运动测量 u 和传感器读数 z 来求解定位问题 (估计 x) 和建图问题 (估计 y)。 只利用相机作为外部感知传感器的 SLAM 称为视觉 SLAM (vSLAM)。相机具有视觉信息丰富、硬件成本低等优点，经典的 vSLAM 系统一般包含前端视觉里程计、 后端优化、 闭环检测和构图四个主要部分。 ● 视觉里程计 (Visual Odometry)：仅有视觉输入的姿态估计； ● 后端优化 (Optimization): 后端接受不同时刻视觉里程计测量的相机位姿，以及闭环检测的信息，对它们进行优化，得到全局一致的轨迹和地图 [5]； ● 闭环检测 (Loop Closing): 指机器人在地图构建过程中，通过视觉等传感器信息检测是否发生了轨迹闭环，即判断自身是否进入历史同一地点 [6]; ● 建图 (Mapping): 根据估计的轨迹，建立与任务要求对应的地图 [5]。 2 VSLAM 算法分类 根据生成方法的不同，SLAM 可以分成两大类：间接方法和直接方法 （1）间接法及其典型系统 间接法首先对测量数据进行预处理来产生中间层，通过稀疏的特征点提取和匹配来实现的，也可以采用稠密规则的光流，或者提取直线或曲线特征来实现。然后计算出地图点坐标或光流向量等几何量，因此间接法优化的是几何误差： 其中 ui 为 Ik-1 中任意像素点，它投影到空间点的坐标为 Pi， u′i 是 Pi 投影到 Ik 上的坐标。之后利用中间层的数值来估计周围环境的三维模型和相机运动。 1&gt; MonoSLAM MonoSLAM 是第一个实时的单目视觉 SLAM 系统 [7]。 MonoSLAM 以 EKF (扩展卡尔曼滤波) 为后端，追踪前端稀疏的特征点，以相机的当前状态和所有路标点为状态量，更新其均值和协方差。在 EKF 中，每个特征点的位置服从高斯分布，可以用一个椭球表示它的均值和不确定性，它们在某个方向上越长，说明在该方向上越不稳定。 该方法的缺点：场景窄、路标数有限、稀疏特征点易丢失等。 2&gt; PTAM PTAM [8] 提出并实现了跟踪和建图的并行化，首次区分出前后端 (跟踪需要实时响应图像数据，地图优化放在后端进行)，后续许多视觉 SLAM 系统设计也采取了类似的方法。PTAM 是第一个使用非线性优化作为后端的方案，而不是滤波器的后端方案。提出了关键帧 (keyframes) 机制，即不用精细处理每一幅图像，而是把几个关键图像串起来优化其轨迹和地图。该方法的缺点是：场景小、跟踪容易丢失。 3&gt; ORB-SLAM (ORB_SLAM2) ORB-SLAM [9] 围绕 ORB 特征计算，包括视觉里程计与回环检测的 ORB 字典。ORB 特征计算效率比 SIFT 或 SURF 高，又具有良好的旋转和缩放不变性。ORB-SLAM 创新地使用了三个线程完成 SLAM，三个线程是：实时跟踪特征点的 Tracking 线程，局部 Bundle Adjustment 的优化线程和全局 Pose Graph 的回环检测与优化线程。该方法的缺点：每幅图像都计算一遍 ORB 特征非常耗时，三线程结构给 CPU 带来了较重负担。稀疏特征点地图只能满足定位需求，无法提供导航、避障等功能。 ORB-SLAM2 [10] 基于单目的 ORB-SLAM 做了如下贡献：第一个用于单目、双目和 RGB-D 的开源 SLAM 系统，包括闭环，重定位和地图重用；RGB-D 结果显示，通过使用 bundle adjustment，比基于迭代最近点 (ICP) 或者光度和深度误差最小化的最先进方法获得更高的精度；通过使用近距离和远距离的立体点和单目观察结果，立体效果比最先进的直接立体 SLAM 更准确；轻量级的本地化模式，当建图不可用时，可以有效地重新使用地图。 （2）直接法及其典型系统 直接法跳过预处理步骤直接使用实际传感器测量值，例如在特定时间内从某个方向接收的光，如下图所示。在被动视觉的情况下，由于相机提供光度测量，因此直接法优化的是光度误差： 1&gt; DTAM DTAM [11] 是单目 VSLAM 系统，是一种直接稠密的方法，通过最小化全局空间规范能量函数来计算关键帧构建稠密深度图，而相机的位姿则使用深度地图通过直接图像匹配来计算得到。对特征缺失、 图像模糊有很好的鲁棒性。该方法的缺点是：计算量非常大，需要 GPU 并行计算。 DTAM 假设光度恒定，对全局照明处理不够鲁棒。 2&gt; LSD-SLAM LSD-SLAM [12] 建了一个大尺度直接单目 SLAM 的框架，提出了一种用来直接估计关键帧之间相似变换、尺度感知的图像匹配算法，在 CPU 上实现了半稠密场景的重建。该方法的缺点：对相机内参敏感和曝光敏感，相机快速运动时容易丢失，依然需要特征点进行回环检测。 3&gt; SVO SVO 是一种半直接法的视觉里程计，它是特征点和直接法的混合使用：跟踪了一些角点，然后像直接法那样，根据关键点周围信息估计相机运动及位置。由于不需要计算大量描述子，因此速度极快，在消费级笔记本电脑上可以达到每秒 300 帧，在无人机上可以达到每秒 55 帧。该方法的缺点是：舍弃了后端优化和回环检测，位姿估计存在累积误差，丢失后重定位困难。 4&gt; DSO DSO 是基于高度精确的稀疏直接结构和运动公式的视觉里程计的方法。不考虑几何先验信息，能够直接优化光度误差。并且考虑了光度标定模型，其优化范围不是所有帧，而是由最近帧及其前几帧形成的滑动窗口，并且保持这个窗口有 7 个关键帧。DSO 中除了完善直接法位姿估计的误差模型外，还加入了仿射亮度变换、 光度标定、 深度优化等。该方法没有回环检测。 5&gt; 基于深度学习的 SLAM 传统的视觉 SLAM 在环境的适应性方面依然存在瓶颈，深度学习有望在这方面发挥较大的作用。目前，深度学习已经在语义地图、重定位、回环检测、特征点提取与匹配以及端到端的视觉里程计等问题上有了相关工作，下面列举一些典型成果： ●CNN-SLAM [17] 在 LSD-SLAM [12] 基础上将深度估计以及图像匹配改为基于卷积神经网络的方法，并且可以融合语义信息，得到了较鲁棒的效果； ● 剑桥大学开发的 PoseNet [18]，是在 GoogleNet [19] 的基础上将 6 自由度位姿作为回归问题进行的网络改进，可以利用单张图片得到对应的相机位姿； ● 《视觉 SLAM 十四讲》[5] 一书的作者高翔，利用深度神经网络而不是常见的视觉特征来学习原始数据的特征，实现了基于深度网络的回环检测 [20]； ● LIFT [21] 利用深度神经网络学习图像中的特征点，相比于 SIFT [22] 匹配度更高，其流程图如下图所示： LIFT (Learned Invariant Feature Transform) 由三个部分组成：Detector，Orientation Estimator 和 Descriptor。每一个部分都基于 CNN 实现，作者用 Spatial Transformers 将它们联系起来，并用 soft argmax 函数替代了传统的非局部最大值抑制，保证了端到端的可微性。 孪生训练架构（Siamese Network）包含四个分支，其中 P1 和 P2 对应同一个点的不同视角，作为训练 Descriptor 的正样本，P3 代表不同的 3D 点，作为 Descriptor 的负样本；P4 不包含特征点，仅作为训练 Detector 的负样本。由大 P，Detector，softargmax 和 Spatial Transformer layer Crop 共同得到的小 p 反馈到 Orientation Estimator，Orientation Estimator 和 Spatial Transformer layer Rot 提供 pθ 给 Descriptor，得到最终的描述向量 d。作者给出了 LIFT 与 SIFT 特征匹配的效果对比。 特征匹配对比图中左列为 SIFT 匹配结果，右列为 LIFT。绿色线条代表正确匹配，红色圆圈代表描述子区域，可以看到，LIFT 得到了比 SIFT 更稠密的匹配效果。 UnDeepVO [23] 能够通过使用深度神经网络估计单目相机的 6 自由度位姿及其视野内的深度，整体系统框架概图见下图。 UnDeepVO 有两个显著的特点：一个是采用了无监督深度学习机制，另一个是能够恢复绝对尺度。UnDeepVO 在训练过程中使用双目图像恢复尺度，但是在测试过程中只使用连续的单目图像。该文的主要贡献包括以下几点： 1. 通过空间和时间几何约束，用无监督的方式恢复了单目视觉里程计的绝对尺度； 2. 利用训练过程中的双目图像对，不仅估计了姿态还估计了稠密的带有绝对尺度的深度图； 3. 在 KITTI 数据集上评价了该系统， UnDeepVO 对于单目相机有良好的位姿估计结果。 UnDeepVO 由位姿估计和深度估计构成，两个估计系统均把单目连续图像作为输入，分别以带有尺度的 6 自由度位姿和深度作为输出。对于位姿估计器，它是基于 VGG 的 CNN 架构。 它把两个连续的单目图像作为输入，并预测它们之间的 6 自由度变换。 由于旋转（由欧拉角表示）具有高非线性，因此与平移相比通常难以训练。 在有监督训练中，一种常用的方法是将旋转损失作为一种归一化方式给予更大的权重。 为了使用无监督学习更好地训练旋转，作者在最后一个卷积层之后用两组独立的全连接层解耦平移和旋转。 这使得作者能够引入一个权重来标准化旋转和平移的预测，以获得更好的性能。对于深度估计器，它基于 encoder-decoder 结构来生成稠密的深度图。 与其他深度估计方法不同的是，该方法从网络中产生视差图像（深度的倒数），UnDeepVO 的深度估计器可以直接预测深度图，以这种方式训练整个系统更容易收敛。系统结构图如下图所示。 3 总结 本文介绍了基于传统算法和深度学习的代表性 SLAM 方法。当前的 SLAM 算法在复杂的机器人运动和环境中很容易失效 (例如：机器人的快速运动， 高度动态性的环境)，通常不能面对严格的性能要求，例如，用于快速闭环控制的高速率估计。大多数的 SLAM 没有自由主动地收集数据，行动方案不够高效，并且，目前 vSLAM 方案中所采用的图像特征的语义级别太低，造成特征的可区别性太弱。因此，今后的视觉 SLAM 将向着主动 SLAM、语义 SLAM 以及与其它传感器（例如 IMU）融合的方向发展。 参考文献 [1] Durrant-Whyte, H, and Bailey, Tim. ”Simultaneous Localization and Mapping: Part I.” IEEE Robotics &amp; Amp Amp Automation Magazine 13.2(2006):99 - 110. [2] Fuentes-Pacheco, Jorge, J. Ruiz-Ascencio, and J. M. Rendón-Mancha. ”Visual simultaneous localization and mapping: a survey.” Artifcial Intelligence Review 43.1(2015):55-81. [3] 陈常，朱华，由韶泽。基于视觉的同时定位与地图构建的研究进展 [J/OL]. 计算机应用研究，2018,(03):1-9 (2017-08-18). [4] Nister, D, O. Naroditsky, and J. Bergen. ”Visual odometry.” Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on IEEE, 2004:I-652-I-659 Vol.1. [5] 高翔。视觉 SLAM 十四讲 [M]. 北京：电子工业出版社，2017. [6] 赵洋等. ” 基于深度学习的视觉 SLAM 综述.” 机器人 39.6 (2017):889-896. [7] Davison, Andrew J., et al. ”MonoSLAM: Real-time single camera SLAM.” IEEE transactions on pattern analysis and machine intelligence 29.6 (2007): 1052-1067. [8] Klein, Georg, and David Murray. ”Parallel tracking and mapping for small AR workspaces.” Mixed and Augmented Reality, 2007. ISMAR 2007. 6th IEEE and ACM International Symposium on. IEEE, 2007. [9] Mur-Artal, Raúl, J. M. M. Montiel, and J. D. Tardós. ”ORB-SLAM: A Versatile and Accurate Monocular SLAM System.” IEEE Transactions on Robotics 31.5(2015):1147-1163. [10] Mur-Artal, Raul, and Juan D. Tardós. ”Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras.” IEEE Transactions on Robotics 33.5 (2017): 1255-1262. [11] Newcombe, Richard A, S. J. Lovegrove, and A. J. Davison. ”DTAM: Dense tracking and mapping in real-time.” International Conference on Computer Vision IEEE Computer Society, 2011:2320-2327. [12] Engel, Jakob, T. Schöps, and D. Cremers. ”LSD-SLAM: Large-Scale Direct Monocular SLAM.” 8690(2014):834-849. [13] Forster, Christian, M. Pizzoli, and D. Scaramuzza. ”SVO: Fast semi-direct monocular visual odometry.” IEEE International Conference on Robotics and Automation IEEE, 2014:15-22. [14] Engel, Jakob, V. Koltun, and D. Cremers. ”Direct Sparse Odometry.” IEEE Transactions on Pattern Analysis &amp; Machine Intelligence PP.99(2016):1-1. [15] Cadena, Cesar, et al. ”Past, Present, and Future of Simultaneous Localization and Mapping:Toward the Robust-Perception Age.” IEEE Transactions on Robotics 32.6(2016):1309-1332. [16] 吕霖华。基于视觉的即时定位与地图重建 (V-SLAM) 综述 [J]. 中国战略新兴产业，2017 (4). [17] Tateno K, Tombari F, Laina I, et al. CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017, 2. [18] Kendall A, Grimes M, Cipolla R. Posenet: A convolutional network for real-time 6-dof camera relocalization[C]//Proceedings of the IEEE international conference on computer vision. 2015: 2938-2946. [19] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9. [20] Gao X, Zhang T. Loop closure detection for visual slam systems using deep neural networks[C]//Control Conference (CCC), 2015 34th Chinese. IEEE, 2015: 5851-5856. [21] Yi K M, Trulls E, Lepetit V, et al. Lift: Learned invariant feature transform[C]//European Conference on Computer Vision. Springer, Cham, 2016: 467-483. [22] Lowe D G. Distinctive image features from scale-invariant keypoints[J]. International journal of computer vision, 2004, 60(2): 91-110. [23] Li R, Wang S, Long Z, et al. Undeepvo: Monocular visual odometry through unsupervised deep learning[C]//2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018: 7286-7291.","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"VSLAM","slug":"VSLAM","permalink":"https://leezhao415.github.io/tags/VSLAM/"}]},{"title":"【精华】VSLAM及3D视觉详解","slug":"【精华】VSLAM及3D视觉详解","date":"2022-03-10T13:35:34.000Z","updated":"2022-03-10T13:37:24.985Z","comments":true,"path":"2022/03/10/【精华】VSLAM及3D视觉详解/","link":"","permalink":"https://leezhao415.github.io/2022/03/10/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91VSLAM%E5%8F%8A3D%E8%A7%86%E8%A7%89%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"文章目录 一 VSLAM 课程总结 二 3D 视觉精华总结 三 3D 视觉从入门到精通 一 VSLAM 课程总结 国内首个面向自动驾驶领域的多传感器融合标定系统学习课程 视觉 - 惯性 SLAM 入门与实践教程（基于 VINS-Fusion） 彻底搞透视觉三维重建：原理剖析、代码讲解、及优化改进 彻底搞懂基于 LOAM 框架的 3D 激光 SLAM: 源码剖析到算法优化 三维点云从入门到精通系统视频学习课程 彻底剖析室内、室外激光 SLAM 关键算法原理、代码和实战 (cartographer+LOAM+LIO-SAM) 国内首个基于结构光投影三维重建系列视频课程 相机标定从入门到精通：基本原理与实战 机械臂位姿估计与抓取（基本原理 + 代码实战） 从零搭建一套结构光 3D 重建系统理论 + 源码 + 实践] 彻底剖析激光 - 视觉 - IMU-GPS 融合 SLAM 算法：理论推导、代码讲解和实战 彻底搞懂视觉 - 惯性 SLAM：基于 VINS-Fusion 视觉三维重建的关键技术及实现 ROS2 从入门到精通：理论与实战 四旋翼飞行器：算法与实战 二 3D 视觉精华总结 1、总结 | VSLAM、点云后处理、相机标定、深度学习等边角知识 2、三维重建 3、3D 视觉相关公司汇总 5、点云后处理 6、3D 视觉会议期刊 7、Elas 立体匹配算法 8、相机标定 demo 9、3D 视觉优质文章汇总 10、3D 视觉入门到精通精华汇总 11、计算机视觉系统学习必读书籍 三 3D 视觉从入门到精通 开源代码（涉及立体匹配、结构光、缺陷检测、点云、三维重建、位姿估计、深度不全、） vip 系列视频课程 + 课件（三维重建、自动驾驶、结构光、ORB-SLAM3、点云、机械臂抓取等系列视频课程） 3D 视觉优质 github 资源汇总 3D 视觉学习路线 3D 视觉顶会 | 期刊 往期帖子与问答汇总（涉及超过近 3000 + 干货帖子与精彩问答） 汇总 | 项目对接与招聘信息（包括 SLAM、点云、结构光、相机标定、三维重建等项目对接与求职信息） 公众号原创文章汇总 工具与课件（涉及编程与 GPU 优化及项目中的编译优化方法） 论文阅读（主要为星主、合伙人及各位嘉宾分享的行业优质论文）","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"VSLAM","slug":"VSLAM","permalink":"https://leezhao415.github.io/tags/VSLAM/"}]},{"title":"【精华】最强六大开源轻量级人脸检测项目分析","slug":"【精华】最强六大开源轻量级人脸检测项目分析","date":"2022-03-06T07:15:55.000Z","updated":"2022-03-06T07:57:42.489Z","comments":true,"path":"2022/03/06/【精华】最强六大开源轻量级人脸检测项目分析/","link":"","permalink":"https://leezhao415.github.io/2022/03/06/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%9C%80%E5%BC%BA%E5%85%AD%E5%A4%A7%E5%BC%80%E6%BA%90%E8%BD%BB%E9%87%8F%E7%BA%A7%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E9%A1%B9%E7%9B%AE%E5%88%86%E6%9E%90/","excerpt":"","text":"文章目录 【精华】最强六大开源轻量级人脸检测项目分析 1 libfacedetection 2 Ultra-Light-Fast-Generic-Face-Detector-1MB 3 A-Light-and-Fast-Face-Detector-for-Edge-Devices 4 CenterFace 5 DBFace 6 RetinaFace MobileNet0.25 【精华】最强六大开源轻量级人脸检测项目分析 随着深度学习的兴起，工业界和学术界越来越多的使用基于深度学习的方法，而不是传统的基于模板匹配，纹理提取或者像素积分图等方法。因为人脸检测本身并不属于特别复杂的任务，因此轻量级的深度学习模型即可满足该任务。本文汇总了六大开源的人脸检测项目。 虽说深度学习是个黑箱，但基于深度学习的通用目标检测算法（例 如 Faster-RCNN，SSD，YoloV3、 RetinaNet 等）的检测效果和鲁棒性，远远的超过基于纹理、边缘、Harr 特征、Sift 特征的传统计算机视觉方法，而且近几年随着模型压缩、量化技术的进步，模型运行速度也越来越快。 对于通用的目标检测算法，以大家常用的 SSD 和 YoloV3 算法为例，因为算法在设计之初，是为 Pascal VOC 或者 COCO 这种 20 类 和 80 类的多类别任务设计的，因此其 backbone 网络，也就是特征提取网络一般使用 VGG16、Darknet53、ResNet18 这种网络，这些网络的一个通用特点是，其卷积层的卷积核数目通常比较多（例如 256，512），导致模型参数量动辄几千万，运算量巨大。 如果我们拿这些通用目标检测算法来检测一类，比如只检测人脸、行人或者车辆时，使用那么多的卷积核数量、那么深的网络其实是不必要的。如果要检测 80 个类别，可能需要更多参数量来拟合，但是对于一两个类别，其实是有点杀鸡用宰牛刀了，如果你的任务不复杂，却用了 ResNet18/34 这种网络，你会发现很多卷积核的激活，其实是 0，导致白白多增加了很多计算量。所以，针对特定的人脸检测任务，其实一些非常轻量级的网络即可满足任务要求。 对于比赛刷榜，我们可以用很大的模型，例如某 AI 公司在 WiderFace 上夺冠的模型，结构用 RetinaNet，backbone 用 ResNet152，另外，FPN 结构也安排上，多模型融合安排上，更多的 anchor 组数安排上，这样的模型对于刷榜非常实用，但是在工业界非常不实用，假如要部署到 ARM 的嵌入式设备上，大概率直接卡死。 随着 AI 落地为王时代的到来，大家越来越注重精度和速度的权衡（trade off），本文精选了六大轻量级的开源人脸检测项目，并对其进行简单赏析和介绍。 言归正传，下面我们按照 Github 上 star 数目从高到低依次介绍。 1 libfacedetection Github star: 9.3k 作者：于仕琪 链接：https://github.com/ShiqiYu/libfacedetection 模型参数量：232 万，体积 3.34M 前几天元峰已经对该项目进行过一次介绍了，该项目使用一个 SSD 架构的人脸检测模型，在酷睿 i7 的 CPU 上，320x240 分辨率下可以达到 296.21 FPS，下图是该项目在不同分辨率和单线程下的速度概览。 模型结构也比较简单，就是一个轻量级的 SSD 架构，共四个定位层，而且借鉴了 RetinFace 的关键点方法，可以同时回归 5 个关键点。模型体积只有 232 万，体积仅有 3.34M 该项目的最大亮点，其实是于老师搞了一个纯 C++ 的推理版本，不依赖第三方深度学习库，非常有利于工程部署。 2 Ultra-Light-Fast-Generic-Face-Detector-1MB Github star: 4.7k 作者：Linzaer 链接：https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB 体积：1.04M， int8 量化后 300KB 该模型是针对边缘计算设备设计的人脸检测模型。并提供了精简网络和 RFB 网络两种，在 320x240 的输入分辨率下 90~109 FPS 左右。 该模型的结构与上述于老师的模型结构非常相似，也是 SSD 架构，共有 4 个定位层，模型结构如下图所示。 另外，该项目提供了 NCNN、MNN、Caffe、Onnx、Opencv 的推理代码，可以给我们部署项目提供大量参考样例。 3 A-Light-and-Fast-Face-Detector-for-Edge-Devices Github star: 897 作者：YonghaoHe 链接：https://github.com/YonghaoHe/A-Light-and-Fast-Face-Detector-for-Edge-Devices 体积：6.1 M 从名字可以看出来，这也是一个面向边缘设备的检测模型，该模型同样是 SSD 架构的，不过相比前述两个模型，该模型有八个定位层，分别对应 tiny、small、medium 和 large 四个尺度，backbone 网络共有 25 个卷积层。 该模型在 Nvidia TX2 下，320x240 分辨率下可以达到 50.92 FPS。 另外，该 repo 还提供了人头检测、行人检测、车辆检测的代码和模型。 4 CenterFace Github star: 607 作者：Star-Clouds 链接：https://github.com/Star-Clouds/CenterFace 体积：7.3 M、同精度小模型 2.3M CenterFace 是 anchor free 的模型结构，应该算是 CenterNet 针对人脸检测任务的特例，这一点上跟 RetinaFace 作为 RetinaNet 的在人脸任务的特例有异曲同工之妙，而且该模型同时回归了五个关键点。 该网络的 backbone 是 MobileNetV2，额外添加了 FPN 结构。 下图是 CenterFace 的速度，在 2080TI 上可以仅 4.4ms。Anchor Free 的模型，没有 NMS 的过程，也能节省很多后处理时间。 5 DBFace Github star: 195 作者：dlunion 链接：https://github.com/dlunion/DBFace 体积：7.03M DBFace 是一个 Anchor Free 的网络结构，模型原理可以说与 CenterFace 非常相似。这里不再展开详细的介绍了。 6 RetinaFace MobileNet0.25 Github star: 不好定义（6.4k） 作者：yangfly 链接：https://github.com/deepinsight/insightface/issues/669 体积：1.68M 这个项目来自于知名的 InsightFace 项目，该项目在 Github 有 6.4k star，InsightFace 提出了知名的 RetinaFace、ArcFace 算法，而且开源了详细的训练代码和预训练模型，可以说是非常良心的开源项目。 但是 RetinaFace（应该是受 RetianNet 的结构启发而成）本身的 backbone 是 ResNet50，yangfly 大佬将其替换为了 MobileNet0.25，模型大小仅 1.68MB。RetinaFace 的模型结构如下，这里的 backbone 网络是 MobileNet 0.25。 ​ 根据作者的开源结果，我们对以上 6 大开源轻量级人脸检测做一个速度和准确度的对比汇总。 名称 模型大小（MB） 速度（FPS） WiderFace Easy WiderFace Medium WiderFace Hard libfacedetection 3.34 296（i7CPU， 320x240） 0.773 0.718 0.485 UltraLightFace 1.04 109（ 320x240） 0.853 0.819 0.539 LightFast 5.81 131（Titan xp， 640x480） 0.910 0.881 0.780 CenterFace 7.3 227（2080TI，640x480） 0.931 0.924 0.870 DBFace 7.03 0.905 0.896 0.794 RetinaFace MobileNet0.25 1.68 0.887 0.87 0.791","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"目标检测（人脸检测）","slug":"目标检测（人脸检测）","permalink":"https://leezhao415.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%EF%BC%89/"}]},{"title":"【精华】主流的深度学习推理架构","slug":"【精华】主流的深度学习推理架构","date":"2022-03-06T07:15:30.000Z","updated":"2022-03-06T07:16:43.769Z","comments":true,"path":"2022/03/06/【精华】主流的深度学习推理架构/","link":"","permalink":"https://leezhao415.github.io/2022/03/06/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E4%B8%BB%E6%B5%81%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E7%90%86%E6%9E%B6%E6%9E%84/","excerpt":"","text":"文章目录 主流的深度学习推理架构 （1）NCNN （2）OpenVino （3）TensorRT （4）MediaPipe （5）ONNX （6）MNN （7）MACE （8）TNN （9）TVM 主流的深度学习推理架构 发布时间: 2021-10-20 12:29 以深度学习为主的人工智能算法模型在日常 AI 应用中逐渐占据主流方向，相关的各类产品也是层出不穷。我们平时所看到的 AI 产品，像刷脸支付、智能语音、银行的客服机器人等，都是 AI 算法的具体落地应用。AI 技术在具体落地应用方面，和其他软件技术一样，也需要具体的部署和实施的。既然要做部署，那就会有不同平台设备上的各种不同的部署方法和相关的部署架构工具，目前在人工智能的落地部署方面，各大平台机构也都是大展身手，纷纷推出自家的部署平台。 目前市场上应用最广泛的部署工具主要有以下几种： 腾讯公司开发的移动端平台部署工具 —— NCNN Intel 公司针对自家设备开开发的部署工具 —— OpenVino NVIDIA 公司针对自家 GPU 开发的部署工具 —— TensorRT Google 针对自家硬件设备和深度学习框架开发的部署工具 —— MediaPipe 由微软、亚马逊、Facebook 和 IBM 等公司共同开发的开放神经网络交换格式 —— ONNX (Open Neural Network Exchange) 阿里巴巴公司开发的移动端部署工具 ——MNN 小米公司开发的移动端平台部署工具 ——MACE 腾讯公司基于 Rapidnet、ncnn 开发的平台部署工具 ——TNN 华盛顿大学的 SAMPL 组开发的平台部署工具 ——TVM 除此之外，还有一些深度学习框架有自己的专用部署服务： TensorFlow 自己提供的部署服务： TensorFlow Serving 、 TensorFlow Lite Pytorch 自己提供的部署服务： libtorch 本文主要是针对这些不同的部署工具做一个简单的分析，对比一下各家不同的部署工具到底有哪些优势和不足之处，方便大家在做部署的时候能够找到适合自己的项目的部署方法。具体的各种不同的部署工具的下载安装和使用方法会在后续的文章中做出详细的教程，关注深度人工智能学院，了解最实用的人工智能干货知识。 （1）NCNN Github 地址 NCNN 是腾讯优图实验室首个开源项目，是一个为手机端极致优化的高性能神经网络前向计算框架。并在 2017 年 7 月正式开源。NCNN 做为腾讯优图最 “火” 的开源项目之一，是一个为手机端极致优化的高性能神经网络前向计算框架，在设计之初便将手机端的特殊场景融入核心理念，是业界首个为移动端优化的开源神经网络推断库。能实现无第三方依赖，跨平台操作，在手机端 CPU 运算速度在开源框架中处于领先水平。基于该平台，开发者能够轻松将深度学习算法移植到手机端，输出高效的执行，进而产出人工智能 APP，将 AI 技术带到用户指尖。 NCNN 从设计之初深刻考虑手机端的部署和使用。无第三方依赖，跨平台，手机端 CPU 的速度快于目前所有已知的开源框架。基于 NCNN，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP，将 AI 带到你的指尖。NCNN 目前已在腾讯多款应用中使用，如 QQ，Qzone，微信，天天 P 图等。 下面是 NCNN 在各大系统平台的应用发展状态情况： 从 NCNN 的发展矩阵可以看出，NCNN 覆盖了几乎所有常用的系统平台，尤其是在移动平台上的适用性更好，在 Linux、Windows 和 Android、以及 iOS、macOS 平台上都可以使用 GPU 来部署模型。 根据官方的功能描述，NCNN 在各方面的性能都比较优良： 支持卷积神经网络，支持多输入和多分支结构，可计算部分分支 无任何第三方库依赖，不依赖 BLAS/NNPACK 等计算框架 纯 C++ 实现，跨平台，支持 android ios 等 ARM NEON 汇编级良心优化，计算速度极快 精细的内存管理和数据结构设计，内存占用极低 支持多核并行计算加速，ARM big.LITTLE cpu 调度优化 支持基于全新低消耗的 vulkan api GPU 加速 整体库体积小于 700K，并可轻松精简到小于 300K 可扩展的模型设计，支持 8bit 量化 和半精度浮点存储，可导入 caffe/pytorch/mxnet/onnx/darknet/keras/tensorflow (mlir) 模型 支持直接内存零拷贝引用加载网络模型 可注册自定义层实现并扩展 除此之外，NCNN 在对各种硬件设备的支持上也非常给力： NCNN 的官方代码地址：https://github.com/Tencent/ncnn 移动端的部署工具除了 NCNN，还有华盛顿大学的 TVM、阿里的 MNN、小米的 MACE、腾讯优图基于 NCNN 开发的 TNN 等推理部署工具。 （2）OpenVino Github 地址 OpenVINO 工具套件全称是 Open Visual Inference &amp; Neural Network Optimization，是 Intel 于 2018 年发布的，开源、商用免费、主要应用于计算机视觉、实现神经网络模型优化和推理计算 (Inference) 加速的软件工具套件。由于其商用免费，且可以把深度学习模型部署在英尔特 CPU 和集成 GPU 上，大大节约了显卡费用，所以越来越多的深度学习应用都使用 OpenVINO 工具套件做深度学习模型部署。 OpenVINO 是一个 Pipeline 工具集，同时可以兼容各种开源框架训练好的模型，拥有算法模型上线部署的各种能力，只要掌握了该工具，你可以轻松的将预训练模型在 Intel 的 CPU 上快速部署起来。 对于 AI 工作负载来说，OpenVINO 提供了深度学习推理套件（DLDT)，该套件可以将各种开源框架训练好的模型进行线上部署，除此之外，还包含了图片处理工具包 OpenCV，视频处理工具包 Media SDK，用于处理图像视频解码，前处理和推理结果后处理等。 在做推理的时候，大多数情况需要前处理和后处理，前处理如通道变换，取均值，归一化，Resize 等，后处理是推理后，需要将检测框等特征叠加至原图等，都可以使用 OpenVINO 工具套件里的 API 接口完成。 OpenVino 目前支持 Linux、Windows、macOS、Raspbian 等系统平台。 OpenVINO 工具套件主要包括：Model Optimizer (模型优化器)—— 用于优化神经网络模型的工具，Inference Engine (推理引擎)—— 用于加速推理计算的软件包。 模型优化器是一个 python 脚本工具，用于将开源框架训练好的模型转化为推理引擎可以识别的中间表达，其实就是两个文件，xml 和 bin 文件，前者是网络结构的描述，后者是权重文件。模型优化器的作用包括压缩模型和加速，比如，去掉推理无用的操作 (Dropout)，层的融合 (Conv + BN + Relu)，以及内存优化。 推理引擎是一个支持 C\\C++ 和 python 的一套 API 接口，需要开发人员自己实现推理过程的开发，开发流程其实非常的简单，核心流程如下： 装载处理器的插件库 读取网络结构和权重 配置输入和输出参数 装载模型 创建推理请求 准备输入 Data 推理 结果处理 OpenVino 工具套件的工作流程图： OpenVino 的官方地址：https://docs.openvinotoolkit.org/latest/index.html 提醒一下，大家不要去下面这个网站，因为这个网站是一个酿酒厂的网站： https://openvino.org/ （3）TensorRT Github 地址 TensorRT 是 NVIDIA 开发的一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT 可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT 现已能支持 TensorFlow、Caffe、Mxnet、Pytorch 等几乎所有的深度学习框架，将 TensorRT 和 NVIDIA 的 GPU 结合起来，能在几乎所有的框架中进行快速和高效的部署推理。 TensorRT 是一个 C 库，从 TensorRT 3 开始提供 C API 和 Python API，主要用来针对 NVIDIA GPU 进行高性能推理（Inference）加速，它可为深度学习推理应用提供低延迟和高吞吐量。在推理期间，基于 TensorRT 的应用比仅 CPU 平台的执行速度快 40 倍。 一般的深度学习项目，训练时为了加快速度，会使用多 GPU 分布式训练。但在部署推理时，为了降低成本，往往使用单个 GPU 机器甚至嵌入式平台（比如 NVIDIA Jetson）进行部署，部署端也要有与训练时相同的深度学习环境，如 caffe，TensorFlow 等。由于训练的网络模型可能会很大（比如，inception，resnet 等），参数很多，而且部署端的机器性能存在差异，就会导致推理速度慢，延迟高。这对于那些高实时性的应用场合是致命的，比如自动驾驶要求实时目标检测，目标追踪等。所以为了提高部署推理的速度，出现了很多轻量级神经网络，比如 squeezenet，mobilenet，shufflenet 等。基本做法都是基于现有的经典模型提出一种新的模型结构，然后用这些改造过的模型重新训练，再重新部署。 而 TensorRT 则是对训练好的模型进行优化。TensorRT 就只是推理优化器。当你的网络训练完之后，可以将训练模型文件直接丢进 TensorRT 中，而不再需要依赖深度学习框架（Caffe，TensorFlow 等） 可以认为 TensorRT 是一个只有前向传播的深度学习推理框架，这个框架可以将 Caffe，TensorFlow，PyTorch 等网络模型解析，然后与 TensorRT 中对应的层进行一一映射，把其他框架的模型统一全部转换到 TensorRT 中，然后在 TensorRT 中可以针对 NVIDIA 自家 GPU 实施优化策略，并进行部署加速。 TensorRT 依赖于 Nvidia 的深度学习硬件环境，可以是 GPU 也可以是 DLA，如果没有的话则无法使用。TensorRT 支持目前大部分的神经网络 Layer 的定义，同时提供了 API 让开发者自己实现特殊 Layer 的操作。 TensorRT 基于 CUDA，NVIDIA 的并行编程模型，能够利用 CUDA-X AI 中的库、开发工具和技术，为人工智能、自动机器、高性能计算和图形优化所有深度学习框架的推理。 TensorRT 的部署分为两个部分： 优化训练好的模型并生成计算流图 使用 TensorRT Runtime 部署计算流图 TensorRT 的部署流程： TensorRT 的模型导入流程： TensorRT 的优化过程： 网络模型在导入至 TensorRT 后会进行一系列的优化，主要优化内容如下图所示 TensorRT 官网下载地址：https://developer.nvidia.com/zh-cn/tensorrt 开发者指南：https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html Github 地址：https://github.com/NVIDIA/TensorRT （4）MediaPipe Github 地址 MediaPipe 是一款由 Google Research 开发并开源的多媒体机器学习模型应用框架。在谷歌，一系列重要产品，如 YouTube、Google Lens、ARCore、Google Home 以及 Nest，都已深度整合了 MediaPipe。 MediaPipe 是一个基于图形的跨平台框架，用于构建多模式（视频，音频和传感器）应用的机器学习管道。MediaPipe 可在移动设备、工作站和服务器上跨平台运行，并支持移动 GPU 加速。使用 MediaPipe，可以将应用的机器学习管道构建为模块化组件的图形。MediaPipe 不仅可以被部署在服务器端，更可以在多个移动端 （安卓和苹果 iOS）和嵌入式平台（Google Coral 和树莓派）中作为设备端机器学习推理 （On-device Machine Learning Inference）框架。 一款多媒体机器学习应用的成败除了依赖于模型本身的好坏，还取决于设备资源的有效调配、多个输入流之间的高效同步、跨平台部署上的便捷程度、以及应用搭建的快速与否。 基于这些需求，谷歌开发并开源了 MediaPipe 项目。除了上述的特性，MediaPipe 还支持 TensorFlow 和 TF Lite 的推理引擎（Inference Engine），任何 TensorFlow 和 TF Lite 的模型都可以在 MediaPipe 上使用。同时，在移动端和嵌入式平台，MediaPipe 也支持设备本身的 GPU 加速。 MediaPipe 专为机器学习（ML）从业者而设计，包括研究人员，学生和软件开发人员，他们实施生产就绪的 ML 应用程序，发布伴随研究工作的代码，以及构建技术原型。MediaPipe 的主要用例是使用推理模型和其他可重用组件对应用机器学习管道进行快速原型设计。MediaPipe 还有助于将机器学习技术部署到各种不同硬件平台上的演示和应用程序中。 MediaPipe 的核心框架由 C++ 实现，并提供 Java 以及 Objective C 等语言的支持。MediaPipe 的主要概念包括数据包（Packet）、数据流（Stream）、计算单元（Calculator）、图（Graph）以及子图（Subgraph）。数据包是最基础的数据单位，一个数据包代表了在某一特定时间节点的数据，例如一帧图像或一小段音频信号；数据流是由按时间顺序升序排列的多个数据包组成，一个数据流的某一特定时间戳（Timestamp）只允许至多一个数据包的存在；而数据流则是在多个计算单元构成的图中流动。MediaPipe 的图是有向的 —— 数据包从数据源（Source Calculator 或者 Graph Input Stream）流入图直至在汇聚结点（Sink Calculator 或者 Graph Output Stream） 离开。 MediaPipe 在开源了多个由谷歌内部团队实现的计算单元（Calculator）的同时，也向用户提供定制新计算单元的接口。创建一个新的 Calculator，需要用户实现 Open ()，Process ()，Close () 去分别定义 Calculator 的初始化，针对数据流的处理方法，以及 Calculator 在完成所有运算后的关闭步骤。为了方便用户在多个图中复用已有的通用组件，例如图像数据的预处理、模型的推理以及图像的渲染等， MediaPipe 引入了子图（Subgraph）的概念。因此，一个 MediaPipe 图中的节点既可以是计算单元，亦可以是子图。子图在不同图内的复用，方便了大规模模块化的应用搭建。 MediaPipe 不支持除了 tensorflow 之外的其他深度学习框架，但是对各种系统平台和语言的支持非常友好： MediaPipe 的官方地址：https://google.github.io/mediapipe/ GitHub 地址：https://github.com/google/mediapipe （5）ONNX Github 地址 Open Neural Network Exchange（ONNX，开放神经网络交换）格式，是一个用于表示深度学习模型的标准，可使模型在不同框架之间进行转移。ONNX 是一种针对机器学习所设计的开放式的文件格式，用于存储训练好的模型。它使得不同的人工智能框架（如 Pytorch, MXNet）可以采用相同格式存储模型数据并交互。ONNX 的规范及代码主要由微软，亚马逊 ，Facebook 和 IBM 等公司共同开发，以开放源代码的方式托管在 Github 上。目前官方支持加载 ONNX 模型并进行推理的深度学习框架有：Caffe2, PyTorch, MXNet，ML.NET，TensorRT 和 Microsoft CNTK，并且 TensorFlow 也非官方的支持 ONNX。 比方说现在某组织因为主要开发用 TensorFlow 为基础的框架，现在有一个深度算法，需要将其部署在移动设备上，以观测变现。传统地我们需要用 caffe2 重新将模型写好，然后再训练参数；试想下这将是一个多么耗时耗力的过程。 此时，ONNX 便应运而生，Caffe2，PyTorch，Microsoft Cognitive Toolkit，Apache MXNet 等主流框架都对 ONNX 有着不同程度的支持。这就便于了我们的算法及模型在不同的框架之间的迁移。无论你使用何种训练框架训练模型（比如 TensorFlow/Pytorch/OneFlow/Paddle），在训练完毕后你都可以将这些框架的模型统一转换为 ONNX 这种统一的格式进行存储。 开放式神经网络交换（ONNX）是迈向开放式生态系统的第一步，它使 AI 开发人员能够随着项目的发展选择合适的工具。ONNX 为 AI 模型提供开源格式。它定义了可扩展的计算图模型，以及内置运算符和标准数据类型的定义。最初的 ONNX 专注于推理（评估）所需的功能。ONNX 解释计算图的可移植，它使用 graph 的序列化格式。它不一定是框架选择在内部使用和操作计算的形式。例如，如果在优化过程中操作更有效，则实现可以在存储器中以不同方式表示模型。 在获得 ONNX 模型之后，模型部署人员自然就可以将这个模型部署到兼容 ONNX 的运行环境中去。这里一般还会设计到额外的模型转换工作，典型的比如在 Android 端利用 NCNN 部署 ONNX 格式模型，那么就需要将 ONNX 利用 NCNN 的转换工具转换到 NCNN 所支持的 bin 和 param 格式。 ONNX 作为一个文件格式，我们自然需要一定的规则去读取我们想要的信息或者是写入我们需要保存信息。ONNX 使用的是 Protobuf 这个序列化数据结构去存储神经网络的权重信息。熟悉 Caffe 或者 Caffe2 的同学应该知道，它们的模型存储数据结构协议也是 Protobuf。 Protobuf 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API（摘自官方介绍）。 Protobuf 协议是一个以 *.proto 后缀文件为基础的，这个文件描述了用户自定义的数据结构。如果需要了解更多细节请参考 0x7 节的资料 3，这里只是想表达 ONNX 是基于 Protobuf 来做数据存储和传输，那么自然 onnx.proto 就是 ONNX 格式文件了。 ONNX 作为框架共用的一种模型交换格式，使用 protobuf 二进制格式来序列化模型，可以提供更好的传输性能我们可能会在某一任务中将 Pytorch 或者 TensorFlow 模型转化为 ONNX 模型 (ONNX 模型一般用于中间部署阶段)，然后再拿转化后的 ONNX 模型进而转化为我们使用不同框架部署需要的类型，ONNX 相当于一个翻译的作用。 ONNX 将每一个网络的每一层或者说是每一个算子当作节点 Node，再由这些 Node 去构建一个 Graph，相当于是一个网络。最后将 Graph 和这个 onnx 模型的其他信息结合在一起，生成一个 model，也就是最终的.onnx 的模型。 构建一个简单的 onnx 模型，实质上，只要构建好每一个 node，然后将它们和输入输出超参数一起塞到 graph，最后转成 model 就可以了。 在计算方面，虽然更高级的表达不同，但不同框架产生的最终结果都是非常接近。因此实时跟踪某一个神经网络是如何在这些框架上生成的，接着使用这些信息创建一个通用的计算图，即符合 ONNX 标准的计算图。 ONNX 为可扩展的计算图模型、内部运算器（Operator）以及标准数据类型提供了定义。在初始阶段，每个计算数据流图以节点列表的形式组织起来，构成一个非循环的图。节点有一个或多个的输入与输出。每个节点都是对一个运算器的调用。图还会包含协助记录其目的、作者等信息的元数据。运算器在图的外部实现，但那些内置的运算器可移植到不同的框架上，每个支持 ONNX 的框架将在匹配的数据类型上提供这些运算器的实现。 Microsoft 和合作伙伴社区创建了 ONNX 作为表示机器学习模型的开放标准。 许多框架（包括 TensorFlow、PyTorch、SciKit-Learn、Keras、Chainer、MXNet、MATLAB 和 SparkML）中的模型都可以导出或转换为标准 ONNX 格式。模型采用 ONNX 格式后，可在各种平台和设备上运行。 ONNX 运行时是一种用于将 ONNX 模型部署到生产环境的高性能推理引擎。它针对云和 Edge 进行了优化，适用于 Linux、Windows 和 Mac。它使用 C++ 编写，还包含 C、Python、C#、Java 和 Javascript (Node.js) API，可在各种环境中使用。ONNX 运行时同时支持 DNN 和传统 ML 模型，并与不同硬件上的加速器（例如，NVidia GPU 上的 TensorRT、Intel 处理器上的 OpenVINO、Windows 上的 DirectML 等）集成。通过使用 ONNX 运行时，可以从大量的生产级优化、测试和不断改进中受益。 ONNX 运行时用于大规模 Microsoft 服务，如必应、Office 和 Azure 认知服务。性能提升取决于许多因素，但这些 Microsoft 服务的 CPU 平均起来可实现 2 倍的性能提升。除了 Azure 机器学习服务外，ONNX 运行时还在支持机器学习工作负荷的其他产品中运行，包括： Windows: 该运行时作为 Windows 机器学习的一部分内置于 Windows 中，在数亿台设备上运行。 Azure SQL 产品系列：针对 Azure SQL Edge 和 Azure SQL 托管实例中的数据运行本机评分。 ML.NET：在 ML.NET 中运行 ONNX 模型。 ONNX 的官方网站：https://onnx.ai/ ONXX 的 GitHub 地址：https://github.com/onnx/onnx （6）MNN Github 地址 MNN 是一个高效、轻量的深度学习框架。由阿里巴巴开源，它支持深度模型推理与训练，尤其在端侧的推理与训练性能在业界处于领先地位。目前，MNN 已经在阿里巴巴的手机淘宝、手机天猫、优酷、钉钉、闲鱼等 20 多个 App 中使用，覆盖直播、短视频、搜索推荐、商品图像搜索、互动营销、权益发放、安全风控等 70 多个场景。此外，IoT 等场景下也有若干应用。 MNN 的架构设计理念与性能数据在 MLSys 2020 上面发表。Paper 在此处。如果 MNN 对你的研究有所助益，欢迎引用 MNN 的论文： （1）整体特点 1&gt; 轻量性 针对端侧设备特点深度定制和裁剪，无任何依赖，可以方便地部署到移动设备和各种嵌入式设备中。 iOS 平台：armv7+arm64 静态库大小 5MB 左右，链接生成可执行文件增加大小 620KB 左右，metallib 文件 600KB 左右。 Android 平台：so 大小 400KB 左右，OpenCL 库 400KB 左右，Vulkan 库 400KB 左右。 2&gt; 通用性 支持 Tensorflow 、 Caffe 、 ONNX 等主流模型文件格式，支持 CNN 、 RNN 、 GAN 等常用网络。 支持 86 个 Tensorflow Op、34 个 Caffe Op；各计算设备支持的 MNN Op 数：CPU 71 个，Metal 55 个，OpenCL 29 个，Vulkan 31 个。 支持 iOS 8.0+、Android 4.3 + 和具有 POSIX 接口的嵌入式设备。 支持异构设备混合计算，目前支持 CPU 和 GPU，可以动态导入 GPU Op 插件，替代 CPU Op 的实现。 3&gt; 高性能 不依赖任何第三方计算库，依靠大量手写汇编实现核心运算，充分发挥 ARM CPU 的算力。 iOS 设备上可以开启 GPU 加速（Metal），常用模型上快于苹果原生的 CoreML。 Android 上提供了 OpenCL 、 Vulkan 、 OpenGL 三套方案，尽可能多地满足设备需求，针对主流 GPU（ Adreno 和 Mali ）做了深度调优。 卷积、转置卷积算法高效稳定，对于任意形状的卷积均能高效运行，广泛运用了 Winograd 卷积算法，对 3x3 -&gt; 7x7 之类的对称卷积有高效的实现。 针对 ARM v8.2 的新架构额外作了优化，新设备可利用半精度计算的特性进一步提速。 4&gt; 易用性 有高效的图像处理模块，覆盖常见的形变、转换等需求，一般情况下，无需额外引入 libyuv 或 opencv 库处理图像。 支持回调机制，可以在网络运行中插入回调，提取数据或者控制运行走向。 支持只运行网络中的一部分，或者指定 CPU 和 GPU 间并行运行。 （2）架构设计 MNN 可以分为 Converter 和 Interpreter 两部分。 Converter 由 Frontends 和 Graph Optimize 构成。前者负责支持不同的训练框架，MNN 当前支持 Tensorflow (Lite)、Caffe 和 ONNX (PyTorch/MXNet 的模型可先转为 ONNX 模型再转到 MNN)；后者通过算子融合、算子替代、布局调整等方式优化图。 Interpreter 由 Engine 和 Backends 构成。前者负责模型的加载、计算图的调度；后者包含各计算设备下的内存分配、Op 实现。在 Engine 和 Backends 中，MNN 应用了多种优化方案，包括在卷积和反卷积中应用 Winograd 算法、在矩阵乘法中应用 Strassen 算法、低精度计算、Neon 优化、手写汇编、多线程优化、内存复用、异构计算等。 （7）MACE Github 地址 Mobile AI Compute Engine (MACE) 是小米开发的一个专为移动端异构计算平台 (支持 Android, iOS, Linux, Windows) 优化的神经网络计算框架。 主要从以下的角度做了专门的优化： 性能 代码经过 NEON 指令，OpenCL 以及 Hexagon HVX 专门优化，并且采用 Winograd 算法来进行卷积操作的加速。 此外，还对启动速度进行了专门的优化。 功耗 支持芯片的功耗管理，例如 ARM 的 big.LITTLE 调度，以及高通 Adreno GPU 功耗选项。 系统响应 支持自动拆解长时间的 OpenCL 计算任务，来保证 UI 渲染任务能够做到较好的抢占调度， 从而保证系统 UI 的相应和用户体验。 内存占用 通过运用内存依赖分析技术，以及内存复用，减少内存的占用。另外，保持尽量少的外部 依赖，保证代码尺寸精简。 模型加密与保护 模型保护是重要设计目标之一。支持将模型转换成 C++ 代码，以及关键常量字符混淆，增加逆向的难度。 硬件支持范围 支持高通，联发科，以及松果等系列芯片的 CPU，GPU 与 DSP (目前仅支持 Hexagon) 计算加速。CPU 模式支持 Android, iOS, Linux 等系统。 模型格式支持 支持 TensorFlow， Caffe 和 ONNX 等模型格式。 （8）TNN Github 地址 TNN 是由腾讯优图实验室开源的高性能、轻量级神经网络推理框架，同时拥有跨平台、高性能、模型压缩、代码裁剪等众多突出优势。TNN 框架在原有 Rapidnet、ncnn 框架的基础上进一步加强了移动端设备的支持以及性能优化，同时借鉴了业界主流开源框架高性能和良好拓展性的特性，拓展了对于后台 X86, NV GPU 的支持。手机端 TNN 已经在手 Q、微视、P 图等众多应用中落地，服务端 TNN 作为腾讯云 AI 基础加速框架已为众多业务落地提供加速支持。欢迎大家参与协同共建，促进 TNN 推理框架进一步完善。 （9）TVM Github 地址 TVM 是一款开源项目，主要由华盛顿大学的 SAMPL 组贡献开发。目前深度学习社区十分活跃，每天都有研究者提出新的 operation 以期望更好的提升模型的准确率。同时，随着越来越多的厂商开始做硬件（比如寒武纪，商汤科技等等），运行神经网络的时候会有越来越多的后端设备可供选择。 而这对于做框架的人来说就比较头疼，既要尝试为新出现的各种 operation 提供支持，又要在新出现的后端设备上实现现有的 operation。TVM 项目因此应运而生，希望达到的目标就是研究人员只用写一次 operation，然后 TVM 自动对各种后端设备生成性能可观的代码。 按照官方的定义，TVM 是一套完整的 stack，包括神经网络图优化（比如 op fusion）和单个 operation 优化等部分。我习惯于将图优化的部分归类做 Relay 项目，而仅仅把单个 operation 优化看做 TVM，因此文章之后提到的 TVM 基本是指单个算子优化这部分。 上面这张摘自 tvm 的官网 (https://tvm.ai/about) 的图片说明了 TVM 处于深度学习框架的位置。TVM 位于神经网络图（High-Level Differentiable IR）的下方，底层硬件（LLVM, CUDA, Metal）的上方。 图片右边的 AutoTVM 我认为比较独立。这个目的是自动调整 TVM 生成的代码的一些参数，试图让 TVM 生成的代码尽可能快。做自动代码优化的优秀项目除了 AutoTVM，还有 Halide，个人认为目前 Halide 做代码自动优化做的更好。TVM 的基本思路参考自 Halide，其中的数据结构也引用了很多 Halide 的实现，强烈推荐感兴趣的朋友研究一下 Halide","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"模型部署","slug":"模型部署","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"}]},{"title":"【精选】三观超正的文案","slug":"【精选】三观超正的文案","date":"2022-03-03T15:37:10.000Z","updated":"2022-03-03T15:37:50.775Z","comments":true,"path":"2022/03/03/【精选】三观超正的文案/","link":"","permalink":"https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E9%80%89%E3%80%91%E4%B8%89%E8%A7%82%E8%B6%85%E6%AD%A3%E7%9A%84%E6%96%87%E6%A1%88/","excerpt":"","text":"文章目录 【三观超正的文案】 永远不要因为你花三分钟看过的文章，去否定一个别人喜欢了好多年的人。 独处时守住心，群处时守住嘴。 不知全貌，不予置评。 未尝君苦，不劝大度。 被误解是表达者的宿命，不随意误解是聆听者的本分。 爱情不分先来后到 但是有礼义廉耻。 知道的太多不会累，想的太多才会。 但行好事，莫问前程。 唯有前途与父母不可辜负。 每件事到最后都会变成好事，如果不是 ，那就是没到最后，加油。 老老实实做事，规规矩矩做人。 爱更是行动，不只是情感或者想法。 你觉得那个有纹身的女孩怎么样？你说这句话的时候就是对她有了偏见。 可以卑微如尘土，不可扭曲如蛆虫。 可以不喜欢，但请别伤害。 穷则独善其身，达则兼济天下。 不要去评论你不了解的人，不要去评价你不了解的事。 只听其歌，不评其人。红时不追，落时不黑。只赏其作，不闻是非。 不要随波逐流，人生是自己的，不要去为了一个兴起的决定而去决定自己的人生。 孔子有云：君子不自大其事，不自尚其功 《菜根谭》中有言：地底成海，人低成王 金庸曾言：情深不寿，慧极必伤 一身清贫怎敢入繁华， 二袖清风怎敢误佳人， 三生有幸怎敢遇见你， 四目相对怎敢目无你， 五行皆空怎敢奢求你， 六道轮回怎敢忘记你， 七情藏心怎敢不爱你， 八面玲珑怎敢敷衍你， 九死一生怎敢辜负你。 一个人赚到的钱一定是他对这个世界认知的变现，否则就算你靠运气赚的钱，也一定会凭实力亏掉。 这世间大雨磅礴，万物苟且而活，莫嫌前路颠簸，人生本就曲折。 欲望上不封顶，能把自己逼上绝境的，一定是因为四个字，追名逐利。 不要因为世俗的眼光，遮挡住我们亮丽的灵魂。 通信不努力，微信在努力，商场不努力，电商在努力，所以我们必须要努力。 我们要做个热血青年，哪怕生活你是个吸血鬼，你咬我一口，我也要烫你一嘴。 其实绕了一圈才发现，自己真正想要的，早就拥有过了，目之所及，皆是回忆，心之所向，皆是过往。 祝酒词 （1）酒水无情人有情，这就不喝哪能行。 （2）喝完啤酒喝白酒，黄金白银咱都有。 （3）火车跑的快，全靠车头带，大家吃好喝好，全靠您来领导。 （4）美酒香飘万里，岂能空杯见底，春雨润物细无声，既然喝酒要喝清。 （5）万水千山总是情，少喝一口行不行？ （6）半斤不当酒，一斤扶墙走，斤半墙走我不走。 （7）感情深，一口闷；感情浅，舔一舔；感情厚，喝不够，感情薄，喝不着；感情铁，喝出血。 （8）老乡见老乡，喝酒要喝光。老乡见老乡，喝酒要喝双。 （9）喝一斤的喝一桶，回头提拔当副总；喝一桶的喝一缸，酒厂厂长让你当。 （10）宁可让胃喝个洞，不让感情留条缝，只要感情不要胃，跟着组织别掉队。 （11）茶水配酒，越喝越有。 （12）一杯情，两杯意，三杯才是好兄弟。一杯干，二杯敬，三杯喝出真感情。 （13）两腿一站，喝了不算。 （14）酒逢知己千杯少，能喝多少喝多少，喝不了赶紧跑。 （15）酒是粮食精，越喝越年轻。 （16）天蓝蓝，海蓝蓝，一杯一杯往下传。 （17）一条大河波浪宽，端起这杯咱就干。 （18）跟着感觉走，这次我喝酒。 （19）危难之处显身手，兄弟替我喝个酒。 （20）一两二两漱漱口，三两四两不算酒，五两六两扶墙走，七两八两还在吼。 （21）屁股一抬，喝了重来；屁股一动，表示尊重。 这个世界上最亏本的事情就是为尚未发生的事担忧，为自己想象的结果而焦虑，从利益出发它要不要做，从风险出发它该不该博，从能力出发它该不该干，从结果出发它划不划算，而不是别人告诉我，我对不对！ 法不轻传，道不贱卖，医不叩门，师不顺路 千金不传无意子，万财不渡忘恩人 自古表白多白表，从来情书难书请 笑谈少年多年少，常与生人道人生 坐在亭前望前亭，亦是情多也多情。 怎奈落花随花落，负了年少无少年。 先为不可胜，以待敌之可胜。不可胜在己，可胜在敌。 举手之劳是我帮忙之后的谦词，而不是你绑架我的说辞 我喜欢这样纯粹的程序员，他们不受外界因素干扰，低调不显摆，谦逊又自我。可能在一些人眼里，他们是带着厚厚的镜片，头发油腻，品味也 low 到爆的形象。 但事实上，他们是懂美学的牛顿，懂人类学的梵高，懂孙子兵法的甘地，他们的血液里充满着智慧，也很少有东西使他们感到困惑。 总之，并不华丽的文字才适合叙写平凡的程序员的平凡生活，程序员都是小人物，每天做着小事情，并且相信未来，白公子不是屌丝，程序猿只是自谦之词，请以后叫程序员为工程师，他们的生活丰富着呢，而且满怀希望。 你的眼光有多大，你人生的舞台就有多大。 站在二楼，你眼里只能看到垃圾； 站在二十楼，你才能看得见美景！ 单丝不成线，独木不成林。 互相搭台，好戏连台；互相拆台，大家垮台。 一定要站在自己所热爱的世界里，闪闪发亮，这归途尚远，要迷人，且倔强 人间失格中有句话说：人生最大的监狱是我们的大脑，走不出自己的观念，到哪里都是被困的囚徒。 提高自信力的技巧 正确地对待需要，期望值不要过高 认真努力地做好自己的本职工作 淡化个人名利，不计较个人得失 处理好家庭、同事、同学等人际关系 加强自我修养，心胸开阔 热爱生活， 志趣广泛 坚持体育锻炼，健身防病 不要压抑自己的情绪 不对自己的期望过高 不对别人希翼的过多 克制自己过激的情绪 正确面对严酷的现实 要会用理智战胜情绪 主动倾吐内心的烦恼 用防卫机制调控情绪 不要把目标定的过高； 要放弃消极自我暗示； 保持积极的情绪体验； 用大自然来陶冶情绪； 用积极的语言来调节； 转移注意到另外事物； 转移不良的消极情绪； 运用挫折升华的机制； 用理智调节战胜感情； 进行适当的心理宣泄 衣贵洁而不贵华，辞贵诚而不贵多。 明确的爱，真诚的喜欢，直接的厌恶，站在太阳底下的坦荡，还有被坚定的选择。 是非在己，毁誉由人。 总以为人生最美好的是相遇，后来才明白是相逢。忽有故人心上过，回首山河已是秋。两处相思同淋雪，此生也算共白头。 通过差异化满足个性化的需求的同时来创造更高的价值。 “我至今仍喜欢‘压岁钱’这三个字，那样粗鄙直接，却说尽了对岁月的惶恐、珍重，和一点点的撒赖与贿赂。” 台上一站，成功一半 要想当总统，必须先拿起话筒；要想当总裁，必须先走上舞台 人有两条路要走，一条是必须走的，一条是想走的，你只有把必须走的路走好，才能走想走的路，十年前，人们根据你父母的收入对待你，十年后，人们会根据你的收入对待你的父母 做一个影响力中心的人，放大你的能力，放大你的能量磁场，让自己影响更多的人 人脉圈的质量决定了我们事业发展的速度 所有的不可能都是自我设限，不是做不到，是不敢做 销售的本质是信赖感，没有信任就没有成交 穷人在十字路口耍十把钢钩，勾不到骨肉血亲 。富人在深山老林舞枪弄棒，打不散宾朋好友！ 格局是被委屈撑大的，温柔是用懂事换来的！ 未经他人苦，莫劝他人善 如果没有躺赢的命，那就站起来奔跑吧 夏虫不可以语冰，井蛙不可以语海，凡夫不可以语道. 从此无心爱良夜、任它明月下西楼 尊严只在剑锋之上，真理只在大炮射程之内。 酒杯太浅敬不到来日方长，巷子太短走不到白发苍苍，不是年少守不住旧情，而是岁月荒芜了人心。 你已经是个大人了；别再因为一点感情问题就失魂落魄。你可以有一段糟糕的爱情，但不能放纵自己过一个烂透的人生。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"且读文摘","slug":"且读文摘","permalink":"https://leezhao415.github.io/tags/%E4%B8%94%E8%AF%BB%E6%96%87%E6%91%98/"}]},{"title":"【Linux】NanoDet Plus之ncnn部署","slug":"【Linux】NanoDet-Plus之ncnn部署","date":"2022-03-03T14:56:48.000Z","updated":"2022-03-03T15:02:16.004Z","comments":true,"path":"2022/03/03/【Linux】NanoDet-Plus之ncnn部署/","link":"","permalink":"https://leezhao415.github.io/2022/03/03/%E3%80%90Linux%E3%80%91NanoDet-Plus%E4%B9%8Bncnn%E9%83%A8%E7%BD%B2/","excerpt":"","text":"文章目录 （1）准备工作： （2）操作步骤： step1 step2 step3 （3）参考教程 本项目实现在 linux 平台使用 ncnn 部署 NanoDet Plus 模型的功能，内容亲测有效，可作为通用 ncnn 模型部署参考项目。 （1）准备工作： 下载安卓的项目包：https://github.com/nihui/ncnn-android-nanodet （2）操作步骤： step1 https://github.com/Tencent/ncnn/releases Download ncnn-YYYYMMDD-android-vulkan.zip or build ncnn for android yourself Extract ncnn-YYYYMMDD-android-vulkan.zip into app/src/main/jni and change the ncnn_DIR path to yours in app/src/main/jni/CMakeLists.txt step2 https://github.com/nihui/opencv-mobile Download opencv-mobile-XYZ-android.zip Extract opencv-mobile-XYZ-android.zip into app/src/main/jni and change the OpenCV_DIR path to yours in app/src/main/jni/CMakeLists.txt step3 Open this project with Android Studio, build it and enjoy! （3）参考教程 安卓部署：手机端 Anchor-free 的目标检测模型 Nanodet 安卓手机部署 nanodet","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"模型部署","slug":"模型部署","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"}]},{"title":"【精华】视频理解研究进展","slug":"【精华】视频理解研究进展","date":"2022-03-03T14:56:32.000Z","updated":"2022-03-03T15:15:27.830Z","comments":true,"path":"2022/03/03/【精华】视频理解研究进展/","link":"","permalink":"https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/","excerpt":"","text":"文章目录 视频理解研究进展 &lt;1&gt; 常用数据集 &lt;2&gt; 经典方法 &lt;3&gt; 其他视频理解任务 &lt;4&gt; 可能的未来方向 视频理解研究进展 相比图像，视频多了一维时序信息。如何利用好视频中的时序信息是研究这类方法的关键。视频理解可以用于多个领域，例如在智能安防领域中可以取代人工来对监控视频进行分析。本文简要回顾视频理解方面的近期研究进展，并对未来可能的研究方向作一展望。 &lt;1&gt; 常用数据集 视频分类主要有两种数据集，剪辑过 (trimmed) 的视频和未经剪辑的视频。剪辑的视频中包含一段明确的动作，时间较短标记唯一，而未剪辑的视频还包含了很多无用信息。如果直接对未剪辑的视频进行处理是未来的一大研究方向。 H. Kuehne, et al. HMDB: A large video database for human motion recognition. In ICCV, pages 2556-2563, 2011. (1) HMDB-51 ：51 类、6,766 剪辑视频、每个视频不超过 10 秒、分辨率 320 240、共 2 GB。视频源于 YouTube 和谷歌视频，内容包括人面部、肢体、和物体交互的动作这几大类。 K. Soomro, et al. UCF101: A dataset of 101 human action classes from videos in the wild. CoRR, abs/1212.0402, 2012. (2) UCF-101 ：101 类、13,320 视频剪辑、每个视频不超过 10 秒、共 27 小时、分辨率 320 240、共 6.5 GB。视频源于 YouTube，内容包含化妆刷牙、爬行、理发、弹奏乐器、体育运动五大类。每类动作由 25 个人做动作，每人做 4-7 组。 A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and F.-F. Li. Large-scale video classification with convolutional neural networks. In CVPR, pages 1725-1732, 2014. (3) Sports-1M ：487 类、1,100,000 视频（70% 训练、20% 验证、10% 测试）。内容包含各种体育运动。 G. A. Sigurdsson, et al. Hollywood in homes: Crowdsourcing data collection for activity understanding. In ECCV, pages 510-526, 2016. (4) Charades ：157 类、9,848 未剪辑视频（7,985 训练、1,863 测试）、每个视频大约 30 秒。每个视频有多个标记，以及每个动作的开始和结束时间。 B. G. F. C. Heilbron, V. Escorcia, B. Ghanem, J. C. Niebles. ActivityNet: A large-scale video benchmark for human activity understanding. In CVPR, pages 961-970, 2015. (5) ActivityNet (v1.3) ：200 类、19,994 未剪辑视频（10,024 训练，4,926 验证，5,044 测试）、共 648 小时。内容包括饮食、运动、家庭活动等。 W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, and A. Zisserman. The Kinetics human action video dataset. CoRR, abs/1705.06950, 2017. (6) Kinetics ：400 类、246k 训练视频、20k 验证视频、每个视频大约 10 秒。视频源于 YouTube。Kinetics 是一个大规模数据集，其在视频理解中的作用有些类似于 ImageNet 在图像识别中的作用，有些工作用 Kinetics 预训练模型迁移到其他视频数据集。 S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan, and S. Vijayanarasimhan. YouTube-8M: A large-scale video classification benchmark. CoRR, abs/1609.08675, 2016. (7) YouTube-8M ：4716 类、7M 视频、共 450,000 小时。不论是下载还是训练都很困难。 R. Goyal, S. E. Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fründ, P. Yianilos, M. Mueller-Freitag, F. Hoppe, C. Thurau, I. Bax, and R. Memisevic. The “Something Something” video database for learning and evaluating visual common sense. In ICCV, pages 5843-5851, 2017. (8) Something-something ：174 类、108,000 视频、每个视频 2 到 6 秒。和 Kinetics 不同，Something-something 数据集需要更加细粒度、更加底层交互动作的区分，例如 “从左向右推” 和 “从右向左推”。 此外，Video Dataset Overview 对常用数据集进行了总结。 G. A. Sigurdsson, O. Russakovsky, and A. Gupta. What actions are needed for understanding human actions in videos? In ICCV, pages 2156-2165, 2017. 相比图像分类，视频的类别 / 动作数目要少很多，而且常常有一定歧义，例如 take 和 put 要和后面名词结合才会有具体含义 (如 take medication, take shoes, take off shoes)。Sigurdsson 等人发现人类对这些动词也容易感到混淆。另外，视频中动作开始和结束的时间也不够明确。 &lt;2&gt; 经典方法 H. Wang, et al. Dense trajectories and motion boundary descriptors for action recognition. IJCV’13. H. Wang and C. Schmid. Action recognition with improved trajectories. ICCV’13. Wang 等人提出 DT 和 iDT 方法。DT 利用光流得到视频中的运动轨迹，再沿着轨迹提取特征。iDT 对相机运动进行了补偿，同时由于人的运动比较显著，iDT 用额外的检测器检测人，以去除人对相邻帧之间投影矩阵估计的影响。这是深度学习方法成熟之前效果最好的经典方法，该方法的弊端是特征维度高 (特征比原始视频还要大)、速度慢。实践中，早期的深度学习方法在和 iDT 结合之后仍能取得一定的效果提升，现在深度学习方法的性能已较 iDT 有大幅提升，因此 iDT 渐渐淡出视线。 （1）逐帧处理融合 这类方法把视频看作一系列图像的集合，每帧图像单独提取特征，再融合它们的深度特征。 A. Karpathy, et al. Large-scale video classification with convolutional neural networks. CVPR’14. Karpathy 等人把视频划分成很多固定长度的片段 (clip)，并设计了多种融合方法。 Single frame. 逐帧单独前馈网络。 Late fusion. 两帧相距 15 帧的图像分别前馈网络，并融合它们的深度卷积特征。 Early fusion. 连续 10 帧图像前馈网络，因此网络第一层的卷积核由 11×11×3 变为 11×11×3×10。Early fusion 的思路最早由 Le 等人提出。 Le, et al. Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. CVPR’11. Slow fusion. 即使用 3D 卷积。连续 10 帧图像前馈网络，第一层卷积核时间方向大小为 4，第二、三层卷积核时间方向大小为 2。 此外，为了加速训练，由于目标通常位于图像中心，Karpathy 等人使用了一个两分支网络：一个分支输入空间大小下采样减半的图像，另一个分支输入原图中心裁剪后的图像。这样，总体的输入图像维数只有原图的一半。这两个分支的深度卷积特征拼接 (concatenate) 起来给全连接层进行分类。 实验发现，3D 卷积的融合效果最好，而不考虑运动信息的 single frame 策略已经是十分强的 baseline。Early fusion 策略效果最差的原因可能是直接从输入图像上捕获运动信息难度太大。 J. Y.-H. Ng, et al. Beyond short snippets: Deep networks for video classification. CVPR’15. Ng 等人先提取每一帧的深度卷积特征，再设计特征融合方法得到最终输出。其中，紫色代表沿时间方向进行最大汇合 (max-pooling)，黄色代表全连接层，绿色代表 3310，stride 5 的 3D 卷积，红色代表 softmax 输出。相比 Karpathy 等人，Ng 等人的输入视频片段的长度更长 (每秒采样 1 帧，一个片段由 120 帧构成)，包含了更多的时序信息。实践中发现 (a) 的效果最好。 B. Fernando and S. Gould. Learning end-to-end video classification with rank-pooling. ICML’16. 在得到每帧图像的深度卷积特征 之后，Fernando 和 Gould 通过解如下的优化问题来对特征进行排序汇合 (rank-pooling)。其动机是靠前的帧 要小一些，而靠后的帧大一些。 X.-S. Wei, et al. Deep bimodal regression of apparent personality traits from short video sequences. TAC’17. 由于相邻帧信息冗余度很高，Wei 等人从视频 (450 帧) 中采样 100 帧，每帧交由 DAN 分别进行预测。在得到 relu5-2/pool5 深度特征之后，DAN 将其全局最大 / 平均汇合以得到深度特征。 A. Kar, et al. AdaScan: Adaptive scan pooling in deep convolutional neural networks for human action recognition in videos. CVPR’17. 由于不同帧的重要性不同，Kar 等人提出 AdaScan 汇合方法。其逐帧提取特征，之后判断不同帧的重要程度，并据此进行特征汇合。 M. Zolfaghari, et al. ECO: Efficient Convolutional network for Online video understanding. arXiv:1804.09066. Zolfaghari 等人提出 ECO。由于相邻帧有信息冗余，ECO 从视频中采样若干帧，每帧单独用 2D 卷积提取特征，之后沿时间方向拼接特征，再用 3D 卷积捕获它们的时序关系。ECO 和 state-of-the-art 方法性能相似，但速度上快了 10-80 倍。在测试时，为了捕获长距离依赖，输入视频片段由已看过的和未看过的视频中采样得到。 （2）ConvLSTM 这类方法是用 CNN 提取每帧图像的特征，之后用 LSTM 挖掘它们之间的时序关系。 J. Y.-H. Ng, et al. Beyond short snippets: Deep networks for video classification. CVPR’15. J. Donahue, et al. Long-term recurrent convolutional networks for visual recognition and description. CVPR’15. Ng 等人在深度特征上，用 5 层隐层结点数 512 的 LSTM 来提取深度特征，每个时刻都进行输出。训练时，一个片段从第 1 帧到最后一帧输出层获得的梯度分别乘以 0.0-1.0 的权重，用以强调后面帧的重要性。测试时，计算这些帧输出的加权和。Donahue 等人也提出了类似的工作。此外，Ng 等人和 Donahue 等人还利用了光流输入。把 x、y 两个方向的光流缩放到 [0, 255] 作为光流图像前两个通道，把光流的大小作为第三个通道。 W. Du, et al. RPAN: An end-to-end recurrent pose-attention network for action recognition in videos. ICCV’17. Du 等人利用人体姿态估计辅助动作识别。 （3）3D 卷积 把视频划分成很多固定长度的片段 (clip)，相比 2D 卷积，3D 卷积可以提取连续帧之间的运动信息。 在视频动作识别中最早提出 3D 卷积的是 M. Baccouche, et al. Sequential deep learning for human action recognition. HBU Workshop’11. S. Ji, et al. 3D convolutional neural networks for human action recognition. TPAMI’13. Baccouche 等人使用第一层卷积核时间方向大小为 5。Ji 等人使用第一、二层卷积核时间方向大小为 3，第三层卷积时由于时间维度大小很小，所以采用 2D 卷积。 此外，为使网络获得更多先验信息，Ji 等人使用了两个技巧：(1). 同时使用原始图像、图像梯度、和相邻帧光流作为输入。(2). 让网络额外地学习运动信息接近手工运动特征。 D. Tran, et al. Learning spatio-temporal features with 3D convolutional networks. ICCV’15. Tran 等人提出 C3D，其将 3×3 卷积扩展到 3×3×3 卷积，2×2 汇合扩展到 2×2×2 汇合。输入片段 16 帧。实验中发现，时域方向卷积核大小为 3 效果最好。相比 2D CNN，3D CNN 的参数量很大，训练变得更困难，且需要更多的训练数据。相比其他类型的方法，C3D 一次处理多帧，所以计算效率很高。 L. Sun, et al. Human action recognition using factorized spatio-temporal convolutional networks. ICCV’15. Sun 等人把 3D 卷积分解为空间方向 2D 卷积和时间方向 1D 卷积。 J. Carreira and A. Zisserman. Quo vadis, action recognition? A new model and the Kinetics dataset. CVPR’17. Carreira 和 Zisserman 提出 I3D，把 two-stream 结构中的 2D 卷积扩展为 3D 卷积。由于时间维度不能缩减过快，前两个汇合层的卷积核大小是 1×2×2，最后的汇合层的卷积核大小是 277。和之前文章不同的是，two-tream 的两个分支是单独训练的，测试时融合它们的预测结果。 Z. Qiu, et al. Learning spatio-temporal representation with pseudo-3D residual networks. ICCV’17. Qiu 等人提出 P3D，用一个 1×3×3 的空间方向卷积和一个 3×1×1 的时间方向卷积近似原 3×3×3 卷积。通过组合三种不同的模块结构，进而得到 P3D ResNet。P3D ResNet 在参数数量、运行速度等方面对 C3D 作出了优化。 D. Tran, et al. A closer look at spatio-temporal convolutions for action recognition. CVPR’18. Tran 等人提出 ResNet (2+1) D，把一个 3D 卷积分解成为一个 2D 卷积空间卷积和一个 1D 时间卷积，注意这里的参数量和原 3D 卷积相同。相比 P3D 有三种形式，(2+1) D 和 P3D-A 最接近。 C. Lea, et al. Temporal convolutional networks for action segmentation and detection. CVPR’17. 受 WaveNet 启发，Lea 等人提出一个编码 - 解码网络，并使用空洞卷积和短路连接，以捕获长距离依赖。实验中发现，这种结构效果优于 RNN。 L. Wang, et al. Appearance-and-relation networks for video classfication. CVPR’18. Wang 等人希望利用 3D 卷积显式地学习类似 two-stream 的结构。Wang 等人通过不同帧之间的乘性关系度量不同帧之间的关系。 K. Hara, et al. Can spatio-temporal 3D CNNs retrace the history of 2D CNNs and ImageNet? CVPR’18. ImageNet 预训练的 2D CNN 在多种视觉任务上取得了成功，Hara 等人尝试了多种 Kinetics 预训练的 3D CNN 模型迁移到其他数据集上的表现。发现直接用 3D ResNet-18 训练 UCF-101，HMDB-51，和 ActivityNet 会过拟合，而 Kinetics 不会。利用 Kinetics 预训练模型会提升其他数据集的性能。 X. Wang, et al. Non-local neural networks. CVPR’18. 可以看作是 3D 卷积的一个扩展。3D 卷积的感受野是有限区域，而 non-local 旨在解决长距离依赖问题。Non-local 的响应是所有空间和时间位置特征的加权平均 其中， 用于度量相似性 计算响应 用于归一化 当 ，non-local 操作退化为全连接层 当 ，non-local 操作 退化为 self-attention。 实验中发现 non-local block 加在底层比加在高层效果要好，加多个 non-local blocks 会有效果提升但不是很明显。 这类方法的弊端是只能考虑比较短的时间片段的运动信息，参数量？ （4）Two-stream K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. NIPS’14. 采用两个分支。一个分支输入单帧图像，用于提取图像信息，即在做图像分类。另一个分支输入连续 10 帧的光流 (optical flow) 运动场，用于提取帧之间的运动信息。由于一个视频片段中的光流可能会沿某个特别方向位移的支配，所以在训练时光流减去所有光流向量的平均值。两个分支网络结构相同，分别用 softmax 进行预测，最后用直接平均或 SVM 两种方式融合两分支结果。 此外，为了加速训练，Simonyan 和 Zisserman 预先计算出光流并保存到硬盘中。为了减小存储大小，他们将光流缩放到 [0, 255] 后用 JPEG 压缩，这会使 UCF101 的光流数据大小由 1.5TB 减小到 27GB。 L. Wang, et al. Action recognition with trajectory-pooled deep-convolutional descriptors. CVPR’15. Wang 等人结合了经典 iDT 手工特征和 two-stream 深度特征，提出 TDD。经典手工特征计算时通常分两步：检测图像中显著和有信息量的区域，并在运动显著的区域提取特征。TDD 将预训练的 two-stream 网络当作固定的特征提取器。得到两者特征之后，TDD 使用时空规范化以保证每个通道的数值范围近似一致，使用通道规范化以保证每个时空位置的描述向量的数值范围近似一致，之后用 trajectory pooling 并用 Fisher 向量构建 TDD 特征，最后用 SVM 分类。 C. Feichtenhofer, et al. Convolutional two-stream network fusion for video action recognition. CVPR’16. Feichtenhofer 等人研究如何融合两分支的深度卷积特征。他们发现级联两个特征到 2D 维再用 1×1 卷积到 D 维的融合方法效果最好，之后再经过 3D 卷积和 3D 汇合后输出。 C. Feichtenhofer, et al. Spatio-temporal residual networks for video action recognition. NIPS’16. Feichtenhofer 将 ResNet 作为 two-stream 的基础网络架构，用预训练网络的权重初始化新的 3D 网络：w (d, t, i, j) = w (d, i, j) / T。此外，有从光流分支到图像分支的信息传递。此外，网络输入不是连续的，而是步长 5 到 15 帧。 C. Feichtenhofer, et al. Spatio-temporal multiplier networks for video action recognition. CVPR’17. Feichtenhofer 等人发现，two-stream 网络在外观分支容易过拟合。Feichtenhofer 等人加入了两个分支之间的信息交互，并发现乘性的信息交互效果最好。 L. Wang, et al. Temporal segment networks: Towards good practices for deep action recognition. ECCV’16. 由于相邻的帧有信息冗余，所以对视频密采样是不需要的。Wang 等人提出 TSN，采用稀疏采样的策略利用整个视频的信息。具体来说，TSN 把视频分成 3 段，每个片段均匀地随机采样一个视频片段，并使用双流网络得到视频片段属于各类得分 (softmax 之前的值)，之后把不同片段得分取平均，最后通过 softmax 输出。训练时，TSN 可以根据整个视频的信息而不只是一个视频片段的信息对网络参数进行学习，这是 TSN 最大的优点 (个人认为)。TSN 获得了 ActivityNet 2016 竞赛的冠军 (93.2% mAP)。 此外，除经典的 RGB 图像和光流外，TSN 还尝试了 RGB difference 和 warped 光流。其中，RGB difference 可以认为是对图像运动信息的一个粗略估计，而 warped 光流借鉴经典 iDT 的思路，试图对相机位移作一估计并对光流进行补偿，使得光流代表的运动信息更集中在前景目标上。实验发现，RGB + 光流 + warped optical flow 输入的效果最好。 Z. Lan, et al. Deep local video feature for action recognition. CVPR’17. 由于不是视频中每帧都包含有用信息，Lan 等人首先用 TSN 提取局部特征，之后再进行聚合。 R. Girdhar, et al. ActionVLAD: Learning spatio-temporal aggregation for action recognition. CVPR’17. 类似于 NetVLAD，Girdhar 等人用 two-stream 提取特征，之后用 VLAD 得到视频的表示。实验中发现，图像和光流两个分支单独处理效果最好。 G. A. Sigurdsson, et al. Asynchronous temporal fields for action recognition. CVPR’17. Sigurdsson 等人利用全连接时序 CRF 对视频的时序关系进行推断。 W. Zhu, et al. A key volume mining deep framework for action recognition. CVPR’16. 一段视频中并非所有的帧都对识别任务同等重要，如果把它们同等看待，有价值的帧信息会淹没在其他无关的帧中。借助多示例学习思路，Zhu 等人交替优化关键帧挖掘和分类两个目标。网络输入 N 个视频片段，输出每个片段对应每个类别的分数。如果该类别对应真实标记，采用随机汇合，否则是 maxout 汇合，其中响应最强的视频片段即为得到的关键帧。 Y. Wang, et al. Spatio-temporal pyramid network for video action recognition. CVPR’16. Wang 等人利用双线性汇合融合两个分支的特征。 A. Diba, et al. Deep temporal linear encoding networks. CVPR’17. Diba 等人对不同帧 / 片段的深度卷积特征逐元素相乘，再通过精简双线性汇合得到最终的特征表示。 R. Girdhar and D. Ramanan. Attentional pooling for action recognition. NIPS’17. 将双线性汇合用于 TSN 的图像分支。在得到深度卷积特征 之后，经典双线性汇合会计算输入属于第 个类的分数 。Girdhar 和 Ramanan 对参数矩阵 做了一个秩 - 1 近似 实验中， Girdhar 和 Ramanan 将 224 大小的 HMDB-51 缩放到 450 大小，以确保最后的深度卷积特征大小不会太小 (14×14)。当特征大小太小时，效果不显著。另一方面，Girdhar 和 Ramanan 只用了图像分支来处理视频，总体性能和 state-of-the-art 还有很大差距。 I. C. Duta, et al. Spatio-temporal vector of locally max-pooled features for action recognition in videos. CVPR’17. Duta 等人研究如何聚合不同特征。 C.-Y. Wu, et al. Compressed video action recognition. CVPR’18. Wu 等人发现：(1). 视频中有很多的冗余信息，这会网络难以提取有用的特征。(2). 相比只利用 RGB 信息，使用光流总是能提升性能。相比之前工作将视频解码为 RGB 图像帧，如果直接输入压缩视频 (如 MPEG，H.264 等)，可以 “免费” 地利用这些编码格式中得到的运动信息。视频在压缩时会把帧分为 I 帧 (保存原始图像) 和 P 帧 (保存和参考帧之间的相对运动信息和残差，参考帧可能是 I 帧也可能是 P 帧)。I 帧可以直接用深度神经网络架构进行处理，而如何对 P 帧信息进行处理是这类方法的难点所在。 Wu 等人通过将运动向量追溯到 I 帧来去除 P 帧对 P 帧的依赖，之后对 I 帧、P 帧的运动信息和残差分别前馈网络。在特征融合部分，Wu 等人发现直接相加效果最好。实际中，为降低计算开销，Wu 等人使用一个大网络 (preResNet-152) 处理 I 帧，用一个小网络 (preResNet-18) 处理 P 帧。 P. Weinzaepfel, et al. DeepFlow: Large displacement optical flow with deep matching. ICCV’13. A. Dosovitskiy, et al. FlowNet: Learning optical flow with convolutional networks. ICCV’15. E. Ilg, et al. FlowNet 2.0: Evolution of optical flow estimation with deep networks. CVPR’17. 由于经典光流算法很慢，因此有工作致力于使用深度神经网络计算光流。DeepFlow 在不同粒度上进行聚合和匹配，FlowNet 基于类似于视觉跟踪的思路使用互相关滤波综合两张图的深度特征最终生成光流。由于标记数据不足，FlowNet 使用人工合成 Chairs 数据集进行训练。FlowNet 2.0 的贡献有三点。(1). 发现如何进行训练对结果影响至关重要，先在简单的 Chairs 数据集上训练、再在更真实的 Things3D 上训练会比混合两个数据集一起训练效果更好。(2). 多个子网络堆叠，并用中间光流结果对图像进行扭曲输入中间子网络。(3). 有一个子网络专注于微小运动。 &lt;3&gt; 其他视频理解任务 时序动作定位 (temporal action localization) 在一段未剪辑的视频中找到动作开始和结束的时间，并对动作进行分类。 Z. Shou, et al. Temporal action localization in untrimmed videos via multi-stage CNNs. CVPR’16. Shou 等人提出 SCNN，用不同大小的滑动窗产生视频片段，之后用 3D 候选区域网络判断该视频片段是前景 / 背景，用 3D 分类网络判断 K+1 个类别的分数 (包括背景)，最后用定位网络判断开始 / 结束时间。后处理使用非最大抑制 (NMS)。 J. Gao, et al. TURN TAP: Temporal unit regression network for temporal action proposals. ICCV’17. 思路类似于 Faster R-CNN。 H. Xu, et al. R-C3D: Region convolutional 3D network for temporal activity detection. ICCV’17. 以 C3D 网络为基础，借鉴 Faster R-CNN，对输入视频片段先提取特征，再生成提取候选时序，最后 RoI 汇合后进行检测。 Z. Shou, et al. CDC: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos. CVPR’17. 类似于语义分割问题的思路，为了得到对应于每一帧的分类预测分数，Shou 等人在 3D 卷积层之后提出 CDC 卷积，在空间方向用卷积进行下采样，在时间方向上用转置卷积进行上采样。 L. Wang, et al. UntrimmedNets for weakly supervised action recognition and detection. CVPR’17. 分类模块用于对每个视频片段进行分类，而选择模块用于给出不同视频片段的重要性。选择模块的实现包括 hard selection 和 soft selection。训练时端到端联合优化。 Y. Zhao, et al. Temporal action detection with structured segment networks. ICCV’17. Zhao 等人提出 SSN，讲视频分为三个部分，最终构成全局特征。分类时有动作性分类器和完整性分类器。 异常检测 (anomaly detection) 通常用于判断监控视频中出现的异常事件。 W. Sultani, et al. Real-world anomaly detection in surveillance videos. CVPR’18. 由于训练时只知道一段视频中有 / 没有异常，而异常事件的种类和发生时刻未知，Sultani 等人利用多示例学习，将异常检测问题转化为一个回归排序问题，让异常样本的排序值高于普通样本，训练时让正负样本之间的距离尽可能远。 视频摘要与视频浓缩 (video summarization and video synopsis) 视频摘要是从原始视频中提取有代表性的关键帧，而视频浓缩将多帧视频合并成一帧。 M. Gygli, et al. Creating summaries from user videos. ECCV’14. X. Li, et al. Surveillance video synopsis via scaling down objects. TIP’16. &quot;看视频说话&quot;(video captioning) 基本思路和看图说话一致，用编码网络提取视频信息，用解码网络生成文字描述。 S. Venugopalan, et al. Sequence to Sequence–Video to Text. ICCV’15. 第一视角视频 (first-person video) 研究第一视角视频可以用于自动驾驶、机器人导航等。 T. Yagi, et al. Future person localization in first-person videos. CVPR’18. Yagi 等人提出行人位置预测任务，即根据行人历史信息，预测下一帧行人的位置。Yagi 等人用 1D 时域卷积来融合不同帧的特征。 视频生成 (next frame generation) 有工作利用生成式模型对视频进行生成。 M. Mathieu, et al. Deep multi-scale video prediction beyond mean square error. ICLR’16. C. Vondrick, et al. Generating videos with scene dynamics. NIPS’16. 目标跟踪 (object tracking) 给定视频第一帧中目标的位置 (以包围盒的形式)，我们需要预测其他帧中该目标的包围盒。目标跟踪类似于目标检测，但目标跟踪的难点在于事先不知道要跟踪的目标具体是什么，因此无法事先收集足够的训练数据以训练一个专门的检测器。一种研究思路是利用孪生网络，一支输入第一帧包围盒内图像，另一支输入其他帧的候选图像区域，通过互相关操作 (卷积)，得到二维的响应图，其中最大响应位置确定了需要预测的包围盒位置。 L. Bertinetto, et al. Fully-convolutional siamese networks for object tracking. ECCV’16 Workshop. M. Danelljan, et al. ECO: Efficient Convolution Operators for tracking. CVPR’17. E. Valmadre, et al. End-to-end representation learning for correlation filter based tracking. CVPR’17. &lt;4&gt; 可能的未来方向 利用多示例学习进行视频分析。未剪辑视频中有很多无关内容，并非视频中所有的帧都对应于该视频标记，这符号多示例学习的设定。虽然 Zhu 等人在 CVPR’16 和 Kar 等人在 CVPR’17 的工作中对这方面已有一些探索，但仍有后续精进的空间。 精度与效率。Two-stream 和 3D 卷积的方法相比，大致来说前者的效果更好，但前者需要逐帧图像前馈网络，而后者一次可以处理多帧，因此前者效率不如后者，尤其是预先计算并保存光流是一个很繁重的负担。如何能同时利用两者的优点是未来一个可能的研究方向，Feichtenhofer 等人在 CVPR’16 已有初步的工作。LSTM 能捕获的长距离依赖程度有限，并且更难训练，速度也更慢，因此 ConvLSTM 的方法在视频分析中用的不多。 资源受限下的视频分析。相比图像数据，处理视频数据需要更大的计算和存储资源。现实应用中很多是资源受限的，如何在这种场景下进行视频分析是一大挑战。将视频解压为能输入网络的一帧帧图像也需要不小的资源开销，Wu 等人在 CVPR’18 提出直接利用原始视频输入，并利用视频压缩编码中的运动信息。 更大、更通用数据集。哪种方法更好和用什么数据集 (解决什么任务) 有很大关系。如果视频本身就比较静止，或者单帧图像已经包含了足够的信息，那么用逐帧单独处理的策略已经可以取得很好的结果。 视频 = 图像 + 音频。视频是一种多模态的数据形式，能否利用音频信息辅助视频分析呢。Aytar 等人在 NIPS’16 的工作中利用图像辅助音频分析。 Y. Aytar, et al. SoundNet: Learning sound representations from unlabeled video. NIPS’16. 最后列出一些相关的综述文章。其中 Tran 等人实验研究了不同采样步长、不同输入大小、不同网络配置等对性能的影响。 Z. Wu, et al. Deep learning for video classification and captioning. arXiv: 1609.06782. D. Tran, et al. ConvNet architecture search for spatio-temporal feature learning. arXiv: 1708:05038. M. Asadi-Aghbolaghi, et al. A survey on deep learning based approaches for action and gesture recognition in image sequences. FG’17. S. Herath, et al. Going deeper into action recognition: A survey. IVC’17.","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"视频理解","slug":"视频理解","permalink":"https://leezhao415.github.io/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/"}]},{"title":"【精华】模型部署实例（OpenVINO、TensorRT）","slug":"【精华】模型部署实例（OpenVINO、TensorRT）","date":"2022-03-03T14:56:11.000Z","updated":"2022-03-03T15:18:59.118Z","comments":true,"path":"2022/03/03/【精华】模型部署实例（OpenVINO、TensorRT）/","link":"","permalink":"https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/","excerpt":"","text":"文章目录 （1）OpenVINO 部署 NanoDet 模型 1&gt; nanodet 简介 2&gt; 环境配置 3&gt; NanoDet 模型训练和转换 ONNX 4&gt; NanoDet 模型部署 5&gt; 核心代码一览 6&gt; 推理时间展示及预测结果展示 （2）TensorRT 部署深度学习模型 1&gt; 背景 2&gt; 相关技术 3&gt; tensorflow 模型 tensorRT 部署教程 4&gt; Caffe 模型 tensorRT 部署教程 5&gt; 为 tensorRT 添加自定义层 6&gt; 为 CaffeParser 添加自定义层支持 7&gt; 心得体会（踩坑记录） （1）OpenVINO 部署 NanoDet 模型 1&gt; nanodet 简介 NanoDet （https://github.com/RangiLyu/nanodet）是一个速度超快和轻量级的 Anchor-free 目标检测模型。想了解算法本身的可以去搜一搜之前机器之心的介绍。 2&gt; 环境配置 Ubuntu：18.04 OpenVINO：2020.4 OpenCV：3.4.2 OpenVINO 和 OpenCV 安装包（编译好了，也可以自己从官网下载自己编译）可以从链接: https://pan.baidu.com/s/1zxtPKm-Q48Is5mzKbjGHeg 密码: gw5c 下载 OpenVINO 安装 123456789tar -xvzf l_openvino_toolkit_p_2020.4.287.tgzcd l_openvino_toolkit_p_2020.4.287sudo ./install_GUI.sh 一路next安装cd /opt/intel/openvino/install_dependenciessudo ./install_openvino_dependencies.shvi ~/.bashrc 把如下两行放置到 bashrc 文件尾 12source /opt/intel/openvino/bin/setupvars.shsource /opt/intel/openvino/opencv/setupvars.sh source ~/.bashrc 激活环境 模型优化配置步骤 12cd /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisitessudo ./install_prerequisites_onnx.sh（模型是从onnx转为IR文件，只需配置onnx依赖） OpenCV 配置 1tar -xvzf opencv-3.4.2.zip # 解压OpenCV到用户根目录即可，以便后续调用。（这是我编译好的版本，有需要可以自己编译） 3&gt; NanoDet 模型训练和转换 ONNX git clone https://github.com/Wulingtian/nanodet.git cd nanodet cd config 配置模型文件，训练模型 定位到 nanodet 目录，进入 tools 目录，打开 export.py 文件，配置 cfg_path model_path out_path 三个参数 定位到 nanodet 目录，运行 python tools/export.py 得到转换后的 onnx 模型 python /opt/intel/openvino/deployment_tools/model_optimizer/mo_onnx.py --input_model onnx 模型 --output_dir 期望模型输出的路径。得到 IR 文件 4&gt; NanoDet 模型部署 sudo apt install cmake 安装 cmake git clone https://github.com/Wulingtian/nanodet_openvino.git （求 star！） cd nanodet_openvino 打开 CMakeLists.txt 文件，修改 OpenCV_INCLUDE_DIRS 和 OpenCV_LIBS_DIR，之前已经把 OpenCV 解压到根目录了，所以按照你自己的路径指定 定位到 nanodet_openvino，cd models 把之前生成的 IR 模型（包括 bin 和 xml 文件）文件放到该目录下 定位到 nanodet_openvino， cd test_imgs 把需要测试的图片放到该目录下 定位到 nanodet_openvino，编辑 main.cpp，xml_path 参数修改为 &quot;…/models/ 你的模型名称.xml&quot; 编辑 num_class 设置类别数，例如：我训练的模型是安全帽检测，只有 1 类，那么设置为 1 编辑 src 设置测试图片路径，src 参数修改为 &quot;…/test_imgs/ 你的测试图片&quot; 定位到 nanodet_openvino mkdir build; cd build; cmake … ;make ./detect_test 输出平均推理时间，以及保存预测图片到当前目录下，至此，部署完成！ 5&gt; 核心代码一览 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113//主要对图片进行预处理，包括resize和归一化std::vector&lt;float&gt; Detector::prepareImage(cv::Mat &amp;src_img)&#123; std::vector&lt;float&gt; result(INPUT_W * INPUT_H * 3); float *data = result.data(); float ratio = float(INPUT_W) / float(src_img.cols) &lt; float(INPUT_H) / float(src_img.rows) ? float(INPUT_W) / float(src_img.cols) : float(INPUT_H) / float(src_img.rows); cv::Mat flt_img = cv::Mat::zeros(cv::Size(INPUT_W, INPUT_H), CV_8UC3); cv::Mat rsz_img = cv::Mat::zeros(cv::Size(src_img.cols*ratio, src_img.rows*ratio), CV_8UC3); cv::resize(src_img, rsz_img, cv::Size(), ratio, ratio); rsz_img.copyTo(flt_img(cv::Rect(0, 0, rsz_img.cols, rsz_img.rows))); flt_img.convertTo(flt_img, CV_32FC3); int channelLength = INPUT_W * INPUT_H; std::vector&lt;cv::Mat&gt; split_img = &#123; cv::Mat(INPUT_W, INPUT_H, CV_32FC1, data + channelLength * 2), cv::Mat(INPUT_W, INPUT_H, CV_32FC1, data + channelLength), cv::Mat(INPUT_W, INPUT_H, CV_32FC1, data) &#125;; cv::split(flt_img, split_img); for (int i = 0; i &lt; 3; i++) &#123; split_img[i] = (split_img[i] - img_mean[i]) / img_std[i]; &#125; return result;&#125; //加载IR模型，初始化网络bool Detector::init(string xml_path,double cof_threshold,double nms_area_threshold,int input_w, int input_h, int num_class, int r_rows, int r_cols, std::vector&lt;int&gt; s, std::vector&lt;float&gt; i_mean,std::vector&lt;float&gt; i_std)&#123; _xml_path = xml_path; _cof_threshold = cof_threshold; _nms_area_threshold = nms_area_threshold; INPUT_W = input_w; INPUT_H = input_h; NUM_CLASS = num_class; refer_rows = r_rows; refer_cols = r_cols; strides = s; img_mean = i_mean; img_std = i_std; Core ie; auto cnnNetwork = ie.ReadNetwork(_xml_path); InputsDataMap inputInfo(cnnNetwork.getInputsInfo()); InputInfo::Ptr&amp; input = inputInfo.begin()-&gt;second; _input_name = inputInfo.begin()-&gt;first; input-&gt;setPrecision(Precision::FP32); input-&gt;getInputData()-&gt;setLayout(Layout::NCHW); ICNNNetwork::InputShapes inputShapes = cnnNetwork.getInputShapes(); SizeVector&amp; inSizeVector = inputShapes.begin()-&gt;second; cnnNetwork.reshape(inputShapes); _outputinfo = OutputsDataMap(cnnNetwork.getOutputsInfo()); for (auto &amp;output : _outputinfo) &#123; output.second-&gt;setPrecision(Precision::FP32); &#125; _network = ie.LoadNetwork(cnnNetwork, &quot;CPU&quot;); return true;&#125;//模型推理及获取输出结果vector&lt;Detector::Bbox&gt; Detector::process_frame(Mat&amp; inframe)&#123; cv::Mat showImage = inframe.clone(); std::vector&lt;float&gt; pr_img = prepareImage(inframe); InferRequest::Ptr infer_request = _network.CreateInferRequestPtr(); Blob::Ptr frameBlob = infer_request-&gt;GetBlob(_input_name); InferenceEngine::LockedMemory&lt;void&gt; blobMapped = InferenceEngine::as&lt;InferenceEngine::MemoryBlob&gt;(frameBlob)-&gt;wmap(); float* blob_data = blobMapped.as&lt;float*&gt;(); memcpy(blob_data, pr_img.data(), 3 * INPUT_H * INPUT_W * sizeof(float)); infer_request-&gt;Infer(); vector&lt;Rect&gt; origin_rect; vector&lt;float&gt; origin_rect_cof; int i=0; vector&lt;Bbox&gt; bboxes; for (auto &amp;output : _outputinfo) &#123; auto output_name = output.first; Blob::Ptr blob = infer_request-&gt;GetBlob(output_name); LockedMemory&lt;const void&gt; blobMapped = as&lt;MemoryBlob&gt;(blob)-&gt;rmap(); float *output_blob = blobMapped.as&lt;float *&gt;(); bboxes = postProcess(showImage,output_blob); ++i; &#125; return bboxes;&#125;//对模型输出结果进行解码及nmsstd::vector&lt;Detector::Bbox&gt; Detector::postProcess(const cv::Mat &amp;src_img, float *output) &#123; GenerateReferMatrix(); std::vector&lt;Detector::Bbox&gt; result; float *out = output; float ratio = std::max(float(src_img.cols) / float(INPUT_W), float(src_img.rows) / float(INPUT_H)); cv::Mat result_matrix = cv::Mat(refer_rows, NUM_CLASS + 4, CV_32FC1, out); for (int row_num = 0; row_num &lt; refer_rows; row_num++) &#123; Detector::Bbox box; auto *row = result_matrix.ptr&lt;float&gt;(row_num); auto max_pos = std::max_element(row + 4, row + NUM_CLASS + 4); box.prob = row[max_pos - row]; if (box.prob &lt; _cof_threshold) continue; box.classes = max_pos - row - 4; auto *anchor = refer_matrix.ptr&lt;float&gt;(row_num); box.x = (anchor[0] - row[0] * anchor[2] + anchor[0] + row[2] * anchor[2]) / 2 * ratio; box.y = (anchor[1] - row[1] * anchor[2] + anchor[1] + row[3] * anchor[2]) / 2 * ratio; box.w = (row[2] + row[0]) * anchor[2] * ratio; box.h = (row[3] + row[1]) * anchor[2] * ratio; result.push_back(box); &#125; NmsDetect(result); return result;&#125; 6&gt; 推理时间展示及预测结果展示 我的老笔记本平均推理时间 15ms 左右，CPU 下实时推理 安全帽检测结果 至此完成了 NanoDet 在 X86 CPU 上的部署，希望有帮助到大家。 （2）TensorRT 部署深度学习模型 原帖：https://zhuanlan.zhihu.com/p/84125533 1&gt; 背景 目前主流的深度学习框架（caffe，mxnet，tensorflow，pytorch 等）进行模型推断的速度都并不优秀，在实际工程中用上述的框架进行模型部署往往是比较低效的。而通过 Nvidia 推出的 tensorRT 工具来部署主流框架上训练的模型能够极大的提高模型推断的速度，往往相比与原本的框架能够有至少 1 倍以上的速度提升，同时占用的设备内存也会更加的少。因此对是所有需要部署模型的同志来说，掌握用 tensorRT 来部署深度学习模型的方法是非常有用的。 2&gt; 相关技术 上面的图片取自 TensorRT 的官网，里面列出了 tensorRT 使用的一些技术。可以看到比较成熟的深度学习落地技术：模型量化、动态内存优化、层的融合等技术均已经在 tensorRT 中集成了，这也是它能够极大提高模型推断速度的原因。总体来说 tensorRT 将训练好的模型通过一系列的优化技术转化为了能够在特定平台（GPU）上以高性能运行的代码，也就是最后图中生成的 Inference engine。目前也有一些其他的工具能够实现类似 tensorRT 的功能，例如 TVM，TensorComprehensions 也能有效的提高模型在特定平台上的推断速度，但是由于目前企业主流使用的都是 Nvidia 生产的计算设备，在这些设备上 nvidia 推出的 tensorRT 性能相比其他工具会更有优势一些。而且 tensorRT 依赖的代码库仅仅包括 C++ 和 cuda，相对与其他工具要更为精简一些。 3&gt; tensorflow 模型 tensorRT 部署教程 实际工程部署中多采用 c 进行部署，因此在本教程中也使用的是 tensorRT 的 CAPI，tensorRT 版本为 5.1.5。具体 tensorRT 安装可参考教程 [深度学习] TensorRT 安装，以及官网的安装说明。 （1）模型持久化 部署 tensorflow 模型的第一步是模型持久化，将模型结构和权重保存到一个.pb 文件当中。 123pb_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), [v.op.name for v in outputs])with tf.gfile.FastGFile(&#x27;./pbmodel_name.pb&#x27;, mode=&#x27;wb&#x27;) as f: f.write(pb_graph.SerializeToString()) 具体只需在模型定义和权重读取之后执行以上代码，调用 tf.graph_util.convert_variables_to_constants 函数将权重转为常量，其中 outputs 是需要作为输出的 tensor 的列表，最后用 pb_graph.SerializeToString () 将 graph 序列化并写入到 pb 文件当中，这样就生成了 pb 模型。 （2）生成 uff 模型 有了 pb 模型，需要将其转换为 tensorRT 可用的 uff 模型，只需调用 uff 包自带的 convert 脚本即可 1python /usr/lib/python2.7/site-packages/uff/bin/convert_to_uff.py pbmodel_name.pb 如转换成功会输出如下信息，包含图中总结点的个数以及推断出的输入输出节点的信息 （3）tensorRT c++ API 部署模型 使用 tensorRT 部署生成好的 uff 模型需要先讲 uff 中保存的模型权值以及网络结构导入进来，然后执行优化算法生成对应的 inference engine。具体代码如下，首先需要定义一个 IBuilder* builder，一个用来解析 uff 文件的 parser 以及 builder 创建的 network，parser 会将 uff 文件中的模型参数和网络结构解析出来存到 network，解析前要预先告诉 parser 网络输入输出输出的节点。解析后 builder 就能根据 network 中定义的网络结构创建 engine。在创建 engine 前会需要指定最大的 batchsize 大小，之后使用 engine 时输入的 batchsize 不能超过这个数值否则就会出错。推断时如果 batchsize 和设定最大值一样时效率最高。举个例子，如果设定最大 batchsize 为 10，实际推理输入一个 batch 10 张图的时候平均每张推断时间是 4ms 的话，输入一个 batch 少于 10 张图的时候平均每张图推断时间会高于 4ms。 123456789101112131415161718IBuilder* builder = createInferBuilder(gLogger.getTRTLogger());auto parser = createUffParser();parser-&gt;registerInput(inputtensor_name, Dims3(INPUT_C, INPUT_H, INPUT_W), UffInputOrder::kNCHW);parser-&gt;registerOutput(outputtensor_name); INetworkDefinition* network = builder-&gt;createNetwork(); if (!parser-&gt;parse(uffFile, *network, nvinfer1::DataType::kFLOAT)) &#123; gLogError &lt;&lt; &quot;Failure while parsing UFF file&quot; &lt;&lt; std::endl; return nullptr; &#125; builder-&gt;setMaxBatchSize(maxBatchSize); builder-&gt;setMaxWorkspaceSize(MAX_WORKSPACE); ICudaEngine* engine = builder-&gt;buildCudaEngine(*network); if (!engine) &#123; gLogError &lt;&lt; &quot;Unable to create engine&quot; &lt;&lt; std::endl; return nullptr; &#125; 生成 engine 之后就可以进行推断了，执行推断时需要有一个上下文执行上下文 IExecutionContext* context，可以通过 engine-&gt;createExecutionContext () 获得。执行推断的核心代码是 1context-&gt;execute(batchSize, &amp;buffers[0]); 其中 buffer 是一个 void * 数组对应的是模型输入输出 tensor 的设备地址，通过 cudaMalloc 开辟输入输出所需要的设备空间（显存）将对应指针存到 buffer 数组中，在执行 execute 操作前通过 cudaMemcpy 把输入数据（输入图像）拷贝到对应输入的设备空间，执行 execute 之后还是通过 cudaMemcpy 把输出的结果从设备上拷贝出来。 更为详细的例程可以参考 TensorRT 官方的 samples 中的 sampleUffMNIST 代码 （4）加速比情况 实际工程中我在 Tesla M40 上用 tensorRT 来加速过 Resnet-50，Inception-resnet-v2，谷歌图像检索模型 Delf（DEep Local Features），加速前后单张图推断用时比较如下图（单位 ms） 4&gt; Caffe 模型 tensorRT 部署教程 相比与 tensorflow 模型 caffe 模型的转换更加简单，不需要有 tensorflow 模型转 uff 模型这类的操作，tensorRT 能够直接解析 prototxt 和 caffemodel 文件获取模型的网络结构和权重。具体解析流程和上文描述的一致，不同的是 caffe 模型的 parser 不需要预先指定输入层，这是因为 prototxt 已经进行了输入层的定义，parser 能够自动解析出输入，另外 caffeparser 解析网络后返回一个 IBlobNameToTensor *blobNameToTensor 记录了网络中 tensor 和 pototxt 中名字的对应关系，在解析之后就需要通过这个对应关系，按照输出 tensor 的名字列表 outputs 依次找到对应的 tensor 并通过 network-&gt;markOutput 函数将其标记为输出，之后就可以生成 engine 了。 1234567891011121314IBuilder* builder = createInferBuilder(gLogger); INetworkDefinition* network = builder-&gt;createNetwork(); ICaffeParser* parser = createCaffeParser(); DataType modelDataType = DataType::kFLOAT; const IBlobNameToTensor *blobNameToTensor = parser-&gt;parse(deployFile.c_str(), modelFile.c_str(), *network, modelDataType); assert(blobNameToTensor != nullptr); for (auto&amp; s : outputs) network-&gt;markOutput(*blobNameToTensor-&gt;find(s.c_str())); builder-&gt;setMaxBatchSize(maxBatchSize); builder-&gt;setMaxWorkspaceSize(1 &lt;&lt; 30); engine = builder-&gt;buildCudaEngine(*network); 生成 engine 后执行的方式和上一节描述的一致，详细的例程可以参考 SampleMNIST （1）加速比情况 实际工程中我在 Tesla M40 上用 tensorRT 加速过 caffe 的 VGG19，SSD 速度变为 1.6 倍，ResNet50，MobileNetV2 加速前后单张图推断用时比较如下图（单位 ms） 5&gt; 为 tensorRT 添加自定义层 tensorRT 目前只支持一些非常常见的操作，有很多操作它并不支持比如上采样 Upsample 操作，这时候就需要我们自行将其编写为 tensorRT 的插件层，从而使得这些不能支持的操作能在 tensorRT 中使用。以定义 Upsample 层为例，我们首先要定义一个继承自 tensorRT 插件基类的 Upsample 类 1class Upsample: public IPluginExt 然后要实现该类的一些必要方法，首先是 2 个构造函数，一个是传参数构建，另一个是从序列化后的比特流构建。 12345678910111213141516 Upsample(int scale = 2) : mScale(scale) &#123; assert(mScale &gt; 0); &#125;//定义上采样倍数 Upsmaple(const void *data, size_t length) &#123; const char *d = reinterpret_cast&lt;const char *&gt;(data), *a = d; mScale = read&lt;int&gt;(d); mDtype = read&lt;DataType&gt;(d); mCHW = read&lt;DimsCHW&gt;(d); assert(mScale &gt; 0); assert(d == a + length); &#125;~Upsample() &#123; &#125; 一些定义层输出信息的方法 1234567891011 int getNbOutputs() const override &#123; return 1; &#125;//模型的输出个数 Dims getOutputDimensions(int index, const Dims *inputs, int nbInputDims) override &#123; // std::cout &lt;&lt; &quot;Get ouputdims!!!&quot; &lt;&lt; std::endl; assert(nbInputDims == 1); assert(inputs[0].nbDims == 3); return DimsCHW(inputs[0].d[0], inputs[0].d[1] * mScale, inputs[0].d[2] * mScale); &#125;//获取模型输出的形状 根据输入的形状个数以及采用的数据类型检查合法性以及配置层参数的方法 1234567891011121314 bool supportsFormat(DataType type, PluginFormat format) const override &#123; return (type == DataType::kFLOAT || type == DataType::kHALF || type == DataType::kINT8) &amp;&amp; format == PluginFormat::kNCHW; &#125;//检查层是否支持当前的数据类型和格式 void configureWithFormat(const Dims *inputDims, int nbInputs, const Dims *outputDims, int nbOutputs, DataType type, PluginFormat format, int maxBatchSize) override &#123; mDtype = type; mCHW.c() = inputDims[0].d[0]; mCHW.h() = inputDims[0].d[1]; mCHW.w() = inputDims[0].d[2]; &#125;//配置层的参数 层的序列化方法 123456789101112 size_t getSerializationSize() override &#123; return sizeof(mScale) + sizeof(mDtype) + sizeof(mCHW); &#125;//输出序列化层所需的长度 void serialize(void *buffer) override &#123; char *d = reinterpret_cast&lt;char *&gt;(buffer), *a = d; write(d, mScale); write(d, mDtype); write(d, mCHW); assert(d == a + getSerializationSize()); &#125;//将层参数序列化为比特流 层的运算方法 1234567 size_t getWorkspaceSize(int maxBatchSize) const override &#123; return 0; &#125;//层运算需要的临时工作空间大小 int enqueue(int batchSize, const void *const *inputs, void **outputs, void *workspace, cudaStream_t stream) override;//层执行计算的具体操作 在 enqueue 中我们调用编写好的 cuda kenerl 来进行 Upsample 的计算 完成了 Upsample 类的定义，我们就可以直接在网络中添加我们编写的插件了，通过如下语句我们就定义一个上采样 2 倍的上采样层。addPluginExt 的第一个输入是 ITensor** 类别，这是为了支持多输出的情况，第二个参数就是输入个数，第三个参数就是需要创建的插件类对象。 12Upsample up(2)；auto upsamplelayer=network-&gt;addPluginExt(inputtensot,1,up) 6&gt; 为 CaffeParser 添加自定义层支持 对于我们自定义的层如果写到了 caffe prototxt 中，在部署模型时调用 caffeparser 来解析就会报错。 还是以 Upsample 为例，如果在 prototxt 中有下面这段来添加了一个 upsample 的层 123456layer &#123; name: &quot;upsample0&quot; type: &quot;Upsample&quot; bottom: &quot;ReLU11&quot; top: &quot;Upsample1&quot;&#125; 这时再调用 1234const IBlobNameToTensor *blobNameToTensor = parser-&gt;parse(deployFile.c_str(), modelFile.c_str(), *network, modelDataType); 就会出现错误 之前我们已经编写了 Upsample 的插件，怎么让 tensorRT 的 caffe parser 识别出 prototxt 中的 upsample 层自动构建我们自己编写的插件呢？这时我们就需要定义一个插件工程类继承基类 nvinfer1::IPluginFactory, nvcaffeparser1::IPluginFactoryExt。 1class PluginFactory : public nvinfer1::IPluginFactory, public nvcaffeparser1::IPluginFactoryExt 其中必须要的实现的方法有判断一个层是否是 plugin 的方法，输入的参数就是 prototxt 中 layer 的 name，通过 name 来判断一个层是否注册为插件 12345678910111213bool isPlugin(const char *name) override &#123; return isPluginExt(name); &#125; bool isPluginExt(const char *name) override &#123; char *aa = new char[6]; memcpy(aa, name, 5); aa[5] = 0; int res = !strcmp(aa, &quot;upsam&quot;); return res;&#125;//判断层名字是否是upsample层的名字 根据名字创建插件的方法，有两中方式一个是由权重构建，另一个是由序列化后的比特流创建，对应了插件的两种构造函数，Upsample 没有权重，对于其他有权重的插件就能够用传入的 weights 初始化层。mplugin 是一个 vector 用来存储所有创建的插件层。 12345678910IPlugin *createPlugin(const char *layerName, const nvinfer1::Weights *weights, int nbWeights) override &#123; assert(isPlugin(layerName)); mPlugin.push_back(std::unique_ptr&lt;Upsample&gt;(new Upsample(2))); return mPlugin[mPlugin.size() - 1].get(); &#125;IPlugin *createPlugin(const char *layerName, const void *serialData, size_t serialLength) override &#123; assert(isPlugin(layerName)); return new Upsample(serialData, serialLength); &#125; std::vector &lt;std::unique_ptr&lt;Upsample&gt;&gt; mPlugin; 最后需要定义一个 destroy 方法来释放所有创建的插件层。 12345 void destroyPlugin() &#123; for (unsigned int i = 0; i &lt; mPlugin.size(); i++) &#123; mPlugin[i].reset(); &#125;&#125; 对于 prototxt 存在多个多种插件的情况，可以在 isPlugin，createPlugin 方法中添加新的条件分支，根据层的名字创建对应的插件层。 实现了 PluginFactory 之后在调用 caffeparser 的时候需要设置使用它，在调用 parser-&gt;parser 之前加入如下代码 12PluginFactory pluginFactory;parser-&gt;setPluginFactoryExt(&amp;pluginFactory); 就可以设置 parser 按照 pluginFactory 里面定义的规则来创建插件层，这样之前出现的不能解析 Upsample 层的错误就不会再出现了。 官方添加插件层的样例 samplePlugin 可以作为参考 7&gt; 心得体会（踩坑记录） 转 tensorflow 模型时，生成 pb 模型、转换 uff 模型以及调用 uffparser 时 register Input，output，这三个过程中输入输出节点的名字一定要注意保持一致，否则最终在 parser 进行解析时会出现错误，找不到输入输出节点。 除了本文中列举的 pluginExt，tensorRT 中插件基类还有 IPlugin，IPluginV2，继承这些基类所需要实现的类方法有细微区别，具体情况可自行查看 tensorRT 安装文件夹下的 include/NvInfer.h 文件。同时添加自己写的层到网络时的函数有 addPlugin，addPluginExt，addPluginV2 这几种和 IPlugin，IPluginExt，IPluginV2 一一对应，不能够混用，否则有些默认调用的类方法不会调用的，比如用 addPlugin 添加的 PluginExt 层是不会调用 configureWithFormat 方法的，因为 IPlugin 类没有该方法。同样的在还有 caffeparser 的 setPluginFactory 和 setPluginFactoryExt 也是不能混用的。 运行程序出现 cuda failure 一般情况下是由于将内存数据拷贝到磁盘时出现了非法内存访问，注意检查 buffer 开辟的空间大小和拷贝过去数据的大小是否一致. 有一些操作在 tensorRT 中不支持但是可以通过一些支持的操作进行组合替代，比如 ，这样可以省去一些编写自定义层的时间。 tensorflow 中的 flatten 操作默认时 keepdims=False 的，但是在转化 uff 文时会默认按照 keepdims=True 转换，因此在 tensorflow 中对 flatten 后的向量进行 transpose、expanddims 等等操作，在转换到 uff 后用 tensorRT 解析时容易出现错误，比如 “Order size is not matching the number dimensions of TensorRT” 。最好设置 tensorflow 的 reduce，flatten 操作的 keepdims=True，保持层的输出始终为 4 维形式，能够有效避免转到 tensorRT 时出现各种奇怪的错误。 tensorRT 中的 slice 层存在一定问题，我用 network-&gt;addSlice 给网络添加 slice 层后，在执行 buildengine 这一步时就会出错 nvinfer1::builder::checkSanity (const nvinfer1::builder::Graph&amp;): Assertion `tensors.size () == g.tensors.size ()’ failed.，构建网络时最好避开使用 slice 层，或者自己实现自定层来执行 slice 操作。 tensorRT 的 github 中有着部分的开源代码以及丰富的示例代码，多多学习能够帮助更快的掌握 tensorRT 的使用","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"模型部署","slug":"模型部署","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"}]},{"title":"ForgeryNet_目前公开最大的深度人脸伪造数据集CVPR2021","slug":"ForgeryNet-目前公开最大的深度人脸伪造数据集CVPR2021","date":"2022-03-03T14:55:49.000Z","updated":"2022-03-03T15:22:22.085Z","comments":true,"path":"2022/03/03/ForgeryNet-目前公开最大的深度人脸伪造数据集CVPR2021/","link":"","permalink":"https://leezhao415.github.io/2022/03/03/ForgeryNet-%E7%9B%AE%E5%89%8D%E5%85%AC%E5%BC%80%E6%9C%80%E5%A4%A7%E7%9A%84%E6%B7%B1%E5%BA%A6%E4%BA%BA%E8%84%B8%E4%BC%AA%E9%80%A0%E6%95%B0%E6%8D%AE%E9%9B%86CVPR2021/","excerpt":"","text":"文章目录 ForgeryNet | 目前公开最大的深度人脸伪造数据集 | CVPR 2021 oral ForgeryNet | 目前公开最大的深度人脸伪造数据集 | CVPR 2021 oral #CVPR 2021 oral## 伪造人脸分析# ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis 逼真的合成技术飞速发展，使真实图像与操纵图像之间的界限开始模糊。因此，基准化和推进数字伪造分析已经成为一个迫在眉睫的问题。 但现有的对此问题所需要的人脸伪造数据集要么多样性有限，要么只支持粗粒度的分析。 作者在本次工作中，针对上述挑战，构建了 ForgeryNet 数据集，是一个极其庞大的人脸伪造数据集，在图像和视频级数据中具有统一的标注，涵盖以下四个任务。 **Image Forgery Classification：** 图像伪造分类，包括双向（真 / 假）、三向（真 / 假与身份置换的伪造方法 / 假与身份保留的伪造方法）和 n 向（真和 15 种各自的伪造方法）分类。 **Spatial Forgery Localization：** 空间伪造定位，将伪造图像的操纵区域与其对应的源头真实图像进行对比分割。 **Video Forgery Classification：** 视频伪造分类，对视频级作假分类进行重新定义，在随机位置上进行操纵帧。因为现实世界中的攻击者可以自由操纵任何目标帧。 **Temporal Forgery Localization：** 时间伪造定位，对被操纵的时间段进行定位。 ForgeryNet 是目前公开的最大的深度人脸伪造数据集，从数据规模（290 万张图像，221247 个视频）、manipulations 操纵（7 种图像级方法，8 种视频级方法）、perturbations 扰动（36 种独立的和更多的混合扰动）和标注（630 万个分类标签，290 万个操纵区域标注和 221247 个时空伪造段标签）来看，它都是最大的。 Forgery 作者 | Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, Ziwei Liu 单位 | 北京邮电大学；商汤；中国科学技术大学；南洋理工大学 论文 | ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis 主页 | ForgeryNet Dataset 备注 | CVPR 2021 oral 52CV/CVPR-2021-Papersgithub.com/52CV/CVPR-2021-Papers)","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"数据集","slug":"数据集","permalink":"https://leezhao415.github.io/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"}]},{"title":"部署方案之模型部署概述","slug":"部署方案之模型部署概述","date":"2022-02-21T14:00:10.000Z","updated":"2022-02-21T15:03:29.124Z","comments":true,"path":"2022/02/21/部署方案之模型部署概述/","link":"","permalink":"https://leezhao415.github.io/2022/02/21/%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88%E4%B9%8B%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%A6%82%E8%BF%B0/","excerpt":"","text":"文章目录 部署方案之模型部署概述 1 模型部署场景 2 模型部署方式 3 部署的核心优化指标 4 部署流程 部署方案之模型部署概述 模型训练重点关注的是如何通过训练策略来得到一个性能更好的模型，其过程似乎包含着各种 “玄学”，被戏称为 “炼丹”。整个流程包含从训练样本的获取（包括数据采集与标注），模型结构的确定，损失函数和评价指标的确定，到模型参数的训练，这部分更多是业务方去承接相关工作。一旦 “炼丹” 完成（即训练得到了一个指标不错的模型），如何将这颗 “丹药” 赋能到实际业务中，充分发挥其能力，这就是部署方需要承接的工作。 因此，一般来说，学术界负责各种 SOTA (State of the Art) 模型的训练和结构探索，而工业界负责将这些 SOTA 模型应用落地，赋能百业。本文将要讲述的是，在 CV 场景中，如何实现模型的快速落地，赋能到产业应用中。模型部署一般无需再考虑如何修改训练方式或者修改网络结构以提高模型精度，更多的是需要明确部署的场景、部署方式（中心服务化还是本地终端部署）、模型的优化指标，以及如何提高吞吐率和减少延迟等，接下来将逐一进行介绍。 1 模型部署场景 这个问题主要源于中心服务器云端部署和边缘部署两种方式的差异 云端部署常见的模式是，模型部署在云端服务器，用户通过网页访问或者 API 接口调用等形式向云端服务器发出请求，云端收到请求后处理并返回结果。边缘部署则主要用于嵌入式设备，主要通过将模型打包封装到 SDK，集成到嵌入式设备，数据的处理和模型推理都在终端设备上执行。 2 模型部署方式 针对上面提到的两种场景，分别有两种不同的部署方案，Service 部署和 SDK 部署。 Service 部署：主要用于中心服务器云端部署，一般直接以训练的引擎库作为推理服务模式。SDK 部署：主要用于嵌入式端部署场景，以 C++ 等语言实现一套高效的前后处理和推理引擎库（高效推理模式下的 Operation/Layer/Module 的实现），用于提供高性能推理能力。此种方式一般需要考虑模型转换（动态图静态化）、模型联合编译等进行深度优化 3 部署的核心优化指标 部署的核心目标是合理把控成本、功耗、性价比三大要素。 成本问题是部署硬件的重中之重，AI 模型部署到硬件上的成本将极大限制用户的业务承受能力。成本问题主要聚焦于芯片的选型，比如，对比寒武纪 MLU220 和 MLU270，MLU270 主要用作数据中心级的加速卡，其算力和功耗都相对于边缘端的人工智能加速卡 MLU220 要低。至于 Nvida 推出的 Jetson 和 Tesla T4 也是类似思路，Tesla T4 是主打数据中心的推理加速卡，而 Jetson 则是嵌入式设备的加速卡。对于终端场景，还会根据对算力的需求进一步细分，比如表中给出的高通骁龙芯片，除 GPU 的浮点算力外，还会增加 DSP 以增加定点算力，篇幅有限，不再赘述，主要还是根据成本和业务需求来进行权衡。 在数据中心服务场景，对于功耗的约束要求相对较低；在边缘终端设备场景，硬件的功耗会影响边缘设备的电池使用时长。因此，对于功耗要求相对较高，一般来说，利用 NPU 等专用优化的加速器单元来处理神经网络等高密度计算，能节省大量功耗。 不同的业务场景对于芯片的选择有所不同，以达到更高的性价比。从公司业务来看，云端相对更加关注是多路的吞吐量优化需求，而终端场景则更关注单路的延时需要。在目前主流的 CV 领域，低比特模型相对成熟，且 INT8/INT4 芯片因成本低，且算力比高的原因已被广泛使用；但在 NLP 或者语音等领域，对于精度的要求较高，低比特模型精度可能会存在难以接受的精度损失，因此 FP16 是相对更优的选择。在 CV 领域的芯片性价比选型上，在有 INT8/INT4 计算精度的芯片里，主打低精度算力的产品是追求高性价比的主要选择之一，但这也为平衡精度和性价比提出了巨大的挑战。 4 部署流程 上面简要介绍了部署的主要方式和场景，以及部署芯片的选型考量指标，接下来以 SDK 部署为例，给大家概括介绍一下 SenseParrots 在部署中的整体流程。SenseParrots 部署流程大致分为以下几个步骤：模型转换、模型量化压缩、模型打包封装 SDK。 4.1 模型转换 模型转换主要用于模型在不同框架之间的流转，常用于训练和推理场景的连接。目前主流的框架都以 ONNX 或者 caffe 为模型的交换格式，SenseParrots 也不例外。SenseParrots 的模型转换主要分为计算图生成和计算图转换两大步骤，另外，根据需要，还可以在中间插入计算图优化，对计算机进行推理加速（诸如常见的 CONV/BN 的算子融合）。 计算图生成是通过一次 inference 并追踪记录的方式，将用户的模型完整地翻译成静态的表达。在模型 inference 的过程中，框架会记录执行算子的类型、输入输出、超参、参数和调用该算子的模型层次，最后把 inference 过程中得到的算子信息和模型信息结合得到最终的静态计算图。 在计算图生成之后与计算图转换之前，可以进行计算图优化，例如去除冗余 op，计算合并等。SenseParrots 原生实现了一批计算图的精简优化 pass，也开放接口鼓励用户对计算图进行自定义的处理和优化操作。 计算图转换是指分析静态计算图的算子，对应转换到目标格式。SenseParrots 支持了多后端的转换，能够转换到各个 opset 的 ONNX、原生 caffe 和多种第三方版本的 caffe。框架通过算子转换器继承或重写的方式，让 ONNX 和 caffe 的不同版本的转换开发变得更加简单。同时，框架开放了自定义算子生成和自定义算子转换器的接口，让第三方框架开发者也能够轻松地自主开发实现 SenseParrots 到第三方框架的转换。 4.2 模型量化压缩 终端场景中，一般会有内存和速度的考虑，因此会要求模型尽量小，同时保证较高的吞吐率。除了人工针对嵌入式设备设计合适的模型，如 MobileNet 系列，通过 NAS (Neural Architecture Search) 自动搜索小模型，以及通过蒸馏 / 剪枝的方式压缩模型外，一般还会使用量化来达到减小模型规模和加速的目的。 量化的过程主要是将原始浮点 FP32 训练出来的模型压缩到定点 INT8 (或者 INT4/INT1) 的模型，由于 INT8 只需要 8 比特来表示，因此相对于 32 比特的浮点，其模型规模理论上可以直接降为原来的 1/4，这种压缩率是非常直观的。另外，大部分终端设备都会有专用的定点计算单元，通过低比特指令实现的低精度算子，速度上会有很大的提升，当然，这部分还依赖协同体系结构和算法来获得更大的加速。 量化的技术栈主要分为量化训练（QAT, Quantization Aware Training）和离线量化（PTQ, Post Training Quantization）, 两者的主要区别在于，量化训练是通过对模型插入伪量化算子（这些算子用来模拟低精度运算的逻辑），通过梯度下降等优化方式在原始浮点模型上进行微调，从来调整参数得到精度符合预期的模型。离线量化主要是通过少量校准数据集（从原始数据集中挑选 100-1000 张图，不需要训练样本的标签）获得网络的 activation 分布，通过统计手段或者优化浮点和定点输出的分布来获得量化参数，从而获取最终部署的模型。两者各有优劣，量化训练基于原始浮点模型的训练逻辑进行训练，理论上更能保证收敛到原始模型的精度，但需要精细调参且生产周期较长；离线量化只需要基于少量校准数据，因此生产周期短且更加灵活，缺点是精度可能略逊于量化训练。实际落地过程中，发现大部分模型通过离线量化就可以获得不错的模型精度（1% 以内的精度损失，当然这部分精度的提升也得益于优化策略的加持），剩下少部分模型可能需要通过量化训练来弥补精度损失，因此实际业务中会结合两者的优劣来应用。 量化主要有两大难点：一是如何平衡模型的吞吐率和精度，二是如何结合推理引擎充分挖掘芯片的能力。比特数越低其吞吐率可能会越大，但其精度损失可能也会越大，因此，如何通过算法提升精度至关重要，这也是组内的主要工作之一。另外，压缩到低比特，某些情况下吞吐率未必会提升，还需要结合推理引擎优化一起对模型进行图优化，甚至有时候会反馈如何进行网络设计，因此会是一个算法与工程迭代的过程。 4.3 模型打包封装 SDK 实际业务落地过程中，模型可能只是产品流程中的一环，用于实现某些特定功能，其输出可能会用于流程的下一环。因此，模型打包会将模型的前后处理，一个或者多个模型整合到一起，再加入描述性的文件（前后处理的参数、模型相关参数、模型格式和版本等）来实现一个完整的功能。因此，SDK 除了需要一些通用前后处理的高效实现，对齐训练时的前后处理逻辑，还需要具有足够好的扩展性来应对不同的场景，方便业务线同学扩展新的功能。可以看到，模型打包过程更多是模型的进一步组装，将不同模型组装在一起，当需要使用的时候将这些内容解析成整个流程（pipeline）的不同阶段（stage），从而实现整个产品功能。 另外，考虑到模型很大程度是研究员的研究成果，对外涉及保密问题，因此会对模型进行加密，以保证其安全性。加密算法的选择需要根据实际业务需求来决定，诸如不同加密算法其加解密效率不一样，加解密是否有中心验证服务器，其核心都是为了保护研究成果。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"模型部署","slug":"模型部署","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"}]},{"title":"【精华】目标检测模型对比","slug":"【精华】目标检测模型对比","date":"2022-02-20T15:08:01.000Z","updated":"2022-02-21T13:57:59.929Z","comments":true,"path":"2022/02/20/【精华】目标检测模型对比/","link":"","permalink":"https://leezhao415.github.io/2022/02/20/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94/","excerpt":"","text":"文章目录 YOLO fastest/YOLOX/YOLO fastestv2/Nanodet/Nanodet Plus 模型对比 （1）网络结构 （2）模型结构差异（优化模块） （3）模型性能 （4）关键概念解析 YOLO fastest/YOLOX/YOLO fastestv2/Nanodet/Nanodet Plus 模型对比 YOLO fastest Paper Github 库 YOLOX-Nano Paper Github 库 YOLO fastestv2 Paper Github 库 NanoDet Paper Github 库 NanoDet Plus Paper Github 库 （1）网络结构 1&gt; YOLO fastest 2&gt; YOLOX-Nano 3&gt; YOLO fastestv2 4&gt; NanoDet 5&gt; NanoDet Plus （2）模型结构差异（优化模块） 1&gt; YOLO fastest Backbone EfficientNet-lite 注重单核的实时推理性能，在满足实时的条件下的低 CPU 占用，不单单能在手机移动端达到实时，还要在 RK3399，树莓派 4 以及多种 Cortex-A53 低成本低功耗设备上满足一定实时性，毕竟这些嵌入式的设备相比与移动端手机要弱很多，但是使用更加广泛，成本更加低廉。 2&gt; YOLOX-Nano Backbone ：YOLOv3（Darknet-53） Head ：使用解耦合检测头 标签匹配策略 ：SimOTA 3&gt; YOLO fastestv2 Backbone Shufflenetv2（相比于 EfficientNet-lite，访存减少了，更加轻量） Anchor 匹配机制：参考 YOLOv5 Head ：参考 YOLOX，使用解耦合检测头。 检测框的回归、前景背景的分类、检测类别的分类 前景背景的分类以及检测类别的分类采用同一网络分支参数共享 检测类别分类的 loss 由 sigmoid 替换为 softmax 输出尺度由 3 个变为 2 个：（11x11、22x22、44x44）变为（11x11、22x22） 4&gt; NanoDet 项目思路 大模型发展历程：Two stage 到 One stage，Anchor-base 到 Anchor-free，Transformer 移动端目标检测：YOLO 系列和 SSD 等 Anchor-base 模型 NanoDet 项目：希望能够开源一个移动端实时的 Anchor-free 的检测模型。能够提供不亚于 YOLO 系列的性能，而且同样方便训练和移植。 思路一：将 FCOS 轻量化 &lt;原因：FCOS 的 centerness 分支在轻量化模型上很难收敛&gt;（效果不佳，不如 MobileNet+YOLOv3） 思路二：GFocalLoss 完美去掉了 FCOS 系列的 centerness 分支，省去了这一分支上的大量卷积，减少了检测头的计算开销，非常适合移动端的轻量化部署。 Backbone ： 尝试了 Mbilenet 系列、GhostNet、Shufflenet、EfficientNet 使用 Shufflenet v2 ：权衡参数量、计算量以及权重大小，该模型在相似精度下体积最小，而且对移动端 CPU 推理比较友好。 使用 Shufflenetv2 1.0x 作为 Backbone，去掉最后一层卷积，并且抽取 8、16、32 倍下采样的特征输入进 PAN 做多尺度的特征融合。 Neck ：PAFPN BiFPN：EfficientDet （性能强大，但堆叠的特征融合操作势必会带来运行速度的降低） PAN：YOLOv4/YOLOv5 （只有自下而上和自上而下的两条通路，非常简洁，是轻量化模型特征融合的不二选择） BalancedFPN PAFPN 完全去掉 PAN 中的所有卷积，只保留从骨干网络特征提取后的 1x1 卷积来进行特征通道维度的对齐，上采样和下采样均使用插值来完成。 与 yolo 使用的 concatenate 操作不同，将多尺度的 Feature Map 直接相加，使得整个特征融合模块的计算量变得非常非常小。 Head 使用 2 个深度可分离卷积模块同时预测分类和回归，并将卷积堆叠的数量从 4 个减少到 2 组，通道由 256 压缩到 96 维（大模型中使用 4 组 256channel 的 3x3 卷积预测分类和回归） 检测头不共享权重：取消 FCOS 系列模型的共享权重策略，由于移动端模型推理由 CPU 进行计算，共享权重并不会对推理过程进行加速，而且在检测头非常轻量的情况下，共享权重使得其检测能力进一步下降，因此还是选择每一层特征使用一组卷积比较合适。 用 BN 代替 GN (Group Normalization)：在推理时能够将其归一化的参数直接融合进卷积中，节省归一化时间。 标签匹配策略 ATSS：根据 IOU 的均值和方差为每一层 feature map 动态选取匹配样本（本质上依然时基于先验信息（中心点和 Anchor）的静态匹配策略） 在每个 FPN 层选取离 gt 框中心点最近的 k 个 anchor，之后对所有选取的 anchor 与 gt 计算 IOU，同时计算 IOU 均值和方差，最后保留 IOU 大于均值加方差的并且中心点在 gt 之内的 anchor 作为正样本。 训练策略 SGD+momentum+MutiStepLr 5&gt; NanoDet Plus Backbone FBNetv5/PicoDet：ESNet（使用 NAS 搜索，在约束了计算量参数量和精度的搜索空间内搜出强的 Backbone） NanoDet Plus：沿用 NanoDet 的 Backbone，后期可修改为 ESNet。（算力霸权下妥协） Neck YOLOX/PicoDet/YOLOv5：CSP-PAN NanoDet： Ghost-PAN （GhostNet 中的 GhostBlock（1x1 和 3x3 的 depthwise））(mAP 提升 2%) Head ThunderNet：轻量级模型中将深度可分离卷积的 depthwise 部分从 3x3 改成 5x5（增加较少参数量的同时提升检测器感受野并提升性能） PicoDet：在原本 NanoDet 的 3 层特征基础上增加一层下采样特征 NanoDet Plus：沿用通用技巧，将检测头的 depthwise 卷积的卷积核大小改成 5x5，并在 NanoDet 的 3 层特征基础上增加一层下采样特征。（mAP 提升 0.7%） 标签匹配策略 （使用 AGM（Assign Guidance Module）并配合动态的软标签分配策略 DSLA（ynamic Soft Label Assigner）来解决轻量级模型中的最优标签匹配问题）(mAP 提升 2.1%) 使用 AGM 预测的分类概率和检测框会送入 DSLA 模块计算 Matching Cost。Cost 函数由三部分组成：classification cost，regression cost 以及 distance cost： 最终的代价函数就是这样： 训练策略 优化器：SGD+momentum 改成 AdamW（对超参数更不敏感且收敛更快） 学习率衰减策略：从 MultiStepLr 改成 CosineAnnealingLR，反向传播计算梯度时加了梯度裁剪。 其他：增加模型平滑策略 EMA 部署优化 NanoDet：使用多尺度检测头，每层都有分类和回归两个输出，加上有三个尺度的特征图，共有 6 个输出。（对不熟悉模型结构的人不友好） NanoDet Plus：将模型输出合为一个，所有的输出 Tensor 都提前 reshape，然后 concatenate 到一起。（略微影响后处理速度，但模型友好） （3）模型性能 YOLO fastest 官方库 Network COCO mAP(0.5) Resolution Run Time(Ncnn 4xCore) Run Time(Ncnn 1xCore) FLOPS Params Weight size Yolo-Fastest-1.1 24.40 % 320X320 5.59 ms 7.52 ms 0.252BFlops 0.35M 1.4M Yolo-Fastest-1.1-xl 34.33 % 320X320 9.27ms 15.72ms 0.725BFlops 0.925M 3.7M Yolov3-Tiny-Prn 33.1% 416X416 %ms %ms 3.5BFlops 4.7M 18.8M Yolov4-Tiny 40.2% 416X416 23.67ms 40.14ms 6.9 BFlops 5.77M 23.1M Yolo-Fastest-1.1 Multi-platform benchmark Equipment Computing backend System Framework Run time Mi 11 Snapdragon 888 Android(arm64) ncnn 5.59ms Mate 30 Kirin 990 Android(arm64) ncnn 6.12ms Meizu 16 Snapdragon 845 Android(arm64) ncnn 7.72ms Development board Snapdragon 835(Monkey version) Android(arm64) ncnn 20.52ms Development board RK3399 Linux(arm64) ncnn 35.04ms Raspberrypi 3B 4xCortex-A53 Linux(arm64) ncnn 62.31ms Orangepi Zero Lts H2+ 4xCortex-A7 Linux(armv7) ncnn 550ms Nvidia Gtx 1050ti Ubuntu(x64) darknet 4.73ms Intel i7-8700 Ubuntu(x64) ncnn 5.78ms Pascal VOC performance index comparison Network Model Size mAP(VOC 2007) FLOPS Tiny YOLOv2 60.5MB 57.1% 6.97BFlops Tiny YOLOv3 33.4MB 58.4% 5.52BFlops YOLO Nano 4.0MB 69.1% 4.51Bflops MobileNetv2-SSD-Lite 13.8MB 68.6% &amp;Bflops MobileNetV2-YOLOv3 11.52MB 70.20% 2.02Bflos Pelee-SSD 21.68MB 70.09% 2.40Bflos Yolo Fastest 1.3MB 61.02% 0.23Bflops Yolo Fastest-XL 3.5MB 69.43% 0.70Bflops MobileNetv2-YOLOv3-Lite 8.0MB 73.26% 1.80Bflops YOLOX-Nano 官方库 Model size mAPval 0.5:0.95 Params (M) FLOPs (G) weights YOLOX-Nano 416 25.8 0.91 1.08 github YOLOX-Tiny 416 32.8 5.06 6.45 github NanoDet 官方库 Model Resolution mAPval 0.5:0.95 CPU Latency (i7-8700) ARM Latency (4xA76) FLOPS Params Model Size NanoDet-m 320*320 20.6 4.98ms 10.23ms 0.72G 0.95M 1.8MB(FP16) | 980KB(INT8) NanoDet-m 416*416 21.7 16.44ms 1.2G 0.95M 1.8MB(FP16) | 980KB(INT8) NanoDet-Plus-m 320*320 27.0 5.25ms 11.97ms 0.9G 1.17M 2.3MB(FP16) | 1.2MB(INT8) NanoDet-Plus-m 416*416 30.4 8.32ms 19.77ms 1.52G 1.17M 2.3MB(FP16) | 1.2MB(INT8) NanoDet-Plus-m-1.5x 320*320 29.9 7.21ms 15.90ms 1.75G 2.44M 4.7MB(FP16) | 2.3MB(INT8) NanoDet-Plus-m-1.5x 416*416 34.1 11.50ms 25.49ms 2.97G 2.44M 4.7MB(FP16) | 2.3MB(INT8) YOLOv3-Tiny 416*416 16.6 - 37.6ms 5.62G 8.86M 33.7MB YOLOv4-Tiny 416*416 21.7 - 32.81ms 6.96G 6.06M 23.0MB YOLOX-Nano 416*416 25.8 - 23.08ms 1.08G 0.91M 1.8MB(FP16) YOLOv5-n 640*640 28.4 - 44.39ms 4.5G 1.9M 3.8MB(FP16) FBNetV5 320*640 30.4 - - 1.8G - - MobileDet 320*320 25.6 - - 0.9G - - YOLO fastestv2 官方库 Network COCO mAP(0.5) Resolution Run Time(4xCore) Run Time(1xCore) FLOPs(G) Params(M) Yolo-FastestV2 24.10 % 352X352 3.29 ms 5.37 ms 0.212 0.25M Yolo-FastestV1.1 24.40 % 320X320 4.23 ms 7.54 ms 0.252 0.35M Yolov4-Tiny 40.2% 416X416 26.00ms 55.44ms 6.9 5.77M 1&gt; YOLO fastest 初衷就是打破算力的瓶颈，能在更多的低成本的边缘端设备实时运行目标检测算法。 基于 NCNN 推理框架开启 BF16s，在树莓派 3b，4 核 A53 1.2Ghz，320x320 图像单次推理时间在 60ms。 在性能更加强劲的树莓派 4b，单次推理 33ms，达到了 30fps 的全实时。 而相比较下应用最广泛的轻量化目标检测算法 MobileNet-SSD 要在树莓派 3b 跑 200ms 左右，Yolo-Fastest 速度整整要快 3 倍 +，而且模型才只有 1.3MB，而 MobileNet-SSD 模型达到 23.2MB，Yolo-Fastest 整整比它小了 20 倍，当然这也是有代价的，在 Pascal voc 上的 mAP，MobileNet-SSD 是 72.7，Yolo-Fastest 是 61.2，带来了接近 10 个点的精度损失 总结：YOLO-Fastest 是个牺牲一定精度 （大约 5% 的 mAP）、大幅提升速度的目标检测模型。 2&gt; YOLOX-Nano 对于 YOLO-Nano，所提方法仅需 0.91M 参数 + 1.08G FLOPs 取得了 25.3% AP 指标，以 1.8% 超越了 NanoDet； 对于 YOLOv3，所提方法将指标提升到了 47.3%，以 3% 超越了当前最佳； 具有与 YOLOv4-CSP、YOLOv5-L 相当的参数量，YOLOX-L 取得了 50.0% AP 指标同事具有 68.9fps 推理速度 (Tesla V100)，指标超过 YOLOv5-L 1.8%; 值得一提的是，YOLOX-L 凭借单模型取得了 Streaming Perception (Workshop on Autonomous Driving at CVPR 2021) 竞赛冠军。 3&gt; YOLO fastestv2 用 0.3% 的精度损失换取 30% 推理速度的提升以及 25% 的参数量的减少 4&gt; NanoDet 在经过对 one-stage 检测模型三大模块（Head、Neck、Backbone）都进行轻量化之后，得到了目前开源的 NanoDet-m 模型，在 320x320 输入分辨率的情况下，整个模型的 Flops 只有 0.72B，而 yolov4-tiny 则有 6.96B，小了将近十倍！模型的参数量也只有 0.95M，权重文件在使用 ncnn optimize 进行 16 位存储之后，只有 1.8mb，非常适合在移动端部署，能够有效减少 APP 体积，同时也对更低端的嵌入式设备更加友好。 尽管模型非常的轻量，但是性能却依旧强劲。对于小模型，往往选择使用 AP50 这种比较宽容的评价指标进行对比，这里我选择用更严格一点的 COCO mAP (0.5:0.95) 作为评估指标，同时兼顾检测和定位的精度。在 COCO val 5000 张图片上测试，并没有使用 Testing-Time-Augmentation 的情况下，320 分辨率输入能够达到 20.6 的 mAP，比 tiny-yolov3 高 4 分，只比 yolov4-tiny 低 1 个百分点，而将输入分辨率与 yolo 保持一致，都使用 416 输入的情况下，得分持平。 最后用 ncnn 部署到手机上之后跑了一下 benchmark，模型前向计算时间只要 10 毫秒左右，对比 yolov3 和 v4 tiny，均在 30 毫秒的量级。在安卓摄像头 demo app 上，算上图片预处理，检测框后处理以及绘制检测框的时间，也能轻松跑到 40+FPS~。 5&gt; NanoDet Plus NanoDet Plus 与上一代 NanoDet 相比，在仅增加 1 毫秒多的延时的情况下，精度提升了 30%。 改进了代码和架构，提出了一种非常简单的训练辅助模块，使模型变得更易训练，同时新版本也更易部署。 （4）关键概念解析 1&gt; 基于 Matching Cost 的动态匹配 简单来说，就是直接使用模型检测头的输出，与每一个 Ground Truth 计算一个匹配的代价，这个代价一般由分类 loss 和回归 loss 组成。Feature Map 上所有的点（N 个）的预测值与所有的 Ground Truth（M 个）计算得到的 NxM 的矩阵，就是所谓的 Cost Matrix，基于这个 Cost Matrix 进行二分图匹配也好还是传输优化也好再或者直接取 TopK 也好，就是一种动态匹配策略。这种策略与之前的基于 Anchor 算 IOU 的匹配最大的不同就是，它不再只依赖先验的静态的信息，而是使用当前的预测结果去动态寻找最优的匹配，只要模型预测的越准确，匹配算法求得的结果也会更优秀。 2&gt; 标签匹配策略 基于位置 基于 Anchor IOU 基于 Matching Cost（直接使用检测头的输出与每一个 Ground Truth 计算一个匹配的代价（分类 Loss 和回归 Loss）） 基于全局的动态匹配策略 DETR ：使用匈牙利匹配算法进行双边匹配 OTA ：使用 Sinkhorn 迭代求解匹配中的最优传输问题（位置约束：使用 5x5 的中心区域限制匹配自由度） YOLOX ：使用 OTA 的近似算法 SimOTA（位置约束：使用 5x5 的中心区域限制匹配自由度） ---- ：使用 LAD（Label Assignment Distillation）用教室网络的结果计算标签匹配来指导学生网络的训练 IQDet ：使用 QDE 模块对每个实例预测 PAA 中提出的高斯混合质量分布的三个参数来指导检测头的训练 NanoDet Plus ：使用 AGM（Assign Guidance Module）并配合动态的软标签分配策略 DSLA（ynamic Soft Label Assigner）来解决轻量级模型中的最优标签匹配问题 3&gt; 主要是指检测算法在训练阶段，如何给特征图上的每个位置进行合适的学习目标的表示，以及如何进行正负样本的分配的。 算法类型 先验 学习目标的表示 正负样本的分配 anchor box anchor box bounding box IoU anchor point center 高斯等 高斯热图等 key point point representative points feature map bin 和 IoU 等 set prediction embedding bounding box Hungarian 算法 （1）ATSS 论文标题： Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection 论文链接 | 代码链接 这篇文章从 anchor-free 和 anchor-base 算法的本质区别出发，通过分析对比 anchor-base 经典算法 retinanet 和 anchor-free 经典算法 FCOS 来说明正负样本分配（label assignment）的重要性。 如上图所示，RetinaNet 使用 IoU 阈值来区分正负 anchor box，处于中间 anchor box 的全部忽略。FCOS 使用空间（spatial）和尺寸（scale）限制来区分正负 anchor point，正样本首先选择在 GT box 内的 anchor points，其次选择 GT 尺寸对应的层 anchor points，其余均为负样本。 最后通过交叉实验，发现在相同正负样本定义下情况下，RetinaNet 和 FCOS 性能几乎一样，而且 spatila and scale constraint 的方式比 IOU 的效果好，如下表： 因此 ATSS 提出了一种新的正负样本选取方式，这种方法几乎不会引入额外的超参数并且更加鲁棒。 主要就是在每个 FPN 层选取离 gt 框中心点最近的 k 个 anchor，之后对所有选取的 anchor 与 gt 计算 IOU，同时计算 IOU 均值和方差，最后保留 IOU 大于均值加方差的并且中心点在 gt 之内的 anchor 作为正样本。 根据下表可以发现，即使 anchor box 数量为 1 的 RetinaNet 和 FOCS 在都加上 ATSS 策略之后，效果都有明显的提升，这也证明了 ATSS 策略的有效性。 （2）SAPD 论文标题： Soft Anchor-Point Object Detection 论文链接 | 代码链接 SAPD 就是对 anchor-free 检测器中的 anchor-point 检测器进行了训练策略的改进。SAPD 分析了两个问题：注意力偏差（attention bias）和特征选择（feature selection）。其中，特征选择的问题对金字塔特征层级做软选择，这里就不深入了。而为了解决注意力偏差（attention bias），SAPD 使用了一个新颖的训练策略：Soft-weighted anchor points。 3.1 Attention bias 注意力偏差 在自然图像中，可能会出现遮挡、背景混乱等干扰，SAPD 发现原始的 anchor-point 检测器在处理这些具有挑战性的场景时存在注意力偏差的问题，即具有清晰明亮 views 的目标会生成过高的得分区域，从而抑制了周围的其他目标的得分区域。 这个问题是由于特征不对齐导致了靠近目标边界的位置会得到不必要的高分 所导致的。 3.2 Soft-weighted anchor points 将目标实际位置与 anchor point（也就是 center）的距离作为一个 anchor 的惩罚权重，加入到损失函数的计算中（仅针对正样本，负样本不做改动）。公式如下： 其中，η 控制递减幅度，权重 范围为 0~1，公式保证了目标边界处的 points 权重为 0，目标中心处的 ponit 权重为 1。 这种通过对 anchor points 做软加权，就是 label assign 的进行优化，减少对靠近边界包含大量背景信息的锚点的关注。 （3）AutoAssign 论文标题： AutoAssign: Differentiable Label Assignment for Dense Object Detection 论文链接 AutoAssign 对 label assignment 进行非常全面的讨论。主要解决了在给定一个 bounding box （x, y, w, h） 后，根据框内的物体形状，动态分配正负样本的问题。如下图所示： （1）RetinaNet 是根据 anchor box 和 ground truth 的 IOU 阈值定义正负样本，这样会每个样本都是打上非正即负以及 ignore 的标签，而且 anchor box 的 num，size，aspect ratios 等等都是超参数； （2）FCOS 通过 centerness、空间和尺度约束来分配正负样本，也引入了很多超参数； （3）AutoAssign 将 label assignment 看做一种连续问题，没有真正意义上的正负样本之分，每个特征图上的位置都有正样本属性和负样本属性，只是权重不同罢了；而且如上图最左变所示，动态分配正负样本更符合目标的形状，可以说有利用分割做检测的思想。 下面是 AutoAssign 的正负样本分配的示意图： 可以看到，比一般的检测算法多了一个 Implict Objectness 分支，用于背景与前景的判断，已解决引入的大量背景位置的问题。 （1）Center Weighting 先使用高斯中心先验确定图像中一个目标正负样本的权重： （2）Confidence Weighting 通过 ImpObj 分支来避免引入大量背景位置 与 FreeAnchor 相似，将分类和定位联合看成极大似然估计问题，学习出样本的置信度 Confidence Weighting，即下面的 C (Pi)： 直观的理解 C (Pi) 就是，分类得分高、框预测的准的 location 拥有较大的 C (Pi) 值的概率就会高。 （3）正负样本的权重（w+/w-） positive weights：通过 Center Weighting 和 Confidence Weighting 得到 Positive weights negative weights：通过最大 IOU 得到 Negative weights 对于前景和背景的 weighting function，有一个共同的特点是 “单调递增”；这就保证了一个位置预测 pos /neg 的置信度越高，那么对应的权重就越大。 （4）loss function 有了对于正负样本的权重之后，对于一个 gt box，其 loss 如下： Positive weights 和 Negative weights 在训练过程中动态调整达到平衡，像是在学该目标的形状。 （4）DETR 论文标题： End-to-End Object Detection with Transformers 论文链接 | 代码链接 4.1 Object detection set prediction DETR 将目标检测任务视为一个图像到集合（image-to-set）的问题，即给定一张图像，模型的预测结果是一个包含了所有目标的无序集合。 那么对于一个目标 ground truth，如何找到对应的 prediction 呢？Detr 用的是 Hungarian algorithm 实现预测值与真值实现最大的匹配，并且是一一对应。 假设有 4 个 prediction（a,b,c,d），有 4 个 ground truth（p,q,r,s），每个 prediction 匹配 ground truth 的好坏都不同，那么便可构造一个代价矩阵（cost matrix，是 cost_bbox、cost_class 和 cost_giou 的加权和），通过求解最优的分配后，得到的每个 prediction 对应 ground truth 最佳分配的结果。 4.2 object queries 传统的 Anchor 是人工设计，铺在特征图上。最初人们给 Anchor 加上 scales 和 aspect ration，后来还有加上了 dense，再到后来，也出现了可学习的 Guided Anchoring，把 anchor 拆解为：位置预测和形状预测。 这种方式的 anchor 有个缺陷是：在推理阶段会产生大量的框，需要 NMS 进行抑制，这说明人工设计的 anchor 是存在冗余的（多个 anchor 匹配到一个 gt 上）。 而 DETR 的 object queries 就是一个 embedding 形式的 learned anchor，目的是让网络自己根据数据集自己学习 anchor。并且 DETR 的实验结果也证明 embedding 已经足够学习 anchor 了。 Detr 也在 coco 2017 val 上对把每个 object query 预测的框做了可视化，如下，选取 N=100 中的 20 个 object query，可以看到不同的 query vector 具有不同的分布（有些注重左下角，有些注重中间…），可以想成：有 N 个不同的人用不同的角度进行观测。 4&gt; 论文地址：https://arxiv.org/pdf/2006.04388.pdf 源码和预训练模型地址：https://github.com/implus/GFocal MMDetection 官方收录地址：https://github.com/open-mmlab/mmdetection/blob/master/configs/gfl/README.md 总结：基于任意 one-stage 检测器上，调整框本身与框质量估计的表示，同时用泛化版本的 GFocal Loss 训练该改进的表示，无 cost 涨点（一般 1 个点出头）AP。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"深度模型（目标检测）","slug":"深度模型（目标检测）","permalink":"https://leezhao415.github.io/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%89/"}]},{"title":"【精华】使用NCNN在移动端部署深度学习模型","slug":"【精华】使用NCNN在移动端部署深度学习模型","date":"2022-02-20T15:07:43.000Z","updated":"2022-02-20T15:10:34.359Z","comments":true,"path":"2022/02/20/【精华】使用NCNN在移动端部署深度学习模型/","link":"","permalink":"https://leezhao415.github.io/2022/02/20/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E4%BD%BF%E7%94%A8NCNN%E5%9C%A8%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"文章目录 使用 NCNN 在移动端部署深度学习模型 一、整体流程概览 二、将 *.pth 转换成 onnx 三、编译 NCNN 框架 四、C++ 调用和封装 五、 编写 JNI C++ 六、java 调用 七、 应用层使用 使用 NCNN 在移动端部署深度学习模型 一、整体流程概览 （1）训练模型，使用各种你熟悉的框架我用的是 pytorch （2）将 *.pth 转换成 onnx， 优化 onnx 模型 （3）使用转换工具转换成可供 ncnn 使用的模型 （4）编译 ncnn 框架，并编写 c 代码调用上一步转换的模型，得到模型的输出结果，封装成可供调用的类 （5）使用 JNIC 调用上一步 C++ 封装的类，提供出接口 （6）在安卓端编写 java 代码再次封装一次，供应用层调用 二、将 *.pth 转换成 onnx 使用 pytorch 自带的 torch.onnx 即可，需要 1.1 版本以上，这里有一点需要注意，torch 的 API 有些是 onnx 不支持的，如果转换的时候报错就把模型里的函数改成 onnx 支持的吧，有些文章里说这里可以设置 opset_version=12 来解决，但是这样的话在后面转换到 ncnn 或者 mnn 的时候造成转换失败，应该是 ncnn 还没支持到更高版本的 onnx 的原因。在最后输出之前有个 torch.randn () 函数，这里的参数格式是 [b,c, w,h] 这里也不是随便写的，b 固定是 1 了，你模型的输入通道是多少就写多少，后面的就是模型的输入，这里一旦固定了，后面在第 5 步的时候 c++ 里的输入也就固定了 1234567891011121314151617181920212223242526272829303132# -*- coding:utf-8 -*-# name: convert_onnx# author: bqh# datetime:2020/6/17 10:31# =========================import torchdef load_model(model, pretrained_path): print(&#x27;Loading pretrained model from &#123;&#125;&#x27;.format(pretrained_path)) pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage) if &quot;state_dict&quot; in pretrained_dict.keys(): pretrained_dict = remove_prefix(pretrained_dict[&#x27;state_dict&#x27;], &#x27;module.&#x27;) else: pretrained_dict = remove_prefix(pretrained_dict, &#x27;module.&#x27;) check_keys(model, pretrained_dict) model.load_state_dict(pretrained_dict, strict=False) return model output_onnx = &#x27;../weights/output.onnx&#x27;raw_weights = &#x27;../weights/model.pth&#x27;# load weightnet = you_net()net = load_model(net, raw_weights)net.eval()print(&#x27;Finished loading model!&#x27;)device = torch.device(&quot;cuda&quot;)net = net.to(device)input_names = [&quot;input0&quot;]output_names = [&quot;output0&quot;]inputs = torch.randn(1, 3, 300, 300).to(device)torch_out = torch.onnx._export(net, inputs, output_onnx, export_params=True, verbose=False,keep_initializers_as_inputs=True, input_names=input_names, output_names=output_names) 安装 onnx 简化工具 1pip3 install onnx-simplifier onnxruntime 简化 onnx 模型 这一步一定要做，否则后面转 onnx 的时候会报错 1python3 -m onnxsim model.onnx model_sim.onnx 三、编译 NCNN 框架 主要参考 ncnn 官网的教程即可，windows 下编译同上一篇的 MNN 的编译都差不多，只有一点需要说明，官网的教程上有 vulkan-sdk 的安装然后打开 - DNCNN_VULKAN=ON 编译选项。我一切照做后编译出来的 ncnn.lib 在运行 ncnn::Extractor ex = Net-&gt;create_extractor (); 这个函数后的所有操作之后，返回的时候就报堆栈溢出错误，包括加载官网给出的例子全部报错；后来不 cmake 的时候这个编译选项不打开编译出来的 ncnn.lib 就一切正常了。可能是自己的问题，也没去深究。反正能用就 OK 了。我把编译出来的 ncnn.lib ncnn.a 和 linux 下的 onnx2ncnn 工具都放在了我的网盘里，不想被编译折磨的就直接去下吧。如果编译遇见问题，也可以给我留言，哈哈～ 说明：ncnnd.lib 是 windows 下的 debug 版本，ncnn.lib 是 release 版本，libncnn.a 是 linux 下的库文件，onnx2ncnn 是 linux 下的转换工具。 下载地址：NCNN 提取码：6cuc 四、C++ 调用和封装 说明 对于 vs 中 lib 库和 include 目录的配置就不赘述了，有不懂的之前的文章有提过，假定工程已经配置完成。大体的调用过程 NCNN 和 MNN 都差不多，先加载模型创建一个指向模型的指针，然后创建 session、创建用于处理输入的 tensor，将 input_tensor 送入 session，运行 session，最后得到网络的输出。如果对 C++ 比较熟悉的话，看着官网的教程比葫芦画瓢即可，只有一个地方需要说明就是对输出的获得。先看下我的代码和官网的代码再说为什么 我的输出： 1234// run net and get outputncnn::Mat out, out1;ret = ex.extract(&quot;output0&quot;, out);ex.extract(&quot;376&quot;, out1); 官网的例子输出： 12ncnn::Mat out;ex.extract(&quot;detection_out&quot;, out); 辣么问题来了，我的 &quot;output0&quot; 和 &quot;376&quot;、官网的 “detection_out” 都哪里来的？有两个地方可以得到，最简单的方法，使用 MNN 框架下的转换工具，在转换完成的时候会给出模型的输入和输出名称，直接拷贝即可 1234567891011&gt;MNNConvert.exe -f ONNX --modelFile model.onnx --MNNModel slime.mnn --bizCode bizMNNConverter Version: 0.2.1.5git - MNN @ 2018Start to Convert Other Model Format To MNN Model...[17:49:58] :29: ONNX Model ir version: 6Start to Optimize the MNN Net...[17:49:58] :20: Inputs: input0[17:49:58] :37: Outputs: output0, Type = Concat[17:49:58] :37: Outputs: 376, Type = SoftmaxConverted Done! 如果没有 MNN 的转换工具，在后面加载模型后单步跟一下，在 Net = new ncnn::Net () 变量中有个 blob 变量，在内存中查看一下，里面存的有模型的各个层的名称。代码中的 img_w,img_h 就是在第二步转换的时候你指定的 w,h。这里只写了核心调用函数，具体使用时还请自行添加一些辅助函数！ C++ 代码 detection.h 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#pragma once#include &lt;opencv2/opencv.hpp&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &quot;net.h&quot;#include &lt;stdio.h&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;fstream&gt;#include &quot;omp.h&quot;struct bbox &#123; float x1; float y1; float x2; float y2; float s;&#125;;struct box &#123; float cx; float cy; float sx; float sy;&#125;;struct ObjectInfo &#123; float x1; //bbox的left float y1; //bbox的top float x2; //bbox的right float y2; //bbox的bottom float prob; //置信度&#125;;class ObjectDetection&#123;private: float _nms = 0.4; float _threshold = 0.6; const float mean_vals[3] = &#123; 104.f, 117.f, 123.f &#125;; const float norm_vals[3] = &#123; 1.0 / 104.0, 1.0 / 117.0, 1.0 / 123.0 &#125;; cv::Mat img; ncnn::Net *Net; int img_w = 300; int img_h = 300; int numThread; int detect_count = 0; static inline bool cmp(bbox a, bbox b);public: ObjectDetection(std::string modelFolder, int num_thread); ~ObjectDetection(); int Detect(unsigned char *inputImage, int inputw, int inputh, std::vector&lt;ObjectInfo &gt; &amp;obj);&#125;; detection.cpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &quot;Detection.h&quot;#include &lt;cmath&gt;ObjectDetection::ObjectDetection(std::string modelFolder, int num_thread)&#123; Net = new ncnn::Net(); std::string model_param = modelFolder + &quot;Detect.param&quot;; std::string model_bin = modelFolder + &quot;Detect.bin&quot;; int ret = Net-&gt;load_param(model_param.c_str()); ret = Net-&gt;load_model(model_bin.c_str()); numThread = num_thread;&#125;ObjectDetection::~ObjectDetection()&#123; if (Net != nullptr) &#123; delete Net; Net = nullptr; &#125;&#125;int ObjectDetection::Detect(unsigned char *inputImage, int inputw, int inputh, std::vector&lt;ObjectInfo &gt; &amp;obj)&#123; int ret = -1; ncnn::Mat in = ncnn::Mat::from_pixels_resize(inputImage, ncnn::Mat::PIXEL_BGR, inputw, inputh, img_w, img_h); in.substract_mean_normalize(mean_vals, norm_vals); ncnn::Extractor ex = Net-&gt;create_extractor(); ex.set_light_mode(true); ret = ex.input(&quot;input0&quot;, in); // run net and get output ncnn::Mat out, out1; // bbox的输出 ret = ex.extract(&quot;output0&quot;, out); ex.extract(&quot;376&quot;, out1); // get result for (int i = 0; i &lt; out.h; ++i) &#123; // 得到网络的具体输出 const float *boxes = out.row(i); const float *scores = out1.row(i); // 执行你自己的操作 &#125; std::sort(total_box.begin(), total_box.end(), cmp); NMS(total_box, _nms); return 0;&#125; 五、 编写 JNI C++ 在 Android Studio 中配置 NDK，具体配置网上有很多教程我就不啰嗦了，假定 android strdio 的 jni c 环境已经配置完成。源码中的函数名的格式是 jni c 要求的，必须这种格式，根据实际情况修改，函数名中的 &quot;com_example_demokit_Detection&quot; 对应到 java 的应用中就是 &quot;com.example.demokit.Detection&quot; 这样就很好理解了。 native-lib.cpp 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;jni.h&gt;#include &lt;string&gt;#include &quot;Detection.h&quot;#include &lt;android/log.h&gt;extern &quot;C&quot; JNIEXPORT jlong JNICALLJava_com_example_demokit_Detection_Create(JNIEnv *env, jobject instance, jstring path) &#123; char* _path; _path = (char*)env-&gt;GetStringUTFChars(path,0); Detection *phandle = new Detection(_path, 2); return (jlong)phandle;&#125;extern &quot;C&quot; JNIEXPORT jintArray JNICALLJava_com_example_demokit_Detection_Detect(JNIEnv *env, jobject instance, jlong handle, jint campos, jint w, jint h, jbyteArray data_) &#123; Detection *gp = NULL; if (handle) gp = (Detection *)handle; else return nullptr; jbyte *data = env-&gt;GetByteArrayElements(data_, NULL); std::vector&lt;ObjectInfo&gt; objects; gp-&gt;Detect((unsigned char*)data, w, h, objects); env-&gt;ReleaseByteArrayElements(data_, data, 0); jintArray jarr = env-&gt;NewIntArray(objects.size()*15+1); jint *arr = env-&gt;GetIntArrayElements(jarr, NULL); arr[0] = objects.size(); for (int i = 0; i &lt; objects.size(); i++) &#123; arr[i*5 + 1] = objects[i].x1; arr[i*5 + 2] = objects[i].y1; arr[i*5 + 3] = objects[i].x2; arr[i*5 + 4] = objects[i].y2; arr[i*5 + 5] = objects[i].prob; &#125; env-&gt;ReleaseIntArrayElements(jarr, arr, 0); return jarr;&#125; 六、java 调用 12345678910111213141516package com.example.demokit;public class Detection &#123; static &#123; System.loadLibrary(&quot;native-lib&quot;); &#125; private long handle; public Detection(String path)&#123; handle = Create(path); &#125; public int[] Detect(int w, int h, byte[] data)&#123; return Detect(handle, w, h, data); &#125; private native long Create(String path); private native int[] Detect(long handle, int w, int h, byte[] data);&#125; 七、 应用层使用 在应用层就可以直接调用上面的 java 类啦，搞定～ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.example.demokit;import android.graphics.Point;import java.util.ArrayList;import java.util.Arrays;import java.util.List;public class DetectTool &#123; private Detection mDetection; private static final int DATA_LENGTH = 5; // 矩形框坐标2个，每个具有x,y两个值；置信度1个； public DetectTool(String dect_model_dir)&#123; /** * @dect_model_dir: 检测模型所在的目录路径 */ mDetection = new Detection(dect_model_dir); &#125; private ObjectInfo ArrayAnalysis(int[] src_array)&#123; /** * 对输入的数组进行解析，返回ObjectInfo对象 * @src_array: 具有DATA_LENGTH所示结构的数组 */ ObjectInfo obj_info = new ObjectInfo(); Point[] pointFaceBox = new Point[2]; // face_bbox 坐标 for(int i = 0; i &lt; 2; i++) &#123; Point point = new Point(); point.x = src_array[2*i]; point.y = src_array[2*i+1]; pointFaceBox[i] = point; &#125; // 置信度 obj_info.setProb(src_array[4]); return obj_info ; &#125; public List&lt;ObjectInfo&gt; GetObjectInfo(int width, int height, byte[] data)&#123; /** * @width：图片宽度 * @height：图片高度 * @data：图片的字节流 */ int[] obj= mDetection.Detect(width, height, data); List&lt;ObjectInfo&gt; obj_list = new ArrayList&lt;&gt;(); int obj_count = obj[0]; for(int i = 0; i &lt; obj_count ; i++)&#123; int[] obj_array = Arrays.copyOfRange(obj, i*DATA_LENGTH + 1, (i + 1) * DATA_LENGTH+1); ObjectInfo obj_info = this.ArrayAnalysis(obj_array); obj_list.add(obj_info); &#125; return obj_list; &#125;&#125; ———————————————— 版权声明：本文为 CSDN 博主「zzubqh103」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/qq_36810544/article/details/106911025","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NCNN部署","slug":"NCNN部署","permalink":"https://leezhao415.github.io/tags/NCNN%E9%83%A8%E7%BD%B2/"}]},{"title":"保姆级教程：深度学习环境配置指南","slug":"保姆级教程：深度学习环境配置指南","date":"2022-02-20T15:07:20.000Z","updated":"2022-02-20T15:32:35.712Z","comments":true,"path":"2022/02/20/保姆级教程：深度学习环境配置指南/","link":"","permalink":"https://leezhao415.github.io/2022/02/20/%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/","excerpt":"","text":"文章目录 一、Windows 系统深度学习环境配置 1.1 打开 Anaconda Prompt 1.2 确定硬件支持的 CUDA 版本 1.3 确定 pytorch 版本，torchvision 版本 1.4 镜像中下载对应的安装包 1.5 本地安装 1.6 测试 1.7 遇到的问题 二、Mac 深度学习环境配置 2.1 安装 Anaconda 2.2 确认下载情况 2.3 虚拟环境和包的下载 2.4 额外情况 三、Ubuntu 深度学习环境配置 3.1 Anacond 安装 3.2 pytorch cpu 版本安装 3.3 pytorch-gpu 安装 四、写在最后 作者丨伍天舟、马曾欧、陈信达 来源丨 Datawhale 编辑丨极市平台 极市导读 俗话说，环境配不对，学习两行泪。本文为保姆级别的教程，详细介绍了 Windows、Mac 和 Ubuntu 的深度学习环境配置方法及问题，帮助卡在环境配置的小伙伴们，解决入门难关。 入门深度学习，很多人经历了从入门到放弃的心酸历程，且千军万马倒在了入门第一道关卡：环境配置问题。俗话说，环境配不对，学习两行泪。 如果你正在面临配置环境的痛苦，不管你是 Windows 用户、Ubuntu 用户还是苹果死忠粉，这篇文章都是为你量身定制的。接下来就依次讲下 Windows、Mac 和 Ubuntu 的深度学习环境配置问题。 一、Windows 系统深度学习环境配置 系统：Win10 64 位操作系统 安装组合：Anaconda+PyTorch (GPU 版)+GTX1060 开源贡献：伍天舟，内蒙古农业大学 1.1 打开 Anaconda Prompt 1、conda create -n pytorch python=3.7.0：创建名为 pytorch 的虚拟环境，并为该环境安装 python=3.7。 2、activate pytorch：激活名为 pytorch 的环境 1.2 确定硬件支持的 CUDA 版本 NVIDIA 控制面板 - 帮助 - 系统信息 - 组件 2020 年 5 月 19 日 16:46:31，我更新了显卡驱动，看到我的 cuda 支持 11 以内的 1.3 确定 pytorch 版本，torchvision 版本 进入 pytorch 官网：https://pytorch.org/get-started/locally/ 因为官方源太慢了，这里使用清华源下载 1.4 镜像中下载对应的安装包 清华镜像： https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/win-64/ pytorch： torchvision： 1.5 本地安装 接着第一步，在 pytorch 环境下进行安装，依次输入如下指令。 然后回到虚拟环境所在目录，用 conda install anaconda 安装环境所需的基础包 1.6 测试 代码 1： 1234from future import print_functionimport torchx = torch.rand(5, 3)print(x) 输出类似于以下的张量： 代码 2： 12import torchtorch.cuda.is_available() 输出：True 如果以上两段代码输出无异常，表明环境搭建成功。 1.7 遇到的问题 用下面命令创建虚拟环境报错 1conda create -n py37_torch131 python=3.7 【解决方法】https://blog.csdn.net/weixin_42329133/article/details/102640763 环境会保存在 Anaconda 目录下的 envs 文件夹内 PackagesNotFoundError: The following packages are not available from current channels 【解决方法】： https://www.cnblogs.com/hellojiaojiao/p/10790273.html conda 下载太慢问题 【解决方案】https://mirror.tuna.tsinghua.edu.cn/help/anaconda/ 我直呼一声清华 NB！ cuda 安装 cuda 历史版本下载：https://developer.nvidia.com/cuda-toolkit-archive 要看 NVIDIA 的组件，自己的 CUDA 支持哪个版本，我是 1060 显卡，所以我下的 10.0 版本的 cuda pytorch 安装 官网没有我的组合，我满脸问号 这里我直接（别忘了先进入刚创建的环境） 1conda install pytorch torchvision cudatoolkit=10.0 -c pytorch jupyter note 如何进入虚拟环境 python -m ipykernel install --name 虚拟环境名 1. 打开 Anaconda Prompt, 输入 conda env list 查看现有环境 2. 输入 activate name（name 是你想切换的环境） 3.conda install ipykernel 安装必要插件 4.python -m ipykernel install --name Name 将环境添加到 Jyputer 中（Name 是此环境显示在 Jyputer 中的名称，可自定义） 删除内核 jupyter kernelspec remove 内核名称 报错 [Errono 13] [Errno 13] Permission denied: ‘/usr/local/share/jupyter’ 为此，需要添加–user 选项，将配置文件生成在本账户的家目录下。 1python -m ipykernel install --user --name py27-caffe-notebook 至此，添加 kernel 完成。查看已有的 kernel： 1jupyter kernelspec list 删除已有的 kernel 1jupyter kernelspec remove kernelname 以上的命令删除仅仅是配置文件，并没有卸载相应虚拟环境的 ipykernel，因此若要再次安装相应 python 虚拟环境的 kernel，只需激活虚拟环境，然后 1python -m ipykernel install --name kernelname conda 安装一半总失败 把文件下载到本地，进入该文件的目录，然后用命令 1conda install --offline 包名 二、Mac 深度学习环境配置 安装组合：Anaconda+PyTorch (GPU 版) 开源贡献：马曾欧，伦敦大学 2.1 安装 Anaconda Anaconda 的安装有两种方式，这里仅介绍一种最直观的 - macOS graphical install。 https://www.anaconda.com/products/individual 里，Anaconda Installers 的位置，选择 Python 3.7 下方的 “64-Bit Graphical Installer (442)”。下载好 pkg 安装包后点击进入，按下一步完成安装即可。默认安装地点为～/opt。想用 command line install 的，请自行参考：https://docs.anaconda.com/anaconda/install/mac-os/ 2.2 确认下载情况 在 Mac 的 Terminal 里，输入 1python --version 确保安装的 Python 是 3.x 版本。在 Terminal 输入 1jupyter notebook 弹出网页，即可进入 notebook。 在网页右上角点击 Quit，或返回 Terminal，command + c，退出 notebook。 2.2.1 常见问题 如果电脑中下载了多个 Anaconda，运行时可能出现冲突。在 Terminal 中输入 1cd ~ 返回 home 目录，输入 1cat .bash_profile 如果只能看到一个 Anaconda 版本就没有问题。如果有多个则下载包时有可能 造成一定的冲突。用 vim、nano 或其他文本编辑器把旧版本 Anaconda 的 1export PATH= ... 删除。 2.3 虚拟环境和包的下载 用 conda 去创建虚拟环境和下载对应的包是很简单的一件事。 2.3.1 Graphic 点击 Anaconda-Navigator，可以看到自己下载好的应用程序，左上角 “Applications on” 应该指向的是 “base (root)”，左边点击 “Environments” 就可以看到自己建立的虚拟环境和对应的包了。点击左下角的 Create 即可创建一个新的虚拟环境。 输入环境名称和 python 的版本，点击 create 进行创建。 之后在 Home 页面，确保左上角指向的是你刚刚创建的环境名，在这个环境下 install jupyter notebook，注意原本装的 notebook 是在 base 里的，不可通用。 回到 Environments 中，可以看到在此环境中的所有包，左上方选择 All，然后输入想要下载的包名 然后选中进行下载 2.3.2 Command Line 用 command line 完成以上的操作也很简洁。这次以 Pytorch 为例。在 Terminal 中输入 1conda create --name env_name 就可以创建一个虚拟环境，叫 “env_name”。输入 1conda env list 即可看到创建了的所有虚拟环境，其中打 * 的就是当前环境。输入 1conda activate env_name 进入环境 1conda deactivate 退出当前虚拟环境，进入 base。 2.3.3 下载 Pytorch 一般情况下 Mac 是不支持 CUDA 的。进入 https://pytorch.org/ 可以看到 pytorch 官网显示当前设备应该用的下载语句。此情况，我的是 1conda install pytorch torchvision -c pytorch 复制下来，粘贴到 Terminal 中运行就可以开始下载了。 下载完成后，在 Terminal 输入 1python3 之后 import 两个刚下载的包，确认下载完成 12345import torchimport torchvisionprint(torch.__version__)print(torchvision.__version__) 如果 import 和输出正常，配置就完成了！输入 1quit() 就 ok 了。 2.4 额外情况 国内如果直接用 conda 下载，可能会很慢。可以换下载源进行加速。换源方法可参考下列资料中的 Linux 部分：https://zhuanlan.zhihu.com/p/87123943。 conda 下载中常会出现 “Solving environment: failed…” 的问题，如果正常创建虚拟环境，这位问题应该就是没有问题的，如果还发生，可以再创建一个虚拟环境。 conda 的社群很大，基本上遇到的问题很有可能有人遇到、有人解答，Google 会是一个很好的解决办法。 三、Ubuntu 深度学习环境配置 安装组合：Anaconda+PyTorch (CPU 版) 或 PyTorch (GPU 版) 开源贡献：陈信达，华北电力大学 3.1 Anacond 安装 Anaconda 和 Python 版本是对应的，所以需要选择安装对应 Python2.7 版本的还是 Python3.7 版本或其他版本的，根据自己的需要下载合适的安装包。 下载链接：https://www.anaconda.com/download/#linux 点击下面的 64-Bit (x86) Installer (522 MB)，下载 64 位的版本。 下载完后的文件名是：Anaconda3-2020.02-Linux-x86_64.sh。 cd 到 Anaconda3-2020.02-Linux-x86_64.sh 所在的目录： 执行 bash Anaconda3-2020.02-Linux-x86_64.sh 开始安装： 一直按回车直到如下界面，然后输入 yes： 这里直接回车安装到默认路径，或者在 &gt;&gt;&gt; 后输入自定义路径 等待安装进度条走完，然后出现下面的提示，yes 是加入环境变量，no 是不加入环境变量，这里我们以输入 no 为例 接下来手动加入环境变量，先 cd 到～，然后编辑.bashrc 文件：sudo vim .bashrc 在最下面添加如下几行 (注意。后有空格)： 12345# 区分anaconda python与系统内置pythonalias python3=&quot;/usr/bin/python3.5&quot;alias python2=&quot;/usr/bin/python2.7&quot;. /home/cxd/anaconda3/etc/profile.d/conda.sh 然后按 esc + : + wq! 保存 输入 source .bashrc 来执行刚修改的初始化文档 下面输入 conda env list 来试试环境变量是否设置成功： 试试刚刚设置的使用内置 python 的命令：python2、python3 如果到这就结束的话，大家安装包的时候肯定会无比煎熬～这里需要将 anaconda 换一下源（加入清华源）： 然后我们创建一个名为 pytorch 的虚拟环境，发现报了下面的错误： 原因是我们没有清除上次安装留下来的源，输入 sudo vim .condarc，修改该文件的内容（记得删除 default 那行）： 然后输入 source .condarc，再次创建虚拟环境： 3.2 pytorch cpu 版本安装 打开 pytorch 官网：https://pytorch.org/ 激活刚刚创建的虚拟环境：conda activate pytorch 在安装之前先添加下面这个源： 1conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch 然后输入下面代码： 1conda install pytorch torchvision cpuonly -c pytorch 等到安装好后测试一下是否安装完成： 12import torchprint(torch.__version__) 输出如下则安装成功： 3.3 pytorch-gpu 安装 3.3.1 GPU 驱动安装 检测显卡类型 执行命令：’ ubuntu-drivers devices’ 123456789101112131415== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==modalias : pci:v000010DEd00001C8Dsv00001028sd0000086Fbc03sc02i00vendor : NVIDIA Corporationmodel : GP107M [GeForce GTX 1050 Mobile]driver : nvidia-driver-390 - distro non-freedriver : nvidia-driver-435 - distro non-freedriver : nvidia-driver-440 - distro non-free recommendeddriver : xserver-xorg-video-nouveau - distro free builtin== /sys/devices/pci0000:00/0000:00:14.3 ==modalias : pci:v00008086d0000A370sv00008086sd000042A4bc02sc80i00vendor : Intel Corporationmodel : Wireless-AC 9560 [Jefferson Peak]manual_install: Truedriver : backport-iwlwifi-dkms - distro free 大家可以看到，这里有个设备是 GTX1050。推荐安装驱动是 440。 安装驱动 安装所有推荐驱动 1sudo ubuntu-drivers autoinstall 安装一个驱动 1sudo apt install nvidia-440 3.3.2 安装 cuda cuda 安装需要对应合适的显卡驱动。下面是驱动和 cuda 的版本对应关系 12345678910111213Table 1. CUDA Toolkit and Compatible Driver VersionsCUDA Toolkit Linux x86_64 Driver Version Windows x86_64 Driver VersionCUDA 10.2.89 &gt;= 440.33 &gt;= 441.22CUDA 10.1 (10.1.105 general release, and updates) &gt;= 418.39 &gt;= 418.96CUDA 10.0.130 &gt;= 410.48 &gt;= 411.31CUDA 9.2 (9.2.148 Update 1) &gt;= 396.37 &gt;= 398.26CUDA 9.2 (9.2.88) &gt;= 396.26 &gt;= 397.44CUDA 9.1 (9.1.85) &gt;= 390.46 &gt;= 391.29CUDA 9.0 (9.0.76) &gt;= 384.81 &gt;= 385.54CUDA 8.0 (8.0.61 GA2) &gt;= 375.26 &gt;= 376.51CUDA 8.0 (8.0.44) &gt;= 367.48 &gt;= 369.30CUDA 7.5 (7.5.16) &gt;= 352.31 &gt;= 353.66CUDA 7.0 (7.0.28) &gt;= 346.46 &gt;= 347.62 cuda 下载链接：http://suo.im/6dY8rL Installer Type 选择第一第二个都可。但是要在获得 cuda 文件后先检测 gcc 版本。下面以第一个 runfile (local) 安装方式为例。 安装 gcc linux 一般会自带了 gcc，我们先检测一下自己系统的 gcc 版本 1gcc --version 而 cuda 的 gcc 依赖版本在官方文档的安装指南上会给出 如果版本和 cuda 依赖 gcc 不对应，就安装 cuda 需要的版本 12sudo apt-get install gcc-7.0sudo apt-get install g++-7.0 安装完成后需要更换系统 gcc 版本 12sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 50sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 50 选择需要的版本 123456sudo update-alternatives --config gcc 选择 路径 优先级 状态------------------------------------------------------------ * 0 /usr/bin/gcc-9 50 自动模式 1 /usr/bin/g++-9 50 手动模式 2 /usr/bin/gcc-7 50 手动模式 输入前面显示的编号即可。 安装 cuda 1sudo sh cuda_你的版本_linux.run 配置环境变量 1sudo vim ~/.bashrc 将下面的命令复制进去 12export PATH=/usr/local/cuda-10.2/bin$&#123;PATH:+:$PATH&#125;&#125; export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125; 检查是否安装成功 1nvcc -V 3.3.3 安装 cudnn https://developer.nvidia.com/cudnn 选择对应 cuda 的版本即可 然后将 cudnn 解压后的 include 和 lib64 文件夹复制到 cuda 中 123sudo cp cuda/include/cudnn.h /usr/local/cuda-10.2/include #解压后的文件夹名字为cuda-10.2 sudo cp cuda/lib64/libcudnn* /usr/local/cuda-10.2/lib64sudo chmod a+r /usr/local/cuda-10.2/include/cudnn.h /usr/local/cuda-10.2/lib64/libcudnn* 3.3.4 安装 pytorch-gpu conda 安装： 12# 选择自己对应的cuda版本conda install pytorch torchvision cudatoolkit=10.2 pip 安装： 12pip install torch torchvision -i https://pypi.mirrors.ustc.edu.cn/simple 四、写在最后 所有的深度学习环境安装指南到这里就结束了，希望能解决你面临的环境配置难题。关于实践项目，可以结合阿里天池的学习赛进行动手实践。 数据挖掘学习赛（进行中，5832 人参与） https://tianchi.aliyun.com/competition/entrance/231784/forum cv 实践学习赛（进行中，1933 参与） https://tianchi.aliyun.com/competition/entrance/531795/forum nlp 实践学习赛（进行中，573 人参与） https://tianchi.aliyun.com/competition/entrance/531810/forum","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"深度学习环境配置","slug":"深度学习环境配置","permalink":"https://leezhao415.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"}]},{"title":"【精华】Linux最强总结","slug":"【精华】Linux最强总结","date":"2022-02-20T15:06:52.000Z","updated":"2022-02-20T15:12:47.865Z","comments":true,"path":"2022/02/20/【精华】Linux最强总结/","link":"","permalink":"https://leezhao415.github.io/2022/02/20/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91Linux%E6%9C%80%E5%BC%BA%E6%80%BB%E7%BB%93/","excerpt":"","text":"文章目录 前言 Linux 基础 操作系统 什么是 Linux （1）Linux 系统内核与 Linux 发行套件的区别 （2）Linux 对比 Windows （3）Linux 系统种类 终端连接阿里云服务器 Shell Shell 的种类 命令 快捷方式 文件和目录 文件的组织 查看路径 浏览和切换目录 浏览和创建文件 文件的复制和移动 文件的删除和链接 用户与权限 用户 群组的管理 文件权限管理 查找文件 locate find 软件仓库 yum 常用命令 切换 CentOS 软件源 阅读手册 man help Linux 进阶 文本操作 grep sort wc uniq cut 重定向 管道 流 重定向 管道 | 流 查看进程 w ps top kill 管理进程 进程状态 前台进程 &amp; 后台进程 守护进程 文件压缩解压 tar gzip / gunzip tar 归档 + 压缩 zcat、zless、zmore zip/unzip 编译安装软件 编译安装 网络 ifconfig host ssh 连接远程服务器 wget 备份 scp rsync 系统 halt reboot poweroff Vim 编辑器 Vim 是什么？ Vim 常用模式 基本操作 标准操作 高级操作 Vim 配置 前言 Linux 的学习对于一个程序员的重要性是不言而喻的。前端开发相比后端开发，接触 Linux 机会相对较少，因此往往容易忽视它。但是学好它却是程序员必备修养之一。 如果本文对你有所帮助，请点个👍 吧。 作者使用的是阿里云服务器 ECS （最便宜的那种） CentOS 7.7 64 位。当然你也可以在自己的电脑安装虚拟机，虚拟机中再去安装 CentOS 系统（这就完全免费了）。至于它的安装教程可以去谷歌搜索下，相关教程非常多。 Linux 基础 操作系统 操作系统 Operating System 简称 OS ，是软件的一部分，它是硬件基础上的第一层软件，是硬件和其它软件沟通的桥梁。 操作系统会控制其他程序运行，管理系统资源，提供最基本的计算功能，如管理及配置内存、决定系统资源供需的优先次序等，同时还提供一些基本的服务程序。 什么是 Linux （1）Linux 系统内核与 Linux 发行套件的区别 Linux 系统内核指的是由 Linus Torvalds 负责维护，提供硬件抽象层、硬盘及文件系统控制及多任务功能的系统核心程序。 Linux 发行套件系统是我们常说的 Linux 操作系统，也即是由 Linux 内核与各种常用软件的集合产品。 总结：真正的 Linux 指的是系统内核，而我们常说的 Linux 指的是 “发行版完整的包含一些基础软件的操作系统”。 （2）Linux 对比 Windows 稳定且有效率； 免费（或少许费用）； 漏洞少且快速修补； 多任务多用户； 更加安全的用户与文件权限策略； 适合小内核程序的嵌入系统； 相对不耗资源。 （3）Linux 系统种类 红帽企业版 Linux ： RHEL 是全世界内使用最广泛的 Linux 系统。它具有极强的性能与稳定性，是众多生成环境中使用的（收费的）系统。 Fedora ：由红帽公司发布的桌面版系统套件，用户可以免费体验到最新的技术或工具，这些技术或工具在成熟后会被加入到 RHEL 系统中，因此 Fedora 也成为 RHEL 系统的试验版本。 CentOS ：通过把 RHEL 系统重新编译并发布给用户免费使用的 Linux 系统，具有广泛的使用人群。 Deepin ：中国发行，对优秀的开源成品进行集成和配置。 Debian ：稳定性、安全性强，提供了免费的基础支持，在国外拥有很高的认可度和使用率。 Ubuntu ：是一款派生自 Debian 的操作系统，对新款硬件具有极强的兼容能力。 Ubuntu 与 Fedora 都是极其出色的 Linux 桌面系统，而且 Ubuntu 也可用于服务器领域。 终端连接阿里云服务器 通过执行 ssh root@121.42.11.34 命令，然后输入服务器连接密码就可以顺利登陆远程服务器。从现在开始我们就可以在本地电脑操作远程服务器。 这个黑色的面板就是终端也就是 Shell （命令行环境）。 ssh root@xxx 这是一条命令，必须要在 Shell 中才能执行。 Shell Shell 这个单词的原意是 “外壳”，跟 kernel （内核）相对应，比喻内核外面的一层，即用户跟内核交互的对话界面。 Shell 是一个程序，提供一个与用户对话的环境。这个环境只有一个命令提示符，让用户从键盘输入命令，所以又称为命令行环境（ command line interface ，简写为 CLI ）。 Shell 接收到用户输入的命令，将命令送入操作系统执行，并将结果返回给用户。 Shell 是一个命令解释器，解释用户输入的命令。它支持变量、条件判断、循环操作等语法，所以用户可以用 Shell 命令写出各种小程序，又称为 Shell 脚本。这些脚本都通过 Shell 的解释执行，而不通过编译。 Shell 是一个工具箱，提供了各种小工具，供用户方便地使用操作系统的功能。 Shell 的种类 Shell 有很多种，只要能给用户提供命令行环境的程序，都可以看作是 Shell 。 历史上，主要的 Shell 有下面这些： Bourne Shell（sh） Bourne Again shell（bash） C Shell（csh） TENEX C Shell（tcsh） Korn shell（ksh） Z Shell（zsh） Friendly Interactive Shell（fish） 其中 Bash 是目前最常用的 Shell 。 MacOS 中的默认 Shell 就是 Bash 。 通过执行 echo $SHELL 命令可以查看到当前正在使用的 Shell 。还可以通过 cat /etc/shells 查看当前系统安装的所有 Shell 种类。 命令 命令行提示符 进入命令行环境以后，用户会看到 Shell 的提示符。提示符往往是一串前缀，最后以一个美元符号 $ 结尾，用户可以在这个符号后面输入各种命令。 执行一个简单的命令 pwd ： 12[root@iZm5e8dsxce9ufaic7hi3uZ ~]# pwd/root 命令解析： root ：表示用户名； iZm5e8dsxce9ufaic7hi3uZ ：表示主机名； ~ ：表示目前所在目录为家目录，其中 root 用户的家目录是 /root 普通用户的家目录在 /home 下； # ：指示你所具有的权限（ root 用户为 # ，普通用户为 $ ）。 执行 whoami 命令可以查看当前用户名； 执行 hostname 命令可以查看当前主机名； 关于如何创建、切换、删除用户，在后面的用户与权限会具体讲解，这里先使用 root 用户进行演示。 [备注] root 是超级用户，具备操作系统的一切权限。 命令格式 1command parameters（命令 参数） 长短参数 12345单个参数：ls -a（a 是英文 all 的缩写，表示“全部”）多个参数：ls -al（全部文件 + 列表形式展示）单个长参数：ls --all多个长参数：ls --reverse --all长短混合参数：ls --all -l 参数值 12短参数：command -p 10（例如：ssh root@121.42.11.34 -p 22）长参数：command --paramters=10（例如：ssh root@121.42.11.34 --port=22） 快捷方式 在开始学习 Linux 命令之前，有这么一些快捷方式，是必须要提前掌握的，它将贯穿整个 Linux 使用生涯。 通过上下方向键 ↑ ↓ 来调取过往执行过的 Linux 命令； 命令或参数仅需输入前几位就可以用 Tab 键补全； Ctrl + R ：用于查找使用过的命令（ history 命令用于列出之前使用过的所有命令，然后输入 ! 命令加上编号 ( !2 ) 就可以直接执行该历史命令）； Ctrl + L ：清除屏幕并将当前行移到页面顶部； Ctrl + C ：中止当前正在执行的命令； Ctrl + U ：从光标位置剪切到行首； Ctrl + K ：从光标位置剪切到行尾； Ctrl + W ：剪切光标左侧的一个单词； Ctrl + Y ：粘贴 Ctrl + U | K | Y 剪切的命令； Ctrl + A ：光标跳到命令行的开头； Ctrl + E ：光标跳到命令行的结尾； Ctrl + D ：关闭 Shell 会话； 文件和目录 文件的组织 查看路径 pwd 显示当前目录的路径 which 查看命令的可执行文件所在路径， Linux 下，每一条命令其实都对应一个可执行程序，在终端中输入命令，按回车的时候，就是执行了对应的那个程序， which 命令本身对应的程序也存在于 Linux 中。 总的来说一个命令就是一个可执行程序。 浏览和切换目录 ls 列出文件和目录，它是 Linux 最常用的命令之一。 【常用参数】 -a 显示所有文件和目录包括隐藏的 -l 显示详细列表 -h 适合人类阅读的 -t 按文件最近一次修改时间排序 -i 显示文件的 inode （ inode 是文件内容的标识） cd cd 是英语 change directory 的缩写，表示切换目录。 123456cd / --&gt; 跳转到根目录cd ~ --&gt; 跳转到家目录cd .. --&gt; 跳转到上级目录cd ./home --&gt; 跳转到当前目录的home目录下cd /home/lion --&gt; 跳转到根目录下的home目录下的lion目录cd --&gt; 不添加任何参数，也是回到家目录 [注意] 输入 cd /ho + 单次 tab 键会自动补全路径 + 两次 tab 键会列出所有可能的目录列表。 du 列举目录大小信息。 【常用参数】 -h 适合人类阅读的； -a 同时列举出目录下文件的大小信息； -s 只显示总计大小，不显示具体信息。 浏览和创建文件 cat 一次性显示文件所有内容，更适合查看小的文件。 1cat cloud-init.log 【常用参数】 -n 显示行号。 less 分页显示文件内容，更适合查看大的文件。 1less cloud-init.log 【快捷操作】 空格键：前进一页（一个屏幕）； b 键：后退一页； 回车键：前进一行； y 键：后退一行； 上下键：回退或前进一行； d 键：前进半页； u 键：后退半页； q 键：停止读取文件，中止 less 命令； = 键：显示当前页面的内容是文件中的第几行到第几行以及一些其它关于本页内容的详细信息； h 键：显示帮助文档； / 键：进入搜索模式后，按 n 键跳到一个符合项目，按 N 键跳到上一个符合项目，同时也可以输入正则表达式匹配。 head 显示文件的开头几行（默认是 10 行） 1head cloud-init.log 【参数】 -n 指定行数 head cloud-init.log -n 2 tail 显示文件的结尾几行（默认是 10 行） 1tail cloud-init.log 【参数】 -n 指定行数 tail cloud-init.log -n 2 -f 会每过 1 秒检查下文件是否有更新内容，也可以用 -s 参数指定间隔时间 tail -f -s 4 xxx.log touch 创建一个文件 1touch new_file mkdir 创建一个目录 1mkdir new_folder 【常用参数】 -p 递归的创建目录结构 mkdir -p one/two/three 文件的复制和移动 cp 拷贝文件和目录 1cp file file_copy --&gt; file 是目标文件，file_copy 是拷贝出来的文件cp file one --&gt; 把 file 文件拷贝到 one 目录下，并且文件名依然为 filecp file one/file_copy --&gt; 把 file 文件拷贝到 one 目录下，文件名为file_copycp *.txt folder --&gt; 把当前目录下所有 txt 文件拷贝到 folder 目录下 【常用参数】 -r 递归的拷贝，常用来拷贝一整个目录 mv 移动（重命名）文件或目录，与 cp 命令用法相似。 1mv file one --&gt; 将 file 文件移动到 one 目录下mv new_folder one --&gt; 将 new_folder 文件夹移动到one目录下mv *.txt folder --&gt; 把当前目录下所有 txt 文件移动到 folder 目录下mv file new_file --&gt; file 文件重命名为 new_file 文件的删除和链接 rm 删除文件和目录，由于 Linux 下没有回收站，一旦删除非常难恢复，因此需要谨慎操作 1rm new_file --&gt; 删除 new_file 文件rm f1 f2 f3 --&gt; 同时删除 f1 f2 f3 3个文件 【常用参数】 -i 向用户确认是否删除； -f 文件强制删除； -r 递归删除文件夹，著名的删除操作 rm -rf 。 ln 英文 Link 的缩写，表示创建链接。 学习创建链接之前，首先要理解链接是什么，我们先来看看 Linux 的文件是如何存储的： Linux 文件的存储方式分为 3 个部分，文件名、文件内容以及权限，其中文件名的列表是存储在硬盘的其它地方和文件内容是分开存放的，每个文件名通过 inode 标识绑定到文件内容。 Linux 下有两种链接类型：硬链接和软链接。 硬链接 使链接的两个文件共享同样文件内容，就是同样的 inode ，一旦文件 1 和文件 2 之间有了硬链接，那么修改任何一个文件，修改的都是同一块内容，它的缺点是，只能创建指向文件的硬链接，不能创建指向目录的（其实也可以，但比较复杂）而软链接都可以，因此软链接使用更加广泛。 1ln file1 file2 --&gt; 创建 file2 为 file1 的硬链接 如果我们用 rm file1 来删除 file1 ，对 file2 没有什么影响，对于硬链接来说，删除任意一方的文件，共同指向的文件内容并不会从硬盘上删除。只有同时删除了 file1 与 file2 后，它们共同指向的文件内容才会消失。 软链接 软链接就类似 windows 下快捷方式。 1ln -s file1 file2 执行 ls -l 命名查看当前目录下文件的具体信息 1total 0-rw-r--r-- 1 root root 0 Jan 14 06:29 file1lrwxrwxrwx 1 root root 5 Jan 14 06:42 file2 -&gt; file1 # 表示file2 指向 file1 其实 file2 只是 file1 的一个快捷方式，它指向的是 file1 ，所以显示的是 file1 的内容，但其实 file2 的 inode 与 file1 并不相同。如果我们删除了 file2 的话， file1 是不会受影响的，但如果删除 file1 的话， file2 就会变成死链接，因为指向的文件不见了。 用户与权限 用户 Linux 是一个多用户的操作系统。在 Linux 中，理论上来说，我们可以创建无数个用户，但是这些用户是被划分到不同的群组里面的，有一个用户，名叫 root ，是一个很特殊的用户，它是超级用户，拥有最高权限。 自己创建的用户是有限权限的用户，这样大大提高了 Linux 系统的安全性，有效防止误操作或是病毒攻击，但是我们执行的某些命令需要更高权限时可以使用 sudo 命令。 sudo 以 root 身份运行命令 1sudo date --&gt; 当然查看日期是不需要sudo的这里只是演示，sudo 完之后一般还需要输入用户密码的 useradd + passwd useradd 添加新用户 passwd 修改用户密码 这两个命令需要 root 用户权限 1useradd lion --&gt; 添加一个lion用户，添加完之后在 /home 路径下可以查看passwd lion --&gt; 修改lion用户的密码 userdel 删除用户，需要 root 用户权限 1userdel lion --&gt; 只会删除用户名，不会从/home中删除对应文件夹userdel lion -r --&gt; 会同时删除/home下的对应文件夹 su 切换用户，需要 root 用户权限 1sudo su --&gt; 切换为root用户（exit 命令或 CTRL + D 快捷键都可以使普通用户切换为 root 用户）su lion --&gt; 切换为普通用户su - --&gt; 切换为root用户 群组的管理 Linux 中每个用户都属于一个特定的群组，如果你不设置用户的群组，默认会创建一个和它的用户名一样的群组，并且把用户划归到这个群组。 groupadd 创建群组，用法和 useradd 类似。 1groupadd friends groupdel 删除一个已存在的群组 1groupdel foo --&gt; 删除foo群组 groups 查看用户所在群组 1groups lion --&gt; 查看 lion 用户所在的群组 usermod 用于修改用户的账户。 【常用参数】 -l 对用户重命名。需要注意的是 /home 中的用户家目录的名字不会改变，需要手动修改。 -g 修改用户所在的群组，例如 usermod -g friends lion 修改 lion 用户的群组为 friends 。 -G 一次性让用户添加多个群组，例如 usermod -G friends,foo,bar lion 。 -a -G 会让你离开原先的群组，如果你不想这样做的话，就得再添加 -a 参数，意味着 append 追加的意思。 chgrp 用于修改文件的群组。 1chgrp bar file.txt --&gt; file.txt文件的群组修改为bar chown 改变文件的所有者，需要 root 身份才能运行。 1chown lion file.txt --&gt; 把其它用户创建的file.txt转让给lion用户chown lion:bar file.txt --&gt; 把file.txt的用户改为lion，群组改为bar 【常用参数】 -R 递归设置子目录和子文件， chown -R lion:lion /home/frank 把 frank 文件夹的用户和群组都改为 lion 。 文件权限管理 chmod 修改访问权限。 1chmod 740 file.txt 【常用参数】 -R 可以递归地修改文件访问权限，例如 chmod -R 777 /home/lion 修改权限的确简单，但是理解其深层次的意义才是更加重要的。下面我们来系统的学习 Linux 的文件权限。 1[root@lion ~]# ls -ldrwxr-xr-x 5 root root 4096 Apr 13 2020 climblrwxrwxrwx 1 root root 7 Jan 14 06:41 hello2.c -&gt; hello.c-rw-r--r-- 1 root root 149 Jan 13 06:14 hello.c 其中 drwxr-xr-x 表示文件或目录的权限。让我们一起来解读它具体代表什么？ d ：表示目录，就是说这是一个目录，普通文件是 - ，链接是 l 。 r ： read 表示文件可读。 w ： write 表示文件可写，一般有写的权限，就有删除的权限。 x ： execute 表示文件可执行。 - ：表示没有相应权限。 权限的整体是按用户来划分的，如下图所示： 现在再来理解这句权限 drwxr-xr-x 的意思： 它是一个文件夹； 它的所有者具有：读、写、执行权限； 它的群组用户具有：读、执行的权限，没有写的权限； 它的其它用户具有：读、执行的权限，没有写的权限。 现在理解了权限，我们使用 chmod 来尝试修改权限。 chmod 它不需要是 root 用户才能运行的，只要你是此文件所有者，就可以用 chmod 来修改文件的访问权限。 数字分配权限 权限 数字 r 4 w 2 x 1 因此要改变权限，只要做一些简单的加法就行： 1chmod 640 hello.c # 分析6 = 4 + 2 + 0 表示所有者具有 rw 权限4 = 4 + 0 + 0 表示群组用户具有 r 权限0 = 0 + 0 + 0 表示其它用户没有权限对应文字权限为：-rw-r----- 用字母来分配权限 u ： user 的缩写，用户的意思，表示所有者。 g ： group 的缩写，群组的意思，表示群组用户。 o ： other 的缩写，其它的意思，表示其它用户。 a ： all 的缩写，所有的意思，表示所有用户。 + ：加号，表示添加权限。 - ：减号，表示去除权限。 = ：等于号，表示分配权限。 1chmod u+rx file --&gt; 文件file的所有者增加读和运行的权限chmod g+r file --&gt; 文件file的群组用户增加读的权限chmod o-r file --&gt; 文件file的其它用户移除读的权限chmod g+r o-r file --&gt; 文件file的群组用户增加读的权限，其它用户移除读的权限chmod go-r file --&gt; 文件file的群组和其他用户移除读的权限chmod +x file --&gt; 文件file的所有用户增加运行的权限chmod u=rwx,g=r,o=- file --&gt; 文件file的所有者分配读写和执行的权限，群组其它用户分配读的权限，其他用户没有任何权限 查找文件 locate 搜索包含关键字的所有文件和目录。后接需要查找的文件名，也可以用正则表达式。 安装 locate 1yum -y install mlocate --&gt; 安装包updatedb --&gt; 更新数据库locate file.txtlocate fil*.txt [注意] locate 命令会去文件数据库中查找命令，而不是全磁盘查找，因此刚创建的文件并不会更新到数据库中，所以无法被查找到，可以执行 updatedb 命令去更新数据库。 find 用于查找文件，它会去遍历你的实际硬盘进行查找，而且它允许我们对每个找到的文件进行后续操作，功能非常强大。 1find &lt;何处&gt; &lt;何物&gt; &lt;做什么&gt; 何处：指定在哪个目录查找，此目录的所有子目录也会被查找。 何物：查找什么，可以根据文件的名字来查找，也可以根据其大小来查找，还可以根据其最近访问时间来查找。 做什么：找到文件后，可以进行后续处理，如果不指定这个参数， find 命令只会显示找到的文件。 根据文件名查找 1find -name &quot;file.txt&quot; --&gt; 当前目录以及子目录下通过名称查找文件find . -name &quot;syslog&quot; --&gt; 当前目录以及子目录下通过名称查找文件find / -name &quot;syslog&quot; --&gt; 整个硬盘下查找syslogfind /var/log -name &quot;syslog&quot; --&gt; 在指定的目录/var/log下查找syslog文件find /var/log -name &quot;syslog*&quot; --&gt; 查找syslog1、syslog2 ... 等文件，通配符表示所有find /var/log -name &quot;*syslog*&quot; --&gt; 查找包含syslog的文件 [注意] find 命令只会查找完全符合 “何物” 字符串的文件，而 locate 会查找所有包含关键字的文件。 根据文件大小查找 1find /var -size +10M --&gt; /var 目录下查找文件大小超过 10M 的文件find /var -size -50k --&gt; /var 目录下查找文件大小小于 50k 的文件find /var -size +1G --&gt; /var 目录下查找文件大小查过 1G 的文件find /var -size 1M --&gt; /var 目录下查找文件大小等于 1M 的文件 根据文件最近访问时间查找 1find -name &quot;*.txt&quot; -atime -7 --&gt; 近 7天内访问过的.txt结尾的文件 仅查找目录或文件 1find . -name &quot;file&quot; -type f --&gt; 只查找当前目录下的file文件find . -name &quot;file&quot; -type d --&gt; 只查找当前目录下的file目录 操作查找结果 1find -name &quot;*.txt&quot; -printf &quot;%p - %u\\n&quot; --&gt; 找出所有后缀为txt的文件，并按照 %p - %u\\n 格式打印，其中%p=文件名，%u=文件所有者find -name &quot;*.jpg&quot; -delete --&gt; 删除当前目录以及子目录下所有.jpg为后缀的文件，不会有删除提示，因此要慎用find -name &quot;*.c&quot; -exec chmod 600 &#123;&#125; \\; --&gt; 对每个.c结尾的文件，都进行 -exec 参数指定的操作，&#123;&#125; 会被查找到的文件替代，\\; 是必须的结尾find -name &quot;*.c&quot; -ok chmod 600 &#123;&#125; \\; --&gt; 和上面的功能一直，会多一个确认提示 软件仓库 Linux 下软件是以包的形式存在，一个软件包其实就是软件的所有文件的压缩包，是二进制的形式，包含了安装软件的所有指令。 Red Hat 家族的软件包后缀名一般为 .rpm ， Debian 家族的软件包后缀是 .deb 。 Linux 的包都存在一个仓库，叫做软件仓库，它可以使用 yum 来管理软件包， yum 是 CentOS 中默认的包管理工具，适用于 Red Hat 一族。可以理解成 Node.js 的 npm 。 yum 常用命令 yum update | yum upgrade 更新软件包 yum search xxx 搜索相应的软件包 yum install xxx 安装软件包 yum remove xxx 删除软件包 切换 CentOS 软件源 有时候 CentOS 默认的 yum 源不一定是国内镜像，导致 yum 在线安装及更新速度不是很理想。这时候需要将 yum 源设置为国内镜像站点。国内主要开源的镜像站点是网易和阿里云。 1、首先备份系统自带 yum 源配置文件 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 2、下载阿里云的 yum 源配置文件到 /etc/yum.repos.d/CentOS7 1wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 3、生成缓存 1yum makecache 阅读手册 Linux 命令种类繁杂，我们凭借记忆不可能全部记住，因此学会查用手册是非常重要的。 man 安装更新 man 1sudo yum install -y man-pages --&gt; 安装sudo mandb --&gt; 更新 man 手册种类 可执行程序或 Shell 命令； 系统调用（ Linux 内核提供的函数）； 库调用（程序库中的函数）； 文件（例如 /etc/passwd ）； 特殊文件（通常在 /dev 下）； 游戏； 杂项（ man(7) ， groff(7) ）； 系统管理命令（通常只能被 root 用户使用）； 内核子程序。 man + 数字 + 命令 输入 man + 数字 + 命令 / 函数，可以查到相关的命令和函数，若不加数字， man 默认从数字较小的手册中寻找相关命令和函数 1man 3 rand --&gt; 表示在手册的第三部分查找 rand 函数man ls --&gt; 查找 ls 用法手册 man 手册核心区域解析：(以 man pwd 为例) 12345678910NAME # 命令名称和简单描述 pwd -- return working directory nameSYNOPSIS # 使用此命令的所有方法 pwd [-L | -P]DESCRIPTION # 包括所有参数以及用法 The pwd utility writes the absolute pathname of the current working directory to the standard output. Some shells may provide a builtin pwd command which is similar or identical to this utility. Consult the builtin(1) manual page. The options are as follows: -L Display the logical current working directory. -P Display the physical current working directory (all symbolic links resolved). If no options are specified, the -L option is assumed.SEE ALSO # 扩展阅读相关命令 builtin(1), cd(1), csh(1), sh(1), getcwd(3) help man 命令像新华词典一样可以查询到命令或函数的详细信息，但其实我们还有更加快捷的方式去查询， command --help 或 command -h ，它没有 man 命令显示的那么详细，但是它更加易于阅读。 Linux 进阶 文本操作 grep 全局搜索一个正则表达式，并且打印到屏幕。简单来说就是，在文件中查找关键字，并显示关键字所在行。 基础语法 1grep text file # text代表要搜索的文本，file代表供搜索的文件# 实例[root@lion ~]# grep path /etc/profilepathmunge () &#123; pathmunge /usr/sbin pathmunge /usr/local/sbin pathmunge /usr/local/sbin after pathmunge /usr/sbin afterunset -f pathmunge 常用参数 -i 忽略大小写， grep -i path /etc/profile -n 显示行号， grep -n path /etc/profile -v 只显示搜索文本不在的那些行， grep -v path /etc/profile -r 递归查找， grep -r hello /etc ，Linux 中还有一个 rgrep 命令，作用相当于 grep -r 高级用法 grep 可以配合正则表达式使用。 1grep -E path /etc/profile --&gt; 完全匹配pathgrep -E ^path /etc/profile --&gt; 匹配path开头的字符串grep -E [Pp]ath /etc/profile --&gt; 匹配path或Path sort 对文件的行进行排序。 基础语法 1sort name.txt # 对name.txt文件进行排序 实例用法 为了演示方便，我们首先创建一个文件 name.txt ，放入以下内容： 1ChristopherShawnTedRockNoahZacharyBella 执行 sort name.txt 命令，会对文本内容进行排序。 常用参数 -o 将排序后的文件写入新文件， sort -o name_sorted.txt name.txt ； -r 倒序排序， sort -r name.txt ； -R 随机排序， sort -R name.txt ； -n 对数字进行排序，默认是把数字识别成字符串的，因此 138 会排在 25 前面，如果添加了 -n 数字排序的话，则 25 会在 138 前面。 wc word count 的缩写，用于文件的统计。它可以统计单词数目、行数、字符数，字节数等。 基础语法 1wc name.txt # 统计name.txt 实例用法 1[root@lion ~]# wc name.txt 13 13 91 name.txt 第一个 13，表示行数； 第二个 13，表示单词数； 第三个 91，表示字节数。 常用参数 -l 只统计行数， wc -l name.txt ； -w 只统计单词数， wc -w name.txt ； -c 只统计字节数， wc -c name.txt ； -m 只统计字符数， wc -m name.txt 。 uniq 删除文件中的重复内容。 基础语法 1uniq name.txt # 去除name.txt重复的行数，并打印到屏幕上uniq name.txt uniq_name.txt # 把去除重复后的文件保存为 uniq_name.txt 【注意】它只能去除连续重复的行数。 常用参数 -c 统计重复行数， uniq -c name.txt ； -d 只显示重复的行数， uniq -d name.txt 。 cut 剪切文件的一部分内容。 基础语法 1cut -c 2-4 name.txt # 剪切每一行第二到第四个字符 常用参数 -d 用于指定用什么分隔符（比如逗号、分号、双引号等等） cut -d , name.txt ； -f 表示剪切下用分隔符分割的哪一块或哪几块区域， cut -d , -f 1 name.txt 。 重定向 管道 流 在 Linux 中一个命令的去向可以有 3 个地方：终端、文件、作为另外一个命令的入参。 命令一般都是通过键盘输入，然后输出到终端、文件等地方，它的标准用语是 stdin 、 stdout 以及 stderr 。 标准输入 stdin ，终端接收键盘输入的命令，会产生两种输出； 标准输出 stdout ，终端输出的信息（不包含错误信息）； 标准错误输出 stderr ，终端输出的错误信息。 重定向 把本来要显示在终端的命令结果，输送到别的地方（到文件中或者作为其他命令的输入）。 输出重定向 &gt; &gt; 表示重定向到新的文件， cut -d , -f 1 notes.csv &gt; name.csv ，它表示通过逗号剪切 notes.csv 文件（剪切完有 3 个部分）获取第一个部分，重定向到 name.csv 文件。 我们来看一个具体示例，学习它的使用，假设我们有一个文件 notes.csv ，文件内容如下： 1Mark1,951/100,很不错1Mark2,952/100,很不错2Mark3,953/100,很不错3Mark4,954/100,很不错4Mark5,955/100,很不错5Mark6,956/100,很不错6 执行命令： cut -d , -f 1 notes.csv &gt; name.csv 最后输出如下内容： 1Mark1Mark2Mark3Mark4Mark5Mark6 【注意】使用 &gt; 要注意，如果输出的文件不存在它会新建一个，如果输出的文件已经存在，则会覆盖。因此执行这个操作要非常小心，以免覆盖其它重要文件。 输出重定向 &gt;&gt; 表示重定向到文件末尾，因此它不会像 &gt; 命令这么危险，它是追加到文件的末尾（当然如果文件不存在，也会被创建）。 再次执行 cut -d , -f 1 notes.csv &gt;&gt; name.csv ，则会把名字追加到 name.csv 里面。 1Mark1Mark2Mark3Mark4Mark5Mark6Mark1Mark2Mark3Mark4Mark5Mark6 我们平时读的 log 日志文件其实都是用这个命令输出的。 输出重定向 2&gt; 标准错误输出 1cat not_exist_file.csv &gt; res.txt 2&gt; errors.log 当我们 cat 一个文件时，会把文件内容打印到屏幕上，这个是标准输出； 当使用了 &gt; res.txt 时，则不会打印到屏幕，会把标准输出写入文件 res.txt 文件中； 2&gt; errors.log 当发生错误时会写入 errors.log 文件中。 输出重定向 2&gt;&gt; 标准错误输出（追加到文件末尾）同 &gt;&gt; 相似。 输出重定向 2&gt;&amp;1 标准输出和标准错误输出都重定向都一个地方 1cat not_exist_file.csv &gt; res.txt 2&gt;&amp;1 # 覆盖输出cat not_exist_file.csv &gt;&gt; res.txt 2&gt;&amp;1 # 追加输出 目前为止，我们接触的命令的输入都来自命令的参数，其实命令的输入还可以来自文件或者键盘的输入。 输入重定向 &lt; &lt; 符号用于指定命令的输入。 1cat &lt; name.csv # 指定命令的输入为 name.csv 虽然它的运行结果与 cat name.csv 一样，但是它们的原理却完全不同。 cat name.csv 表示 cat 命令接收的输入是 notes.csv 文件名，那么要先打开这个文件，然后打印出文件内容。 cat &lt; name.csv 表示 cat 命令接收的输入直接是 notes.csv 这个文件的内容， cat 命令只负责将其内容打印，打开文件并将文件内容传递给 cat 命令的工作则交给终端完成。 输入重定向 &lt;&lt; 将键盘的输入重定向为某个命令的输入。 1sort -n &lt;&lt; END # 输入这个命令之后，按下回车，终端就进入键盘输入模式，其中END为结束命令（这个可以自定义）wc -m &lt;&lt; END # 统计输入的单词 管道 | 把两个命令连起来使用，一个命令的输出作为另外一个命令的输入，英文是 pipeline ，可以想象一个个水管连接起来，管道算是重定向流的一种。 举几个实际用法案例： 123456cut -d , -f 1 name.csv | sort &gt; sorted_name.txt # 第一步获取到的 name 列表，通过管道符再进行排序，最后输出到sorted_name.txtdu | sort -nr | head # du 表示列举目录大小信息# sort 进行排序,-n 表示按数字排序，-r 表示倒序# head 前10行文件grep log -Ir /var/log | cut -d : -f 1 | sort | uniq# grep log -Ir /var/log 表示在log文件夹下搜索 /var/log 文本，-r 表示递归，-I 用于排除二进制文件# cut -d : -f 1 表示通过冒号进行剪切，获取剪切的第一部分# sort 进行排序# uniq 进行去重 流 流并非一个命令，在计算机科学中，流 stream 的含义是比较难理解的，记住一点即可：流就是读一点数据，处理一点点数据。其中数据一般就是二进制格式。 上面提及的重定向或管道，就是把数据当做流去运转的。 到此我们就接触了，流、重定向、管道等 Linux 高级概念及指令。其实你会发现关于流和管道在其它语言中也有广泛的应用。 Angular 中的模板语法中可以使用管道。 Node.js 中也有 stream 流的概念。 查看进程 在 Windows 中通过 Ctrl + Alt + Delete 快捷键查看软件进程。 w 帮助我们快速了解系统中目前有哪些用户登录着，以及他们在干什么。 1[root@lion ~]# w 06:31:53 up 25 days, 9:53, 1 user, load average: 0.00, 0.01, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 118.31.243.53 05:56 1.00s 0.02s 0.00s w 06:31:53：表示当前时间up 25 days, 9:53：表示系统已经正常运行了“25天9小时53分钟”1 user：表示一个用户load average: 0.00, 0.01, 0.05：表示系统的负载，3个值分别表示“1分钟的平均负载”，“5分钟的平均负载”，“15分钟的平均负载” USER：表示登录的用于 TTY：登录的终端名称为pts/0 FROM：连接到服务器的ip地址 LOGIN@：登录时间 IDLE：用户有多久没有活跃了 JCPU：该终端所有相关的进程使用的 CPU 时间，每当进程结束就停止计时，开始新的进程则会重新计时 PCPU：表示 CPU 执行当前程序所消耗的时间，当前进程就是在 WHAT 列里显示的程序 WHAT：表示当下用户正运行的程序是什么，这里我运行的是 w ps 用于显示当前系统中的进程， ps 命令显示的进程列表不会随时间而更新，是静态的，是运行 ps 命令那个时刻的状态或者说是一个进程快照。 基础语法 12345[root@lion ~]# ps PID TTY TIME CMD 1793 pts/0 00:00:00 bash 4756 pts/0 00:00:00 ps PID：进程号，每个进程都有唯一的进程号 TTY：进程运行所在的终端 TIME：进程运行时间 CMD：产生这个进程的程序名，如果在进程列表中看到有好几行都是同样的程序名，那么就是同样的程序产生了不止一个进程 常用参数 -ef 列出所有进程； -efH 以乔木状列举出所有进程； -u 列出此用户运行的进程； -aux 通过 CPU 和内存使用来过滤进程 ps -aux | less ; -aux --sort -pcpu 按 CPU 使用降序排列， -aux --sort -pmem 表示按内存使用降序排列； -axjf 以树形结构显示进程， ps -axjf 它和 pstree 效果类似。 top 获取进程的动态列表。 1top - 07:20:07 up 25 days, 10:41, 1 user, load average: 0.30, 0.10, 0.07Tasks: 67 total, 1 running, 66 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.7 us, 0.3 sy, 0.0 ni, 99.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 1882072 total, 552148 free, 101048 used, 1228876 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 1594080 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 956 root 10 -10 133964 15848 10240 S 0.7 0.8 263:13.01 AliYunDun 1 root 20 0 51644 3664 2400 S 0.0 0.2 3:23.63 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.05 kthreadd 4 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H top - 07:20:07 up 25 days, 10:41, 1 user, load average: 0.30, 0.10, 0.07 相当 w 命令的第一行的信息。 展示的这些进程是按照使用处理器 %CPU 的使用率来排序的。 kill 结束一个进程， kill + PID 。 1kill 956 # 结束进程号为956的进程kill 956 957 # 结束多个进程kill -9 7291 # 强制结束进程 管理进程 进程状态 主要是切换进程的状态。我们先了解下 Linux 下进程的五种状态： 状态码 R ：表示正在运行的状态； 状态码 S ：表示中断（休眠中，受阻，当某个条件形成后或接受到信号时，则脱离该状态）； 状态码 D ：表示不可中断（进程不响应系统异步信号，即使用 kill 命令也不能使其中断）； 状态码 Z ：表示僵死（进程已终止，但进程描述符依然存在，直到父进程调用 wait4() 系统函数后将进程释放）； 状态码 T ：表示停止（进程收到 SIGSTOP 、 SIGSTP 、 SIGTIN 、 SIGTOU 等停止信号后停止运行）。 前台进程 &amp; 后台进程 默认情况下，用户创建的进程都是前台进程，前台进程从键盘读取数据，并把处理结果输出到显示器。例如运行 top 命令，这就是一个一直运行的前台进程。 后台进程的优点是不必等待程序运行结束，就可以输入其它命令。在需要执行的命令后面添加 &amp; 符号，就表示启动一个后台进程。 &amp; 启动后台进程，它的缺点是后台进程与终端相关联，一旦关闭终端，进程就自动结束了。 1cp name.csv name-copy.csv &amp; nohup 使进程不受挂断（关闭终端等动作）的影响。 1nohup cp name.csv name-copy.csv nohup 命令也可以和 &amp; 结合使用。 1nohup cp name.csv name-copy.csv &amp; bg 使一个 “后台暂停运行” 的进程，状态改为 “后台运行”。 1bg %1 # 不加任何参数的情况下，bg命令会默认作用于最近的一个后台进程，如果添加参数则会作用于指定标号的进程 实际案例 1： 11. 执行 grep -r &quot;log&quot; / &gt; grep_log 2&gt;&amp;1 命令启动一个前台进程，并且忘记添加 &amp; 符号2. ctrl + z 使进程状态转为后台暂停3. 执行 bg 将命令转为后台运行 实际案例 2： 1前端开发时我们经常会执行 yarn start 启动项目此时我们执行 ctrl + z 先使其暂停然后执行 bg 使其转为后台运行这样当前终端就空闲出来可以干其它事情了，如果想要唤醒它就使用 fg 命令即可（后面会讲） jobs 显示当前终端后台进程状态。 1[root@lion ~]# jobs[1]+ Stopped top[2]- Running grep --color=auto -r &quot;log&quot; / &gt; grep_log 2&gt;&amp;1 &amp; fg fg 使进程转为前台运行，用法和 bg 命令类似。 我们用一张图来表示前后台进程切换： 我们可以使程序在后台运行，成为后台进程，这样在当前终端中我们就可以做其他事情了，而不必等待此进程运行结束。 守护进程 一个运行起来的程序被称为进程。在 Linux 中有些进程是特殊的，它不与任何进程关联，不论用户的身份如何，都在后台运行，这些进程的父进程是 PID 为 1 的进程， PID 为 1 的进程只在系统关闭时才会被销毁。它们会在后台一直运行等待分配工作。我们将这类进程称之为守护进程 daemon 。 守护进程的名字通常会在最后有一个 d ，表示 daemon 守护的意思，例如 systemd 、 httpd 。 systemd systemd 是一个 Linux 系统基础组件的集合，提供了一个系统和服务管理器，运行为 PID 1 并负责启动其它程序。 1[root@lion ~]# ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.2 51648 3852 ? Ss Feb01 1:50 /usr/lib/systemd/systemd --switched-root --system --deserialize 22 通过命令也可以看到 PID 为 1 的进程就是 systemd 的系统进程。 systemd 常用命令（它是一组命令的集合）： 1systemctl start nginx # 启动服务systemctl stop nginx # 停止服务systemctl restart nginx # 重启服务systemctl status nginx # 查看服务状态systemctl reload nginx # 重载配置文件(不停止服务的情况)systemctl enable nginx # 开机自动启动服务systemctl disable nginx # 开机不自动启动服务systemctl is-enabled nginx # 查看服务是否开机自动启动systemctl list-unit-files --type=service # 查看各个级别下服务的启动和禁用情况 文件压缩解压 打包：是将多个文件变成一个总的文件，它的学名叫存档、归档。 压缩：是将一个大文件（通常指归档）压缩变成一个小文件。 我们常常使用 tar 将多个文件归档为一个总的文件，称为 archive 。然后用 gzip 或 bzip2 命令将 archive 压缩为更小的文件。 tar 创建一个 tar 归档。 基础用法 1tar -cvf sort.tar sort/ # 将sort文件夹归档为sort.tartar -cvf archive.tar file1 file2 file3 # 将 file1 file2 file3 归档为archive.tar 常用参数 -cvf 表示 create （创建）+ verbose （细节）+ file （文件），创建归档文件并显示操作细节； -tf 显示归档里的内容，并不解开归档； -rvf 追加文件到归档， tar -rvf archive.tar file.txt ； -xvf 解开归档， tar -xvf archive.tar 。 gzip / gunzip “压缩 / 解压” 归档，默认用 gzip 命令，压缩后的文件后缀名为 .tar.gz 。 1gzip archive.tar # 压缩gunzip archive.tar.gz # 解压 tar 归档 + 压缩 可以用 tar 命令同时完成归档和压缩的操作，就是给 tar 命令多加一个选项参数，使之完成归档操作后，还是调用 gzip 或 bzip2 命令来完成压缩操作。 1tar -zcvf archive.tar.gz archive/ # 将archive文件夹归档并压缩tar -zxvf archive.tar.gz # 将archive.tar.gz归档压缩文件解压 zcat、zless、zmore 之前讲过使用 cat less more 可以查看文件内容，但是压缩文件的内容是不能使用这些命令进行查看的，而要使用 zcat、zless、zmore 进行查看。 1zcat archive.tar.gz zip/unzip “压缩 / 解压” zip 文件（ zip 压缩文件一般来自 windows 操作系统）。 命令安装 1# Red Hat 一族中的安装方式yum install zip yum install unzip 基础用法 1unzip archive.zip # 解压 .zip 文件unzip -l archive.zip # 不解开 .zip 文件，只看其中内容zip -r sort.zip sort/ # 将sort文件夹压缩为 sort.zip，其中-r表示递归 编译安装软件 之前我们学会了使用 yum 命令进行软件安装，如果碰到 yum 仓库中没有的软件，我们就需要会更高级的软件安装 “源码编译安装”。 编译安装 简单来说，编译就是将程序的源代码转换成可执行文件的过程。大多数 Linux 的程序都是开放源码的，可以编译成适合我们的电脑和操纵系统属性的可执行文件。 基本步骤如下： 下载源代码 解压压缩包 配置 编译 安装 实际案例 1、下载 我们来编译安装 htop 软件，首先在它的官网下载源码：bintray.com/htop/source…[1] 下载好的源码在本机电脑上使用如下命令同步到服务器上： 1scp 文件名 用户名@服务器ip:目标路径scp ~/Desktop/htop-3.0.0.tar.gz root@121.42.11.34:. 也可以使用 wegt 进行下载： 1wegt+下载地址wegt https://bintray.com/htop/source/download_file?file_path=htop-3.0.0.tar.gz 2、解压文件 1tar -zxvf htop-3.0.0.tar.gz # 解压cd htop-3.0.0 # 进入目录 3、配置 执行 ./configure ，它会分析你的电脑去确认编译所需的工具是否都已经安装了。 4、编译 执行 make 命令 5、安装 执行 make install 命令，安装完成后执行 ls /usr/local/bin/ 查看是否有 htop 命令。如果有就可以执行 htop 命令查看系统进程了。 网络 ifconfig 查看 ip 网络相关信息，如果命令不存在的话， 执行命令 yum install net-tools 安装。 1[root@lion ~]# ifconfigeth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.31.24.78 netmask 255.255.240.0 broadcast 172.31.31.255 ether 00:16:3e:04:9c:cd txqueuelen 1000 (Ethernet) RX packets 1592318 bytes 183722250 (175.2 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1539361 bytes 154044090 (146.9 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 参数解析： eth0 对应有线连接（对应你的有线网卡），就是用网线来连接的上网。 eth 是 Ethernet 的缩写，表示 “以太网”。有些电脑可能同时有好几条网线连着，例如服务器，那么除了 eht0 ，你还会看到 eth1 、 eth2 等。 lo 表示本地回环（ Local Loopback 的缩写，对应一个虚拟网卡）可以看到它的 ip 地址是 127.0.0.1 。每台电脑都应该有这个接口，因为它对应着 “连向自己的链接”。这也是被称之为 “本地回环” 的原因。所有经由这个接口发送的东西都会回到你自己的电脑。看起来好像并没有什么用，但有时为了某些缘故，我们需要连接自己。例如用来测试一个网络程序，但又不想让局域网或外网的用户查看，只能在此台主机上运行和查看所有的网络接口。例如在我们启动一个前端工程时，在浏览器输入 127.0.0.1:3000 启动项目就能查看到自己的 web 网站，并且它只有你能看到。 wlan0 表示无线局域网（上面案例并未展示）。 host ip 地址和主机名的互相转换。 软件安装 1yum install bind-utils 基础用法 12[root@lion ~]# host github.combaidu.com has address 13.229.188.59 [root@lion ~]# host 13.229.188.5959.188.229.13.in-addr.arpa domain name pointer ec2-13-229-188-59.ap-southeast-1.compute.amazonaws.com. ssh 连接远程服务器 通过非对称加密以及对称加密的方式（同 HTTPS 安全连接原理相似）连接到远端服务器。 1ssh 用户@ip:port1、ssh root@172.20.10.1:22 # 端口号可以省略不写，默认是22端口2、输入连接密码后就可以操作远端服务器了 配置 ssh config 文件可以配置 ssh ，方便批量管理多个 ssh 连接。 配置文件分为以下几种： 全局 ssh 服务端的配置： /etc/ssh/sshd_config ； 全局 ssh 客户端的配置： /etc/ssh/ssh_config （很少修改）； 当前用户 ssh 客户端的配置： ~/.ssh/config 。 【服务端 config 文件的常用配置参数】 服务端 config 参数 作用 Port sshd 服务端口号（默认是 22） PermitRootLogin 是否允许以 root 用户身份登录（默认是可以） PasswordAuthentication 是否允许密码验证登录（默认是可以） PubkeyAuthentication 是否允许公钥验证登录（默认是可以） PermitEmptyPasswords 是否允许空密码登录（不安全，默认不可以） [注意] 修改完服务端配置文件需要重启服务 systemctl restart sshd 【客户端 config 文件的常用配置参数】 客户端 config 参数 作用 Host 别名 HostName 远程主机名（或 IP 地址） Port 连接到远程主机的端口 User 用户名 配置当前用户的 config ： 1# 创建configvim ~/.ssh/config# 填写一下内容Host lion # 别名 HostName 172.x.x.x # ip 地址 Port 22 # 端口 User root # 用户 这样配置完成后，下次登录时，可以这样登录 ssh lion 会自动识别为 root 用户。 [注意] 这段配置不是在服务器上，而是你自己的机器上，它仅仅是设置了一个别名。 免密登录 ssh 登录分两种，一种是基于口令（账号密码），另外一种是基于密钥的方式。 基于口令，就是每次登录输入账号和密码，显然这样做是比较麻烦的，今天主要学习如何基于密钥实现免密登录。 （1）基于密钥验证原理 客户机生成密钥对（公钥和私钥），把公钥上传到服务器，每次登录会与服务器的公钥进行比较，这种验证登录的方法更加安全，也被称为 “公钥验证登录”。 （2）具体实现步骤 1、在客户机中生成密钥对（公钥和私钥） ssh-keygen （默认使用 RSA 非对称加密算法） 运行完 ssh-keygen 会在 ~/.ssh/ 目录下，生成两个文件： id_rsa.pub ：公钥 id_rsa ：私钥 2、把客户机的公钥传送到服务 执行 ssh-copy-id root@172.x.x.x （ ssh-copy-id 它会把客户机的公钥追加到服务器 ~/.ssh/authorized_keys 的文件中）。 执行完成后，运行 ssh root@172.x.x.x 就可以实现免密登录服务器了。 配合上面设置好的别名，直接执行 ssh lion 就可以登录，是不是非常方便。 wget 可以使我们直接从终端控制台下载文件，只需要给出文件的 HTTP 或 FTP 地址。 1wget [参数][URL地址]wget http://www.minjieren.com/wordpress-3.1-zh_CN.zip wget 非常稳定，如果是由于网络原因下载失败， wget 会不断尝试，直到整个文件下载完毕。 常用参数 -c 继续中断的下载。 备份 scp 它是 Secure Copy 的缩写，表示安全拷贝。 scp 可以使我们通过网络，把文件从一台电脑拷贝到另一台电脑。 scp 是基于 ssh 的原理来运作的， ssh 会在两台通过网络连接的电脑之间创建一条安全通信的管道， scp 就利用这条管道安全地拷贝文件。 1scp source_file destination_file # source_file 表示源文件，destination_file 表示目标文件 其中 source_file 和 destination_file 都可以这样表示： user@ip:file_name ， user 是登录名， ip 是域名或 ip 地址。 file_name 是文件路径。 12scp file.txt root@192.168.1.5:/root # 表示把我的电脑中当前文件夹下的 file.txt 文件拷贝到远程电脑scp root@192.168.1.5:/root/file.txt file.txt # 表示把远程电脑上的 file.txt 文件拷贝到本机 rsync rsync 命令主要用于远程同步文件。它可以同步两个目录，不管它们是否处于同一台电脑。它应该是最常用于 “增量备份” 的命令了。它就是智能版的 scp 命令。 软件安装 1yum install rsync 基础用法 1rsync -arv Images/ backups/ # 将Images 目录下的所有文件备份到 backups 目录下rsync -arv Images/ root@192.x.x.x:backups/ # 同步到服务器的backups目录下 常用参数 -a 保留文件的所有信息，包括权限，修改日期等； -r 递归调用，表示子目录的所有文件也都包括； -v 冗余模式，输出详细操作信息。 默认地， rsync 在同步时并不会删除目标目录的文件，例如你在源目录中删除一个文件，但是用 rsync 同步时，它并不会删除同步目录中的相同文件。如果向删除也可以这么做： rsync -arv --delete Images/ backups/ 。 系统 halt 关闭系统，需要 root 身份。 1halt reboot 重启系统，需要 root 身份。 1reboot poweroff 直接运行即可关机，不需要 root 身份。 Vim 编辑器 Vim 是什么？ Vim 是从 vi 发展出来的一个文本编辑器。其代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。和 Emacs 并列成为类 Unix 系统用户最喜欢的编辑器。 Vim 常用模式 交互模式 插入模式 命令模式 可视模式 交互模式 也成为正常模式，这是 Vim 的默认模式，每次运行 Vim 程序的时候，就会进入这个模式。 例如执行 vim name.txt 则会进入交互模式。 交互模式特征： 在这个模式下，你不能输入文本； 它可以让我们在文本间移动，删除一行文本，复制黏贴文本，跳转到指定行，撤销操作，等等。 插入模式 这个模式是我们熟悉的文本编辑器的模式，就是可以输入任何你想输入的内容。进入这个模式有几种方法，最常用的方法是按字母键 i （ i、I、a、A、o、O 都可以进入插入模式，只是所处的位置不同），退出这种模式，只需要按下 Esc 键。 i, I 进入输入模式 Insert mode ： i 为 “从目前光标所在处输入”， I 为 “在目前所在行的第一个非空格符处开始输入”； a, A 进入输入模式 Insert mode ： a 为 “从目前光标所在的下一个字符处开始输入”， A 为 “从光标所在行的最后一个字符处开始输入”； o, O 进入输入模式 Insert mode ： o 为 “在目前光标所在的下一行处输入新的一行”； O 为在目前光标所在处的上一行输入新的一行。 命令模式 命令模式也称为底线命令模式，这个模式下可以运行一些命令例如 “退出”，“保存”，等动作。 也可以用这个模式来激活一些 Vim 配置，例如语法高亮，显示行号，等。甚至还可以发送一些命令给终端命令行，例如 ls、cp 。 为了进入命令模式，首先要进入交互模式，再按下冒号键。 用一张图表示三种模式如何切换： 基本操作 打开 Vim 在终端命令行中输入 vim 回车后 Vim 就会被运行起来，也可以用 Vim 来打开一个文件，只需要在 vim 后面再加文件名。如 vim file.name ，如果文件不存在，那么会被创建。 插入 进入文件之后，此时处于交互模式，可以通过输入 i 进入插入模式。 移动 在 Vim 的交互模式下，我们可以在文本中移动光标。 h 向左移动一个字符 j 向下移动一个字符 k 向上移动一个字符 i 向右移动一个字符 当然也可以使用四个方向键进行移动，效果是一样的。 跳至行首和行末 行首：在交互模式下，为了将光标定位到一行的开始位置，只需要按下数字键 0 即可，键盘上的 Home 键也有相同效果。 行末：在交互模式下，为了将光标定位到一行的末尾，只需要按下美元符号键 $ 即可，键盘上的 End 键也有相同效果。 按单词移动 在交互模式下，按字母键 w 可以一个单词一个单词的移动。 退出文件 在交互模式下，按下冒号键 : 进入命令模式，再按下 q 键，就可以退出了。 如果在退出之前又修改了文件，就直接想用 :q 退出 Vim ，那么 Vim 会显示一个红字标明错误信息。此时我们有两个选择： 保存并退出 :wq 或 :x ； 不保存且退出 :q! 。 标准操作 删除字符 在交互模式下，将光标定位到一个你想要删除的字符上，按下字母键 x 你会发现这个字符被删除了。 也可以一次性删除多个字符，只需要在按 x 键之前输入数字即可。 删除（剪切）单词，行 删除一行：连按两次 d 来删除光标所在的那一行。 删除多行：例如先输入数字 2 ，再按下 dd ，就会删除从光标所在行开始的两行。 删除一个单词：将光标置于一个单词的首字母处，然后按下 dw 。 删除多个单词：例如先按数字键 2 再按 dw 就可以删除两个单词了。 从光标所在位置删除至行首： d0 。 从光标所在位置删除至行末： d$ 。 复制单词，行 复制行：按两次 y 会把光标所在行复制到内存中，和 dd 类似， dd 用于 “剪切” 光标所在行。 复制单词： yw 会复制一个单词。 复制到行末： y$ 是复制从光标所在处到行末的所有字符。 复制到行首： y0 是复制光标所在处到行首的所有字符。 粘贴 如果之前用 dd 或者 yy 剪切复制过来的，可以使用 p 来粘贴。同样也可以使用 数字+p 来表示复制多次。 替换一个字符 在交互模式下，将光标置于想要替换的字符上。按下 r 键，接着输入你要替换的字符即可。 撤销操作 如果要撤销最近的修改，只需要按下 u 键，如果想要撤销最近四次修改，可以按下 4，再按下 u 。 重做 取消撤销，也就是重做之前的修改使用 ctrl + r 。 跳转到指定行 Vim 编辑的文件中，每一行都有一个行号，行号从 1 开始，逐一递增。 行号默认是不显示，如果需要它显示的话，可以进入命令模式，然后输入 set nu ，如果要隐藏行号的话，使用 set nonu 。 跳转到指定行： 数字+gg ，例如 7gg ，表示跳转到第 7 行。 要跳转到最后一行，按下 G 。 要跳转到第一行，按下 gg 。 高级操作 查找 处于交互模式下，按下 / 键，那么就进入查找模式，输入你要查找的字符串，然后按下回车。光标就会跳转到文件中下一个查找到的匹配处。如果字符串不存在，那么会显示 &quot;pattern not found&quot; 。 n 跳转到下一个匹配项； N 跳转到上一个匹配项。 [注意] 用斜杠来进行的查找是从当前光标处开始向文件尾搜索，如果你要从当前光标处开始，向文件头搜索则使用 ? ，当然也可以先按下 gg 跳转到第一行在进行全文搜索。 查找并替换 替换光标所在行第一个匹配的字符串： 1# 语法:s/旧字符串/新字符串# 实例:s/one/two 替换光标所在行所有旧字符串为新字符串： 1# 语法:s/旧字符串/新字符串/g 替换第几行到第几行中所有字符串： 1# 语法:n,m s/旧字符串/新字符串/g# 实例:2,4 s/one/two/g 最常用的就是全文替换了： 1# 语法:%s/旧字符串/新字符串/g 合并文件 可以用冒号 +r ( :r ) 实现在光标处插入一个文件的内容。 1:r filename # 可以用Tab键来自动补全另外一个文件的路径 分屏 Vim 有一个特别便捷的功能那就是分屏，可以同时打开好几个文件，分屏之后，屏幕每一块被称为一个 viewport ，表示 “视口”。 横向分屏 :sp 文件名 垂直分屏 :vsp 文件名 分屏模式下的快捷键 Ctrl + w 再加 Ctrl + w ，表示从一个 viewport 移动光标到另外一个 viewport ； Ctrl + w 再加 “方向键”，就可以移动到这个方向所处的下一个视口了； Ctrl + w 再加 + 号，表示扩大当前视口； Ctrl + w 再加 - 号，表示缩小当前视口； Ctrl + w 再加 = 号，表示平均当前视口； Ctrl + w 再加 r 键，会反向调换视口位置； Ctrl + w 再加 q 键，会关闭当前视口； Ctrl + w 再加 o 键，会关闭除当前视口以外的所有视口； 运行外部命令 :! 在 Vim 中可以运行一些终端命令，只要先输入 :! ，然后接命令名称。 例如： 1:!ls # 在Vim中打开的文件所在的目录运行ls命令 可视模式 前面只讲了 Vim 的三种模式，其实还有一种模式叫做可视模式。 进入它的三种方式（都是从交互模式开始）： v 字符可视模式，进入后配合方向键选中字符后，然后再按 d 键可以删除选中。 V 行可视模式，进入后光标所在行默认被选中，然后再按 d 键可以删除所在行。 Ctrl + v 块可视模式，它是可视模式最有用的功能了，配合 d 和 I 键可以实现删除选中的内容和插入内容。 同时选中多行，并在选中行头部插入内容的具体操作步骤： 12341. ctrl + v 进入块可视模式2. 使用方向键进行选中（上下左右）假设选中5行3. 输入 I 键进行多行同时插入操作4. 插入完成后连续按两下 esc 键，实现多行同时插入相同字符 进入可视模式之后的操作键： d 键，表示删除选中； I 键，表示在选中之前插入； u 键，表示选中变为小写； U 键，表示选中变为大写； Vim 配置 选项参数 在 Vim 被启动后，可以运行一些指令来激活一些选项参数，但是这些选项参数的配置在退出 Vim 时会被忘记，例如前面讲解的激活行号。如果希望所在的配置是永久性的，那么需要在家目录（ cd ~ ）创建一个 Vim 的配置文件 .vimrc 。 .vimrc 12345set number &quot; 显示行号syntax on &quot; 激活语法高亮set showcmd &quot; 实时看到输入的命令set ignorecase &quot; 搜索时不区分大小写set mouse=a &quot; 激活鼠标，用鼠标选中时相当于进入可视模式 Vim 配置非常丰富，我们可以通过个性化配置把 Vim 打造成属于自己的 IDE 等等。在 github 上也可以搜索到一些强大的 Vim 配置文件。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"Linux","slug":"Linux","permalink":"https://leezhao415.github.io/tags/Linux/"}]},{"title":"深度学习模型参数量_FLOPs计算","slug":"深度学习模型参数量-FLOPs计算","date":"2022-02-20T15:06:30.000Z","updated":"2022-02-20T15:14:16.180Z","comments":true,"path":"2022/02/20/深度学习模型参数量-FLOPs计算/","link":"","permalink":"https://leezhao415.github.io/2022/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E9%87%8F-FLOPs%E8%AE%A1%E7%AE%97/","excerpt":"","text":"文章目录 1. 对 CNN 而言，每个卷积层的参数量计算如下： 2. 对 CNN 而言，每个卷积层的运算量计算如下： 3. 对全连接层而言，其参数量非常容易计算： 4. 对全连接层而言，其运算量计算如下： 本文是对卷积神经网络模型参数量和浮点运算量的计算推导公式和方法，使用 API 自动计算这些数据请移步另一篇博客：自动计算模型参数量、FLOPs、乘加数以及所需内存等数据 1. 对 CNN 而言，每个卷积层的参数量计算如下： 其中 表示输出通道数， 表示输入通道数， 表示卷积核宽， 表示卷积核高。 括号内的 表示一个卷积核的权重数量，+1 表示 bias，括号表示一个卷积核的参数量， 表示该层有 个卷积核。 若卷积核是方形的，即，则上式变为： 需要注意的是，使用 Batch Normalization 时不需要 bias，此时计算式中的 + 1 项去除。 2. 对 CNN 而言，每个卷积层的运算量计算如下： FLOPs 是英文 floating point operations 的缩写，表示浮点运算量，中括号内的值表示卷积操作计算出 feature map 中一个点所需要的运算量（乘法和加法）， 表示一次卷积操作中的乘法运算量， 表示一次卷积操作中的加法运算量，+ 1 表示 bias，W 和 H 分别表示 feature map 的长和宽， 表示 feature map 的所有元素数。 若是方形卷积核，即，则有： 上面是乘运算和加运算的总和，将一次乘运算或加运算都视作一次浮点运算。 在计算机视觉论文中，常常将一个‘乘 - 加’组合视为一次浮点运算，英文表述为’Multi-Add’，运算量正好是上面的算法减半，此时的运算量为： 3. 对全连接层而言，其参数量非常容易计算： 值得注意的是，最初由 feature map flatten 而来的向量视为第一层全连接层，即此处的。 可以这样理解上式：每一个输出神经元连接着所有输入神经元，所以有 个权重，每个输出神经元还要加一个 bias。 也可以这样理解：每一层神经元 (O 这一层) 的权重数为，bias 数量为 O。 4. 对全连接层而言，其运算量计算如下： 其中 中括号的值表示计算出一个神经元所需的运算量，第一个 表示乘法运算量， 表示加法运算量，+1 表示 bias， 表示计算 O 个神经元的值。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"模型性能指标","slug":"模型性能指标","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/"}]},{"title":"寒窑赋","slug":"寒窑赋","date":"2022-01-20T13:06:50.000Z","updated":"2022-01-20T13:11:17.860Z","comments":true,"path":"2022/01/20/寒窑赋/","link":"","permalink":"https://leezhao415.github.io/2022/01/20/%E5%AF%92%E7%AA%91%E8%B5%8B/","excerpt":"","text":"文章目录 《寒窑赋》 天有不测风云，人有旦夕祸福。 蜈蚣百足，行不及蛇；雄鸡两翼，飞不过鸦。 马有千里之程，无骑不能自往；人有冲天之志，非运不能自通。 盖闻：人生在世，富贵不能淫，贫贱不能移。 文章盖世，孔子厄于陈邦；武略超群，太公钓于渭水。 颜渊命短，殊非凶恶之徒；盗跖年长，岂是善良之辈。 尧帝明圣，却生不肖之儿；瞽叟愚顽，反生大孝之子。 张良原是布衣，萧何曾为县吏。 晏子身无五尺，封作齐国宰相；孔明卧居草庐，能作蜀汉军师。 楚霸虽雄，败于乌江自刎；汉王虽弱，竟有万里江山。 李广有射虎之威，到老无封；冯唐有乘龙之才，一生不遇。 韩信未遇之时，无一日三餐，及至遇行，腰悬三齐玉印，一旦时衰，死于阴人之手。 有先贫而后富，有老壮而少衰。满腹文章，白发竟然不中；才疏学浅，少年及第登科。 深院宫娥，运退反为妓妾；风流妓女，时来配作夫人。 青春美女，却招愚蠢之夫；俊秀郎君，反配粗丑之妇。 蛟龙未遇，潜水于鱼鳖之间；君子失时，拱手于小人之下。 衣服虽破，常存仪礼之容；面带忧愁，每抱怀安之量。 时遭不遇，只宜安贫守份；心若不欺，必然扬眉吐气。 初贫君子，天然骨骼生成；乍富小人，不脱贫寒肌体。 天不得时，日月无光；地不得时，草木不生；水不得时，风浪不平；人不得时，利运不通。 注福注禄，命里已安排定，富贵谁不欲？人若不依根基八字，岂能为卿为相？ 余者，居洛阳之时，朝投僧寺，夜宿破窑。 布衣不能遮其体，饘粥不能充其饥。 上人嫌，下人憎，皆言余之贱也，余曰：非贱也，乃时也，运也，命也。 余后登高及第，入中书，官至极品，位列三公，思衣则有绮罗千箱，思食则有百味珍馐，有挞百僚之杖，有斩佞臣之剑，出则壮士执鞭，入则佳人扶袂，廪有余粟，库有余财，人皆言余之贵也，余曰：非贵也，乃时也，运也，命也。 嗟呼！人生在世，富贵不可尽用，贫贱不可自欺，听由天地循环，周而复始焉。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"且读文摘","slug":"且读文摘","permalink":"https://leezhao415.github.io/tags/%E4%B8%94%E8%AF%BB%E6%96%87%E6%91%98/"},{"name":"寒窑赋","slug":"寒窑赋","permalink":"https://leezhao415.github.io/tags/%E5%AF%92%E7%AA%91%E8%B5%8B/"}]},{"title":"【精华】常用计算机视觉库","slug":"【精华】常用计算机视觉库","date":"2022-01-20T13:06:30.000Z","updated":"2022-01-20T13:27:02.747Z","comments":true,"path":"2022/01/20/【精华】常用计算机视觉库/","link":"","permalink":"https://leezhao415.github.io/2022/01/20/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E5%B8%B8%E7%94%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%BA%93/","excerpt":"","text":"文章目录 1 常用计算机视觉库 （1）计算机视觉库 OpenCV （2）人脸识别 faceservice.cgi （3）OpenCVDotNet（OpenCV 的.NET 版 ） （4）人脸检测算法 jViolajones （5）Java 视觉处理库 JavaCV （6）运动检测程序 QMotion （7）视频监控系统 OpenVSS （8）手势识别 hand-gesture-detection （9）人脸检测识别 mcvai-tracking （10）人脸检测与跟踪库 asmlibrary （11）Lua 视觉开发库 libecv （12）OpenCVSharp（OpenCV 的.Net 封装） （13）3D 视觉库 fvision2010 （14）基于 QT 的计算机视觉库 QVision （15）图像特征提取 cvBlob （16）实时图像 / 视频处理滤波开发包 GShow （17）视频捕获 API VideoMan （18）开放模式识别项目 OpenPR （19）OpenCV 的 Python 封装 pyopencv （20）视觉快速开发平台 qcv （21）图像捕获 libv4l2cam （22）计算机视觉算法 OpenVIDIA （23）高斯模型点集配准算法 gmmreg （24）模式识别和视觉库 RAVL （25）图像处理和计算机视觉常用算法库 LTI-Lib （26）OpenCV 优化 opencv-dsp-acceleration （27）C++ 计算机视觉库 Integrating Vision Toolkit （28）计算机视觉和机器人技术的工具包 EGT （29）OpenCV 的扩展库 ImageNets （30）libvideogfx （31）Matlab 计算机视觉包 mVision （32）Scilab 的计算机视觉库 SIP （33）STAIR Vision Library 2 Python 计算机视觉库 一、python 计算机视觉中常用的库 二、基本操作 三、常用的函数 3 常用十大图像处理算法库 （1）scikit Image （2）Numpy （3）Scipy （4）PIL/ Pillow （5）OpenCV-Python （6）SimpleCV （7）Mahotas （8）SimpleITK （9）pgmagick （10）Pycairo 1 常用计算机视觉库 （1）计算机视觉库 OpenCV OpenCV 是 Intel? 开源计 算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个 C 函数的跨平台的中、高层 API。它不依赖于其它的外部库 —— 尽管也可以使用某些外部库。 OpenCV 对非商业… （2）人脸识别 faceservice.cgi faceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice 是采用 OpenCV 库进行开发的。 （3）OpenCVDotNet（OpenCV 的.NET 版 ） OpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。 （4）人脸检测算法 jViolajones jViolajones 是人脸检测算法 Viola-Jones 的一个 Java 实现，并能够加载 OpenCV XML 文件。 示例代码：http://www.oschina.net/code/snippet_12_2033 （5）Java 视觉处理库 JavaCV JavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture 和 FFmpeg。此外，该工具可以很容易地使用 Java 平台的功能。 JavaCV 还带有硬件加速的全屏幕图像显示 (CanvasFrame)，易于在多个内核中执行并行代码 (并… （6）运动检测程序 QMotion QMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。 （7）视频监控系统 OpenVSS OpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架 (VAF) 的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块 (OpenCV 的集成)，以及多核心架构。 （8）手势识别 hand-gesture-detection 手势识别，用 OpenCV 实现 （9）人脸检测识别 mcvai-tracking 提供人脸检测、识别与检测特定人脸 的功能，示例代码 123cvReleaseImage( &amp;gray ); cvReleaseMemStorage(&amp;storage); cvReleaseHaarClassifierCascade(&amp;cascade);... （10）人脸检测与跟踪库 asmlibrary Active Shape Model Library (ASMLibrary?) SDK, 用 OpenCV 开发，用于人脸检测与跟踪。 （11）Lua 视觉开发库 libecv ECV 是 lua 的计算机视觉开发库 (目前只提供 linux 支持) （12）OpenCVSharp（OpenCV 的.Net 封装） OpenCVSharp 是一个 OpenCV 的.Net wrapper，应用最新的 OpenCV 库开发，使用习惯比 EmguCV 更接近原始的 OpenCV，有详细的使用样例供参考。 （13）3D 视觉库 fvision2010 基于 OpenCV 构建的图像处理和 3D 视觉库。 示例代码： 1234ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(&quot;c:/a/im_d.jpg&quot;, 0, 20); //ImageSequenceReader* reader = factory.avi(&quot;a.avi&quot;); if (reader == NULL) &#123; ... （14）基于 QT 的计算机视觉库 QVision 基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。 （15）图像特征提取 cvBlob cvBlob 是计算机视觉应用中在二值图像里寻找连通域的库。能够执行连通域分析与特征提取. （16）实时图像 / 视频处理滤波开发包 GShow GShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像 / 视频 处理滤波开发包，集成 DiretX11。… （17）视频捕获 API VideoMan VideoMan 提供一组视频捕获 API 。支持多种视频流同时输入 (视频传输线、USB 摄像头和视频文件等)。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。 （18）开放模式识别项目 OpenPR Pattern Recognition project (开放模式识别项目)，致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。 （19）OpenCV 的 Python 封装 pyopencv OpenCV 的 Python 封装， 主要特性包括： 提供与 OpenCV 2.x 中最新的 C 接口极为相似的 Python 接口，并且包括 C 中不包括的 C 接口 提供对 OpenCV 2.x 中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost… （20）视觉快速开发平台 qcv 计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。 （21）图像捕获 libv4l2cam 对函数库 v412 的封装，从网络摄像头等硬件获得图像数据，支持 YUYV 裸数据输出和 BGR24 的 OpenCV IplImage 输出 （22）计算机视觉算法 OpenVIDIA OpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API’… （23）高斯模型点集配准算法 gmmreg 实现了基于混合高斯模型的点集配准 算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了 C++/Matlab/Python 接口… （24）模式识别和视觉库 RAVL Recognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。 （25）图像处理和计算机视觉常用算法库 LTI-Lib LTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具 （26）OpenCV 优化 opencv-dsp-acceleration 优化了 OpenCV 库在 DSP 上的速度。 （27）C++ 计算机视觉库 Integrating Vision Toolkit Integrating Vision Toolkit (IVT) 是一个强大而迅速的 C++ 计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台 GUI 组件，另外可以选择集成 OpenCV （28）计算机视觉和机器人技术的工具包 EGT The Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se… （29）OpenCV 的扩展库 ImageNets ImageNets 是对 OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用 Nokia 的 QT 编写界面。 （30）libvideogfx 视频处理、计算机视觉和计算机图形学的快速开发库。 （31）Matlab 计算机视觉包 mVision Matlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。 （32）Scilab 的计算机视觉库 SIP SIP 是 Scilab (一种免费的类 Matlab 编程环境) 的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。 （33）STAIR Vision Library STAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。 2 Python 计算机视觉库 一、python 计算机视觉中常用的库 一般我们在处理计算机视觉任务时会同时调用多种库，初学者在复现论文中的代码时往往会不知所措，不明白各种库的作用。这里笔者简单介绍一下自己平时在处理计算机视觉任务时的是如何将这些库函数与我们的计算机视觉任务进行关联，不足之处也请各位可以指出。 计算机视觉任务需要调用的库大致可分为三类： 图像处理类 （PIL、OpenCV、Matplotlib 等）， 数学类 （Numpy）， 神经网络类 （Tensorflow、Pytorch 等）。首先，我们通过调用图像处理类的库函数，进行图像处理的基本操作，例如：图像读取、颜色空间变换、以及常用的图像处理算法。然后，当你需要实现自己的图像处理算法时，需要将图像数据类型转换为数组类型进行运算，这时候需要使用数学类的库函数。最后，使用神经网络时，则需要将数组类型的数据转换到张量类型。 （一）PIL（Python Image Library）图像处理库 （二）Matplotlib （三）Numpy （四）Pytorch （五）torchvision （六）SKimage （七）OpenCV 二、基本操作 （一）利用 PIL 读取图像数据 12345from PIL import ImageimgPath = &quot;F:/path/test.png&quot;#图像路径img = Image.open(imgPath) #读取图像，保存为PIL.Image类型，默认为RGB格式#&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1728x2304 at 0x241C742AA90&gt; （二）使用 Matplotlib 显示图像 1234import matplotlib.pyplot as pltplt.imshow(img)#plt.imshow()函数负责对图像进行处理，并显示其格式，但是不能显示图像内容。plt.axis(&#x27;off&#x27;)#显示的图像不展示轴线plt.show() #显示图像内容 （三）PIL 类型与 Numpy 类型转换 123import numpy as npimg = np.array(img)#将PIL.Image类型转换为np类型img = Image.fromarray(img.astype(&#x27;uint8&#x27;))#将np类型转换为PIL.Image类型 （四) Numpy 类型与 torch 类型互换 123import torchimg = torch.from_numpy(img).float()#将np类型转换为张量img = img.numpy()#将张量转换为np类型 （五）保存张量为图像 1234import torchvisionimgPath = &quot;F:/path/test.png&quot;torchvision.utils.save_image(img,imgPath) 三、常用的函数 （一）打开图像，返回张量 123456789101112131415161718192021222324252627from PIL import Imageimport numpy as npimport torchfrom PIL import Imageimport numpy as npimport torch#输入图片路径，返回四维张量def openImage(path, w=-1, h=-1, mode=&#x27;RGB&#x27;): img = Image.open(path) #打开路径下的图片，保存维PIL.Image类型 if(w==-1): w,_ = img.size if(h==-1): _,h = img.size img = img.resize((w,h),Image.ANTIALIAS)#修改图像尺寸 img = img.convert(mode) #转换颜色空间 img = np.array(img) #将Image类型转换维Numpy类型 img = img/255.0 #将图像进行归一化 img = torch.from_numpy(img).float() #将Numpy类型转换维张量 d = img.dim() if(d==2): img = img.unsqueeze(0).unsqueeze(0) elif(d==3): img = img.permute(2,0,1) #更换维度，因为Image表示通道在第三维，变为张量后转换到第一维 img = img.unsqueeze(0) #增加维度 return img （二）显示使用张量表示的图像 12345678910111213141516171819from PIL import Imageimport numpy as npimport torchimport matplotlib.pyplot as plt#输入一个四维张量，转换为PIL格式后显示def showImage(img, mode=&#x27;RGB&#x27;): _,c,_,_ = img.size() #获取img的尺寸 if(c==1): #判断是否为单通道 img = img[0][0] else: img = img[0] img = img.permute(1,2,0) img = img.numpy() #将张量转换为np数组 img = img*255.0 img = Image.fromarray(img.astype(&#x27;uint8&#x27;)).convert(mode)#将np数组转换为PIL类型 plt.imshow(img) plt.axis(&#x27;off&#x27;) plt.show() （三）保存张量为图像 1234567891011121314151617from PIL import Imageimport numpy as npimport torchimport matplotlib.pyplot as plt#输入一个四维张量，转换为PIL格式后保存def saveImage(img,path,mode=&#x27;RGB&#x27;): _,c,_,_ = img.size() #获取img的尺寸 if(c==1): #判断是否为单通道 img = img[0][0] elif(c==3): img = img[0] img = img.permute(1,2,0) img = img.numpy() #将张量转换为np数组 img = img*255.0 img = Image.fromarray(img.astype(&#x27;uint8&#x27;)).convert(mode)#将np数组转换为PIL类型 img.save(path) 3 常用十大图像处理算法库 当今世界充满了各种数据，而图像是其中高的重要组成部分。然而，若想其有所应用，我们需要对这些图像进行处理。图像处理是分析和操纵数字图像的过程，旨在提高其质量或从中提取一些信息，然后将其用于某些方面。 图像处理中的常见任务包括显示图像，基本操作（如裁剪、翻转、旋转等），图像分割，分类和特征提取，图像恢复和图像识别等。 Python 之成为图像处理任务的最佳选择，是因为这一科学编程语言日益普及，并且其自身免费提供许多最先进的图像处理工具。 让我们看一下用于图像处理任务的一些常用 Python 库。 （1）scikit Image scikit-image 是一个基于 numpy 数组的开源 Python 包。 它实现了用于研究、教育和工业应用的算法和实用程序。 即使是对于那些刚接触 Python 的人，它也是一个相当简单的库。 此库代码质量非常高并已经过同行评审，是由一个活跃的志愿者社区编写的。 使用说明文档 ： https://scikit-image.org/docs/stable/user_guide.html 用法举例 ：图像过滤、模版匹配 可使用 “skimage” 来导入该库。大多数功能都能在子模块中找到。 123456import matplotlib.pyplot as plt%matplotlib inlinefrom skimage import data,filtersimage = data.coins()# ... or any other NumPy array!edges = filters.sobel(image)plt.imshow(edges, cmap=&#x27;gray&#x27;) 模版匹配（使用 match_template 函数） gallery 上还有更多例子。 https://scikit-image.org/docs/dev/auto_examples/ （2）Numpy Numpy 是 Python 编程的核心库之一，支持数组结构。 图像本质上是包含数据点像素的标准 Numpy 数组。 因此，通过使用基本的 NumPy 操作 —— 例如切片、脱敏和花式索引，可以修改图像的像素值。 可以使用 skimage 加载图像并使用 matplotlib 显示。 使用说明文档 ： http://www.numpy.org/ 用法举例 ：使用 Numpy 来对图像进行脱敏处理 12345678910import numpy as npfrom skimage import dataimport matplotlib.pyplot as plt%matplotlib inlineimage = data.camera()type(image)numpy.ndarray #Image is a numpy arraymask = image &lt; 87image[mask]=255plt.imshow(image, cmap=&#x27;gray&#x27;) （3）Scipy scipy 是 Python 的另一个核心科学模块，就像 Numpy 一样，可用于基本的图像处理和处理任务。值得一提的是，子模块 scipy.ndimage 提供了在 n 维 NumPy 数组上运行的函数。 该软件包目前包括线性和非线性滤波、二进制形态、B 样条插值和对象测量等功能。 使用说明文档 ： https://docs.scipy.org/doc/scipy/reference/tutorial/ndimage.html#correlation-and-convolution 用法举例 ：使用 SciPy 的高斯滤波器对图像进行模糊处理 1234from scipy import misc,ndimageface = misc.face()blurred_face = ndimage.gaussian_filter(face, sigma=3)very_blurred = ndimage.gaussian_filter(face, sigma=5)#Resultsplt.imshow() （4）PIL/ Pillow PIL （Python Imaging Library）是一个免费的 Python 编程语言库，它增加了对打开、处理和保存许多不同图像文件格式的支持。 然而，它的发展停滞不前，其最后一次更新还是在 2009 年。幸运的是， PIL 有一个正处于积极开发阶段的分支 Pillow，它非常易于安装。Pillow 能在所有主要操作系统上运行并支持 Python 3。该库包含基本的图像处理功能，包括点操作、使用一组内置卷积内核进行过滤以及颜色空间转换。 使用说明文档 ： https://pillow.readthedocs.io/en/3.1.x/index.html 用法举例 ：使用 ImageFilter 增强 Pillow 中的图像 123456from PIL import Image, ImageFilter#Read imageim = Image.open( &#x27;image.jpg&#x27; )#Display imageim.show()from PIL import ImageEnhanceenh = ImageEnhance.Contrast(im)enh.enhance(1.8).show(&quot;30% more contrast&quot;) （5）OpenCV-Python OpenCV（ 开源计算机视觉库，Open Source Computer Vision Library）是计算机视觉应用中使用最广泛的库之一。OpenCV-Python 是 OpenCV 的 python API。 OpenCV-Python 不仅速度快（因为后台由用 C / C ++ 编写的代码组成），也易于编码和部署（由于前端的 Python 包装器）。 这使其成为执行计算密集型计算机视觉程序的绝佳选择。 使用说明文档 ： https://github.com/abidrahmank/OpenCV2-Python-Tutorials 用法举例 ：使用 Pyramids 创建一个名为’Orapple’的新水果的功能 （6）SimpleCV SimpleCV 也是用于构建计算机视觉应用程序的开源框架。 通过它可以访问如 OpenCV 等高性能的计算机视觉库，而无需首先了解位深度、文件格式或色彩空间等。学习难度远远小于 OpenCV，并且正如他们的标语所说，“它使计算机视觉变得简单”。支持 SimpleCV 的一些观点是： 即使是初学者也可以编写简单的机器视觉测试 摄像机、视频文件、图像和视频流都可以交互操作 使用说明文档 ： https://simplecv.readthedocs.io/en/latest/ 用法举例 （7）Mahotas Mahotas 是另一个用于 Python 的计算机视觉和图像处理库。 它包含传统的图像处理功能（如滤波和形态学操作）以及用于特征计算的更现代的计算机视觉功能（包括兴趣点检测和局部描述符）。 该接口使用 Python，适用于快速开发，但算法是用 C ++ 实现的，并且针对速度进行了优化。Mahotas 库运行很快，它的代码很简单，（对其它库的）依赖性也很小。 建议阅读他们的官方文档以了解更多内容。 使用说明文档 ： https://mahotas.readthedocs.io/en/latest/install.html 用法举例 Mahotas 库使用简单的代码来完成工作。 对于 “寻找 Wally” 的问题，Mahotas 完成的得很好，而且代码量非常小。 （8）SimpleITK ITK（Insight Segmentation and Registration Toolkit）是一个开源的跨平台系统，为开发人员提供了一整套用于图像分析的软件工具。 其中， SimpleITK 是一个建立在 ITK 之上的简化层，旨在促进其在快速原型设计、教育以及脚本语言中的使用。SimpleITK 是一个包含大量组件的图像分析工具包，支持一般的过滤操作、图像分割和配准。 SimpleITK 本身是用 C++ 编写的，但可用于包括 Python 在内的大量编程语言。 使用说明文档 ： https://github.com/hhatto/pgmagick 这里有大量说明了如何使用 SimpleITK 进行教育和研究活动的 Jupyter notebook。notebook 中演示了如何使用 SimpleITK 进行使用 Python 和 R 编程语言的交互式图像分析。 用法举例 下面的动画是使用 SimpleITK 和 Python 创建的可视化的严格 CT / MR 配准过程。 （9）pgmagick pgmagick 是 GraphicsMagick 库基于 Python 的包装器。GraphicsMagick 图像处理系统有时被称为图像处理的瑞士军刀。它提供了强大而高效的工具和库集合，支持超过 88 种主要格式图像的读取、写入和操作，包括 DPX，GIF，JPEG，JPEG-2000，PNG，PDF，PNM 和 TIFF 等重要格式。 使用说明文档 ： https://github.com/hhatto/pgmagick 用法举例 ：图片缩放、边缘提取 （10）Pycairo Pycairo 是图形库 cairo 的一组 python 绑定。 Cairo 是一个用于绘制矢量图形的 2D 图形库。 矢量图形很有趣，因为它们在调整大小或进行变换时不会降低清晰度。Pycairo 库可以从 Python 调用 cairo 命令。 使用说明文档 ： https://github.com/pygobject/pycairo 用法 ：Pycairo 可以绘制线条、基本形状和径向渐变","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"计算机视觉库","slug":"计算机视觉库","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%BA%93/"}]},{"title":"【详解】YOLOX训练Objects365指定类别数据","slug":"【详解】YOLOX训练Objects365指定类别数据","date":"2022-01-20T13:06:12.000Z","updated":"2022-01-20T13:09:08.806Z","comments":true,"path":"2022/01/20/【详解】YOLOX训练Objects365指定类别数据/","link":"","permalink":"https://leezhao415.github.io/2022/01/20/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91YOLOX%E8%AE%AD%E7%BB%83Objects365%E6%8C%87%E5%AE%9A%E7%B1%BB%E5%88%AB%E6%95%B0%E6%8D%AE/","excerpt":"","text":"文章目录 （一）YOLOX 训练 Objects365_172 （1）数据集预处理 （2）YOLOX 训练 （二）注意事项： （1）多 GPU 训练问题（可选） （2）训练中生成临时标签文件报错问题 （一）YOLOX 训练 Objects365_172 （1）数据集预处理 &lt;1&gt; 数据集格式 1234567├─datasets│ └─VOCdevkit│ └─VOC2007│ ├─Annotations│ ├─ImageSets│ │ └─Main│ └─JPEGImages &lt;2&gt; 数据集抽取 从 Objects365 中抽取指定 172 类并转为 VOC 数据集类型 xml ​ obj365_main.py 链接: https://pan.baidu.com/s/1ePq7KB1Ho5R-uNXK-XOa-Q 提取码: 8008 根据抽取的标签获取图片列表 ​ img_list_txt.py 链接: https://pan.baidu.com/s/1IyP7yEibYzR1UL_fZfbccg 提取码: mgk3 根据图片列表抽取源数据 img_list_txt.py 链接:https://pan.baidu.com/s/10C0TULHWCQ1YkjkVzY6jaQ 提取码:ju1j 生成 ImageSets 文件夹，切分 train、val、test、trainval ​ VOC_ImageSets_Main_txt.py 链接: https://pan.baidu.com/s/1hHLI3-30nXb-kqEsNLnHEQ 提取码: om41 （2）YOLOX 训练 &lt;1&gt; 文件修改 （1） yolox/data/datasets/voc_classes.py 1234VOC_CLASSES = (&quot;panda&quot;,&quot;tiger&quot;,) （2） yolox/exp/yolox_base.py 1234567(self.num_classes可以不用修改，因为后面的exps/example/yolox_voc/yolox_voc_s.py会对self.num_classes进行重载)可选修改 self.num_classes修改为自己的类别数可选修改 self.inputsize, self.random_size 改变训练尺寸大小可选修改 self.test_size 改变测试的尺寸大小 （3） yolox/data/datasets/voc.py _do_python_eval() 方法 123annopath = os.path.join(rootpath, &quot;Annotations&quot;, &quot;&#123;:s&#125;.xml&quot;)修改为annopath = os.path.join(rootpath, &quot;Annotations&quot;, &quot;&#123;&#125;.xml&quot;) （4） exps/example/yolox_voc/yolox_voc_s.py 全局超参数 1修改 self.num_classes = 172 # 自定义的类别数 get_data_loder 方法 12修改VOCDetection下的 image_sets=[(&#x27;2007&#x27;, &#x27;trainval&#x27;), (&#x27;2012&#x27;, &#x27;trainval&#x27;)],修改为 image_sets=[(&#x27;2007&#x27;, &#x27;train&#x27;)] get_eval_loader 方法 12修改VOCDetection下的 image_sets=[(&#x27;2007&#x27;, &#x27;test&#x27;)],修改为 image_sets=[(&#x27;2007&#x27;, &#x27;val&#x27;)] （5） tools/train.py 12345678910111213141516171819202122232425设置 default=&quot;Animals&quot;， 训练后结果就会保存在 tools/YOLOX_outputs/Animals下parser.add_argument(&quot;-expn&quot;, &quot;--experiment-name&quot;, type=str, default=None)设置 model_name，我也不太清楚这是不是必须项 (我觉得不是)parser.add_argument(&quot;-n&quot;, &quot;--name&quot;, type=str, default=&quot;yolox-s&quot;, help=&quot;model name&quot;)设置 batch_sizeparser.add_argument(&quot;-b&quot;, &quot;--batch-size&quot;, type=int, default=64, help=&quot;batch size&quot;)设置gpu，因为我只有一张卡，所以设 default=0parser.add_argument( &quot;-d&quot;, &quot;--devices&quot;, default=0, type=int, help=&quot;device for training&quot;)设置你的数据配置的路径，default=&quot;../exps/example/yolox_voc/yolox_voc_s.py&quot;parser.add_argument( &quot;-f&quot;, &quot;--exp_file&quot;, default=&quot;../exps/example/yolox_voc/yolox_voc_s.py&quot;, type=str, help=&quot;plz input your expriment description file&quot;,)设置权重路径， default=&quot;../weights/yolox_s.pth&quot;parser.add_argument(&quot;-c&quot;, &quot;--ckpt&quot;, default=&quot;../weights/yolox_s.pth&quot;, type=str, help=&quot;checkpoint file&quot;) （6） tools/demo.py 由于 demo.py 默认调用的是 COCO_CLASSES，所以想要正确显示结果，就要把 yolox/data/datasets/coco_classes.py 下的 COCO_CLASSES 改成你的数据类别 或者如果想修改为调用 VOC_CLASSES 在 yolox/data/datasets/__init__.py 里将 from .voc import VOCDetection 修改为 from .voc import VOCDetection, VOC_CLASSES 在 tools/demo.py 123from yolox.data.datasets import COCO_CLASSES 修改为 from yolox.data.datasets import COCO_CLASSES, VOC_CLASSES 将 tools/demo.py 里所有用到 COCO_CLASSES 的地方全部改为 VOC_CLASSES 即可 &lt;2&gt; 模型训练 12345678910111213141516# 单GPU训练python tools/train.py -n yolox-s -d 1 -b 16 --fp16 -c yolox/weights/yolox_s.pth -o [--cache]python tools/train.py -f exps/example/yolox_voc/yolox_voc_s.py -d 0 -b 16 -c yolox/weights/yolox_s.pth# 多GPU训练python tools/train.py -f exps/example/yolox_voc/yolox_voc_nano.py -d 4 -b 64 --fp16 -c yolox/weights/yolox_nano.pth#* -d: number of gpu devices#* -b: total batch size, the recommended number for -b is num-gpu * 8#* --fp16: mixed precision training#* --cache: caching imgs into RAM to accelarate training, which need large system RAM. &lt;3&gt; 模型推理 123python tools/demo.py image -f exps/example/yolox_voc/yolox_voc_s.py -c YOLOX_outputs/yolox_s/best_ckpt.pth --path TestFiles/testimg1.jpg --conf 0.3 --nms 0.65 --tsize 640 --save_result --device cpupython tools/demo.py image -f exps/example/yolox_voc/yolox_voc_nano.py -c YOLOX_outputs/yolox_voc_nano/latest_ckpt.pth --path TestFiles/testimg1.jpg --conf 0.3 --nms 0.65 --tsize 640 --save_result --device cpu （二）注意事项： （1）多 GPU 训练问题（可选） 多 GPU 训练模型时，需将 os.environ [‘CUDA_VISIBLE_DEVICES’] 修改为实际需要的 12import osos.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0,1,2,3&#x27; （2）训练中生成临时标签文件报错问题 YOLOX 训练过程中出现验证生成临时标签 txt 文件时，因文件名中包含 “/”, 而无法生成临时标签 txt 文件的问题 解决方法：重新生成标签文件，将 “/” 修改为 “_” 生成标签文件时将 object365_dict.txt 文件中对应标签的 “/” 修改为 “_” /YOLOX/yolox/data/dataset/voc_classes.py 中的 VOC_CLASSES 类别中的 “/” 修改为 “_”","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"深度模型","slug":"深度模型","permalink":"https://leezhao415.github.io/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/"}]},{"title":"人工智能六大领域","slug":"人工智能六大领域","date":"2022-01-20T13:05:47.000Z","updated":"2022-01-20T13:07:49.333Z","comments":true,"path":"2022/01/20/人工智能六大领域/","link":"","permalink":"https://leezhao415.github.io/2022/01/20/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%85%AD%E5%A4%A7%E9%A2%86%E5%9F%9F/","excerpt":"","text":"文章目录 人工智能六大领域 1）深度学习 2）自然语言处理 3）计算机视觉 4）智能机器人 5）自动程序设计 6）数据挖掘 人工智能六大领域 1）深度学习 深度学习是基于现有的数据进行学习操作，是机器学习研究中的一个新的领域，机在于建立、模拟人脑进行分析学习的神经网 络，它模仿人脑的机制来解释数据，例如图像，声音和文本。深度学习是无监督学习的一种。 2）自然语言处理 自然语言处理是用自然语言同计算机进行通讯的一种技术。人工智能的分支学科，研究用电子计算机模拟人的语言交际过程， 使计算机能理解和运用人类社会的自然语言如汉语、英语等，实现人机之间的自然语言通信，以代替人的部分脑力劳动， 包括查询资料、解答问题、摘录文献、汇编资料以及一切有关自然语言信息的加工处理。例如生活中的电话机器人的核心技术 之一就是自然语言处理 3）计算机视觉 计算机视觉是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适 合人眼观察或传送给仪器检测的图像。计算机视觉就是用各种成象系统代替视觉器官作为输入敏感手段，由计算机来代替大脑完 成处理和解释。计算机视觉的最终研究目标就是使计算机能像人那样通过视觉观察和理解世界，具有自主适应环境的能力。 计算机视觉应用的实例有很多，包括用于控制过程、导航、自动检测等方面。 4）智能机器人 如今我们的身边逐渐开始出现很多智能机器人，他们具备形形色色的内部信息传感器和外部信息传感器，如视觉、听觉、触觉、 嗅觉。除具有感受器外，它还有效应器，作为作用于周围环境的手段。这些机器人都离不开人工智能的技术支持。 科学家们认为，智能机器人的研发方向是，给机器人装上 “大脑芯片”，从而使其智能性更强，在认知学 习、自动组织、对模糊信 息的综合处理等方面将会前进一大步。 5）自动程序设计 自动程序设计是指根据给定问题的原始描述，自动生成满足要求的程序。它是软件工程和人工智能相结合的研究课题。自动程序 设计主要包含程序综合和程序验证两方面内容。前者实现自动编程，即用户只需告知机器 “做什么”，无须告诉 “怎么做”，这后一步 的工作由机器自动完成；后者是程序的自动验证，自动完成正确性的检查。其目的是提高软件生产率和软件产品质量。 自动程序设计的任务是设计一个程序系统，接受关于所设计的程序要求实现某个目标非常高级描述作为其输入，然后自动生成一 个能完成这个目标的具体程序。该研究的重大贡献之一是把程序调试的概念作为问题求解的策略来使用。 6）数据挖掘 数据挖掘一般是指从大量的数据中通过算法搜索隐藏于其中信息的过程。它通常与计算机科学有关，并通过统计、在线分析处 理、情报检索、机器学习、专家系统（依靠过去的经验法则）和模式识别等诸多方法来实现上述目标。它的分析方法包括：分 类、估计、预测、相关性分组或关联规则、聚类和复杂数据类型挖掘。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"概述","slug":"概述","permalink":"https://leezhao415.github.io/tags/%E6%A6%82%E8%BF%B0/"}]},{"title":"YOLOX目标检测_原理与源码解析","slug":"YOLOX目标检测-原理与源码解析","date":"2021-12-25T13:42:55.000Z","updated":"2021-12-25T14:51:30.602Z","comments":true,"path":"2021/12/25/YOLOX目标检测-原理与源码解析/","link":"","permalink":"https://leezhao415.github.io/2021/12/25/YOLOX%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-%E5%8E%9F%E7%90%86%E4%B8%8E%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"文章目录 YOLOX 目标检测：原理与源码解析 【为什么要学习这门课】 【课程内容与收获】 【相关课程】 YOLOX 目标检测：原理与源码解析 课程链接：YOLOX 目标检测：原理与源码解析–计算机视觉视频教程 - 人工智能 - CSDN 程序员研修院 【为什么要学习这门课】 Linux 创始人 Linus Torvalds 有一句名言：Talk is cheap. Show me the code. 冗谈不够，放码过来！代码阅读是从基础到提高的必由之路。 YOLOX 是旷视科技新近推出的高性能实时目标检测网络，性能超越了 YOLOv3/YOLOv4 /YOLOv5。YOLOX 使用 PyTorch 实现，含有很多业界前沿和常用的技巧，可以作为很好的代码阅读案例，让我们深入探究其实现原理，其中不少知识点的代码可以作为相关项目的借鉴。 【课程内容与收获】 本课程将在 Ubuntu 和 Windows 系统演示 YOLOX 对 PASCAL VOC 数据集和训练和测试，并详细解析 YOLOX 的实现原理和源码，对关键代码使用 PyCharm 的 debug 模式逐行分析解读。 本课程将提供注释后的 YOLOX 的源码程序文件。 【相关课程】 除本课程《YOLOX 目标检测：原理与源码解析》外，请关注 YOLOX 系列的其它课程，包括： 《YOLOX 目标检测实战：训练自己的数据集》https://edu.csdn.net/course/detail/35586 《YOLOX 目标检测实战：TensorRT 加速部署》https://edu.csdn.net/course/detail/35716 《YOLOX 目标检测实战：OpenVINO 部署》https://edu.csdn.net/course/detail/35791 《YOLOX 目标检测实战：Flask 部署》https://edu.csdn.net/course/detail/35815 ———————————————— 版权声明：本文为 CSDN 博主「bai666ai」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/bai666ai/article/details/120265651","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"YOLOX","slug":"YOLOX","permalink":"https://leezhao415.github.io/tags/YOLOX/"}]},{"title":"【精华】史上最全三维建模软件汇总","slug":"【精华】史上最全三维建模软件汇总","date":"2021-12-25T13:40:54.000Z","updated":"2021-12-25T14:43:37.574Z","comments":true,"path":"2021/12/25/【精华】史上最全三维建模软件汇总/","link":"","permalink":"https://leezhao415.github.io/2021/12/25/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8%E4%B8%89%E7%BB%B4%E5%BB%BA%E6%A8%A1%E8%BD%AF%E4%BB%B6%E6%B1%87%E6%80%BB/","excerpt":"","text":"文章目录 史上最全三维建模软件汇总 一、通用全功能 3D 设计软件 二、行业性的 3D 设计软件 三、3D 雕刻建模软件：笔刷式高精度建模软件 四、基于照片的 3D 建模软件 五、基于扫描（逆向设计）的 3D 建模软件 六、基于草图的 3D 建模软件 七、其他 3D 建模软件 八、虚拟现实软件和平台 九、推荐三维软件 史上最全三维建模软件汇总 常用的 3D 三维建模软件大全有哪些？本期为大家分享一篇以 3D 技术为主，展示各种 3D 建模的软件和工具。 一、通用全功能 3D 设计软件 1、 3D Studio Max，简称 3DS MAX，是当今世界上销售量最大的三维建模、动画及渲染软件。可以说 3DSMAX 是最容易上手的 3D 软件，其最早应用于计算机游戏中的动画制作，后开始参与影视片的特效制作，例如《X 战警》、《最后的武士》等。 2、Maya Maya 是世界顶级的三维动画软件，应用对象是专业的影视广告，角色动画，电影特技等。Maya 功能完善，工作灵活，易学易用，制作效率极高，渲染真实感极强，是电影级别的高端制作软件。 Maya 售价高昂，声名显赫，是制作者梦寐以求的制作工具，掌握了 Maya，会极大的提高制作效率和品质，调节出仿真的角色动画，渲染出电影一般的真实效果，向世界顶级动画师迈进。 3、Rhino Rhinocero，简称 Rhino，又叫犀牛，是一款三维建模工具。不过不要小瞧它，它的基本操作和 AutoCAD 有相似之处，拥有 AutoCAD 基础的初学者更易于掌握犀牛。目前广泛应用于工业设计、建筑、家具、鞋模设计，擅长产品外观造型建模。 4、Zbrush ZBrush 是一个数字雕刻和绘画软件，它以强大的功能和直观的工作流程著称。它界面简洁，操作流畅，以实用的思路开发出的功能组合，激发了艺术家的创作力，让艺术家无约束地自由创作。它的出现完全颠覆了过去传统三维设计工具的工作模式，解放了艺术家们的双手和思维，告别过去那种依靠鼠标和参数来笨拙创作的模式，完全尊重设计师的创作灵感和传统工作习惯。 5、Google Sketchup Sketchup 是一套直接面向设计方案创作过程的设计工具，其创作过程不仅能够充分表达设计师的思想而且完全满足与客户即时交流的需要，它使得设计师可以直接在电脑上进行十分直观的构思，是三维建筑设计方案创作的优秀工具。 SketchUp 是一个极受欢迎并且易于使用的 3D 设计软件，官方网站将它比喻作电子设计中的 “铅笔”。它的主要卖点就是使用简便，人人都可以快速上手。并且用户可以将使用 SketchUp 创建的 3D 模型直接输出至 GoogleEarth 里，非常的酷！ 6、Poser Poser 是 Metacreations 公司推出的一款三维动物、人体造型和三维人体动画制作的极品软件。Poser 更能为你的三维人体造型增添发型、衣服、饰品等装饰，让人们的设计与创意轻松展现。 7、Blender Blender 是一款开源的跨平台全能三维动画制作软件，提供从建模、动画、材质、渲染、到音频处理、视频剪辑等一系列动画短片制作解决方案。Blender 为全世界的媒体工作者和艺术家而设计，可以被用来进行 3D 可视化，同时也可以创作广播和电影级品质的视频，另外内置的实时 3D 游戏引擎，让制作独立回放的 3D 互动内容成为可能。 有了 Blender，喜欢 3D 绘图的玩家们不用花大钱，也可以制作出自己喜爱的 3D 模型了。它不仅支持各种多边形建模，也能做出动画！ 8、FormZ FormZ 是一个备受赞赏、具有很多广泛而独特的 2D/3D 形状处理和凋塑功能的多用途实体和平面建模软件。 对于需要经常处理有关 3D 空间和形状的专业人士 (例如建筑师、景观建筑师、城市规划师、工程师、动画和插画师、工业和室内设计师) 来说是一个有效率的设计工具。 9、LightWave 3D 美国 NewTek 公司开发的 LightWave3D 是一款高性价比的三维动画制作软件，它的功能非常强大，是业界为数不多的几款重量级三维动画软件之一。被广泛应用在电影、电视、游戏、网页、广告、印刷、动画等各领域。它的操作简便，易学易用，在生物建模和角色动画方面功能异常强大；基于光线跟踪、光能传递等技术的渲染模块，令它的渲染品质几尽完美。 10、C4D C4D 全名 CINEMA 4D，德国 MAXON 出的 3D 动画软体。Cinema4D 是一个老牌的三维软件。能够进行顶级的建模、动画和渲染的 3D 工具包。 C4D 是一款容易学习、容易使用、非常高效，并且享有电影级视觉表达能力的 3D 制作软件，C4D 由于其出色的视觉表达能力已成为视觉设计师首选的三维软件。这个始于德国 1989 年的软件，至今已历时 30 年，现在功能越来越强大完善。 C4D 是集万千宠爱于一身的设计界网红，C4D 技术现在流行于电商设计，在平面设计、UI 设计、工业设计、影视制作方面也是广泛运用，很多电影大片的人物建模也都是用 C4D 来完成。 二、行业性的 3D 设计软件 1、AutoCAD AutoCAD 是 Autodesk 公司的主导产品，用于二维绘图、详细绘制、设计文档和基本三维设计，现已经成为国际上广为流行的绘图工具。AutoCAD 具有良好的用户界面，通过交互菜单或命令行方式便可以进行各种操作。它的多文档设计环境，让非计算机专业人员也能很快地学会使用。 2、CATIA CATIA 属于法国达索（DassaultSystemesS.A）公司，是高端的 CAD/CAE/CAM 一体化软件。在 20 世纪 70 年代，CATIA 第一个用户就是世界著名的航空航天企业 DassaultAviation。目前，CATIA 其强大的功能已得到各行业的认可，其用户包括波音、宝马、奔驰等知名企业。 3、UG UG（UnigraphicsNX）是 Siemens 公司出品的一款高端软件，它为用户的产品设计及加工过程提供了数字化造型和验证手段。UG 最早应用于美国麦道飞机公司，目前已经成为模具行业三维设计的主流应用之一。 4、Solidworks Solidworks 属于法国达索（DassaultSystemesS.A）公司，专门负责研发与销售机械设计软件的视窗产品。Solidworks 帮助设计师减少设计时间，增加精确性，提高设计的创新性，并将产品更快推向市场。Solidwords 是世界上第一个基于 Windows 开发的三维 CAD 系统。该软件功能强大，组件繁多，使得 Solidworks 成为领先的、主流的三维 CAD 解决方案。 5、Pro/E Pro/Engineer（简称 Pro/E）是美国 PTC 公司研制的一套由设计至生产的机械自动化软件，广泛应用于汽车、航空航天、消费电子、模具、玩具、工业设计和机械制造等行业。 6、Cimatron Cimatron 是以色列 Cimatron 公司（现已被美国 3DSystems 收购）开发的软件。该系统提供了灵活的用户界面，主要用于模具设计、模型加工，在国际上模具制造业备受欢迎。 Cimatron 公司团队基于 Cimatron 软件开发了金属 3D 打印软 3DXpert。这是全球第一款覆盖了整个设计流程的金属 3D 打印软件，从设计直到最终打印成型，甚至是在后处理的 CNC 处理阶段，3DXpert 软件也能够发挥它的作用。 三、3D 雕刻建模软件：笔刷式高精度建模软件 1、ZBrush 美国 Pixologic 公司开发的 ZBrush 软件是世界上第一个让艺术家感到无约束自由创作的 3D 设计工具。 ZBrush 能够雕刻高达 10 亿多边形的模型，所以说限制只取决于的艺术家自 身的想象力。 2、 MudBox MudBox 是 Autodesk 公司的 3D 雕刻建模软件，它和 ZBrush 相比各有千秋。在某些人看 来，MudBox 的功能甚至超过了 ZBrush，可谓 ZBrush 的超级杀手。 3、MeshMixer Autodesk 公司又开发出一款笔刷式 3D 建模工具 MeshMixer，它能让用户通过笔刷 式的交互来融合现有的模型来创建 3D 模型（似乎是类似与 Poisson 融合或 Laplacian 融合的技 术），比如类似 “牛头马面” 的混合 3D 模型。值得注意的是，最新版本的 MeshMixer 还添加 3D 打印支撑优化新算法。 4、3DCoat 3d-coat 这是由乌克兰开发的数字雕塑软件，我们可以先看看官方的介绍：3D -Coat 是专为游戏美工设计的软件，它专注于游戏模型的细节设计，集三维模型实时纹理绘制和细节雕刻功能为一身，可以加速细节设计流程，在更短的时间内创造出更多的内容。只需导入一个低精度模型，3D-Coat 便可为其自动创建 UV，一次性绘制法线贴图、置换贴图、颜色贴图、透明贴图、高光贴图。 5、Sculptris Sculptris 是一款虚拟建模软件，其核心重点在于建模粘土的概念，如果用户像创建小雕像，那么这款软件十分适合使用。 6、Modo modo 是一款高级多边形细分曲面，建模、雕刻、3D 绘画、动画与渲染的综合性 3D 软件。由 Luxology, LLC 设计并维护。该软件具备许多高级技术，诸如 N-gons（允许存在边数为 4 以上的多边形），多层次的 3D 绘画与边权重工具，可以运行在苹果的 Mac OS X 与微软的 Microsoft Windows 操作平台。 四、基于照片的 3D 建模软件 1、Autodesk 123D Autodesk 123D Catch 是建模软件的重点，用户使用相机或手机来从不同角度拍摄物体、人物或场景，然后上传到云，然后该软件利用云计算的强大计算能力，可将 数码照片中几分钟的时间内转换为 3D 模型，而且还自动带上纹理信息。我们试用 过几次，感觉它的使用还是很方便的。但是其生成的 3D 几何的细节不多，主要是 通过纹理信息来表现真实感的。有时软件也会失败，生成的几何是不正确的。 2、3DSOM Pro 3DSOM Pro 是一款从高质量的照片来生成 3D 建模的软件，它可以通过一个真实物体的 照片来进行 3D 建模，并且制作的模型可以在网络上以交互的方式呈现。 3、PhotoSynth 微软开发了一款产品 PhotoSynth，可将大量的照片做 3D 处理，但是它不是真正创建 3D 模型，而是根据照片之间的相机参数及空间对应关系，建构一个虚拟的 3D 场景，使得用户 能够在从不同角度和位置来查看该场景，而显示的场景图像是由给定的照片所合成的。 五、基于扫描（逆向设计）的 3D 建模软件 1、Geomagic Geomagic (俗称 “杰魔”) 包括系列软件 Geomagic Studio、Geomagic Qualify 和 Geomagic Piano。其中 Geomagic Studio 是被广泛使用的逆向工程软件，具有下述所有特点：确保完美 无缺的多边形和 NURBS 模型处理复杂形状或自由曲面形状时，生产效率比传统 CAD 软件提高数倍；可与主要的三维扫描设备和 CAD/CAM 软件进行集成；能够作为一个独立的应 用程序运用于快速制造，或者作为对 CAD 软件的补充。是我们学生科研的必备软件之一。 2、ImageWare Imageware 由美国 EDS 公司出品，后被德国 Siemens PLM Software 所收购，现在并入旗下的 NX 产品线，是最著名的逆向工程软件，Imageware 因其强大的点云处理能力、曲面编辑能力和 A 级曲面的构建能力而被广泛应用于汽车、航空、航天、消费家电、模具、计算机零部件等设计与制造领域。 3、RapidForm RapidForm 是韩国 INUS 公司出品的逆向工程软件，提供了新一代运算模式，可实时将点 云数据运算出无接缝的多边形曲面，使它成为 3D 扫描数据的最佳化的接口，是很多 3D 扫 描仪的 OEM 软件。我们购买的 Konica Minolta 的激光扫描仪 Range 7 就是用 RapidForm 来进 行逆向设计。 4、ReconstructMe ProFactor 公司开发的 ReconstructMe 是一个功能强大且易于使用的三维重建软件，能够 使用微软的 Kinect 或华硕的 Xtion 进行实时 3D 场景扫描（核心算法是 Kinect Fusion），几分 钟就可以完成一张全彩 3D 场景。我们尝试过，效果还可以。ReconstructMeQt 提供了一个实 时三维重建利用 ReconstructMe SDK（开源）的图形用户界面。 注：法国 ManCTL 公司开发的 Skanect 为 Mac 平台的第一款 3D 扫描软件，也支持者华硕的 Xtion 或者微软的 Kinect 进行实时 3D 扫描 5、Artec Studio Artec 公司出品的 Artec Eva, Artec Spider 等手持式的结构光 3D 扫描仪，重量轻且易于使 用，成为许多 3D 体验馆扫描物体的首选产品。我试用过 Artec Eva 后感觉还是需要较多的技 巧才能扫描好物体，而且后期需要用软件进行较多的处理，比如数据的去噪、修复、光 滑、补洞等。同时，Artec 公司还开发了一款软件 Artec Studio，可以和微软的 Kinect 或华硕的 Xtion 以 及其他厂商的体感周边外设配合使用，使其成为三维扫描仪。Kinect 通过 Artec Studio 可以 完成模型扫描，然后进行后期处理，填补漏洞、清理数据、进行测量、导出数据等。不确 定它是否也使用了 Kinect Fusion 算法。 6、PolyWorks PolyWorks 是加拿大 InnovMetric 公司开发的点云处理软件，提供工程和制造业 3D 测量解 决方案，包含点云扫描、尺寸分析与比较、CAD 和逆向工程等功能。 7、CopyCAD CopyCAD 是由英国 DELCAM 公司出品的功能强大的逆向工程系统软件，它能允许从已存在的零件或实体模型中产生三维 CAD 模型。该软件为来自数字化数据的 CAD 曲面的产生提供了复杂的工具。CopyCAD 能够接受来自坐标测量机床的数据，同时跟踪机床和激光扫描器。 六、基于草图的 3D 建模软件 1、SketchUp SketchUp 是一套面向普通用户的易于使用的 3D 建模软件。使用 SketchUp，创建 3D 模型就像我们使用铅笔在图纸上作图一般，软件能 自动识别你画的这些线条，加以自动捕捉。它的建模流程简单明了，就是画线成面，而后 拉伸成体，这也是建筑或室内场景建模最常用的方法。 2、Teddy Teddy 是一款基于草图的 3D 建模软件，可以通过绘制自由形状笔画来制作有趣的 3D 模型。Teddy 需要在您的机器上安装 Java，主要是为 Windows 设计的。 3、EasyToy EasyToy 是一款 3D 建模软件。它使用基于草图的建模方法和 3D 绘画方法。用户界面非常友好，操作非常简单。通过组合几个简单的操作，可以快速创建复杂的 3D 模型。与现有的 3D 系统相比，EasyToy 易于学习且易于使用。EasyToy 具有广泛的应用，包括玩具设计，图形，动画和教育。 4、Magic Canvas Magic Canvas 一款可以从手绘草图中交互设计三维场景原型的软件，它将场景中模型的简单 2D 草图作为 3D 场景构造的输入。然后，系统自动识别数据库中的相应模型与用户输入的草图相匹配。 5、FiberMesh FiberMesh 是一款专门的网格生成工具。它可以动态创建真实几何体，也可以作为新的 SubTool 添加到现有模型中。在 FiberMesh 子调色板中的设置，可以为纤维，头发，毛发甚至植被生成完全不同的形状。 七、其他 3D 建模软件 1、人体建模软件 关于构建人体模型及动画，首推 Metacreations 公司的 Poser 软件（俗称 “人物造型大 师”）和开源的 MakeHuman 软件。这两款软件都是基于大量人类学形态特征数据，可以快速形成不同年龄段的男女脸部及肢体模型，并对局部体形进行调整。可以轻松快捷地设计 人体造型、动作和动画。我读博期间用过 Poser 构建人体模型来做 morphing，还是蛮方便的。 2、城市建模软件 加拿大 Esri 公司的 CityEngine 是三维城市建模的首选软件，可以利用二维数据快速创建三维场景，并能高效的进行规划设计。应用于数字城市、城市规划、轨道交通、管线、建 筑、游戏开发和电影制作等领域。另外，CityEngine 对 ArcGIS 的完美支持，使很多已有的 基础 GIS 数据不需转换即可迅速实现三维建模，缩短了三维 GIS 系统的建设周期。该软件早期是 ETH Zurich 大学的 Pascal Mueller 研究小组创办的 Procedural 公司开发的，后被 Esri 公司收购。 3、网页 3D (Web3D) 建模工具 一些基于网页 (web) 开发的 3D 模型设计软件，即基于 WebGL，可以在浏览 器中完成 3D 建模的工具。比如 3DTin，TinkerCAD（被 Autodesk 收购）等，它们的界面 简单直观，有 Chrome 等浏览器插件插件，可以在线生成 3D 模型，直接存在云端，并在社区分享模型。 4、其他小巧的 3D 建模软件 这些软件大部分都非常小巧，而且是开源且完全免费的。有很多媒体工作者和艺术家用这些小软件来制作 3D 作品，其中 Blender, K-3D, Art of Illusion, Seamless3d, Wings3D 等软件的使用面稍微广泛些。 Blender K-3D Art of Illusion SOFTIMAGE|XSI Mod Tool Zmodeler TopMod3d Google SketchUp 6 AutoQ3D Community - 3D Editor Anim8or Seamless3d BRL-CAD 3DPlus 3D Canvas eDrawings link 3D Minos freeCAD Bishop3D K3DSurf DesignWorkshop Lite GDesign 2.0 Sweet Home 3D trueSpace Alibre Design Xpress 3DVia Shape 八、虚拟现实软件和平台 虚拟现实软件本质上不是用于 3D 建模的，而是用来对生成好的 3D 模型和场景提供关于 视觉、听觉、触觉等虚拟的模拟，让用户如同身历其境一般。相关软件也有很多，只大致提及几个比较常见的。 1、 VirTools 和 Quest3D 法国 VirTools 公司的 VirTools 和美国 Act-3D 公司的 Quest3D 都是元老级的虚拟现实制作软 件，简单来说，就是工业或游戏用的实时图形渲染引擎，是 3D 虚拟和互动技术的集成工 具。可以让没有程序基础的美术人员利用内置的行为模块快速制作出许多不同用途的 3D 产 品，如网际网络、计算机游戏、多媒体、建筑设计、交互式电视、教育训练、仿真与产品 展示等 。网上的学习资料比较多。 2、Unity3D (U3D) Unity Technologies 开发的 Unity3D (U3D) 是最近几年冒出来的新秀，是一个全面整合的 专业虚拟 3D 和游戏引擎。它在制作虚拟现实及 3D 游戏方面上手非常容易，操作简单，互 动性好，有强大的地形渲染器。我们的学生使用 U3D 可以很快地制作一个 3D 游戏，因此也 强烈推荐大家学习使用。 3、Vega Vega 是 MultiGen-Paradigm 公司开发的用于实时视觉模拟和虚拟现实应用的开发引擎， 提供很多的 C/C++ 语言的应用程序接口 API，结合其应用程序的图形用户 GUI 界面软件 LynX，可以迅速创建各种实时交互的 3D 环境。对于开发 3D 游戏和 3D 场景漫游的项目非常 方便。 4、OSG (Open Scene Graph) OSG (Open Scene Graph) 是一套开源的基于 C++ 平台的应用程序接口 API，能够让开发者 快速、便捷地创建高性能、跨平台的交互式图形程序。它将 3D 场景定义为空间中一系列连 续的对象，能够对 3D 场景进行有效的管理。由于 OSG 是开源和完全免费的，很多 3D 应用 的软件都选用 OSG 作为基础架构。几年前，我们与一个公司合作开发的义齿软件就选用 OSG 作为管理 3D 数据的框架，使得开发非常方便。 九、推荐三维软件 对于没有设计基础的朋友来说相当不容易。下面介绍几款面向学校教育以及个人爱好者的简单三维软件。 1、Tinkercad Tinkercad 是一款基于网页的 3D 建模工具，设计界面色彩鲜艳可爱，如搭积木般简单易用，适合青少年儿童使用并进行建模。 国外一名叫 Emily 的 3D 打印爱好者使用 Tinkercad 建模然后打印出酿酒屋。从图中可以看到，利用 Tinkercad 同样可以完成漂亮的细节和优质的外观表现。 2、123D Design 123D Design 通过简单图形的堆砌和编辑生成复杂形状。这种 “傻瓜式” 的建模方式，即使你不是一个 CAD 建模工程师，也能随心所欲地在 123D Design 里建模。 3、123D Sculpt 123D Sculpt 是一款运行在 ipad 上的应用程序，它可以让每一个喜欢创作的人轻松创作出属于自己的雕塑模型。 4、123D Creature 123D Creature 可根据用户的想象来创造各种生物模型。无论是现实生活中存在的，还是只存在于想象中的，都可以创造出来。 5、123D Make 123D Make 将三维模型，转换为二维图案利用硬纸板、木料再现模型。它可创作美术、家具、雕塑或其他简单的物体。 6、123D Catch 利用云计算的强大能力，可将数码照片迅速转换为逼真的三维模型。只要使用傻瓜相机、手机或高级数码单反相机抓拍物体、人物或场景，人人都能利用 123D Catch 将照片转换成生动鲜活的三维模型。除 PC 外，现已推出手机 APP，手机也能当三维扫描仪。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"三维建模","slug":"三维建模","permalink":"https://leezhao415.github.io/tags/%E4%B8%89%E7%BB%B4%E5%BB%BA%E6%A8%A1/"}]},{"title":"使用YoloX训练自建数据集","slug":"使用YoloX训练自建数据集","date":"2021-12-12T10:45:17.000Z","updated":"2021-12-12T10:46:40.167Z","comments":true,"path":"2021/12/12/使用YoloX训练自建数据集/","link":"","permalink":"https://leezhao415.github.io/2021/12/12/%E4%BD%BF%E7%94%A8YoloX%E8%AE%AD%E7%BB%83%E8%87%AA%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86/","excerpt":"","text":"文章目录 1 Yolox 代码环境搭建 1.1 下载 Yolox 代码 1.2 搭建测试环境 1.3 代码测试：Demo 效果测试 1.3.1 下载 Yolox_s.pth 文件 1.3.2 Demo 测试 2 Yolox 自有数据集训练 2.1 数据集准备：标注数据 2.2 数据集准备：训练 &amp; 验证集划分 2.2.1 数据集介绍 2.2.2 模仿 VOC 格式排布 2.2.3 划分训练集和验证集 2.3 训练准备：修改训练配置参数 2.3.1 修改类别标签和数量 2.3.2 修改训练集信息 2.3.3 修改验证集信息 2.3.4 修改不同的网络结构 2.3.5 修改其他相关 2.4 Yolox 训练及常见问题 2.4.1 开始训练 2.4.2 常见问题 2.5 训练效果测试 随着旷视科技发布 Yolox 的论文和代码后，Yolox 得到了广泛的关注。但由于训练代码和之前的 Yolov3、Yolov4、Yolov5 的代码都不相同。且代码中的训练案例，以 COCO 和 VOC 格式为基准，和平时大家标注的文件，并不是完全相同的格式。而且训练自有数据的讲解流程，很多人不太熟悉。 因此本文以自有标注的人头数据集为案例，一步步和大家一起学习，整体的训练和测试全流程。 1 Yolox 代码环境搭建 在 Yolox 代码训练之前，我们先下载 Yolox 代码，将测试环境搭建起来。 测试的 Demo 跑通了，训练的环境也就没问题了。 1.1 下载 Yolox 代码 Yolox 代码链接：https://github.com/Megvii-BaseDetection/YOLOX 1.2 搭建测试环境 电脑系统为 Ubuntu 18.04 版本。 而 Yolox 测试环境的搭建，其实在代码中的，README.md 中 “Quick Start” 这部分。 首先为了测试环境更加独立，以 conda 为例，新建一个 Yolox 环境。 （1）新建一个 Conda 环境 1conda create -n Yolox_3.7 python=3.7 （2）进入 Conda 环境 1source activate Yolox_3.7 # 进入Conda环境中，并到下载好的YOLOX文件夹下。 （3）安装代码依赖的库文件 1pip3 install -U pip &amp;&amp; pip3 install -r requirements.txt （4）通过 setup.py 安装一些库文件 1python3 setup.py develop （5）下载 apex 文件并安装 apex 12345git clone https://github.com/NVIDIA/apexcd apexsudo pip3 install -v --disable-pip-version-check --no-cache-dir --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext&quot; ./ （6）下载 pycocotools 123pip3 install cythonpip3 install &#x27;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#x27; 1.3 代码测试：Demo 效果测试 1.3.1 下载 Yolox_s.pth 文件 搭建好环境后，就可以下载官方的预训练模型，对图片进行测试了。 预训练权重的下载链接，在官方代码的说明中。 我们先下载 Yolox_s.pth 文件，尝试测试效果。 https://github.com/Megvii-BaseDetection/YOLOX/blob/main/README.md 下载好 yolox_s.pth.tar 后，放到 YOLOX 代码的根目录下。 1.3.2 Demo 测试 使用代码中自带的图片，进行 Demo 测试。 在 YOLOX 文件夹的终端页面输入： 1python3 tools/demo.py image -n yolox-s -c yolox_s.pth.tar --path assets/dog.jpg --conf 0.3 --nms 0.5 --tsize 640 --save_result --device [gpu] YOLOX 的代码中，会新建一个 YOLOX_outputs 文件夹，在其中的 yolox_s/vis_res/，可以看到带有检测效果的图片。 到此 Yolox 的测试环境，以及测试效果都实现了，下面我们再进行 Yolox 的自有数据集训练。 2 Yolox 自有数据集训练 2.1 数据集准备：标注数据 在数据集中，大白采用教室场景下的一个人头数据集，和大家一起尝试整个流程。 ① 标注的工具：采用 Labelimg 标注软件 ② 标注的图片：3000 张人头图片 ③ 标签的类别：head。 ④ 下载链接： ​ PartA of SCUT-HEAD [Google Drive][Baidu Drive] PartB of SCUT-HEAD [Google Drive] [Baidu Drive] 2.2 数据集准备：训练 &amp; 验证集划分 2.2.1 数据集介绍 当下载好人头数据集后，可以看到 head 的数据集文件夹，数据集只有一个标签：head（人头）。 进入 head 文件夹中，其中包含两个文件夹： ① JPEGImages 文件夹：数据集的图片 ② Annotations 文件夹：与图片对应的所有 xml 文件。 我们前面说明，总共有 3000 张图片，即有对应的 3000 个 xml 文件。 2.2.2 模仿 VOC 格式排布 Yolox 的代码中有 VOC、和 COCO 两个数据集加载的格式，这里大白主要演示 VOC 加载的方式。 那么我们首先看一下 VOC 格式的分布： 在 VOC 这些文件夹中，我们主要用到： ① JPEGImages 文件夹：数据集图片 ② Annotations 文件夹：与图片对应的 xml 文件 ③ ImageSets/Main 文件夹：将数据集分为训练集和验证集，因此产生的 train.txt 和 val.txt。 从 Voc 的文件夹排布，和 head 的文件夹排布，可以看出：还缺少一个 ImageSets/Main 文件夹。 因此在 head 文件夹中，新建一个 ImageSets 文件集，再在其中新建一个 Main 子文件夹。 即得到下图这样的文件夹结构： 12345|--head |--Annotations |--ImageSets |--Main |---JPEGImages 2.2.3 划分训练集和验证集 因为自有标注好的图片数据，都是放在一起的。 而训练过程中，需要划分为训练集和验证集。 因此还需要编写脚本，将数据集分为训练集和验证集，并且生成对应的 train.txt，和 val.txt，放在 Main 文件夹中。 1234567891011121314151617181920212223242526272829303132333435# 对head文件夹进行拆分，分为train.txt和val.txtimport osimport randomimage_path = &quot;JPEGImages/&quot;xmls_path = &quot;Annotations/&quot;train_val_txt_path = &quot;ImageSets/Main/&quot;val_percent = 0.1images_list = os.listdir(images_path)random.shuffle(images_list)# 划分训练集和验证集的数量train_images_count = int((1-val_percent)*len(images_list))val_images_count = int(val_percent*len(images_list))# 生成训练集的train.txt文件train_txt = open(os.path.join(train_val_txt_path,&quot;train.txt&quot;),&quot;w&quot;)train_count = 0for i in range(train_images_count): text = images_list[i].split(&quot;.jpg&quot;)[0] + &quot;\\n&quot; train_txt.write(text) train_count +=1 print(&quot;train_count:&quot; + str(train_count))train_txt.close()# 生成验证集的val.txt文件val_txt = open(os.path.join(train_val_txt_path,&quot;val.txt&quot;),&quot;w&quot;)val_count = 0for i in range(val_images_count): text = images_list[train_images_count + i].split(&quot;.jpg&quot;)[0] + &quot;\\n&quot; val_txt.write(text) val_count+=1 print(&quot;val_count:&quot; + str(val_count))val_txt.close() 下载好代码后，将脚本文件 train_val_data_split.py 放在 JPEGImages 同路径下： 1234|--Annotations|--ImageSets|--JPEGImagestrain_val_data_split.py 并进行运行后，在 ImageSets/Main 文件夹下，就会生成对应的 train.txt 和 val.txt。 主要注意的是：代码中，训练集和验证集的比例，为 9：1，大家也可以自行调整。 2.3 训练准备：修改训练配置参数 2.3.1 修改类别标签和数量 ① 修改类别标签 因此前面自有的数据集只有一个类别，head。 将 yolox/data/datasets/voc_classes.py 中的标签信息，进行修改。 123VOC_CLASSES = ( &quot;head&quot;,) 注意：类别后面都要加逗号，例如 “head” 后面加了一个逗号 “，”。 ② 修改类别数量 （1）修改 exps/example/yolox_voc/yolox_voc_s.py 中的 self.num_classes ​ 因为只有 head 一种，所以 self.num_classes=1。 （2）修改 yolox/exp/yolox_base.py 中的 self.num_classes ​ 将 self.num_classes=80 修改为 1。 2.3.2 修改训练集信息 （1）修改 exps/example/yolox_voc/yolox_voc_s.py 中的 VOCDetection。 因为是自己的数据集，所以修改为： 12data_dir = &quot;/mnt/data/head/&quot;image_sets=[(&#x27;train&#x27;)], data_dir 是前面 2.2 节中 head 的绝对路径，images_sets 修改为 train。 此外，max_labels，表示图片最多的目标数量，这里大白因为使用的是人头，数量较多，所以改为 100。 （2）修改 yolox/data/datasets/voc.py 中，VOCDection 函数中的读取 txt 文件。 因为自有的数据集，没有 year 年代的信息，所以修改为： 123456将：self._year = yearrootpath = os.path.join(self.root, &quot;VOC&quot; + year)修改为：rootpath = self.root 2.3.3 修改验证集信息 修改 exps/example/yolox_voc/yolox_voc_s.py 中的 get_eval_loader 函数。 因为是自己的验证数据集，所以修改为： 1234567将：data_dir = os.path.join(get_yolox_datadir(),&quot;VOCdevkit&quot;),image_sets = [(&#x27;2007&#x27;,&#x27;test&#x27;)]修改为：data_dir = &quot;/mnt/data/head/&quot;image_sets = [(&#x27;val&#x27;)] data_dir 是前面 2.2 节中 head 的绝对路径，images_sets 修改为 val。 2.3.4 修改不同的网络结构 以 Yolox_s 网络为例，比如在 exps/default/yolox_s.py 中，self.depth= 0.33 ，self.width= 0.5 。和 Yolov5 中的不同网络调用方式一样。 为了统一不同的网络结构，继续修改 exps/example/yolox_voc/yolox_voc_s.py 中的，self.depth 和 self.width。 再修改 yolox/exp/yolox_base.py 中的，self.depth 和 self.width。 2.3.5 修改其他相关 （1）删除 year 等信息 因为自有数据集中，没有 year 信息，所以需要删除。 即修改 yolox/data/datasets/voc.py 中，_get_voc_results_file_template 函数。 所以将第三行的 year 等删除，如下图所示： 12345将：filedir = os.path.join(self.root, &quot;results&quot;, &quot;VOC&quot; + self._year, &quot;Main&quot;)修改为：filedir = os.path.join(self.root, &quot;results&quot;) 在训练过程中，在原始的 head 数据集中，会生成一个 results 的文件夹，保存历史信息。 （２）修改验证 epoch 的数量 目前代码中是训练迭代 10 个 epoch，再对验证集做１次验证，但大白想每迭代 1 个 epoch，即做一个验证，及时看到效果。 参数在 yolox/exp/yolox_base.py 的 class Exp 中： 1234567将self.print_interval = 10self.eval_interval = 10修改为：self.print_interval = 1self.eval_interval = 1 设置为每迭代一个 epoch，即使用验证集验证一次。 （３）修改验证时的相关信息 主要对读取验证信息的相关代码进行调整，代码在 yolox/data/datasets/voc.py 中_do_python_eval 函数中。 ① 因为自有数据集没有 year 信息，所以将其中的 rootpath 和 name： 12345将：rootpath = os.path.join(self.root, &quot;VOC&quot; + self._year)修改为：rootpath = self.root ② 因为没有 year 信息，所以将其中的 cachedir: 12345将：cachedir = os.path.join(self.root, &quot;annotations_cache&quot;, &quot;VOC&quot; + self._year,name)修改为：cachedir = os.path.join(self.root, &quot;annotations_cache&quot;) 在训练过程中，在原始的 head 数据集中，会生成一个 annotations_cache 的文件夹，保存历史信息。 12345|--Annotations|--annotations_cache|--ImageSets|--JPEGImages|--results ③ 因为没有 year 信息，所以修改 use_07_metric 的信息。 12345将：use_07_metric = True if int(self._year) &lt; 2010 else False修改为：use_07_metric = True 2.4 Yolox 训练及常见问题 2.4.1 开始训练 （1）终端训练 将下载好的 yolox_s.pth.tar 放到 YOLOX 文件夹中，打开终端，在终端中输入： 1python3 tools/train.py -f exps/example/yolox_voc/yolox_voc_s.py -d 0 -b 64 -c yolox_s.pth.tar （2）Pycharm 训练 代码运行时，常常需要 Debug 的方式，进行调试执行。 所以可以修改 train.py 的几个配置参数，采用 Debug 或者 Run 的方式进行执行。 主要需要修改以下参数： ① batch-size 根据自己机器的配置，设置 batch-size 的参数，比如大白这里设置的 64。 ② devices 参数 如果 GPU 服务器只有 1 张卡，将 devices 的 default 修改为 0。 ③ exp_file 参数 将 exp_file 的 default 修改为 yolox_voc_s.py 的路径（如代码版本更新，可重置路径）。 ④ ckpt 参数 如果使用预训练权重，将 ckpt 的 default 修改为模型权重的路径。 2.4.2 常见问题 在运行 tools/train.py 时，可能会出现以下问题，如没有可以跳过： 问题 1：apex 路径报错 123File &quot;/mnt/code/YOLOX/yolox/core/trainer.py&quot;, line9, in &lt;module&gt; from apex import amp ImportError: cannot import name &#x27;amp&#x27; from &#x27;apex&#x27;(unknown location) 因为 YOLOX 内的 apex 文件夹，还有一个 apex 文件夹，所以引用路径有点问题。 解决方法： 这时在每个调用 apex 的地方，添加一个 apex. 即可。 12345将：from apex import amp修改为：from apex.apex import amp 需要注意的是，不少的地方，需要添加 apex.，大概有 10 处左右，修改完之后，错误即可解决。 问题 2：probubuf 报错 12from google.protobuf.internal import enum_type_wrapperModuleNotFoundError: No module named &#x27;google.protobuf.internal&#x27; 解决方法： （1）pip3 uninsall probobuf （2）pip3 install google （3）pip3 install protobuf 2.5 训练效果测试 在上面训练好模型后，我们可以得到一个精度测试最优的网络模型： ① best_ckpt.pth.tar：在 tools/YOLOX_outputs/yolox_voc_s 文件夹中。 ② 为了方便测试，再挑选一张人头测试图片，放到 assets 文件夹中。 在 YOLOX 文件夹的终端页面输入： 1python3 tools/demo.py image -n yolox-s -c tools/YOLOX_outputs/yolox_voc_s/best_ckpt.pth.tar --path assets/head.jpg --conf 0.3 --nms 0.5 --tsize 640 --save_result --device [gpu] 在 YOLOX_outputs/yolox_s/vis_res，根据时间新建的文件夹下，可以看到检测出的效果图片。 ** 注意：** 如最后的类别都显示 person，将 coco_classes.py 中的类别，也修改为 “head”。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"YOLOX目标检测","slug":"YOLOX目标检测","permalink":"https://leezhao415.github.io/tags/YOLOX%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"【精华】使用YOLOX进行物体检测（附数据集）","slug":"【精华】使用YOLOX进行物体检测（附数据集）","date":"2021-12-12T10:40:37.000Z","updated":"2021-12-12T10:44:42.419Z","comments":true,"path":"2021/12/12/【精华】使用YOLOX进行物体检测（附数据集）/","link":"","permalink":"https://leezhao415.github.io/2021/12/12/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E4%BD%BF%E7%94%A8YOLOX%E8%BF%9B%E8%A1%8C%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%EF%BC%88%E9%99%84%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%89/","excerpt":"","text":"文章目录 使用 YOLOX 进行物体检测（附数据集） 一、 配置环境 二、 制作数据集 三、 修改数据配置文件 四、 训练 五、 测试 六、 保存测试结果 七、遇到的错误 使用 YOLOX 进行物体检测（附数据集） YOLOX: Exceeding YOLO Series in 2021 ・代码：https://github.com/Megvii-BaseDetection/YOLOX ・论文：https://arxiv.org/abs/2107.08430 YOLOX 是旷视开源的高性能检测器。旷视的研究者将解耦头、数据增强、无锚点以及标签分类等目标检测领域的优秀进展与 YOLO 进行了巧妙的集成组合，提出了 YOLOX，不仅实现了超越 YOLOv3、YOLOv4 和 YOLOv5 的 AP，而且取得了极具竞争力的推理速度。如下图： 其中 YOLOX-L 版本以 68.9 FPS 的速度在 COCO 上实现了 50.0% AP，比 YOLOv5-L 高出 1.8% AP！还提供了支持 ONNX、TensorRT、NCNN 和 Openvino 的部署版本，本文将详细介绍如何使用 YOLOX 进行物体检测。 一、 配置环境 本机的环境： 1.1 下载源码 GitHub 地址：https://github.com/Megvii-BaseDetection/YOLOX，下载完成后放到 D 盘根目录，然后用 PyCharm 打开。 1.2 安装依赖包 点击 “Terminal”, 如下图， 然后执行下面的命令，安装所有的依赖包。 1.3 安装 yolox 看到如下信息，则说明安装完成了 1.4 安装 apex APEX 是英伟达开源的，完美支持 PyTorch 框架，用于改变数据格式来减小模型显存占用的工具。其中最有价值的是 amp（Automatic Mixed Precision），将模型的大部分操作都用 Float16 数据类型测试，一些特别操作仍然使用 Float32。并且用户仅仅通过三行代码即可完美将自己的训练代码迁移到该模型。实验证明，使用 Float16 作为大部分操作的数据类型，并没有降低参数，在一些实验中，反而由于可以增大 Batch size，带来精度上的提升，以及训练速度上的提升。 安装步骤： 到官网下载 apex，地址：mirrors /nvidia/apex・CODE CHINA (csdn.net)[1] 下载完成后，解压后，在 Shell 里，进入到 apex-master 中。 执行安装命令 看到如下 log，则表明安装成功。 1.5 安装 pycocotools 1.6 验证环境 下载预训练模型，本文选用的是 YOLOX-s， 下载地址：https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.pth。 下载完成后，将预训练模型放到工程的根目录，如下图： 然后验证环境，执行： 参数说明 查看运行结果： 看到上图说明环境没有问题了。 二、 制作数据集 数据集我们采用 VOC 数据集，原始数据集是 Labelme 标注的数据集。 下载地址：https://pan.baidu.com/s/1kj-diqEK2VNVqd2n4ROa5g （提取码 rrnz） 新建 labelme2voc.py 文件 运行上面的代码就可以得到 VOC2007 数据集。如下图所示： VOC 的目录如下，所以要新建 data/VOCdevkit 目录，然后将上面的结果复制进去 到这里，数据集制作完成。 三、 修改数据配置文件 3.1 修改类别 文件路径：exps/example/yolox_voc/yolox_voc_s.py，本次使用的类别有 2 类，所以将 num_classes 修改为 2。 打开 yolox/data/datasets/voc_classes.py 文件，修改为自己的类别名： 3.2 修改数据集目录 文件路径：exps/example/yolox_voc/yolox_voc_s.py，data_dir 修改为 “./data/VOCdevkit”，image_sets 删除 2012 的，最终结果如下： 接着往下翻，修改 test 的路径，如下图： 打开 yolox/data/datasets/voc.py, 这里面有个错误。画框位置，将大括号的 “% s” 去掉，否则验证的时候一直报找不到文件的错误。 修改完成后，执行 重新编译 yolox。 四、 训练 推荐使用命令行的方式训练。 执行命令： 就可以开始训练了。如果不喜欢使用命令行的方式，想直接运行 train.py，那就需要就如 train.py 修改参数了。首先把 train.py 从 tools 里面复制一份到工程的根目录（建议这样做，否则需要修改的路径比较多，新手容易犯错误），如图： 打开，修改里面的参数。需要修改的参数如下： 按照上面的参数配置就可以运行了，如下图： 如果训练了一段时间，再想接着以前的模型再训练，应该如何做呢？修改 train.py 的参数即可，需要修改的参数如下： 命令行： 再次训练，你发现 epoch 不是从 0 开始了。 五、 测试 修改 yolox/data/datasets/_init_.py，导入 “VOC_CLASSES”，如下图： 修改 tools/demo.py 中代码，将 “COCO_CLASSES”，改为 “VOC_CLASSES”。 将 “295” 行的 Predictor 类初始化传入的 “COCO_CLASSES” 改为 “VOC_CLASSES”，如下图： 5.1 单张图片预测 使用训练好的模型进行测试。测试调用 tools/demo.py, 先用命令行的方式演示： 运行结果： 如果不想使用命令行，将 demo.py 复制一份放到工程的根目录，然后修改里面的参数。 然后直接运行 demo.py, 运行结果如下图： 5.2 批量预测 批量预测很简单，将 path 参数由文件路径改为图片的文件夹路径就可以。例： 这样就可以预测 assets 文件夹下面所有的图片了。 六、 保存测试结果 demo.py 只有将结果画到图片上，没有保存结果，所以要增加这部分的功能。 在 demo.py 的 178 行增加获取结果，并返回上层方法，如下图： 然后在 182，修改 image_demo 函数，增加获取结果，保存结果的逻辑，具体代码如下： 然后运行 demo.py, 就可以将结果保存到 txt 中。 七、遇到的错误 1、RuntimeError: DataLoader worker (pid(s) 9368, 12520, 6392, 7384) exited unexpectedly 打开 yolox/exp/yolox_base.py, 将 data_num_workers 设置为 0，如下图： 将 num_workers 设置为 0，程序报错，并提示设置环境变量 KMP_DUPLICATE_LIB_OK=TRUE 那你可以在设置环境变量 KMP_DUPLICATE_LIB_OK=TRUE 或者使用临时环境变量：（在代码开始处添加这行代码) 2、RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR 执行命令 报的错误，把 -“-o” 去掉后就正常了","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"YOLOX目标检测","slug":"YOLOX目标检测","permalink":"https://leezhao415.github.io/tags/YOLOX%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"多任务学习_YOLOP-全景驾驶感知，一个网络同时完成三大任务","slug":"多任务学习-YOLOP-全景驾驶感知，一个网络同时完成三大任务","date":"2021-12-12T10:35:51.000Z","updated":"2021-12-12T10:40:12.965Z","comments":true,"path":"2021/12/12/多任务学习-YOLOP-全景驾驶感知，一个网络同时完成三大任务/","link":"","permalink":"https://leezhao415.github.io/2021/12/12/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0-YOLOP-%E5%85%A8%E6%99%AF%E9%A9%BE%E9%A9%B6%E6%84%9F%E7%9F%A5%EF%BC%8C%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%90%8C%E6%97%B6%E5%AE%8C%E6%88%90%E4%B8%89%E5%A4%A7%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"文章目录 一、YOLOP 多任务学习 二、YOLOP 方法细节 A. 编码器 B. 解码器 C. 损失函数 D. 训练范式 三、实验 A. 设置 B. 结果 C. 消融研究 四、总结 YOLOP: You Only Look Once for Panoptic Driving Perception 全景驾驶感知系统是自动驾驶的重要组成部分，高精度、实时的感知系统可以辅助车辆在行驶中做出合理的决策。本文提出了一个全景驾驶感知网络（YOLOP）来同时执行交通目标检测、可行驶区域分割和车道检测。它由一个用于特征提取的编码器和三个用于处理特定任务的解码器组成。所提模型在具有挑战性的 BDD100K 数据集上表现非常出色，在准确性和速度方面在所有三个任务上都达到了 SOTA。此外，本文通过消融研究验证了所提多任务学习模型对联合训练的有效性，应该第一个可以在嵌入式设备 Jetson TX2（23 FPS）上用一个网络实时同时处理三个视觉感知任务并保持出色精度的工作。 paper: https://arxiv.org/abs/2108.11250 code: https://github.com/hustvl/YOLOP 一、YOLOP 多任务学习 全景驾驶感知系统中通常会涉及物体检测，以帮助车辆避开障碍物并遵守交通规则，还需要可行驶区域分割和车道检测，这对于规划车辆的行驶路线至关重要。已经有许多方法在分别处理这些任务，例如，Faster R-CNN 和 YOLOv4 处理对象检测，UNet 、DeepLab 和 PSPNet 来执行语义分割，SCNN 和 SAD-ENet 用于检测车道。 但在自动驾驶汽车常用的嵌入式设备上部署全景驾驶感知系统时，需要考虑有限的计算资源和延迟，单独处理这些任务需要更长的耗时。另外，交通场景理解中的不同任务往往有很多相关的信息，如图 1 所示，车道往往是可行驶区域的边界，可行驶区域通常紧密围绕着交通目标。多任务网络更适合这种情况，因为（1）它可以通过一次处理多个任务而不是一个接一个地处理来加速图像分析过程，（2）它可以在多个任务之间共享信息，这可以提升每一个任务的性能，因为多任务网络的每个任务通常共享相同的特征提取主干。因此，探索自动驾驶中的多任务方法至关重要。 多任务学习的目标是通过多个任务之间的共享信息来学习更好的表示，尤其是基于 CNN 的多任务学习方法还可以实现网络结构的卷积共享。 Mask R-CNN 扩展了 Faster R-CNN，增加了一个预测对象掩码的分支，有效地结合了实例分割和对象检测任务，这两个任务可以相互提升性能。 MultiNet 共享一个编码器和三个独立的解码器，同时完成场景分类、目标检测和驾驶区域分割三个场景感知任务。它在这些任务上表现良好，并在 KITTI 可行驶区域分割任务上达到了最先进的水平。然而，分类任务在控制车辆方面不如车道检测那么重要。 DLT-Net 继承了编码器 - 解码器结构，将交通目标检测、可行驶区域分割和车道检测结合在一起，并提出上下文张量来融合解码器之间的特征图，以在任务之间共享指定信息。虽然具有竞争力的性能，但它并没有达到实时性。 张等人提出了车道区域分割和车道边界检测之间相互关联的子结构，同时提出了一种新的损失函数来将车道线限制在车道区域的外轮廓上，以便它们在几何上重叠。然而，这个先验的假设也限制了它的应用，因为它只适用于车道线紧紧包裹车道区域的场景。 更重要的是，多任务模型的训练范式也值得思考。康等人指出只有当所有这些任务确实相关时，联合训练才是合适和有益的，否则需要采用交替优化。所以 Faster R-CNN 采用实用的 4 步训练算法来学习共享特征，这种范式有时可能会有所帮助，但也乏善可陈。 本文为全景驾驶感知系统构建了一个高效的多任务网络 YOLOP，包括目标检测、可行驶区域分割和车道检测任务，并且可以在部署 TensorRT 的嵌入式设备 Jetson TX2 上实现实时。通过同时处理自动驾驶中的这三个关键任务，本文方法减少了全景驾驶感知系统的推理时间，将计算成本限制在一个合理的范围内，并提高了每个任务的性能。 为了获得高精度和快速的速度，YOLOP 设计了一个简单高效的网络架构。本文使用轻量级 CNN 作为编码器从图像中提取特征，然后将这些特征图馈送到三个解码器以完成各自的任务。检测解码器基于当前性能最佳的单级检测网络 YOLOv4，主要有两个原因：（1）单级检测网络比两级检测网络更快，(2) 单级检测器基于网格的预测机制与其他两个语义分割任务相关，而实例分割通常与基于区域的检测器相结合。编码器输出的特征图融合了不同级别和尺度的语义特征，分割分支可以利用这些特征图出色地完成像素级语义预测。 除了端到端的训练策略外，本文还尝试了一些交替优化范式，逐步训练模型。一方面，将不相关的任务放在不同的训练步骤中，以防止相互限制，另一方面，先训练的任务可以指导其他任务，所以这种范式有时虽然繁琐但效果很好。然而，实验表明本文所提的模型并没有这个必要，因为端到端训练的模型可以表现得足够好。最终，所设计的全景驾驶感知系统在单个 NVIDIA TITAN XP 上达到了 41 FPS，在 Jetson TX2 上达到了 23 FPS；同时，它在 BDD100K 数据集的三个任务上取得 SOTA。 YOLOP 主要贡献是：（1）提出了一个高效的多任务网络，可以一个网络模型同时处理自动驾驶中的三个关键任务：物体检测、可行驶区域分割和车道检测，减少推理时间的同时提高了每项任务的性能，显著节省计算成本。应该是第一个在嵌入式设备上一个模型同时跑三个任务实现实时性，同时在 BDD100K 数据集上保持 SOTA 的工作。(2) 设计了消融实验验证了所提多任务处理方案的有效性，证明了三个任务可以联合学习，无需繁琐的交替优化。 二、YOLOP 方法细节 本文提出了一个简单高效的前馈网络，可以同时完成交通目标检测、可行驶区域分割和车道检测任务。如图 2 所示，本文的全景驾驶感知单次网络 YOLOP，包含一个共享编码器和三个后续解码器来解决特定任务。不同解码器之间没有复杂和冗余的共享块，这减少了计算消耗并使网络能够轻松地进行端到端的训练。 A. 编码器 网络共享一个编码器，它由骨干网络和颈部网络组成。 1. Backbone 骨干网络用于提取输入图像的特征。通常将一些经典的图像分类网络作为主干。鉴于 YOLOv4 在物体检测上的优异性能，本文选择 CSPDarknet 作为主干，其解决了优化 Cspnet 时梯度重复的问题。它支持特征传播和特征重用，减少了参数量和计算量，有利于保证网络的实时性。 2. Neck Neck 用于融合 backbone 产生的特征。YOLOP 的 Neck 主要由空间金字塔池（SPP）模块和特征金字塔网络（FPN）模块组成。SPP 生成并融合不同尺度的特征，FPN 融合不同语义层次的特征，使得生成的特征包含多尺度、多语义层次的信息，采用串联的方法来融合这些特征。 B. 解码器 网络中的三个头是三个任务的特定解码器。 1. Detect Head 与 YOLOv4 类似，采用基于 anchor 的多尺度检测方案。首先，使用一种称为路径聚合网络（PAN）的结构，其是一种自下而上的特征金字塔网络。FPN 自顶向下传递语义特征，PAN 自底向上传递定位特征，将它们结合起来以获得更好的特征融合效果，直接使用 PAN 中的多尺度融合特征图进行检测。多尺度特征图的每个网格将被分配三个不同长宽比的先验 anchor，检测头将预测位置的偏移量、尺度化的高度和宽度，以及每个类别的对应概率和预测的置信度。 2. Drivable Area Segment Head &amp; Lane Line Segment Head 可行驶区域分割头和车道线分割头采用相同的网络结构，将 FPN 的底层馈送到分割分支，大小为 (W/8, H/8, 256)。分割分支非常简单，经过三个上采样过程，将输出特征图恢复到 (W, H, 2) 的大小，两个通道分别代表了输入图像中每个像素对于可行驶区域 / 车道线和背景的概率。由于颈部网络中共享 SPP，本文没有像其他人通常所做的那样添加额外的 SPP 模块到分割分支，因为这不会对所提网络的性能带来任何改善。此外，上采样层使用最邻近插值方法来降低计算成本而不是反卷积。因此，模型的分割解码器不仅获得了高精度的输出，而且在推理过程中也非常快。 C. 损失函数 D. 训练范式 本文尝试了不同的范式来训练模型。最简单的一种是端到端的训练，可以联合学习三个任务。当所有任务确实相关时，这种训练范式很有用。 此外，还尝试了一些交替优化算法，逐步训练模型。在每一步中，模型都可以专注于一个或多个相关的任务，而不管那些不相关的任务。即使不是所有的任务都是相关的，所提模型仍然可以通过这种范式对每个任务进行充分的学习。算法 1 说明了一种逐步训练方法的过程。 算法 1 一种逐步训练多任务模型的方法 三、实验 A. 设置 1. 数据集设置 BDD100K 数据集支持自动驾驶领域多任务学习的研究，拥有 10 万帧图片和 10 个任务的注释，是最大的驾驶视频数据集。由于数据集具有地理、环境和天气的多样性，在 BDD100K 数据集上训练的算法足够健壮，可以迁移到新环境，因此用 BDD100K 数据集来训练和评估网络是很好的选择。 BDD100K 数据集分为三部分，70K 图像的训练集，10K 图像的验证集，20K 图像的测试集。由于测试集的标签不是公开的，所以在验证集上进行模型评估。 2. 实现细节 为了提高模型的性能，根据经验采用了一些实用的技术和数据增强方法。 为了使检测器能够获得更多交通场景中物体的先验知识，使用 k-means 聚类算法从数据集的所有检测帧中获取先验锚点。使用 Adam 作为优化器来训练模型，初始学习率、β1 和 β2 分别设置为 0.001、0.937 和 0.999。在训练过程中使用预热（Warm-up）和余弦退火来调整学习率，旨在引导模型更快更好地收敛。 使用数据增强来增加图像的变化，使模型在不同环境中具有鲁棒性。训练方案中考虑了光度失真和几何畸变，调整图像的色调（hue）、饱和度（saturation）和像素值进行光度失真，使用随机旋转、缩放、平移、剪切和左右翻转进行几何畸变。 3. 实验设置 实验中选择了一些优秀的多任务网络和专注于单个任务的网络与本文所提网络进行比较。 MultiNet 和 DLT-Net 都同时处理多个全景驾驶感知任务，并且在 BDD100K 数据集上的目标检测和可行驶区域分割任务中取得了很好的性能，Faster-RCNN 是两阶段目标检测网络的杰出代表，YOLOv5 是在 COCO 数据集上实现最先进性能的单级网络，PSPNet 凭借其超强的聚合全局信息能力在语义分割任务上取得了出色的表现。通过在 BDD100k 数据集上重新训练上述网络，与本文在目标检测和可行驶区域分割任务上的网络进行比较。 由于在 BDD100K 数据集上没有合适的现有多任务网络处理车道检测任务，本文将所提网络与 Enet、SCNN 和 Enet-SAD 这三个先进的车道检测网络进行了比较。 另外，将联合训练范式的性能与多种交替训练范式进行了比较，将经过训练以处理多项任务的多任务模型的准确性和速度与经过训练以执行特定任务的模型进行比较。实验中将 BDD100K 数据集中的图像从 1280×720×3 调整为 640×384×3，所有对照实验都遵循相同的实验设置和评估指标，均在 NVIDIA GTX TITAN XP 上运行。 B. 结果 通过简单的端到端训练 YOLOP，将其与所有三个任务的其他代表性模型进行比较。 1. 交通目标检测结果 由于 Multinet 和 DLT-Net 只能检测车辆，我们只考虑 BDD100K 数据集上 5 个模型的车辆检测结果。如表 I 所示，使用 Recall 和 mAP50 作为检测精度的评估指标。YOLOP 在检测精度上超过了 Faster RCNN、MultiNet 和 DLT-Net，并且可以与实际上使用更多技巧的 YOLOv5s 相媲美。速度上模型可以实时推断，YOLOv5s 更快是因为它没有车道线头和可行驶区域头。交通目标检测的可视化如图 3 所示。 2. 可行驶区域分割结果 本文将 BDD100K 数据集中的 “区域 / 可行驶” 和 “区域 / 替代” 类都归为 “可行驶区域”，模型只需要区分图像中的可行驶区域和背景。mIoU 用于评估不同模型的分割性能。从结果表 II 可以看出，YOLOP 分别优于 MultiNet、DLT-Net 和 PSPNet 19.9%、20.2% 和 1.9%，而且推理速度快 4 到 5 倍。可行驶区域分割的可视化结果如图 4 所示。 3. 车道检测结果 BDD100K 数据集中的车道线用两条线标注，所以直接使用标注非常棘手。实验遵循侯等人的设置以便进行比较，首先根据两条标线计算中心线，然后将训练集中的车道线宽度设置为 8 个像素，同时保持测试集的车道线宽度为 2 个像素，使用像素精度和车道的 IoU 作为评估指标。如表 III 所示，YOLOP 的性能大幅超过了其他三个模型。车道检测的可视化结果如图 5 所示。 C. 消融研究 本文设计了以下两个消融实验来进一步说明所提方案的有效性。 1. End-to-end vs Step-by-step 表 IV 比较了联合训练范式与多种交替训练范式的性能。可以看出 YOLOP 通过端到端的训练已经表现得非常好，已经不再需要进行交替优化。有趣的是，端到端范式训练检测任务首先似乎表现更好，这可能主要是因为整个模型更接近一个完整的检测模型，并且模型在执行检测任务时更难收敛（更难的任务用更适配的模型能达到更好的效果）。另外，由三步组成的范式略胜于两步，类似的交替训练可以运行更多的步骤，但改进已可以忽略不计。 注：E, D, S 和 W 分别表示 Encoder, Detect head, two Segment heads and whole network。比如 ED-S-W 表示先只训练 Encoder and Detect head，然后冻结 Encoder and Detect head 训练 two Segmentation heads，最后联合三个任务训练 whole network。 2. Multi-task vs Single task 为了验证多任务学习方案的有效性，对多任务方案和单任务方案的性能进行了比较。表 V 显示了这两种方案在每个特定任务上的性能比较，可以看出采用多任务方案训练的模型性能接近于专注单个任务的性能，更重要的是，与单独执行每个任务相比，多任务模型可以节省大量时间。 四、总结 本文提出了一个简单高效的多任务网络 YOLOP，它可以同时处理物体检测、可行驶区域分割和车道线检测三个典型的驾驶感知任务，并且可以进行端到端的训练。训练出的模型在具有挑战性的 BDD100k 数据集上表现异常出色，在所有三个任务上都达到或大大超过了最先进的水平，并且可以在嵌入式设备 Jetson TX2 上进行实时推理，使得模型可以在真实场景中使用。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"多任务学习模型","slug":"多任务学习模型","permalink":"https://leezhao415.github.io/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"}]},{"title":"Pytorch与PaddlePaddle  API映射表","slug":"Pytorch与PaddlePaddle-API映射表","date":"2021-12-12T10:28:31.000Z","updated":"2021-12-12T11:07:28.043Z","comments":true,"path":"2021/12/12/Pytorch与PaddlePaddle-API映射表/","link":"","permalink":"https://leezhao415.github.io/2021/12/12/Pytorch%E4%B8%8EPaddlePaddle-API%E6%98%A0%E5%B0%84%E8%A1%A8/","excerpt":"","text":"文章目录 PyTorch-PaddlePaddle API 映射表 X2Paddle 介绍 API 映射表目录 基础操作类 API 映射列表 组网类 API 映射列表 Loss 类 API 映射列表 工具类 API 映射列表 视觉类 API 映射列表 本文档基于 X2Paddle 研发过程梳理了 PyTorch（v1.8.1) 常用 API 与 PaddlePaddle 2.0.0 API 对应关系与差异分析。通过本文档，帮助开发者快速迁移 PyTorch 使用经验，完成模型的开发与调优。 X2Paddle 致力于帮助其它主流深度学习框架开发者快速迁移至飞桨框架，目前提供三大功能 预测模型转换 支持 Caffe/TensorFlow/ONNX/PyTorch 的模型一键转为飞桨的预测模型，并使用 PaddleInference/PaddleLite 进行 CPU/GPU/Arm 等设备的部署 PyTorch 训练项目转换 支持 PyTorch 项目 Python 代码（包括训练、预测）一键转为基于飞桨框架的项目代码，帮助开发者快速迁移项目，并可享受 AIStudio 平台对于飞桨框架提供的海量免费计算资源 API 映射文档 详细的 API 文档对比分析，帮助开发者快速从 PyTorch 框架的使用迁移至飞桨框架的使用，大大降低学习成本 详细的项目信息与使用方法参考 X2Paddle 在 Github 上的开源项目: https://github.com/PaddlePaddle/X2Paddle 类别 简介 基础操作类 主要为 torch.XX 类 API 组网类 主要为 torch.nn.XX 类下组网相关的 API Loss 类 主要为 torch.nn.XX 类下 loss 相关的 API 工具类 主要为 torch.nn.XX 类下分布式相关的 API 和 torch.utils.XX 类 API 视觉类 主要为 torchvision.XX 类 API 该文档梳理了基础操作的 PyTorch-PaddlePaddle API 映射列表，主要包括了构造 Tensor、数学计算、逻辑计算相关的 API。 序号 PyTorch API PaddlePaddle API 备注 1 torch.set_default_dtype paddle.set_default_dtype 功能一致 2 torch.get_default_dtype paddle.get_default_dtype 功能一致 3 torch.numel paddle.numel 功能一致，参数名不一致 4 torch.tensor paddle.to_tensor 差异对比 5 torch.from_numpy paddle.to_tensor 差异对比 6 torch.zeros paddle.zeros 差异对比 7 torch.zeros_like paddle.zeros_like 差异对比 8 torch.ones paddle.ones 差异对比 9 torch.ones_like paddle.ones_like 差异对比 10 torch.empty paddle.empty 差异对比 11 torch.empty_like paddle.empty_like 差异对比 12 torch.full paddle.full 功能一致，参数不一致 13 torch.full_like paddle.full_like 差异对比 14 torch.arange paddle.arange 功能一致，参数不一致 15 torch.range paddle.arange 功能一致，参数不一致 16 torch.linspace paddle.linspace 功能一致，参数不一致 17 torch.eye paddle.eye 功能一致，参数不一致 18 [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=torch cat#torch.cat) paddle.concat 功能一致，参数名不一致 19 torch.chunk paddle.chunk 功能一致，参数名不一致 20 torch.gather paddle.gather 差异对比 21 torch.index_select paddle.index_select 功能一致，参数名不一致 22 torch.masked_select paddle.masked_select 功能一致，参数名不一致 23 torch.narrow paddle.slice 差异对比 24 torch.nonzero paddle.nonzero 功能一致，参数名不一致 25 torch.reshape paddle.reshape 功能一致，参数名不一致 26 torch.split paddle.split 差异对比 27 torch.squeeze paddle.squeeze 功能一致，参数名不一致 28 torch.stack paddle.stack 功能一致，参数名不一致 29 torch.t paddle.t 功能一致，参数名不一致 30 torch.transpose paddle.transpose 差异对比 31 torch.unbind paddle.unbind 功能一致，参数名不一致 32 torch.unsqueeze paddle.unsqueeze 功能一致，参数名不一致 33 torch.where paddle.where 功能一致 34 torch.bernoulli paddle.bernoulli 功能一致，参数不一致 35 torch.multinomial paddle.multinomial 功能一致，参数不一致 36 torch.normal paddle.normal 差异对比 37 torch.rand paddle.rand 差异对比 38 torch.randint paddle.randint 功能一致，参数不一致 39 [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html?highlight=ran dn#torch.randn) paddle.randn 差异对比 40 torch.randperm paddle.randperm 功能一致，参数不一致 41 torch.save paddle.save 差异对比 42 torch.load paddle.load 差异对比 43 torch.abs paddle.abs 功能一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 44 torch.absolute paddle.abs 功能一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 45 [torch.acos](https://pytorch.org/docs/stable/generated/torch.acos.html?highlight=torch acos#torch.acos) paddle.acos 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 46 torch.arccos paddle.acos 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 47 torch.add padle.add 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 48 torch.asin paddle.asin 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 49 torch.arcsin paddle.asin 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 50 torch.atan paddle.atan 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 51 torch.arctan paddle.atan 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 52 torch.ceil paddle.ceil 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 53 torch.clamp paddle.clip 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 54 torch.conj paddle.conj 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 55 torch.cos paddle.cos 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 56 torch.cosh paddle.cosh 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 57 torch.div paddle.divide 差异对比 58 torch.divide paddle.divide 差异对比 59 torch.erf paddle.erf 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 60 torch.exp paddle.exp 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 61 torch.floor paddle.floor 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 62 torch.floor_divide paddle.floor_divide 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 63 torch.fmod paddle.mod 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 64 torch.log paddle.log 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 65 torch.log10 paddle.log10 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 66 torch.log1p paddle.log1p 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 67 torch.log2 paddle.log2 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 68 torch.logical_and paddle.logical_and 功能一致，参数名不一致 69 torch.logical_not paddle.logical_not 功能一致，参数名不一致 70 torch.logical_or paddle.logical_or 功能一致，参数名不一致 71 torch.logical_xor paddle.logical_xor 功能一致，参数名不一致 72 [torch.mul](https://pytorch.org/docs/stable/generated/torch.mul.html?highlight=torch mul#torch.mul) paddle.multiply 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 73 torch.multiply paddle.multiply 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 74 torch.pow paddle.pow 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 75 torch.real paddle.real 功能一致，参数名不一致 76 torch.reciprocal paddle.reciprocal 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 77 torch.remainder paddle.mod 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 78 torch.round paddle.round 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 79 torch.rsqrt paddle.rsqrt 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 80 torch.sign paddle.sign 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 81 torch.sin paddle.sin 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 82 torch.sinh paddle.sinh 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 83 torch.sqrt paddle.sqrt 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 84 torch.argmax paddle.argmax 功能一致，参数名不一致 85 torch.argmin paddle.argmin 功能一致，参数名不一致 86 torch.max paddle.max 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 87 torch.min paddle.min 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 88 torch.square paddle.square 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 89 torch.sub paddle.subtract 差异对比 90 torch.subtract paddle.subtract 差异对比 91 torch.tanh paddle.tanh 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 92 torch.true_divide paddle.divide 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 93 torch.dist paddle.dist 功能一致，参数名不一致 94 torch.logsumexp paddle.logsumexp 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 95 torch.mean paddle.mean 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 96 torch.median paddle.median 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 97 torch.norm paddle.norm 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 以及 dtype 参数代表输出 Tensor 类型 98 torch.prod paddle.prod 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 99 torch.std paddle.std 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 100 torch.std_mean 无对应实现 组合实现 101 torch.sum paddle.sum 功能一致，参数名不一致 102 torch.unique paddle.unique 差异对比 103 torch.var paddle.var 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 104 torch.var_mean 无对应实现 组合实现 105 torch.allclose paddle.allclose 功能一致，参数名不一致 106 torch.argsort paddle.argsort 功能一致，参数名不一致 107 torch.eq paddle.equal 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 108 torch.equal paddle.equal_all 功能一致，参数名不一致 109 torch.ge paddle.greater_equal 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 110 torch.gt paddle.greater_than 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 111 torch.isfinite paddle.isfinite 功能一致，参数名不一致 112 torch.isinf paddle.isinf 功能一致，参数名不一致 113 torch.isnan paddle.isnan 功能一致，参数名不一致 114 torch.kthvalue 无对应实现 组合实现 115 torch.le paddle.less_equal 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 116 torch.lt paddle.less_than 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 117 torch.maximum paddle.maximum 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 118 torch.minimum paddle.minimum 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 119 torch.ne paddle.not_equal 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 120 torch.sort paddle.argsort 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 121 torch.topk paddle.topk 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 122 torch.cross paddle.cross 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 123 torch.any paddle.any 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 124 torch.cumsum paddle.cumsum 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 125 torch.diag paddle.diag 差异对比 126 torch.diag_embed paddle.nn.functional.diag_embed 功能一致 127 torch.einsum paddlenlp.ops.einsum 功能一致，需要安装 paddlenlp 128 torch.flatten paddle.flatten 功能一致，参数名不一致 129 torch.flip paddle.flip 功能一致，参数名不一致 130 torch.rot90 无对应实现 组合实现 131 torch.meshgrid paddle.meshgrid 功能一致 132 torch.roll paddle.roll 功能一致，参数名不一致 133 torch.tril paddle.tril 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 134 torch.triu paddle.triu 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 135 torch.bmm paddle.bmm 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 136 torch.cholesky paddle.cholesky 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 137 torch.dot paddle.dot 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 138 torch.inverse paddle.inverse 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 139 torch.trace paddle.trace 差异对比 140 torch.addmv 无对应实现 组合实现 141 torch.addr 无对应实现 组合实现 142 torch.baddbmm 无对应实现 组合实现 143 torch.addmm paddle.addmm 功能一致，参数名不一致，PaddlePaddle 未定义 out 参数代表输出 Tensor 144 torch.chain_matmul 无对应实现 组合实现 145 torch.cholesky_inverse 无对应实现 组合实现 146 torch.cholesky_solve 无对应实现 组合实现 147 torch.matmul paddle.matmul 差异对比 148 torch.mm paddle.matmul 差异对比 149 torch.mv 无对应实现 组合实现 持续更新… 该文档梳理了与构造网络相关的 PyTorch-PaddlePaddle API 映射列表。 序号 PyTorch API PaddlePaddle API 备注 1 [torch.nn.Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html?highlight=torch nn conv1d#torch.nn.Conv1d) paddle.nn.Conv1D 差异对比 2 torch.nn.Conv2d paddle.nn.Conv2D 差异对比 3 torch.nn.Conv3d paddle.nn.Conv3D 差异对比 4 [torch.nn.ConvTranspose1d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html?highlight=torch nn convtranspose1d#torch.nn.ConvTranspose1d) paddle.nn.Conv1DTranspose 差异对比 5 torch.nn.ConvTranspose2d paddle.nn.Conv2DTranspose 差异对比 6 torch.nn.ConvTranspose3d paddle.nn.Conv3DTranspose 差异对比 7 torch.nn.Linear paddle.nn.Linear 差异对比 8 torch.nn.MaxPool1d paddle.nn.MaxPool1D 差异对比 9 torch.nn.MaxPool2d paddle.nn.MaxPool2D 差异对比 10 torch.nn.MaxPool3d paddle.nn.MaxPool3D 差异对比 11 torch.nn.MaxUnpool1d 无对应实现 组合实现 12 torch.nn.MaxUnpool2d 无对应实现 组合实现 13 torch.nn.MaxUnpool3d 无对应实现 组合实现 14 torch.nn.AvgPool1d paddle.nn.AvgPool1D 差异对比 15 torch.nn.AvgPool2d paddle.nn.AvgPool2D 差异对比 16 torch.nn.AvgPool3d paddle.nn.AvgPool3D 差异对比 17 torch.nn.AdaptiveMaxPool1d paddle.nn.AdaptiveMaxPool1D 功能一致，参数名不一致 18 torch.nn.AdaptiveMaxPool2d paddle.nn.AdaptiveMaxPool2D 功能一致，参数名不一致 19 torch.nn.AdaptiveMaxPool3d paddle.nn.AdaptiveMaxPool3D 功能一致，参数名不一致 20 torch.nn.AdaptiveAvgPool1d paddle.nn.AdaptiveAvgPool1D 功能一致，参数名不一致 21 torch.nn.AdaptiveAvgPool2d paddle.nn.AdaptiveAvgPool2D 功能一致，参数名不一致 22 torch.nn.AdaptiveAvgPool3d paddle.nn.AdaptiveAvgPool3D 功能一致，参数名不一致 23 torch.nn.ConstantPad1d paddle.nn.Pad1D 差异对比 24 torch.nn.ConstantPad2d paddle.nn.Pad2D 差异对比 25 torch.nn.ConstantPad3d paddle.nn.Pad3D 差异对比 26 torch.nn.ReflectionPad1d paddle.nn.Pad1D 差异对比 27 torch.nn.ReflectionPad2d paddle.nn.Pad2D 差异对比 28 torch.nn.ReplicationPad1d paddle.nn.Pad1D 差异对比 29 torch.nn.ReplicationPad2d paddle.nn.Pad2D 差异对比 30 torch.nn.ReplicationPad3d paddle.nn.Pad3D 差异对比 31 [torch.nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html?highlight=torch nn batchnorm1d#torch.nn.BatchNorm1d) paddle.nn.BatchNorm1D 差异对比 32 torch.nn.BatchNorm2d paddle.nn.BatchNorm2D 差异对比 33 [torch.nn.BatchNorm3d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html?highlight=torch nn batchnorm3d#torch.nn.BatchNorm3d) paddle.nn.BatchNorm3D 差异对比 34 torch.nn.Upsample paddle.nn.Upsample 差异对比 35 torch.nn.Dropout paddle.nn.Dropout 差异对比 36 torch.nn.Dropout2d paddle.nn.Dropout2D 差异对比 37 torch.nn.Dropout3d paddle.nn.Dropout3D 差异对比 38 torch.nn.LSTM paddle.nn.LSTM 差异对比 39 [torch.nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=torch nn gru#torch.nn.GRU) paddle.nn.GRU 差异对比 40 torch.nn.Embedding paddle.nn.Embedding 差异对比 41 torch.nn.ELU paddle.nn.ELU 功能一致，PaddlePaddle 未定义 inplace 参数表示在不更改变量的内存地址的情况下，直接修改变量的值 42 torch.nn.Hardsigmoid paddle.nn.Hardsigmoid 功能一致，PaddlePaddle 未定义 inplace 参数表示在不更改变量的内存地址的情况下，直接修改变量的值 43 torch.nn.LeakyReLU paddle.nn.LeakyReLU 功能一致，PaddlePaddle 未定义 inplace 参数表示在不更改变量的内存地址的情况下，直接修改变量的值 44 torch.nn.PReLU paddle.nn.PReLU 功能一致 45 torch.nn.ReLU paddle.nn.ReLU 功能一致，PaddlePaddle 未定义 inplace 参数表示在不更改变量的内存地址的情况下，直接修改变量的值 46 torch.nn.Softmax paddle.nn.Softmax 功能一致，参数名不一致 持续更新… 该文档梳理了计算 loss 相关的 PyTorch-PaddlePaddle API 映射列表。 序号 PyTorch API PaddlePaddle API 备注 1 torch.nn.L1Loss paddle.nn.L1Loss 功能一致，PyTroch 存在废弃参数 size_average 和 reduce 。 2 torch.nn.MSELoss paddle.nn.MSELoss 功能一致，PyTroch 存在废弃参数 size_average 和 reduce 。 3 torch.nn.CrossEntropyLoss paddle.nn.CrossEntropyLoss 差异对比 4 torch.nn.KLDivLoss paddle.nn.KLDivLoss 差异对比 5 torch.nn.BCELoss paddle.nn.BCELoss 功能一致，PyTroch 存在废弃参数 size_average 和 reduce 。 6 torch.nn.BCEWithLogitsLoss paddle.nn.BCEWithLogitsLoss 功能一致，PyTroch 存在废弃参数 size_average 和 reduce 。 7 [torch.nn.SmoothL1Loss](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html?highlight=torch nn smoothl1loss#torch.nn.SmoothL1Loss) paddle.nn.SmoothL1Loss 功能一致，参数名不一致，PyTroch 存在废弃参数 size_average 和 reduce 。 持续更新… 该文档梳理了与数据处理、分布式处理等相关的 PyTorch-PaddlePaddle API 映射列表。 序号 PyTorch API PaddlePaddle API 备注 1 torch.nn.DataParallel paddle.DataParallel 差异对比 2 [torch.nn.parameter.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html?highlight=torch nn parameter#torch.nn.parameter.Parameter) paddle.create_parameter 差异对比 3 torch.nn.utils.clip_grad_value_ 无对应实现 组合实现 4 torch.utils.data.DataLoader paddle.io.DataLoader 差异对比 5 torch.utils.data.random_split 无对应实现 组合实现 6 torch.utils.data.distributed.DistributedSampler 无对应实现 组合实现 7 [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html?highlight=torch utils data dataset#torch.utils.data.Dataset) paddle.io.Dataset 功能一致 8 torch.utils.data.BatchSampler paddle.io.BatchSampler 差异对比 9 torch.utils.data.Sampler paddle.io.Sampler 功能一致 持续更新… 该文档梳理了与视觉处理相关的 PyTorch-PaddlePaddle API 映射列表。 序号 PyTorch API PaddlePaddle API 备注 1 torchvision.transforms.Compose paddle.vision.transforms.Compose 功能一致 2 torchvision.transforms.ToPILImage 无对应实现 组合实现 3 torchvision.transforms.Resize paddle.vision.transforms.Resize 功能一致 4 torchvision.transforms.ToTensor paddle.vision.transforms.ToTensor 功能一致 5 torchvision.transforms.RandomHorizontalFlip paddle.vision.transforms.RandomHorizontalFlip 功能一致 6 torchvision.transforms.CenterCrop paddle.vision.transforms.CenterCrop 功能一致 7 torchvision.transforms.ColorJitter paddle.vision.transforms.ColorJitter 功能一致 8 torchvision.transforms.Grayscale paddle.vision.transforms.Grayscale 功能一致 9 torchvision.transforms.Normalize paddle.vision.transforms.Normalize 差异对比 10 torchvision.transforms.RandomResizedCrop paddle.vision.transforms.RandomResizedCrop 功能一致 11 torchvision.transforms.Pad paddle.vision.transforms.Pad 功能一致 12 torchvision.transforms.RandomCrop paddle.vision.transforms.RandomCrop 功能一致 13 torchvision.transforms.RandomRotation paddle.vision.transforms.RandomRotation 功能一致 14 torchvision.transforms.RandomVerticalFlip paddle.vision.transforms.RandomVerticalFlip 功能一致 15 torchvision.transforms.Lambda 无对应实现 组合实现 17 torchvision.utils.save_image 无对应实现 组合实现 18 [torchvision.models 系列模型](https://pytorch.org/vision/stable/models.html?highlight=torchvision models) X2Paddle 提供 使用方式","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"PaddlePaddle","slug":"PaddlePaddle","permalink":"https://leezhao415.github.io/tags/PaddlePaddle/"}]},{"title":"基于 Multiple Teacher Single Student 框架的多领域对话模型","slug":"基于-Multiple-Teacher-Single-Student-框架的多领域对话模型","date":"2021-12-12T10:23:56.000Z","updated":"2021-12-12T10:51:38.357Z","comments":true,"path":"2021/12/12/基于-Multiple-Teacher-Single-Student-框架的多领域对话模型/","link":"","permalink":"https://leezhao415.github.io/2021/12/12/%E5%9F%BA%E4%BA%8E-Multiple-Teacher-Single-Student-%E6%A1%86%E6%9E%B6%E7%9A%84%E5%A4%9A%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"文章目录 多领域对话系统中棘手的状态表示问题 来源于生活的 MTSS（Multiple Teachers Single Student）模型 MTSS 的模型结构 Model Overview Teacher Model Student Model MTSS 中的知识迁移方法 文本级别的知识迁移 实验与结果分析 数据集 实验设定 结果分析 总结 后记 导读：一个源于高考的高性能多领域对话模型。 作者 | 珂蓝、元淳 来源 | 凌云时刻（微信号：linuxpk） 朴素的思想：多个师傅教出综合徒弟 一切都从高考开始谈起。 每一个高考生在学习语文、数学、外语、物理等学科时，并非是自己抱着一摞各个学科的书籍独自啃读，而是向相应学科的老师进行学习，参加高考时，学得好的学生在高考这种多学科测试中，往往要比单个学科的老师表现出色。毕竟，学生是博采百家之长。 多领域对话系统中棘手的状态表示问题 在对话处理的过程中，对话状态表示是个至关重要的部分，对话管理模块往往需要精确记录当前对话的状态才能做出正确的对话决策。 任务型对话系统上有两种常用的对话状态表示方法： 状态追踪（State Tracking）：基于状态追踪的方式一般采用一个额外的信念状态追踪模块，从原始的用户文本中抽取出必要的对话本体信息，这些本体信息包括实体值、槽位等，用来表示一段特定的对话状态。 隐向量状态表示：隐向量的状态表示方法则直接将所有的对话历史信息进行压缩，得到用于表示状态的隐向量。 这两种方法在单个领域的对话任务，诸如机票预订的场景下，有着良好的性能表现。这是由于在单一领域下，所涉及的实体值和槽位数量往往很少。然而，当场景切换到多领域对话时，这两种方法的性能会急剧下降: 对于状态追踪方法来说，多领域下本体信息空间成倍地增加，从而导致状态追踪模块在实体值和槽位识别上的准确率大幅下降，进而影响了对话系统。 而对于隐向量的状态表示来说，由于直接使用了隐向量而非人工定义的状态，训练数据中关于实体和槽位的人工标注信息无法被充分利用。除此之外，隐向量的状态不具备可读性，是一个黑盒式的表示，难以针对特定领域的问题进行描述。 综上，多领域下低质量低精度的状态表示影响了对话系统的对话策略，进一步影响了对话系统的整体表现。 来源于生活的 MTSS（Multiple Teachers Single Student）模型 为了解决上述问题并建立一个高性能的多领域对话模型，我们提出了多教师 — 单学生对话模型 MTSS（Multiple Teachers Single Student）。 这种模型能分块式地学习多领域下复杂的对话状态表示，最终得到多领域下优异的对话策略。MTSS 模型使用了多个教师模型，每个模型学习一个领域下优秀的对话策略。多个教师模型再将知识和策略教授给一个通用的学生模型，使得学生模型成为所有领域的专家对话模型。这种多教师 - 单学生的场景能很好地解决多领域对话的问题。 在多领域对话任务下的每一个领域，使用一个独立的教师模型，来学习领域特定的知识，同时也额外学习领域下人工标注的语意信息。每个领域下的教师模型使用用户文本语句和基于人工语意标注的状态表示作为输入，从这些特定领域下的文本和状态中，领域教师模型能学到高质量的对话策略方法。 这些训练好的、领域特定的教师模型将各自学到的知识和对话策略传递到一个通用的学生模型中，通过基于文本的引导和基于策略的引导两种方式将这些知识信息传递到学生模型，传递的过程则采用知识蒸馏算法来实现。通过从领域特定的教师模型学习，通用学生模型既学到了多领域的知识，同时也学到了额外的语意标注信息，从而有着良好的多领域对话表现。 MTSS 的模型结构 Model Overview MTSS 的整体框架如图所示（图中仅画出了两个教师模型，实际教师模型的数量取决于多领域对话的领域数量）。 主要由两个部分组成：教师模型和学生模型。 教师模型通常是由多领域场景下的领域数决定，通常每个领域下建立一个教师模型。 在训练过程中，教师模型和学生模型所接收的输入是不同的： 教师模型接收当前用户语句和人工标注的状态表示作为输入。由于状态表示由人工标注，因此具有非常高的准确度，从而保证了教师模型从这些语意信息中能获得足够多的正确信息用于对话决策和回复生成。 学生模型仅使用当前用户输入和历史对话作为输入。 在经过充分训练后，教师模型再通过文本引导和策略引导两种方式将知识传递到学生模型。使用文本引导使得学生模型和教师模型生成相似的回复，使用策略引导则使得学生模型学习教师模型的对话策略，从而保证学生模型能充分接收来自教师模型的知识。下一节将详细描述教师和学生之间的交互方式。 完成上述的训练过程后，学生模型获得了充分的多领域知识以及足够正确的对话策略。在模型测试和部署时，学生模型只需要原始的文本作为输入，不需要额外的语意和状态标注即可生成高质量的回复。 Teacher Model 我们采用了 Budzianowski 提出的对话模型作为教师模型。如下图所示， 教师模型由三部分构成：编码器，解码器以及中间的策略模型。策略模型同时使用了来自编码器的句表示向量 utut 和来自人工定义的特征向量 etet 作为输入。特征向量又由两个部分组成，第一部分是置信状态向量 vbvb，置信状态向量的每一个维度均使用了独热（one-hot）编码，映射到领域下某个特定的槽位，槽位中的实体值需要由用户给出。当某个值在对话中已经被给定，则当前位置的编码设定为 1，否则设定为 0。因此，置信状态向量 vbvb 代表在当前对话状态下，系统所记录的必要信息。在对话每一轮，置信状态都会根据语意标注信息进行更新。特征的另一部分是数据库指示向量 vkbvkb。数据库指示向量表示满足用户需求的实体在数据库中的个数。使用 4 维的独热编码来记录这个数值，每一维分别表示数量为 0，1，2 或者大于等于 3 个候选实体。并将将以上 3 个向量 —— 句向量 vutvtu，置信状态向量 vbvb 和数据库指示向量 vkbvkb—— 拼接到一起，就得到了当前对话完整的状态表示 stst。 得到拼接向量后，将拼接向量输入策略层，策略层由一层的次线性全连接构成，使用 tanhtanh 作为激活函数，并由策略层得到决策向量 atat： at=tanh(w⋅[vut;vb;vkb]),at=tanh(w⋅[vtu;vb;vkb]), 其中 [;][;] 表示拼接操作。决策向量 atat 最终被输入解码器模块，解码器由输入的决策向量和注意力机制增强来产生最终的输出。由于每个教师模型各自独立，因此每个教师模型所学习的状态信息是各不相同的。在教师模型经过充分预训练后，将教师模型作为引导模型来训练学生模型。 教师模型的训练过程中直接使用真值回复数据作为训练模板。对于教师模型，给定用户输入 uu 和状态表示 ss，模型旨在优化真实回复 r={wr0,wr1,…,wrm} r={w0r,w1r,…,wmr}。和生成回复 rr 之间的负对数似然损失： JNLL(r|u,s)=−m∑i=0∑wi∈V1}logp(wi|u,s,wr0∼i−1;ϕ),JNLL(r|u,s)=−∑i=0m∑wi∈V1{w^i=wir}log⁡p(wi|u,s,w0∼i−1r;ϕ), 其中 VV 表示生成词构成的词表， ϕϕ 表示教师模型的参数，1 {⋅} 1 {⋅} 表示指示函数（当括号内表达式成立时值为 1，否则为 0）。 Student Model 学生模型，也是通用多领域的对话模型，是 MTSS 框架最终产出的模型。通用学生模型不使用额外的状态信息作为输入，它本身应当具有建模完整对话上下文的能力。基于此，我们选用 HRED 模型作为通用学生模型的基本框架。 如图，首先使用一个编码器来建模当前用户的输入语句，并将其编码为一个句向量表示。HRED 中所有对话历史的句向量又由一个上下文编码器编码得到上下文向量。在 tt 时刻，对于包含 mm 个词 (w0,w1,…,wm)(w0,w1,…,wm) 的输入语句 utut，使用 LSTM 作为编码器编码： ht=vwtm=LSTMe(h0;wt0,wt1,…wtm),ht=vtmw=LSTMe(h0;wt0,wt1,…wtm), 然后将 LSTM 的隐状态视为句向量表示 vut=htvtu=ht。而上下文编码器则同时作为策略模块，根据所有的历史语句得到决策向量 aat。同样使用 LSTM 作为上下文编码器实现： at=LSTMc(vu0,vu1,…,vut),at=LSTMc(v0u,v1u,…,vtu), 其中 atat 作为抽象的决策向量，用于表示对当前回复的决策。通过这种将上下文编码向量视为决策向量的方式以及使用教师 — 学生框架的策略层引导方法，能有效提升学生模型的表现。 在模型的最后，我们使用 NLG 模型生成自然语言回复 rtrt，上一层的决策向量作为 NLG 模块 LSTM 模型的初始状 vri=LSTMd(at,vw0∼m,vr0∼i−1),vir=LSTMd(at,v0∼mw,v0∼i−1r), 其中 vwjvjw 是编码器对第 jj 个输入词的编码向量。 与教师模型相同，学生模型同样学习真值回复数据。但在训练过程中，学生模型的输入数据有所不同，没有额外的状态表示作为输入，学生模型需要从文本中总结隐状态信息。此外，学生模型同时学习教师模型的指导信息。 MTSS 中的知识迁移方法 文本级别的知识迁移 知识迁移过程旨在让学生模型学习与教师模型相似的回复输出。在每个时间步骤学生模型的生成词概率需要分别与教师模型一致。为实现这一点，我们使用交叉熵损失来衡量学生和教师之间词概率的相似度。损失的表达式是： JKD=−m∑i=0∑wri∈Vp(wri|u,s,wr0∼i−1;ϕ)logp(wri|u,c,wr0∼i−1;θ),JKD=−∑i=0m∑wir∈Vp(wir|u,s,w0∼i−1r;ϕ)log⁡p(wir|u,c,w0∼i−1r;θ), 其中 ϕϕ 为教师模型的参数，θθ 为学生模型的参数，VV 为生成词表。基于真值回复的训练在每个生成位置仅学习了一个独热的词的概率，而对于文本级别的蒸馏方法，来自教师模型的输出提供的一个更加平滑全词概率分布，这种基于概率的蒸馏方法可使得学生模型的生成更加自然，正确率更高。 策略级别的知识引导方法 策略级别的知识引导旨在让学生模型学到教师模型的对话策略。即在相同的输入下，学生模型和教师模型能做出相同的对话决策。将教师模型的中间层输出，表示决策的向量 aTaT 同样视为目标数据，由学生模型来学习。对于同样作为隐向量的 aTaT 和 aSaS，在训练过程中使用均方误差（Mean Squared Error, MSE）来作为训练的损失从而让学生学习教师的决策： JKD−π=1kk∑i=0(aTi−aSi)2.JKD−π=1k∑i=0k(aiT−aiS)2. 我们同时使用了真值（ground truth）回复和教师模型的引导来作为训练目标。将文本蒸馏和策略蒸馏产生的两种损失加到真值训练损失当中。为了调整教师模型引导的影响力和不同损失之间的权重，使用了两个权重张量，α1α1 作为文本蒸馏损失的权值，α2α2 作为策略蒸馏的权值，最终得到用于训练学生模型的联合损失 JθJθ 可以表示为： Jθ=JNLL+α1JKD+α2JKD−π,Jθ=JNLL+α1JKD+α2JKD−π, 然后通过最小化这个损失值来训练学生模型，实现教师对学生的引导。 实验与结果分析 数据集 在实验数据上，我们选择了 MultiWOZ 多领域任务对话数据集作为对话数据集和模型效果测试数据。MultiWOZ 多领域数据集共包含了 7 个对话领域，分别是：饭店预订，酒店预订，景点查询，出租车预约，火车票预订，医院信息查询和警察局信息查询。MultiWOZ 对话数据集的目的在于获取用户意图，并根据用户意图提供满足用户需求的实体相关的信息。MultiWOZ 数据集中每段对话包含大约 14 个用户与系统之间的对话轮次。部分对话段落的话题仅仅只包含一个领域相关的信息，但更多的对话中，一段对话会包含 2 到 5 个领域的任务，用户会在不同领域之间切换。每个领域下，系统需要从用户中获取大致 4 个左右不同实体的实体值，同时需要向用户提供约 3 个相关实体信息。例如，饭店预订领域，用户需要提供想要预订饭店的地区，价格范围，菜种 3 个信息，系统则需返回用户相关饭店的地址，电话，预约号以及其他一些必要信息。 为了成功训练模型，我们先对数据进行了预处理。首先进行了去词化（Delexicalization）操作，将所有对话语句中实体的具体实体值替换为占位符，然后根据事先定义的置信状态设计，对每一轮对话根据人工标注的语意信息生成置信状态向量和数据库指示向量。并将这些向量作为教师模型的额外输入。然后对每一轮对话进行领域标注，并根据标注将对话划分到 7 个领域下。领域标注的依据是按照每轮对话中所出现的实体值所属于的领域。对于某些对话来说，无法将这些对话划分到以上 7 个领域的其中一个，例如对话开始的问候和对话结束的招呼语句。为此定义一个通用领域，所有不属于以上 7 个领域的对话轮次被划分到这个通用领域下，从而最终得到 8 个领域的训练数据集，数据集大小如表所示。 实验设定 在数据集上，我们构建了两个不同的词表，输入词表和输出词表。对于输入词表，筛选去掉了词频小于 5 的所有的词。最终剩余的输入词表大小约为 1300 左右。对于输出词表，按词频排序后直接筛选保留了词频最高的 500 个词。对于输入和输出词表采用了两套不同的词向量。两种词向量的维度均设置为 50。对于其中所有涉及的 LSTM 模型，隐层维度均设计为 150 维。对每个教师模型，首先在训练集上训练，然后选择验证集上指标最好的训练结果作为引导模型。对于学生模型，选择 Adam 优化器，学习率为 0.005，对于用于平衡不同损失的权重值 α1α1 和 α2α2，值均设为 0.005。为了得到稳定的结果。所有模型均经过 3 次或 5 次的重复实验并取了平均值。 在教师模型的训练中发现，部分领域的训练数据量有限。例如，警察局领域的数据量在整个训练数据中仅占 0.82%，最终在该领域下训练的模型效果较差。为了解决这个问题，我们使用了一种暖启动策略：首先基于所有领域下的数据集，训练一个通用预训练模型 TallTall 作为起点。然后，每个领域下特定的教师模型使用 TallTall 在领域上进行微调训练。这种暖启动的策略能保证每个领域特定的教师模型其性能不弱于或明显强于 TallTall。 我们选取了以下评价指标来评价每个模型生成的回复。 BLEU：我们首先使用了 BLEU4 得分作为评价指标，用于评价生成回复与真实回复之间的相似度。 Inform 指标和 Success 指标：这两种评价指标是由 Budzianowski 提出，作为 MultiWOZ 数据集在对话文本生成的任务上所用的一种评价指标。这两种指标均是从段级别对对话进行评测。其中，Inform 率反映在一整段对话中，系统是否推荐给用户一个符合用户需求的实体信息。Success 率衡量整段对话是否成功，包括是否 Inform 成功，以及用户需要的所有信息是否由系统提供给到了用户。 结果分析 下表表示我们的模型和其他基准模型在多领域场景下的结果表现。 从表中可以看出，与基准模型诸如 Seq2Seq 和 HRED 模型相比，本文的模型结果优于这些基准方法。通过加入教师 - 学生框架，本文所使用的 HRED 模型相比普通的 HRED 模型，在 Inform 和 Success 两项指标上相比分别提升了 6.8% 和 4.0%。与当前最好的模型，LaRL 和 HDSA 两种对话生成模型，以及当前最好的状态追踪模型 TRADE 的组合相比，模型能达到与之相近的结果表现。然而，这两种最新的对话生成模型相比于我们的模型有以下不足： ・HDSA 模型使用了 BERT 这个语言模型作为组成的一部分。然而，BERT 虽然能很好地提升模型性能的表现，但同时，也使得模型的体积十分臃肿，在部署到线上使用时会有严重的延迟。 ・LaRL 使用了强化学习的方法，用来优化段级别的最优回馈值，如 Inform 指标或 Success 指标，从而在这种被优化的指标上取得较好的表现，而在未优化的指标诸如 BLEU 上则表现很差。 此外，在与使用人工状态标记的设定场景下，我们模型与理想化的 Seq2Seq 或者 HRED 模型相比能取得相近或者更高的效果。结果同样表明，一个额外的状态追踪模块对于 Inform 指标有所帮助，而对 Success 指标毫无帮助。 总结 我们基于教师学生框架，提出了一种新颖的用于构建高质量多领域对话模型的方法 MTSS。通过构建多个特定领域的教师模型，分块学习多领域下复杂的状态知识，帮助一个通用的学生模型训练，使之成多领域对话专家。为了充分利用领域教师所学到的知识，我们创造性地使用了文本层次的引导和策略层次的引导，来将教师模型的知识传授给学生。 与此同时，我们的方法存在可以进一步完善的地方。例如，目前在 MTSS 框架中所使用的模型为最基础的 Seq2Seq 模型和 HRED 模型。作为顶层框架，MTSS 有足够的潜力应用到更加新型的对话模型上来。 后记 多领域对话系统在我们的业务场景中扮演着越来越重要的地位，举个例子，如果准备出境游：订机票、订景点、订酒店是相辅相成 (【机票到达时间】影响了【酒店位置和入住时间】的选择，同时【酒店】又会进一步影响【当地景点的选择】)。今后我们的智能服务形式更多的是这种多领域交织的形式。我们的 MTSS 模型无疑为多领域对话提供了一种解决方案：博采百家之长，服务综合领域。 ———————————————— 版权声明：本文为 CSDN 博主「凌云时刻」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/bjchenxu/article/details/109006931","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"知识蒸馏","slug":"知识蒸馏","permalink":"https://leezhao415.github.io/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"}]},{"title":"【AI数据集】手部手势、图像视频形状、对象数据库、人与人体姿势","slug":"【AI数据集】手部手势、图像视频形状、对象数据库、人与人体姿势","date":"2021-12-12T10:21:47.000Z","updated":"2021-12-12T10:23:08.461Z","comments":true,"path":"2021/12/12/【AI数据集】手部手势、图像视频形状、对象数据库、人与人体姿势/","link":"","permalink":"https://leezhao415.github.io/2021/12/12/%E3%80%90AI%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%91%E6%89%8B%E9%83%A8%E6%89%8B%E5%8A%BF%E3%80%81%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%BD%A2%E7%8A%B6%E3%80%81%E5%AF%B9%E8%B1%A1%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%81%E4%BA%BA%E4%B8%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E5%8A%BF/","excerpt":"","text":"文章目录 （1）手，手抓，手动作和手势数据库 （2）图像，视频和形状数据库检索 （3）对象数据库 （4）人（静态和动态），人体姿势 （1）手，手抓，手动作和手势数据库 11k 手 - 190 个对象的 11,076 手图像（1600 x 1200 像素），年龄在 18-75 之间，具有元数据（标识，性别，年龄，肤色，手性，手，配件等）。（Mahmoud Afifi）[19/12/28 之前] 200 亿个小丑 - 标记密集的视频片段，显示人类在笔记本电脑摄像头或网络摄像头前进行预定义手势的动作（二十亿美元的神经元有限公司）[19/12/28 之前] 具有单个深度图像的 3D 关节姿势估计（Tang，Chang，Tejani，Kim，Yu）[19/12/28 之前] A-STAR 标注的手掌深度图像数据集及其性能评估 - 深度数据和数据手套数据，30 位志愿者的 29 张图像，中文数字计数和美国手语（许和郑）[19/12/28 之前] 博斯普鲁斯海峡的手形数据库和手静脉数据库（波加奇齐大学）[19 年 12 月 28 日之前] ContactPose- 具有手 - 物体接触，手和物体姿势以及 2.9 M RGB-D 抓取图像（Brahmbhatt，Tang，Twigg，Kemp，Hays）的大规模功能抓取数据集 [30/12/2020] 人为操作动作的数据集 - 25 个对象和 6 个动作的 RGB-D（Alessandro Pieropan）[19/12/28 之前] DemCare 数据集 - DemCare 数据集由来自不同传感器的一组不同数据集组成，可用于可穿戴 / 深度和静态 IP 摄像机的人类活动识别，用于 Alzheimmer 疾病检测的语音识别以及用于步态分析和异常检测的生理数据。（K. Avgerinakis，A.Karakostas，S.Vrochidis 和 I. Kompatsiaris）[19 年 12 月 28 日之前] DVS128 手势数据集 - 基于事件的数据集，包含 11 个手势的序列，由 29 位受试者在多个光照条件下执行，并使用 DVS128 传感器捕获。每个序列都带有每个手势的开始和结束时间。（埃米尔，塔巴，伯格，梅拉诺，麦金斯特里，迪・诺弗，纳亚克，安德鲁普洛斯，加洛，门多萨，库斯尼兹，黛博勒，埃塞尔，德尔布鲁克，弗利克纳和莫达）[7/1/20] EgoDaily - 以人为中心的手部检测数据集，其人，活动和地点具有可变性，以模拟日常生活情况（克鲁兹，陈）[30/12/2020] EgoGesture 数据集 - 第一人称视角手势，具有 83 类，50 个主题，6 个场景，24161 个 RGB-D 视频样本（张，曹，成，路）[19/12/28 之前] EgoHands- 一个大型数据集，其中有超过 15,000 个像素级细分的手，这些手是从以人为中心的人与人互动的相机中记录下来的。（Sven Bambach）[19/12/28 之前] Ego3DHands- 用于 3D 全局姿势估计的 RGB-D 合成的大规模以自我为中心的双手数据集（Lin，Wilhelm）[28/12/2020] EgoYouTubeHands 数据集 - 以自我为中心的手部分割数据集由来自 YouTube 视频的 1290 个带批注的帧组成，这些帧是在不受限制的实际设置中记录的。这些视频在环境，参与者人数和动作方面各不相同。该数据集对于研究无约束条件下的手部分割问题很有用。（Aisha Urji，Aisha Urooj）[19/12/28 之前] FORTH 手跟踪库（FORTH）[19/12/28 之前] 一般 HANDS：一般的手部检测和姿势挑战 - 22 个序列，具有不同的手势，活动和视点（UC Irvine）[19/12/28 之前] GRASP MultiCam 数据集 - 将同步的立体声单色摄像机和 IMU 的视频与飞行时间深度传感器的深度图像结合在一起，从而实现精确的视觉惯性里程表（VIO）并从深度传感器点云中恢复 3D 结构（ Pfrommer，Owens，Shariati，Skandan，Taylor，Daniilidis）[2020 年 12 月 27 日] 掌握理解（GUN-71）数据集 - 使用 71 个细粒度抓点的分类法注释的 12,000 个对象操纵场景的第一人称 RGB-D 图像。（罗杰兹，苏潘西奇和拉马南）[19 年 12 月 28 日之前] 手势检测数据集（Javier Molina 等）[19/12/28 之前] 手势和海洋轮廓（Euripides GM Petrakis）[19/12/28 之前] HandNet：带关节的手的带注释的深度图像 214971 由手姿势的 RealSense RGBD 传感器捕获的带手的带注释的深度图像。注释：每个像素类，6D 指尖姿势，热图。火车：202198，测试：10000，验证：2773。在 Technion 的 GIP 实验室记录。[19/12/28 之前] HandOverFace 数据集 - 一个手部分割数据集由来自网络的 300 个带注释的帧组成，用于研究手遮挡脸问题。（Aisha Urji，Aisha Urooj）[19/12/28 之前] IDIAP 手势 / 手势数据集（Sebastien Marcel）[19/12/28 之前] Kinect 和 Leap 运动手势识别数据集 - 该数据集包含通过 Leap Motion 和 Kinect 设备（Giulio Marin，Fabio Dominio，Pietro Zanuttigh）获得的 1400 种不同手势 [19/12/28 之前] Kinect 和 Leap 运动手势识别数据集 - 该数据集包含使用 Creative Senz3D 相机获取的几种不同的静态手势。（A. Memo，L。Minto 和 P. Zanuttigh）[19 年 12 月 28 日之前] LISA CVRR-HANDS 3D- 由 8 个对象作为汽车驾驶员和乘客（Ohn-Bar 和 Trivedi）进行的 19 个手势 [19/12/28 之前] 用于评估 3D 关节手运动跟踪的 MPI Dexter 1 数据集 - Dexter 1：7 个具有挑战性，缓慢和快速的手运动序列，RGB + 深度（Sridhar，Oulasvirta，Theobalt）[19/12/28 之前] 深度 MSR 实时和鲁棒的手部跟踪 -（钱，孙，魏，唐，孙）[19/12/28 之前] 移动和网络摄像头手持图像数据库 - MOHI 和 WEHI-200 人，每人 30 张图像（Ahmad Hassanat）[19/12/28 之前] NTU-Microsoft Kinect 手势数据集 - 这是一个 RGB-D 手势数据集，包含 10 个对象 x 10 个手势 x 10 个变化形式。（周仁，袁俊松，孟晶晶和张正有）[19/12/28 之前] NUIG_Palm1- 使用消费类设备在无限制条件下获取的掌纹图像数据库，用于掌纹识别实验。（Adrian-Stefan Ungureanu）[19/12/28 之前] NYU 手姿势数据集 - 带有地面真实姿势的捕获的 RGBD 数据的 8252 测试集和 72757 训练集帧，有 3 个视图（Tompson，Stein，Lecun 和 Perlin）[19 年 12 月 28 日之前] PRAXIS 手势数据集 - 来自 29 个手势的 RGB-D 上身数据，64 位志愿者，多次重复，许多志愿者都有一些认知障碍（Farhood Negin，INRIA）[19/12/28 之前] 渲染的手部数据集 - 用于 2D / 3D 手部估计的合成数据集，每手具有 RGB，深度，分割蒙版和 21 个关键点（克里斯蒂安・齐默尔曼和托马斯・布罗克斯）[19/12/28 之前] ROSHAMBO17-RoShamBo 岩石剪刀纸游戏 DVS 数据集 -“记录了大约 20 个人的数据集，分别显示了大约 2m 的岩石，剪刀和纸符号，每个手势，距离，姿势，左 / 右手。”（Lungu，乌拉圭和苏黎世联邦理工学院神经信息研究所，德拉布鲁克，科拉迪（Corradi）[27/12/2020] RWTH-Boston-50 和 RWTH-Boston-104- 美国手语手势视频数据集，包含由 4 个摄像机（2 个黑白立体声，1 个彩色，一侧侧视图黑白）以 30 fps 和 312 捕获的 201 条带注释的句子 * 242 像素 50 个数据集具有 483 个 50 个单词的语音。（Dreuw，Keysers，Forster，Deselaers，Rybach，Zahedi，Ney）[14/3/20] Sahand LMC 手语数据库 - 该数据库由网络摄像头和 Leap Motion 控制器（LMC）收集，该模块包括 32 个类，其中包括 24 个美国字母（由于它们是动态手势，因此不包括 J 和 Z）以及 0 到 9 的数字（代表 6 和 w，9 和 F 的手势也相同）。每个数据库类包含 2000 个样本。（Ebrahimnezhad 的 Mahdikhanlou）[2020 年 12 月 27 日] Sahand 动态手势数据库 - 此数据库包含 11 个动态手势，旨在将鼠标和触摸屏的功能传达给计算机。（Behnam Maleki，Hossein Ebrahimnezhad）[19/12/28 之前] 谢菲尔德手势数据库 - 2160 个 RGBD 手势序列，6 个对象，10 个手势，3 个姿势，3 个背景，2 个照明（凌绍）[19/12/28 之前] SL-ANIMALS-DVS 数据库 - SL-ANIMALS-DVS 数据库由 DVS 记录组成，这些记录是人类以极低的延迟连续不断的尖峰流动来执行各种动物的手语手势（Serrano-Gotarredona，Linares-Barranco）[27/12 / 2020] UT 抓取数据集 - 4 个主题以各种方式抓住各种对象（蔡，北谷，佐藤）[19/12/28 之前] WLASL- 单词级美国手语语言数据集，包含 2,000 个常用词和 21k RGB 视频，由一百多个本地签名者（李，罗德里格斯，于，李）进行表演 [27/12/2020] 耶鲁人的掌握数据集 - 27 小时的视频，带有来自两名管家和两名机械师（伯洛克，费克斯，美元）的带标签的掌握，对象和任务数据 [19 年 12 月 28 日之前] （2）图像，视频和形状数据库检索 2D 到 3D 变形草图 - 与相同类别的可变形 3D 网格点向对应的可变形 2D 轮廓的集合；提供了大约 10 种对象类别，包括人类和动物。（罗纳，罗多拉）[19/12/28 之前] 杂波中的 3D 变形对象 - 3D 杂波中的可变形对象的数据集，在数百个场景中跨越多个类别（人类，动物）具有逐点地面真实性对应。（Cosmo，Rodola，Masci，Torsello 和 Bronstein）[19 年 12 月 28 日之前] ANN_SIFT1M- 由 128D SIFT 描述符（Jegou 等人）编码的 1M Flickr 图像 [19/12/28 之前] Brown Univ 25/99/216 形状数据库（Ben Kimia）[19/12/28 之前] CIFAR-10-10 类 60K 32x32 图像，带有 512D GIST 描述符（Alex Krizhevsky）[19/12/28 之前] CLEF-IP 2011 对专利图片的评估 [19/12/28 之前] 等高线图数据集 - 5,000 个配对图像和等高线图的数据集，用于研究视觉理解和草图生成（Li，Lin，Měch，Yumer 和 Ramanan）[9/1/20] DeepFashion- 大型时装数据库（刘子玮，罗平，邱秋，王小刚，唐小鸥）[19/12/28 之前] EMODB -picsearch 图像搜索引擎中图像的缩略图以及 picsearch 情感关键字（Reiner Lenz 等）[19 年 12 月 28 日之前] ETU10 轮廓数据集 - 该数据集由 720 个轮廓（包含 10 个对象）组成，每个对象有 72 个视图。（M. Akimaliev 和 MF Demirci）[19 年 12 月 28 日之前] 2013 年欧洲洪水 - 中欧发生洪水事件的 3,710 张图像，并带有与 3 个图像检索任务（多标签）和重要图像区域有关的注释。（耶拿，弗里德里希・席勒大学，德意志联邦理工大学波茨坦分校）[19 年 12 月 28 日之前] Fashion-MNIST - 类似于 MNIST 的时装产品数据库。（Han Xiao，Zalando Research）[19 年 12 月 28 日之前] 鱼形数据库 - 它是具有 100 个 2D 点设置形状的鱼形数据库。（Adrian M. Peter）[19/12/28 之前] Flickr 30K - 图像，动作和标题（Peter Young 等）[19 年 12 月 28 日之前] Flickr15k - 基于草图的图像检索（SBIR）基准 - 330 个草图和 15,024 张照片的数据集，包括 33 个对象类别，通常用于评估基于草图的图像检索（SBIR）算法的基准数据集。（Hu 和 Collomosse，CVIU 2013）[19/12/28 之前] 动手（HIC）IJCV 数据集 - 用于跟踪 1 手或 2 手有 / 没有 1 个对象的数据（图像，模型，运动）。包括单视图 RGB-D 序列（1 个主题，带 18 个注释序列，4 个对象，完整 RGB 图像）和多视图 RGB 序列（1 个主题，HD，8 个视图，8 个序列 - 1 个带注释，2 个对象）。（Dimitrios Tzionas，Luca Ballan，Abhilash Srikantha，Pablo Aponte，Marc Pollefeys 和 Juergen Gall）[19 年 12 月 28 日之前] IAPR TC-12 图片基准测试（Michael Grubinger）[19/12/28 之前] IAPR-TC12 分段和带注释的图像基准（SAIAPR TC-12）：（ Hugo Jair Escalante）[19 年 12 月 28 日之前] ImageCLEF 2010 概念检测和注释任务（Stefanie Nowak）[19/12/28 之前] ImageCLEF 2011 概念检测和注释任务 - Flickr 照片中的多标签分类挑战 [19/12/28 之前] INRIA Copydays 数据集 - 用于评估副本检测：JPEG，裁剪和 “强” 副本攻击。（INRIA）[19/12/28 之前] INRIA Holidays 数据集 - 用于图像搜索评估：500 个查询和 991 个相应的相关图像（Jegou，Douze 和 Schmid）[19/12/28 之前] MA14KD（电影吸引力 14K 数据集）数据集 - 14K 电影 / 电视预告片，每个具有 10 个功能，链接到评级数据集（Elahi，Moghaddam，Hosseini，Trattner，Tkalčič）[19/12/28 之前] METU 商标数据集 METU 数据集由超过 90 万个真实徽标组成，这些徽标属于全球公司。（Usta Bilgi Sistemleri AS 和 Grup Ofis Marka Patent AS）[19/12/28 之前] 麦吉尔 3D 形状基准测试（Siddiqi，Zhang，Macrini，Shokoufandeh，Bouix，Dickinson）[19/12/28 之前] MPEG-7 核心实验 CE-Shape-1 - 将 1400 个 2D 二维形状分为 70 个类别，每个类别中有 20 个形状（Latecki，Lakamper，Eckhardt）[29/12/2020] MPI MANO 和 SMPL + H 数据集 - 统计模型 MANO（手动）和 SMPL + H（身体 + 手）的模型，4D 扫描和配准。对于 MANO，对 31 个对象进行了约 2k 静态 3D 扫描，最多执行 51 个姿势。对于 SMPL + H，我们包括 11 个对象的 39 个 4D 序列。（Javier Romero，Dimitrios Tzionas 和 Michael J Black）[19 年 12 月 28 日之前] 多视图立体评估 - 每个数据集都注册有通过激光扫描过程获得的 “真实” 3D 模型（Steve Seitz 等）[19/12/28 之前] NIST SHREC-2014 NIST 检索竞赛数据库和链接（美国国家标准技术研究院）[19/12/28 之前] NIST SHREC-2013 NIST 检索竞赛数据库和链接（美国国家标准技术研究院）[ 19 年 12 月 28 日之前] NIST SHREC 2010 - 非刚性 3D 模型的形状检索竞赛（美国国家标准技术研究院）[ 19 年 12 月 28 日之前] NIST TREC 视频检索评估数据库（美国国家标准技术研究院）[19/12/28 之前] NUS- WIDE - 带有 81 个概念标签的 269K Flickr 图像标注为 500D BoVW 描述符（Chau 等人）[19/12/28 之前] 普林斯顿形状基准测试（普林斯顿形状检索和分析组）[19/12/28 之前] PairedFrames-3D 姿态跟踪错误的评估 - 合成数据和实数数据集，用于以接近 / 远离极小值的姿态初始化测试 3D 姿态跟踪 / 优化。建立难度增加的测试帧对，以单独测量姿态估计误差，而无需使用完整的跟踪管线。（Dimitrios Tzionas，Juergen Gall）[19 年 12 月 28 日之前] 昆士兰跨媒体数据集 - 数百万个用于 “跨媒体” 检索的图像和文本文档（易阳）[19/12/28 之前] 从 RGB-D 视频重建铰接的索具模型（RecArt-D） - 操作过程中对象变形的数据集。包括 4 个 RGB-D 序列（完整的 RGB 图像），每个对象的可变形跟踪结果，以及每个对象的 3D 网格和 Ground-Truth 3D 骨架。（Dimitrios Tzionas，Juergen Gall）[19 年 12 月 28 日之前] 通过手与对象的交互（R-HOI）进行重构 - 一只手与未知对象进行交互的数据集。包括 4 个 RGB-D 序列，总共 4 个对象，RGB 图像完成。包括跟踪的 3D 运动和对象的地面真实网格。（Dimitrios Tzionas，Juergen Gall）[19 年 12 月 28 日之前] 重访牛津和巴黎（RevisitOP） - 带有 1M 干扰图像的著名地标 / 建筑物检索数据集的改进版本和更具挑战性的版本（已修复的错误，新的注释和评估协议，新的查询图像）。（F. Radenovic，A。Iscen，G。Tolias，Y。Avrithis，O。Chum）[19 年 12 月 28 日之前] SBU 字幕数据集 - 从 Flickr（Ordonez，Kulkarni 和 Berg）收集的 100 万张图像的图像字幕 [19/12/28 之前] SHREC’16 可变形部分形状匹配 - 大约 400 种 3D 可变形形状的集合，这些形状正在经历强烈的局部性转换，其中包括点对点地面真相对应。（Cosmo，Rodola，Bronstein 和 Torsello）[19 年 12 月 28 日之前] SHREC 2016 - 基于 3D 草图的 3D 形状检索 - 使用通用 3D 模型数据集上的手绘 3D 草图查询数据集来评估基于 3D 草图的 3D 模型检索算法的性能的数据（李波）[28/12 之前 / 19] SHREC’17 可变形部分形状检索 - 大约 4000 种可变形 3D 形状的集合，这些形状正在经历严重的部分转换，形式为不规则的缺失零件和范围数据；提供了地面实况课程信息。（罗纳，罗多拉）[19/12/28 之前] SHREC Watertight Models Track（SHREC 2007 年） -400 个水密 3D 模型（Daniela Giorgi）[19/12/28 之前] SHREC 部分模型跟踪（SHREC 2007 年发布） -400 个水密 3D DB 模型和 30 个简化的水密查询模型（Daniela Giorgi）[19/12/28 之前] Sketch Me That Shoe- 在细粒度设置中基于草图的对象检索。将草图与特定的鞋子和椅子相匹配。（钱宇，QMUL，爱丁堡大学医学院，T。Hospedales Edinburgh / QMUL）。[19/12/28 之前] SPARE3D- 包含各种针对深度网络而设计的基于线描的空间 IQ 测试（形状一致性，相机姿态和形状生成），其中，最先进的网络几乎像随机猜测一样运行（NYU AI4CE 实验室）[27 / 12/2020] TOSCA 3D 形状数据库（Bronstein，Bronstein，Kimmel）[19 年 12 月 28 日之前] 完全看起来像 - 用于评估预测基于人的图像相似性的基准（Amir Rosenfeld，Markus D. Solbach，John Tsotsos）[19/12/28 之前] UCF-CrossView 数据集：用于城市环境中地理定位的跨视图图像匹配 - 用于跨视图图像地理定位的街景和鸟瞰图像的新数据集。（中央佛罗里达大学计算机视觉研究中心）[19/12/28 之前] YouTube-8M 数据集 - 用于视频理解研究的大而多样化的视频数据集。（Google Inc。）[19/12/28 之前] （3）对象数据库 各种对象和场景的 2.5D / 3D 数据集（Ajmal Mian）[19/12/28 之前] 3D 对象识别立体声数据集该数据集包含 9 个对象和 80 个测试图像。（Akash Kushal 和 Jean Ponce）[19/12/28 之前] 3D 摄影数据集是在我们的实验室中捕获的十个多视图数据集的集合（古隆安孝和让・庞塞）[19/12/28 之前] 3D 打印的 RGB-D 对象数据集 - 5 个具有地面真实 CAD 模型和相机轨迹的对象，并使用各种质量的 RGB-D 传感器（西门子和 TUM）记录下来 [19/12/28 之前] 3DNet 数据集 - 3DNet 数据集是一个免费资源，可用于从点云数据进行对象类识别和 6DOF 姿态估计。（John Folkesson 等人）[19 年 12 月 28 日之前] ABC 数据集 - 一百万个 CAD 模型，包括地面分析描述（样条曲线），密集网格，点云，法线。（科赫，马特维耶夫，姜，威廉姆斯，阿特莫夫，本那夫，亚历山德拉，佐林，帕诺佐）[2/1/20] 对齐后的各种对象的 2.5D / 3D 数据集 - 用于从单个深度视图重建对象的合成数据集和真实世界数据集。（杨波，史蒂芬诺・罗莎，安德鲁・马克汉姆，妮基・特里戈尼，洪凯文）[19/12/28 之前] 阿姆斯特丹对象图像图书馆（ALOI）：1 万个对象的 100K 视图（阿姆斯特丹大学 / 智能感官信息系统）[19/12/28 之前] ATRW - 在野外重新识别东北虎 - 92 个人（MakerCollider 和 WWF）的 8,000 东北虎视频剪辑 [26/1/20] 具有属性的动物 2-37322（免费许可）包含 50 种动物类的图像，每类具有 85 个二元属性。（Christoph H. Lampert，IST 奥地利）[19/12/28 之前] ASU Office-Home 数据集 - 用于领域适应（Venkateswara，Eusebio，Chakraborty，Panchanathan）的日常对象的对象识别数据集 [19/12/28 之前] ATIS Planes 数据集 - ATIS Planes 数据集是基于事件的自由落体飞机模型。（Afshar，Tapson，van Schaik，Cohen）[2020 年 12 月 27 日] B3DO：伯克利 3-D 对象数据集 - 家庭对象检测（Janoch 等）[19/12/28 之前] Bristol 以自我为中心的对象交互数据集 - 具有同步注视的以自我为中心的对象交互（Dima Damen）[19/12/28 之前] CIFAR-10H- 新的软标签数据集，反映了 10,000 张图像的 CIFAR-10 测试集（Peterson，Battleday，Griffiths，Russakovsky）的人的感知不确定性 [14/1/20] CORE 图像数据集 - 帮助学习更详细的模型并探索对象识别中的跨类别归纳。（Ali Farhadi，Ian Endres，Derek Hoiem 和 David A. Forsyth）[19 年 12 月 28 日之前] CTU 色差服装的颜色和深度图像数据集 - 带有带批注角的色差服装的图像。（Wagner，L.，Krejov D. 和 Smutn V.（布拉格的捷克技术大学））[19/12/28 之前] 加州理工学院 101（现在为 256）类别对象识别数据库（李飞飞，马可・安德雷托，马克・奥雷利奥・兰佐托）[19/12/28 之前] 卡塔尼亚鱼类物种识别 - 15 种鱼类，带有大约 20,000 个样本训练图像和其他测试图像（Concetto Spampinato）[19/12/28 之前] COCO-COntext 中的常见对象 - 大型对象检测，分割和字幕数据集：330K 图像，200K 标记，1.5m 对象实例，80 个对象类别，91 个类别，250K 人（Lin，Patterson，Ronchi，Cui， Maire，Belongie，Bourdev，Girshick，Hays，Perona，Ramanan，Zitnick，美元）[12/08/20] COCO-Stuff 数据集 - 带有 “事物” 和 “事物” 标记的 164K 图像（凯撒，乌伊林斯，法拉利）[19 年 12 月 28 日之前] COCO - 任务 - 来自可可数据集的 40k 图像带有最适合解决 14 个任务的对象的注释（波恩大学）[27/12/2020] Columbia COIL-100 3D 对象多重视图（哥伦比亚大学）[19/12/28 之前] CompCars- 汽车和零件的图像。163 辆汽车的网络上有 136,726 幅来自 1,716 辆汽车的图像。50,000 张正视图监控图像。（杨，罗，来，唐）[1/6/20] 野生国家 / 地区的国旗 - 手动裁剪了 224 个不同国家 / 地区的国旗的 12854 张火车图像和 6,110 张测试图像，以轻松地适应上面的旗帜。（Jetley）[19/12/28 之前] COWC- 带有上下文的汽车架空。32,716 辆独特的带注释的汽车。58,247 个独特的负面例子。在六个不同的位置，每像素分辨率 15 厘米。（劳伦斯・利弗莫尔国家实验室）[19 年 12 月 28 日之前] DAWN：恶劣天气下的车辆检测 - 真实交通环境中的 1000 张图像集合，分为四组天气条件：雾，雪，雨和沙尘暴（肯克，哈萨巴拉）[28/12/2020] 更深入，更广泛和更专业的领域概括 - 领域概括任务数据集。（QMUL 达里）[19/12/28 之前] 密集采样的对象视图：2 个对象的 2500 个视图，例如用于基于视图的识别和建模（Gabriele Peters，多特蒙德大学）[19/12/28 之前] 爱丁堡厨房用具数据库 - 20 种厨房用具的 897 张原始图像和二进制图像，这是一种用于培训未来家庭助理机器人的资源（D. Fullerton，A。Goel 和 RB Fisher）[19 年 12 月 28 日之前] EDUB- Obj - 用于对象定位和分割的以自我为中心的数据集。（MarcBolaños 和 Petia Radeva。）[19/12/28 之前] 椭圆查找数据集（Dilip K. Prasad 等）[19/12/28 之前] FGVC 飞机基准测试 - 10,200 架飞机的图像，其中 102 种不同飞机模型变体（Maji，Kannala，Rahtu，Blaschko，Vedaldi）每幅图像 100 张 [19 年 12 月 28 日之前] FIN-Benthic- 这是用于对底栖大型无脊椎动物进行自动细粒度分类的数据集。有来自 64 个类别的 15074 张图像。每个类别的图像数量从 577 到 7。（Jenni Raitoharju，Ekaterina Riabchenko，Iftikhar Ahmad，Alexandros Iosifidis，Moncef Gabbouj，Serkan Kiranyaz，Ville Tirronen 和 Johanna Arje）[19/12/28 之前] GERMS- 我们用于 GERMS 数据收集的对象集由 136 种不同微生物的填充玩具组成。玩具分为 7 个较小的类别，由玩具微生物的语义划分形成。将对象划分为较小类别的动机是提供具有不同难度的基准。（Malmir M，Sikka K，Forster D，Movellan JR，Cottrell G。）[19 年 12 月 28 日之前] GDXray：用于 X 射线测试和计算机视觉的 X 射线图像 - GDXray 包括五组图像：铸件，焊缝 *，行李，自然和设置。（智利天主教大学 Domingo Mery）[19 年 12 月 28 日之前] GMU 厨房数据集 - 来自 9 个不同厨房（乔治・梅森大学）的 BigBird 数据集中 11 种常见家用产品的实例级别注释 [19/12/28 之前] 抓狂 - 自然生活对象的以自我为中心的视频数据集。7 个厨房中有 16 个对象。（贝努瓦 - 皮诺，拉鲁斯，德・鲁吉）[19 年 12 月 28 日之前] GRAZ-02 数据库（自行车，汽车，人）（A。平茨）[19/12/28 之前] GREYC 3D -GREYC 3D 彩色网格数据库是一组 15 个真实对象的集合，这些对象具有不同的颜色，几何形状和纹理，这些对象是使用 3D 彩色激光扫描仪获取的。（Anass Nouri，Christophe Charrier，Olivier Lezoray）[19 年 12 月 28 日之前] GTSDB：德国交通标志检测基准和 GTSRB：德国交通标志识别基准（波鸿鲁尔大学）[19/12/28 之前] ICubWorld -iCubWorld 数据集是通过在观察日常物体的同时从 iCub 人形机器人的摄像机记录下来而获得的图像集合。（Giulia Pasquale，Carlo Ciliberto，Giorgio Metta，Lorenzo Natale，Francesca Odone 和 Lorenzo Rosasco。）[19/12/28 之前] 工业 3D 对象检测数据集（MVTec ITODD）-3500 个标记场景中 28 个对象的深度和灰度值数据，用于 3D 对象检测和姿势估计，重点关注工业设置和应用（MVTec Software GmbH，慕尼黑）[28/12 之前 / 19] Instagram Food 数据集 - 在 6 周内发布到 Instagram 的 800,000 张食物图像和相关元数据的数据库。支持食物类型识别和社交网络分析。（T. Hospedales。爱丁堡 / QMUL）[19/12/28 之前] Keypoint-5 数据集 - 带有 2D 关键点标签的五种家具的数据集（Jiajun Wu，Xianfan Xue，Joseph Lim，Tiandong Tian，Josh Tenenbaum，Antonio Torralba，Bill Freeman）[19/12/28 之前] KTH-3D-TOTAL- 带有注释的桌面上对象的 RGB-D 数据。20 张桌子，每天 19 次，每天 3 次。（John Folkesson 等人）[19 年 12 月 28 日之前] Laval 6 DOF 对象跟踪数据集 - 297 个 RGB-D 序列的数据集，其中有 11 个对象用于 6 DOF 对象跟踪。（Mathieu Garon，Denis Laurendeau，Jean-Francois Lalonde）[19 年 12 月 28 日之前] LISA 交通灯数据集 - 在各种照明条件下（詹森，菲利普森，莫吉莫斯，莫斯隆德和特里维第）的 6 种照明等级 [19/12/28 之前] LISA 交通标志数据集 - 在 6610 帧（Mogelmose，Trivedi 和 Moeslund）上的 47 种美国标志类型的视频，带有 7855 个注释（19 年 12 月 28 日之前） Linkoping 3D 对象姿势估计数据库（Fredrik Viksten 和 Per-Erik Forssen）[19/12/28 之前] Linkoping 交通标志数据集 - 以 20K 图像显示 3488 个交通标志（Larsson 和 Felsberg）[19/12/28 之前] 长期标记 - 此数据集包含来自长期数据集（上面的长期数据集）的观测值的子集。（John Folkesson 等人）[19 年 12 月 28 日之前] 主要产品检测数据集 - 包含时尚产品及其图像的文本元数据，以及主要产品的边框（由文本指代的框）。（A. Rubio，L。Yu，E。Simo-Serra 和 F. Moreno-Noguer）[19/12/28 之前] MCIndoor20000- 来自三种不同室内对象类别的 20,000 张数字图像：门，楼梯和医院标志。（Bashiri，LaRose，Peissig 和 Tafti）[19 年 12 月 28 日之前] Mexculture142- 墨西哥文化遗产物体和眼动仪注视装置（Montoya Obeso，Benois-Pineau，Garcia-Vazquez，Ramirez Acosta）[19/12/28 之前] MinneApple：Apple 检测和分割的基准数据集 - 在果园中获取的高分辨率图像，其中 1000 个图像中有超过 40000 个带注释的对象实例。可用于检测，簇蛋白，产量估算（Haeni，Roy，Isler）[30/12/2020] MIO-TCD -786,702 张车辆图像以及 648,959 个分类图像和 137,743 个本地化图像。在一天中的不同时间和一年中的不同时期被成千上万的交通摄像机所采集。（罗，查伦，勒梅尔，康拉德，李，米斯拉，阿赫卡尔，埃歇尔，乔杜因）[1/6/20] MIT CBCL 汽车数据（生物和计算学习中心）[19/12/28 之前] MIT CBCL StreetScenes 挑战框架：（Stan Bileschi）[19 年 12 月 28 日之前] Microsoft COCO - 上下文中的公共对象（林宗怡等）[19/12/28 之前] Microsoft 对象类识别图像数据库（Antonio Criminisi，Pushmeet Kohli，Tom Minka，Carsten Rother，Toby Sharp，Jamie Shotton，John Winn）[19/12/28 之前] Microsoft 显着对象数据库（由边界框标记）（刘，孙政，唐，沉）[19/12/28 之前] MNIST-DVS 和 FLASH-MNIST-DVS 数据库 - 该数据集基于原始的基于帧的 MNIST 数据集，并包含 DVS（动态视觉传感器）记录。（Yousefzadeh，Serrano-Gotarredona，Linares-Barranco）[27/12 / 2020] 移动标签 - 此数据集将长期数据集扩展到 KTH 在同一办公室环境中的更多位置。（John Folkesson 等人）[19 年 12 月 28 日之前] N-Caltech101（Neuromorphic-Caltech101） - 该数据集是原始基于帧的 Caltech101 数据集的尖峰版本。（Orchard，Cohen，Jayawant，Thakor）[27/12/2020] N- Cars-“数据集由 12336 个汽车样本和 11,693 个非汽车样本（背景）组成，用于通过 ATIS 摄像机记录。”（Sironi，Brambilla，Bourdis，Lagorce，Benosman）[27/12/2020] N-MNIST（Neuromorphic-MNIST） - 数据集是原始帧的手写数字 MNIST 数据集的尖峰版本。（Orchard，Cohen，Jayawant，Thakor）[27/12/2020] N-SOD 数据集 -“神经形态单对象数据集（N-SOD），包含三个对象，这些对象具有基于事件的传感器记录的时间长度不同的样本。”（Ramesh，Ussa，Vedovs，Yang，Orchard）[27/12 / 2020] NABirds 数据集 - 北美常见的 400 种鸟类的 70,000 张带批注的照片（格兰特・范・霍恩）[19/12/28 之前] NEC 玩具动物物体识别或分类数据库（Hossein Mobahi）[19/12/28 之前] NORB 50 玩具图片数据库（NYU）[19/12/28 之前] NTU-VOI：NTU 视频对象实例数据集 - 具有对象实例的帧级边界框注释的视频剪辑，用于评估大规模视频中的对象实例搜索和本地化。（孟晶晶等）[19/12/28 之前] 对象姿态估计数据库 - 此数据库包含 16 个对象，每个对象沿两个旋转轴（F. Viksten 等）以 5 度角增量采样 [19/12/28 之前] 物体识别数据库该数据库具有八个物体的建模镜头和包含多个物体的 51 个混乱的测试镜头。（Fred Rothganger，Svetlana Lazebnik，Cordelia Schmid 和 Jean Ponce。）[19/12/28 之前] Omniglot -16 个来自 50 个不同字母（Lake，Salakhutdinov，Tenenbaum）的手写字符 [19/12/28 之前] 打开 600 个类别中的打开图像数据集 V6 15,851,536 个框，在 19,794 个类别中打开 59,919,574 个图像级标签。在 350 个类别上进行了 2,785,498 个实例细分。在 1,466 个关系上的 3,284,282 个关系注释。507,444 个本地化叙述。478,000 个众包图像，包含 6,000 多个类别。（法拉利，Duerig，Gomes）[19 年 12 月 28 日之前] 开放式博物馆识别挑战赛（开放式 MIC）开放式 MIC 包含在多个博物馆的 10 个不同展览空间（绘画，雕塑，珠宝等）中捕获的展品照片，以及适用于领域调整和少量学习问题的协议。（P. Koniusz，Y。Tas，H。Zhang，M。Harandi，F。Porikli 和 R. Zhang）[19 年 12 月 28 日之前] 奥斯纳布吕克综合可扩展多维数据集数据集 - 从 12 种不同的视角捕获了 830000 个多维数据集，用于 ANN 训练（Schöning，Behrens，Faion，Kheiri，Heidemann 和 Krumnack）[19/12/28 之前] Princeton ModelNet -127,915 个 CAD 模型，662 个对象类别，带有标注方向的 10 个类别（吴，宋，科斯拉，于，张，唐，肖）[19/12/28 之前] PacMan 数据集 - 用于可抓握的炊具和陶器的 RGB 和 3D 合成和真实数据（Jeremy Wyatt）[19/12/28 之前] PACS（摄影艺术卡通素描） - 用于测试领域概括的对象类别识别数据集数据集：在一个领域中的对象图像上训练的分类器如何很好地识别另一个领域中的对象？（大理 QMUL，T。Hospedales。爱丁堡 / QMUL）[19 年 12 月 28 日之前] PASCAL 2007 Challange 图像数据库（摩托车，汽车，奶牛）（PASCAL 联盟）[19 年 12 月 28 日之前] PASCAL 2008 Challange 图像数据库（PASCAL 联盟）[19/12/28 之前] PASCAL 2009 Challange 图像数据库（PASCAL 联盟）[19/12/28 之前] PASCAL 2010 Challange 图像数据库（PASCAL 联盟）[19/12/28 之前] PASCAL 2011 Challange 图像数据库（PASCAL 联盟）[19/12/28 之前] PASCAL 2012 Challange 图像数据库类别分类，检测和分割以及静止图像操作分类（PASCAL 联合会）[19/12/28 之前] PASCAL 图像数据库（摩托车，汽车，奶牛）（PASCAL 联合会）[19 年 12 月 28 日之前] PASCAL 零件数据集 - 具有对象语义部分的分段注释的 PASCAL VOC（Alan Yuille）[19/12/28 之前] PASCAL-Context 数据集 - 400 多个附加类别的注释（Alan Yuille）[19/12/28 之前] PASCAL 3D / 超越 PASCAL：野外 3D 对象检测的基准 - 12 类，每张带有 3000 个图像的 3D 注释（于翔，Roozbeh Mottaghi，Silvio Savarese）[19 年 12 月 28 日之前] POKER-DVS 数据库 -“POKER-DVS 数据库包含从三个独立的 DVS 记录中跟踪和提取的 131 个扑克点符号集，同时可以非常快速地浏览扑克牌。”（Serrano-Gotarredona，Linares-Barranco）[27/12 / 2020] 物理 101 数据集 - 在五个不同场景中的 101 个对象的视频数据集（Jiajun Wu，Joseph Lim，张宏毅，Josh Tenenbaum，Bill Freeman）[19 年 12 月 28 日之前] 植物幼苗数据集 - 12 种杂草种类的高分辨率图像。（奥胡斯大学）[19 年 12 月 28 日之前] 雨滴检测 - 结合使用形状和显着度描述符以及场景上下文隔离来改进雨滴检测 - 评估数据集（Breckon，Toby P.，Webster 和 Dereck D.）[19/12/28 之前] ReferIt 数据集（IAPRTC-12 和 MS-COCO） - 引用 IAPRTC-12 和 MS-COCO 数据集（Kazemzadeh，Maten，Ordonez 和 Berg）中图像中对象的表达式 [19/12/28 之前] roboflow 国际象棋棋盘对象检测数据集 - 国际象棋棋盘照片和各种棋子的数据集。所有照片都是从固定角度（板子左侧的三脚架）拍摄的。所有片段的边界框都用边界框注释。292 个图像中有 2894 个标签（）[29/12/2020] SAIL- VOS - 语义非模态实例级视频对象分割（SAIL-VOS）数据集提供了准确的地面真相注释，以开发用于推理对象被遮挡部分的方法，同时能够考虑时间信息（Hu，Chen，Hui，Huang， Schwing）[29/12/19] SeaShips- 从监视视频（邵，吴，王，杜，李）中提取的 7455 类陆地船只的 31455 侧面影像 [19/12/28 之前] ShapeNet -55 个常见对象类别的 3D 模型，具有约 51K 独特的 3D 模型。还有 270 个类别的 12K 型号。（Princeton，Stanford 和 TTIC）[19 年 12 月 28 日之前] SHORT-100 数据集 - 在典型的购物清单中找到 100 种产品类别。它旨在对用于从使用手持式或可穿戴式摄像机获取的快照或视频中识别手持对象的算法的性能进行基准测试。（Jose Rivera-Rubio，Saad Idrees，Anil A. Bharath）[19 年 12 月 28 日之前] SkelNetOn -SkelNetOn 挑战赛围绕四个领域的形状理解而构建：形状轮廓，RGB 图像，点云和参数表示。我们提供形状数据集，一些补充资源（例如，前 / 后处理，采样和数据增强脚本），以及用于四类骨骼提取的测试平台。（学分）[29/12/2020] SLOW-POKER-DVS 数据库 -“SLOW-POKER-DVS 数据库由 4 个独立的 DVS 记录组成，同时在摄像机前缓慢移动一个扑克符号约 3 分钟。”（Serrano-Gotarredona，Linares-Barranco）[27 / 12/2020] SOR3D -SOR3D 数据集由超过 2 万个人对对象交互实例，14 种对象类型和 13 种对象提供能力组成。（吡啶瓶保温瓶）[19/12/28 之前] 空间物体姿态估计挑战数据集 - 用于训练的 12000 个合成图像，2998 个相似的合成测试图像和 305 个真实图像（太空交会实验室（SLAB））[26/1/20] Stanford Dogs 数据集 - Stanford Dogs 数据集包含来自世界各地的 120 个犬种的图像。此数据集是使用 ImageNet 的图像和注释构建的，用于完成细粒度的图像分类任务。（Aditya Khosla，Nityananda Jayadevaprakash，Bangpeng 姚，李飞飞，斯坦福大学）[19 年 12 月 28 日之前] Stream-51- 用于流媒体持续学习（分类）的数据集，包括来自 51 种不同对象类别的时间相关图像和训练分布之外的其他评估类，以测试新颖性（开放集）识别（Roady，Hayes，Vaidya，Kanan） [26/12/2020] SVHN：街景门牌号码数据集 - 类似于 MNIST，但标注的数据量大了一个数量级（超过 60 万个数字图像），并且来自一个更加困难，尚未解决的现实世界问题（识别自然场景图像中的数字和数字）。（Netzer，Wang，Coates，Bissacco，Wu，Ng）[19 年 12 月 28 日之前] 瑞典树叶数据集 - 这些图像包含 15 种树类的树叶（Oskar JO S？derkvist）[19/12/28 之前] T-LESS- 用于无纹理物体的 6D 姿态估计的 RGB-D 数据集。（Tomas Hodan，Pavel Haluza，Stepan Obdrzalek，Jiri Matas，Manolis Lourakis，Xenophon Zabulis）[19 年 12 月 28 日之前] 淘宝商品数据集 - TCD 包含 800 个商品图像（衣服，牛仔裤，T 恤，鞋子和帽子），用于在淘宝网站上的商店中检测图像显着物体。（王克则，施克扬，林良，李成龙）[19/12/28 之前] TenCent 开源多标签图像数据库 - 17,609,752 培训和 88,739 验证图像 URL，这些图像 URL 最多带有 11,166 个类别（Wu，Chen，Fan，Zhang，Hou，Liu，Zhang）[16/4/20] tieredImageNet 数据集 - ILSVRC-12 的较大子集，具有 608 个类别（779,165 张图像），分为 ImageNet 人类管理层次结构中的 34 个更高级别的节点。（Ren，Triantafillou，Ravi，Snell，Swersky，Tenenbaum，Larochelle 和 Zemel）[17/1/20] ToolArtec 点云 - 从 Artec EVA 扫描仪进行 50 次厨房工具 3D 扫描（层）。另请参见 ToolKinect- 使用 Kinect 2 和 ToolWeb 进行的 13 次扫描 - 116 个合成家用工具的点云，具有质量和负担能力，可完成 5 个任务。（Paulo Abelha）[19/12/28 之前] TUW 对象实例识别数据集 - 从各种角度观察到的室内场景混乱的对象实例及其 6DoF 姿态的注释，并表示为 Kinect RGB-D 点云（Thomas，A。Aldoma，M。Zillich，M。Vincze）[在 28 / 12/19] TUW 数据集 - TUW 的一些 RGB-D 地面真相和带注释的数据集。（John Folkesson 等人）[19 年 12 月 28 日之前] UAH 交通标志数据集（Arroyo 等）[19 年 12 月 28 日之前] UIUC 汽车图像数据库（UIUC）[19/12/28 之前] 3D 对象类别（S. Savarese 和 L. Fei-Fei）的 UIUC 数据集 [19/12/28 之前] USPS 手写数字数据集 - 7291 火车和 2007 测试图像。图像为 16 * 16 灰度像素（外壳）[19/12/28 之前] VAIS -VAIS 包含从码头获取的船舶的同时获取的未注册热图像和可见图像，并且创建该图像是为了促进自主船舶的开发。（张伯宝，蔡恩，迈克尔・沃尔夫，科斯塔斯・达尼利迪斯，克里斯托弗・卡南）[19 年 12 月 28 日之前] 威尼斯 3D 杂波物体识别和分割（Emanuele Rodola）[19/12/28 之前] 视觉属性超过 500 个对象类别（无生命和无生命）的数据集视觉属性批注均在 ImageNet 中表示。每个对象类别均基于 636 个属性的分类法（例如，具有皮毛，金属制成，是圆形的）以视觉属性进行注释。[19/12/28 之前] 视觉船体数据集视觉船体数据集的集合（Svetlana Lazebnik，Yasutaka Furukawa 和 Jean Ponce）[19/12/28 之前] VOC-360- 鱼眼图像（Fu，Bajic 和 Vaughan）中用于对象检测和分割的数据集 [29/12/19] YCB 基准测试–对象和模型集 - 5 个类别（食品，厨房，工具，形状，任务）中的 77 个对象，每个都有 600 RGBD 和高分辨率 RGB 图像，校准数据，分割蒙版，网格模型（Calli，Dollar，Singh， Walsman，Srinivasa，Abbeel）[19 年 12 月 28 日之前] YouTube-BoundingBoxes- 从 24 万个 YouTube 视频中跨帧跟踪的 23 个对象类别中的 560 万个准确的带人类注释的 BB，重点关注人物类（130 万个盒子）（真实，Shlens，Pan，Mazzocchi，Vanhoucke，Khan， Kakarla 等人，[19 年 12 月 28 日之前] （4）人（静态和动态），人体姿势 3D 铰接式车身 - 通过旋转和平移对铰接式车身进行 3D 重建。单摄像头，可变焦。每个场景都可能有关节运动。包括四种数据集。包含的样本重建结果仅使用场景的四个图像。（Jihun 公园教授）[19/12/28 之前] BUFF 数据集 - 约有 1 万次扫描衣服中的人以及下面的人的估计身体形状。扫描包含纹理，因此易于生成合成视频 / 图像。（Zhang，Pujades，Black 和 Pons-Moll）[19 年 12 月 28 日之前] CAPE 数据集 - 对服装中的人进行 4D 扫描的 140K SMPL 网格配准，包括 15 个对象，约 600 个运动序列，以及对服装下的地面真实身体形状的配准扫描（Ma，Yang，Ranjan，Pujades，Pons-Moll 和 Tang ，黑色）[28/12/2020] CASR：骑车人手臂手势识别 - 约 10 秒的小片段，显示骑车人正在执行手臂手势。这些视频是通过消费者分级的相机获取的。共有 219 条手臂手势动作被注释。（方志杰，安东尼奥・洛佩兹）[13/1/20] 动态动态 - 超过 40K 4D 60fps 的高分辨率扫描和人物模型非常准确地注册。扫描包含纹理，因此易于生成合成视频 / 图像。（Pons-Moll，Romero，Mahmood 和 Black）[19 年 12 月 28 日之前] 动态浮士 - 对人进行的 40K 4D 60fps 高分辨率扫描非常准确。扫描包含纹理，因此易于生成合成视频 / 图像。（波哥，罗梅罗，庞斯・莫尔和布莱克）[19 年 12 月 28 日之前] EHF 数据集 - 穿着最少的衣服，对一个对象的 100 个策划帧（+ 代码）执行涉及身体，手和脸的各种表达姿势。每帧都包含全身 RGB 图像，检测到的 2D OpenPose 特征（身体，手，脸），对象的 3D 扫描以及作为伪地面真相的 3D SMPL-X 网格（Pavlakos，Choutas，Ghorbani，Bolkart，奥斯曼（Tzionas），布莱克（Black）[19 年 12 月 28 日之前] 扩展的 Chictopia 数据集 - 14K 图像 Chictopia 数据集，带有附加的已处理注释（面部）和 SMPL 人体模型适合这些图像。（Lassner，Pons-Moll 和 Gehler）[19 年 12 月 28 日之前] 电影院中标有标签的帧（FLIC） - 标有人体姿势（Sapp，Taskar）的 20928 个帧 [19/12/28 之前] GPA：几何姿态可承受度数据集 - 与真实 3D 场景进行交互的真实 3D 人的数据集。8 个场景中的 13 个对象的 300k 静态 RGB 帧具有真实的场景网格，运动捕获脚本着重于对象和场景几何体之间的交互作用，人体动力学以及围绕场景几何体的人类动作的模仿。（王，陈，拉索尔，申，福克斯）[29/12/19] KIDS 数据集 - 包含 30 种高分辨率 3D 形状的集合，这些形状经历了近等轴测和非等轴测变形，具有点对点地面真实性以及左右左右两侧对称性的地面真实性。（Rodola，Rota Bulo，Windheuser，Vestner，Cremers）[19 年 12 月 28 日之前] Kinect2 人体姿势数据集（K2HPD） -Kinect2 人体姿势数据集（K2HPD）包含约 100K 深度图像，具有挑战性场景下的各种人体姿势。（王克则，林良，翟胜富，董登科）[19/12/28 之前] 利兹运动姿势数据集 - 2000 构成了大多数运动人物（约翰逊，埃弗林汉姆）的带注释的图像 [19/12/28 之前] 研究人员数据集 - 50,000 张带有精细像素像素注释的图像，带有 19 个语义人体部位标签，以及带有 16 个关键点的 2D 姿态。（龚亮，张，沉，林）[19/12/28 之前] Manga109：漫画（漫画）数据集 - 109 卷，超过 21,000 页，109 卷，超过 21,000 页（相泽清治）[29/12/19] 通过 RGB 网络摄像头的人体床内姿势数据集 - 通过常规网络摄像头在东北大学模拟的病房中收集该床内姿势数据集。（ACLab 的 Liu Shuangjun 和 Sarah Ostadabbas）[19 年 12 月 28 日之前] 人体模型 IRS 床内数据集 - 该床内姿势数据集是通过我们的红外选择性（IRS）系统在东北大学的模拟病房中收集的。（ACLab 的 Liu Shuangjun 和 Sarah Ostadabbas）[19 年 12 月 28 日之前] MoPoTS-3D- 用于基于单眼 RGB 的方法的多人 3D 身体姿势基准，在室内和室外设置有 20 个序列（信息学 MPI）[19/12/28 之前] MoVi：大型多功能人体运动和视频数据集 - MoVi 是第一个包含大量人（Ghorbani，Mahdaviani，Thaler，Kording，Cook，Blohm，Troje）的同步姿势，身体网格物体和视频记录的人体运动数据集 [27/12/2020] MPI-INF-3DHP- 单人 3D 身体姿势数据集和评估基准，涵盖广泛的姿势，涵盖广泛的活动范围，并具有广泛的外观增强功能。多视图 RGB 帧可用于训练集，而单眼视图帧可用于测试集。（信息学 MPI）[19 年 12 月 28 日之前] MPI MANO 和 SMPL + H 数据集 - 统计模型 MANO（手动）和 SMPL + H（身体 + 手）的模型，4D 扫描和配准。对于 MANO，对 31 个对象进行了约 2k 静态 3D 扫描，最多执行 51 个姿势。对于 SMPL + H，我们包括 11 个对象的 39 个 4D 序列。（Javier Romero，Dimitrios Tzionas 和 Michael J Black）[19 年 12 月 28 日之前] MPII 人类姿势数据集 - 25K 图像，包含超过 40K 具有注释的人体关节的人，410 次人类活动（Andriluka，Pishchulin，Gehler，Schiele）[19 年 12 月 28 日之前] MPII 人体姿态数据集 - MPII 人体姿态数据集是用于评估关节式人体姿态估计的事实上的标准基准。（Mykhaylo Andriluka，Leonid Pishchulin，Peter Gehler，Bernt Schiele）[19 年 12 月 28 日之前] MuCo-3DHP- 从 MPI-INF-3DHP 数据集（信息学 MPI）生成的具有 3D 姿态注释的合成多人 RGB 图像的大规模数据集 [19/12/28 之前] MVOR：用于 2D 和 3D 人体姿势估计的多视图多人 RGB-D 手术室数据集 - 在实际临床干预期间（帕多尼）由 3 台 RGB-D 摄像机捕获的多视图图像 [19 年 12 月 28 日之前] 相册中的人 - 社交媒体照片数据集，其中包含来自 Flickr 的图像，以及在人头及其身份上的手动注释。（张宁和 Manohar Paluri 和 Yaniv Taigman 和 Rob Fergus 和 Lubomir Bourdev）[19/12/28 之前] 人员快照数据集 - 在固定摄像机前旋转的 24 个对象的单目视频。提供了分段和 2D 关节位置形式的注释。（Alldieck，Magnor，Xu，Theobalt，Pons-Moll）[19 年 12 月 28 日之前] 个人照片集中的人识别 - 我们引入了三个更难的划分，用于评估和长期属性注释以及每张照片的时间戳元数据。（哦，Seong Joon 和 Benenson，Rodrigo 和 Fritz，Mario 和 Schiele，Bernt）[19 年 12 月 28 日之前] 指向’04 ICPR 讲习班的头姿势图像数据库 [19/12/28 之前] 姿势估算 - 该数据集共有 155,530 张图像。这些图像是通过在 4 个会话中记录 CIDIS 成员而获得的。总共获得了 10 个视频，每个视频的时长为 4 分钟。要求参与者带来不同的衣服，以使图像更具多样性。此后，以每秒 5 帧的速率分离视频的帧。所有这些图像都是从顶视图角度捕获的。原始图像的分辨率为 1280x720 像素。（CIDIS）[19/12/28 之前] PROX 数据集 - 真实 3D 人与真实 3D 场景进行交互的数据集（+ 代码）。“定量 PROX”：1 个场景中 1 个对象的 180 个静态 RGB-D 静态帧，带有真实的 SMPL-X 网格。“定性代理”：使用伪地面真 SMPL-X 网格在 12 个场景中的 20 个对象的 100K 动态 RGB-D 序列。（哈桑（Hassan），乔塔斯（Choutas），齐奥纳斯（Tzionas），布莱克（Black））[19 年 12 月 28 日之前] SHREC’16 拓扑孩子 - 40 种高分辨率和低分辨率 3D 形状的集合，除了强大的拓扑伪像，自接触和网格粘合以及点对点地面真相外，它们还经历了近等轴测变形。（罗纳，罗多拉）[19/12/28 之前] SIZER -A 姿势（Tiwari，Pons-Moll）中 100 种不同服装样式和尺寸的对象（约 2000 次扫描）的 3D 扫描，服装细分，标签和 SMPL + G 注册数据集 [27/12/2020] SURREAL -60,000 个合成视频，包含形状，纹理，视点和姿势各不相同的人的视频。（瓦罗尔，罗梅罗，马丁，马哈茂德，布莱克，拉普捷夫，施密德）[19/12/28 之前] TNT 15 数据集 - 四肢佩戴的 10 个惯性传感器（IMU）同步的几段视频。（von Marcard，Pons-Moll 和 Rosenhahn）[19 年 12 月 28 日之前] UC-3D 运动数据库 - 可用的数据类型包括高分辨率运动捕获，该运动捕获是通过 MVN Suit 从 Xsens 和 Microsoft Kinect RGB 以及深度图像获取的。（系统和机器人研究所，葡萄牙科英布拉）[19/12/28 之前] 联合人（UP）数据集 - 约 8,000 张具有关键点和前景分段注释以及 3D 人体模型拟合的图像。（Lassner，Romero，Kiefel，Bogo，Black，Gehler）[19 年 12 月 28 日之前] VGG 人体姿势估计数据集，包括 BBC 姿势（20 个带重叠手语翻译的视频），扩展 BBC 姿势（72 个附加培训视频），BBC 短姿势（5 个一小时的手语签名视频）和 ChaLearn 姿势（23 小时） 27 个人执行 20 个义大利手势的 Kinect 数据集）。（Charles，Everingham，Pfister，Magee，Hogg，Simonyan，Zisserman）[19 年 12 月 28 日之前] VRLF：视觉唇读的可行性 - 用西班牙语（Fernandez-Lopez，Martinez 和 Sukno）录制的 24 位演讲者的视听语料库 [19/12/28 之前] xR-EgoPose- 从以自我为中心的角度进行 3D 人体姿势估计（Denis Tome）[27/12/2020]","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"CV数据集","slug":"CV数据集","permalink":"https://leezhao415.github.io/tags/CV%E6%95%B0%E6%8D%AE%E9%9B%86/"}]},{"title":"跨镜追踪技术导读","slug":"跨镜追踪技术导读","date":"2021-11-21T14:22:34.000Z","updated":"2021-11-21T14:38:21.489Z","comments":true,"path":"2021/11/21/跨镜追踪技术导读/","link":"","permalink":"https://leezhao415.github.io/2021/11/21/%E8%B7%A8%E9%95%9C%E8%BF%BD%E8%B8%AA%E6%8A%80%E6%9C%AF%E5%AF%BC%E8%AF%BB/","excerpt":"","text":"文章目录 导读 一、ReID 定义及技术难点 （一）ReID 定义 （二）ReID 技术难点 二、常用数据集与评价指标简介 第一种，表征学习。 第二种，度量学习方案。 第三种，局部特征学习。 三、多粒度网络（MGN）的结构设计与技术实现 （一）多粒度网络（MGN）- 设计思路。 （二）多粒度网络（MGN）—— 网络结构 （三）多粒度网络（MGN）——Loss 设计 （四）多粒度网络（MGN）—— 实验参数 （五）多粒度网络（MGN）—— 实验结果 （六）多粒度网络（MGN）—— 有趣的对比实验 （七）多粒度网络（MGN）—— 多粒度网络效果示例 四、应用场景与技术展望 （一）ReID 的应用场景 （二）ReID 的技术展望 导读 跨镜追踪（Person Re-Identification，简称 ReID）技术是现在计算机视觉研究的热门方向，主要解决跨摄像头跨场景下行人的识别与检索。该技术能够根据行人的穿着、体态、发型等信息认知行人，与人脸识别结合能够适用于更多新的应用场景，将人工智能的认知水平提高到一个新阶段。 本期大本营公开课，我们邀请到了云从科技资深算法研究员袁余锋，袁老师将通过以下四个方面等四个方面来讲解本次的课题： ReID 的定义及技术难点； 常用数据集与评价指标简介； 多粒度网络（MGN）的结构设计与技术实现； ReID 在行人跟踪中的应用分析与技术展望 以下是公开课文字版整理内容: ReID 是行人智能认知的其中一个研究方向，行人智能认知是人脸识别之后比较重要的一个研究方向，特别是计算机视觉行业里面，首先简单介绍 ReID 里比较热门的几项内容： 行人检测。任务是在给定图片中检测出行人位置的矩形框，这个跟之前的人脸检测、汽车检测比较类似，是较为基础的技术，也是很多行人技术的一个前置技术。 行人分割以及背景替换。行人分割比行人检测更精准，预估每个行人在图片里的像素概率，把这个像素分割出来是人或是背景，这时用到很多 P 图的场景，比如背景替换。举一个例子，一些网红在做直播时，可以把直播的背景替换成外景，让体验得到提升。 骨架关键点检测及姿态识别。一般识别出人体的几个关键点，比如头部、肩部、手掌、脚掌，用到行人姿态识别的任务中，这些技术可以应用在互动娱乐的场景中，类似于 Kinnect 人机互动方面，关键点检测技术是非常有价值的。 行人跟踪 “MOT” 的技术。主要是研究人在单个摄像头里行进的轨迹，每个人后面拖了一根线，这根线表示这个人在摄像头里行进的轨迹，和 ReID 技术结合在一起可以形成跨镜头的细粒度的轨迹跟踪。 动作识别。动作识别是基于视频的内容理解做的，技术更加复杂一点，但是它与人类的认知更加接近，应用场景会更多，这个技术目前并不成熟。动作识别可以有非常多的应用，比如闯红灯，还有公共场合突发事件的智能认知，像偷窃、聚众斗殴，摄像头识别出这样的行为之后可以采取智能措施，比如自动报警，这有非常大的社会价值。 行人属性结构化。把行人的属性提炼出来，比如他衣服的颜色、裤子的类型、背包的颜色。 跨境追踪及行人再识别 ReID 技术。 一、ReID 定义及技术难点 （一）ReID 定义 我们把 ReID 叫 “跨镜追踪技术”，它是现在计算机视觉研究的热门方向，主要解决跨摄像头跨场景下行人的识别与检索。该技术可以作为人脸识别技术的重要补充，可以对无法获取清晰拍摄人脸的行人进行跨摄像头连续跟踪，增强数据的时空连续性。 给大家举个例子，右图由四张图片构成，黄色这个人是之前新闻报道中的偷小孩事件的人，这个人会出现在多个摄像头里，现在警察刑侦时会人工去检索视频里这个人出现的视频段。这就是 ReID 可以应用的场景，ReID 技术可以根据行人的穿着、体貌，在各个摄像头中去检索，把这个人在各个不同摄像头出现的视频段关联起来，然后形成轨迹，这个轨迹对警察刑侦破案有一定帮助。这是一个应用场景。 （二）ReID 技术难点 右边是 ReID 的技术特点：首先，ReID 是属于行人识别，是继人脸识别后的一个重要研究方向。另外，研究的对象是人的整体特征，包括衣着、体形、发行、姿态等等。它的特点是跨摄像头，跟人脸识别做补充。 二、常用数据集与评价指标简介 很多人都说过深度学习其实也不难，为什么？只要有很多数据，基本深度学习的数据都能解决，这是一个类似于通用的解法。那我们就要反问，ReID 是一个深度认知问题，是不是用这种逻辑去解决就应该能够迎刃而解？准备了很多数据，ReID 是不是就可以解决？根据我个人的经验回答一下：“在 ReID 中，也行！但仅仅是理论上的，实际操作上非常不行！” 为什么？第一，ReID 有很多技术难点。比如 ReID 在实际应用场景下的数据非常复杂，会受到各种因素的影响，这些因素是客观存在的，ReID 必须要尝试去解决。 第一组图，无正脸照。最大的问题是这个人完全看不到正脸，特别是左图是个背面照，右图戴个帽子，没有正面照。 第二组图，姿态。绿色衣服男子，左边这张图在走路，右图在骑车，而且右图还戴了口罩。 第三组图，配饰。左图是正面照，但右图背面照出现了非常大的背包，左图只能看到两个肩带，根本不知道背包长什么样子，但右图的背包非常大，这张图片有很多背包的信息。 第四组图，遮挡。左图这个人打了遮阳伞，把肩部以上的地方全部挡住了，这是很大的问题。 图片上只列举了四种情况，还有更多情况，比如： 相机拍摄角度差异大； 监控图片模糊不清； 室内室外环境变化； 行人更换服装配饰，如之前穿了一件小外套，过一会儿把外套脱掉了； 季节性穿衣风格，冬季、夏季穿衣风格差别非常大，但从行人认知来讲他很可能是同一个人； 白天晚上的光线差异等。 从刚才列举的情况应该能够理解 ReID 的技术难点，要解决实际问题是非常复杂的。 ReID 常用的数据情况如何？右图列举了 ReID 学术界最常用的三个公开数据集： 第一列，Market1501。用得比较多，拍摄地点在清华大学，图片数量有 32000 张左右，行人数量是 1500 个，相当于每个人差不多有 20 张照片，它是用 6 个摄像头拍的。 第二列，DukeMTMC-reID，拍摄地点是在 Duke 大学，有 36000 张照片，1800 个人，是 8 个摄像头拍的。 第三列，CUHK03，香港中文大学，13000 张照片，1467 个 ID，10 个摄像头拍的。 看了这几个数据集之后，应该能有一个直观的感受，就是在 ReID 研究里，现在图片的数量集大概在几万张左右，而 ID 数量基本小于 2000，摄像头大概在 10 个以下，而且这些照片大部分都来自于学校，所以他们的身份大部分是学生。 这可以跟现在人脸数据集比较一下，人脸数据集动辄都是百万张或者千万张照片，一个人脸的 ID 多的数据集可以上百万，而且身份非常多样。这个其实就是 ReID 面对前面那么复杂的问题，但是数据又那么少的一个比较现实的情况。 这里放三个数据集的照片在这里，上面是 Market1501 的数据集，比如紫色这个人有一些照片检测得并不好，像第二张照片的人只占图片的五分之三左右，并不是一个完整的人。还有些照片只检测到了局部，这是现在数据集比较现实的情况。 总结一下 ReID 数据采集的特点： 必须跨摄像头采集，给数据采集的研发团队和公司提出了比较高的要求； 公开数据集的数据规模非常小； 影响因素复杂多样； 数据一般都是视频的连续截图； 同一个人最好有多张全身照片； 互联网提供的照片基本无法用在 ReID； 监控大规模搜集涉及到数据，涉及到用户的隐私问题。 这些都是 ReID 数据采集的特点，可以归结为一句话：“数据获取难度大，会对算法提出比较大的挑战。” 问题很复杂，数据很难获取，那怎么办？现在业内尽量在算法层面做更多的工作，提高 ReID 的效果。 这里讲一下评价指标，在 ReID 用得比较多的评价指标有两个： 第一个是 Rank1； 第二个是 mAP。 ReID 终归还是排序问题，Rank 是排序命中率核心指标。Rank1 是首位命中率，就是排在第一位的图有没有命中他本人，Rank5 是 1-5 张图有没有至少一张命中他本人。更能全面评价 ReID 技术的指标是 mAP 平均精度均值。 这里我放了三个图片的检索结果，是 MGN 多粒度网络产生的结果，第一组图 10 张，从左到右是第 1 张到第 10 张，全是他本人图片。第二组图在第 9 张图片模型判断错了，不是同一个人。第三组图，第 1 张到第 6 张图是对的，后面 4 张图检索错了，不是我们模型检索错了，是这个人在底库中总共就 6 张图，把前 6 张检索出来了，其实第三个人是百分之百检索对的。 详细介绍评价指标 mAP。因为 Rank1 只要第一张命中就可以了，有一系列偶然因素在里面，模型训练或者测试时有一些波动。但是 mAP 衡量 ReID 更加全面，为什么？因为它要求被检索人在底库中所有的图片都排在最前面，这时候 mAP 的指标才会高。 给大家举个例子，这里放了两组图，图片 1 和图片 2 是检索图，第一组图在底库中有 5 张图，下面有 5 个数字，我们假设它的检索位置，排在第 1 位、第 3 位、第 4 位、第 8 位，第 20 位，第二张图第 1 位、第 3 位、第 5 位。 它的 mAP 是怎么算的？对于第一张图平均精度有一个公式在下面，就是 0.63 这个位置。第一张是 1 除以 1，第二张是除以排序实际位置，2 除以 3，第三个位置是 3 除以 4，第四个是 4 除以 8，第五张图是 5 除以 20，然后把它们的值求平均，再总除以总的图片量，最后得出的 mAP 值大概是 0.63。 同样的算法，算出图片 2 的精度是 0.756。最后把所有图片的 mAP 求一个平均值，最后得到的 mAP 大概是 69.45。从这个公式可以看到，这个检索图在底库中所有的图片都会去计算 mAP，所以最好的情况是这个人在底库中所有的图片都排在前面，没有任何其他人的照片插到他前面来，就相当于同一个人所有的照片距离都是最近的，这种情况最好，这种要求是非常高的，所以 mAP 是比较能够综合体现这个模型真实水平的指标。 再来看一下 ReID 实现思路与常见方案。ReID 从完整的过程分三个步骤： 第一步，从摄像头的监控视频获得原始图片； 第二步，基于这些原始图片把行人的位置检测出来； 第三步，基于检测出来的行人图片，用 ReID 技术计算图片的距离，但是我们现在做研究是基于常用数据集，把前面图像的采集以及行人检测的两个工作做过了，我们 ReID 的课题主要研究第三个阶段。 ReID 研究某种意义上来讲，如果抽象得比较高，也是比较清晰的。比如大家看下图，假设黄色衣服的人是检索图，后面密密麻麻很多小图组成的相当于底库，从检索图和底库都抽出表征图像的特征，特征一般都抽象为一个向量，比如 256 维或者 2048 维，这个 Match 会用距离去计算检索图跟库里所有人的距离，然后对距离做排序，距离小的排在前面，距离大的排在后面，我们理解距离小的这些人是同一个人的相似度更高一点，这是一个比较抽象的思维。 刚才讲到核心是把图像抽象成特征的过程，我再稍微详细的画一个流程，左图的这些图片会经过 CNN 网络，CNN 是卷积神经网络，不同的研究机构会设计自己不同的网络结构，这些图片抽象成特征 Feature，一般是向量表示。 然后分两个阶段，在训练时，我们一般会设计一定的损失函数，在训练阶段尽量让损失函数最小化，最小化过程反向把特征训练得更加有意义，在评估阶段时不会考虑损失函数，直接把特征抽象出来，用这个特征代表这张图片，放到前面那张 PPT 里讲的，去计算它们的距离。 因为现在 ReID 的很多研究课题都是基于 Resnet50 结构去修改的。Resnet 一般会分为五层，图像输入是 （224,224,3）,3 是 3 个通道，每层输出的特征图谱长宽都会比上一层缩小一半，比如从 224 到 112，112 到 56，56 到 28，最后第五层输出的特征图谱是 （7，7，2048）。 最后进行池化，变成 2048 向量，这个池化比较形象的解释，就是每个特征图谱里取一个最大值或者平均值。最后基于这个特征做分类，识别它是行人、车辆、汽车。我们网络改造主要是在特征位置（7，7，2048）这个地方，像我们的网络是 384×128，所以我们输出的特征图谱应该是 （12，4，2048）。 下面，我讲一下 ReID 里面常用的算法实现： 第一种，表征学习。 给大家介绍一下技术方案，图片上有两行，上面一行、下面一行，这两行网络结构基本是一样的，但是两行中间这个地方会把两行的输出特征进行比较，因为这个网络是用了 4096 的向量，两个特征有一个对比 Loss，这个网络用了两种 Loss，第一个 Loss 是 4096 做分类问题，然后两个 4096 之间会有一个对比 Loss。 这个分类的问题是怎么定义的？在我们数据集像 mark1501 上有 751 个人的照片组成，这个分类相当于一张图片输入这个网络之后，判断这个人是其中某一个人的概率，要把这个图片分类成 751 个 ID 中其中一个的概率，这个地方的 Loss 一般都用 SoftmaxLoss。机器视觉的同学应该非常熟悉这个，这是非常基本的一个 Loss，对非机器视觉的同学，这个可能要你们自己去理解，它可以作为分类的实现。 这个方案是通过设计分类损失与对比损失，来实现对网络的监督学习。它测试时取的是 4096 这个向量来表征图片本人。这个文章应该是发在 2016 年，作者当时报告的效果在当时的时间点是有一定竞争力的，它的 Rank1 到了 79.51%，mAP 是 59.87% 第二种，度量学习方案。 基于 TripletLoss 三元损失的 ReID 方案。TripletLoss 计算机视觉里另外一个常用的 Loss。 它的设计思路是左图下面有三个点，目的是从数据里面选择三个图片，这三个图片由两个人构成，其中两张图片是同一个人，另外一张图片不是同一个人，当这个网络在没有训练的时候，我们假设这同一个人的两张照片距离要大于这个人跟不是同一个人两张图片的距离。 它强制模型训练，使得同一个人两张图片的距离小于第三张图片，就是刚才那张图片上箭头表示的过程。它真正的目的是让同类的距离更近，不同类的距离更远。这是 TripletLoss 的定义，大家可以去网上搜一下更详细的解释。 在 ReID 方案里面我给大家介绍一个 Batchhard 的策略，因为 TripletLoss 在设计时怎么选这三张图是有很多文章在实现不同算法，我们的文章里用的是 Batchhard 算法，就是我们从数据集随机抽取 P 个人，每个人 K 张图片形成一个 Batch，每个人的 K 张图片之间形成一个 K×（K-1）个 ap 对，再在剩下其他人里取一个与该 ap 距离最近的 negtive，组成 apn 组，然后我们这个模型使得 apn 组成的 Loss 尽量小。 这个 Loss 怎么定义？右上角有一个公式，就是 ap 距离减 an 距离，m 是一个 gap，这个值尽量小，使得同类之间尽量靠在一起，异类尽量拉开。右图是 TripletLoss 的实验方案，当时这个作者报告了一个成果，Rank1 到了 84.92%，mAP 到了 69%，这个成果在他发文章的那个阶段是很有竞争力的结果。 第三种，局部特征学习。 1、基于局部区域调整的 ReID 解决方案。多粒度网络也是解决局部特征和全局特征的方案。这是作者发的一篇文章，他解释了三种方案。 左图第一种方案是把整张图输进网络，取整张图的特征； 第二种方案是把图从上到下均分为三等，三分之一均分，每个部分输入到网络，去提出一个特征，把这三个特征又串连起来； 第三种方案是文章的核心，因为他觉得第二种均分可能出现问题，就是有些图片检测时，因为检测技术不到位，检测的可能不是完整人，可能是人的一部分，或者是人在图里面只占一部分，这种情况如果三分之一均分出来的东西互相比较时就会有问题。 所以他设计一个模型，使得这个模型动态调整不同区域在图片中的占比，把调整的信息跟原来三分的信息结合在一起进行预估。作者当时报告的成果是 Rank1 为 80% 左右，mAP 为 57%，用现在的眼光来讲，这个成果不是那么显著，但他把图片切分成细粒度的思路给后面的研究者提供了启发，我们的成果也受助于他们的经验。 2、基于姿态估计局部特征调整。局部切割是基于图片的，但对里面的语义不了解，是基于姿态估计局部位置的调整怎么做？先通过人体关键点的模型，把这个图片里面人的关节位置取出来，然后按照人类对人体结构的理解，把头跟头比较，手跟手比较，按照人类的语义分割做一些调整，这相对于刚才的硬分割更加容易理解。基于这个调整再去做局部特征的优化，这个文章是发表在 2017 年，当时作者报告的成果 Rank1 为 84.14%，mAP 为 63.41%。 3、PCB。发表在 2018 年 1 月份左右的文章，我们简称为 PCB，它的指标效果在现在来看还是可以的，我们多粒度网络有一部分也是受它的启发。下图左边这个特征图较为复杂，可以看一下右边这张图，右图上部分蓝色衣服女孩这张图片输入网络后有一个特征图谱，大概个矩形体组成在这个地方，这是特征图谱。这个图谱位置的尺寸应该是 24×8×2048，就是前面讲的那个特征图谱的位置。 它的优化主要是在这个位置，它干了个什么事？它沿着纵向将 24 平均分成 6 份，纵向就是 4，而横向是 8，单个特征图谱变为 4×8×2048，但它从上到下有 6 个局部特征图谱。6 个特征图谱变为 6 个向量后做分类，它是同时针对每个局部独立做一个分类，这是这篇文章的精髓。这个方式看起来非常简单，但这个方法跑起来非常有效。作者报告的成果在 2018 年 1 月份时 Rank1 达到了 93.8%，mAP 达到了 81.6%，这在当时是非常好的指标了。 三、多粒度网络（MGN）的结构设计与技术实现 刚才讲了 ReID 研究方面的 5 个方案。接下来要讲的是多粒度网络的结构设计与实现。有人问 MGN 的名字叫什么，英文名字比较长，中文名字是对英文的一个翻译，就是 “学习多粒度显著特征用于跨境追踪技术（行人在识别）”，这个文章是发表于 4 月初。 （一）多粒度网络（MGN）- 设计思路。 设计思想是这样子的，一开始是全局特征，把整张图片输入，我们提取它的特征，用这种特征比较 Loss 或比较图片距离。但这时我们发现有一些不显著的细节，还有出现频率比较低的特征会被忽略。比如衣服上有个 LOGO，但不是所有衣服上有 LOGO，只有部分人衣服上有 LOGO。全局特征会做特征均匀化，LOGO 的细节被忽略掉了。 我们基于局部特征也去尝试过，用关键点、人体姿态等。但这种有一些先验知识在里面，比如遮挡、姿态大范围的变化对这种方案有一些影响，效果并不是那么强。 后来我们想到全局特征跟多粒度局部特征结合在一起搞，思路比较简单，全局特征负责整体的宏观上大家共有的特征的提取，然后我们把图像切分成不同块，每一块不同粒度，它去负责不同层次或者不同级别特征的提取。 相信把全局和局部的特征结合在一起，能够有丰富的信息和细节去表征输入图片的完整情况。在观察中发现，确实是随着分割粒度的增加，模型能够学到更详细的细节信息，最终产生 MGN 的网络结构。 下面演示一下多粒度特征，演示两张图，左边第一列有 3 张图，中间这列把这 3 张图用二分之一上下均分，你可以看到同一个人有上半身、下半身，第三列是把人从上到下分成三块 —— 头部、腹胸、腿部，它有 3 个粒度，每个粒度做独立的引导，使得模型尽量对每个粒度学习更多信息。 右图表示的是注意力的呈现效果，这不是基于我们模型产生的，是基于之前的算法看到的。左边是整张图在输入时网络在关注什么，整个人看着比较均匀，范围比较广一点。第三栏从上到下相当于把它切成 3 块，每一块看的时候它的关注点会更加集中一点，亮度分布不会像左边那么均匀，更关注局部的亮点，我们可以理解为网络在关注不同粒度的信息。 （二）多粒度网络（MGN）—— 网络结构 这是 MGN 的网络架构完整的图，这个网络图比较复杂，第一个，网络从结构上比较直观，从效果来讲是比较有效的，如果想复现我们的方案还是比较容易的。如果你是做深度学习其他方向的，我们这个方案也有一定的普适性，特别是关注细粒度特征时，因为我们不是只针对 ReID 做的。我们设计的结构是有一定普适性，我把它理解为 “易迁移”，大家可以作为参考。 首先，输入图的尺寸是 384×128，我们用的是 Resnet50，如果在不做任何改变的情况下，它的特征图谱输出尺寸，从右下角表格可以看到，global 这个地方就相当于对 Resnet 50 不做任何的改变，特征图谱输出是 12×4。 下面有一个 part-2 跟 part-3，这是在 Res4_1 的位置，本来是有一个 stride 等于 2 的下采样的操作，我们把 2 改成 1，没有下采样，这个地方的尺寸就不会缩小 2，所以 part-2 跟 part-3 比 global 大一倍的尺寸，它的尺寸是 24×8。为什么要这么操作？因为我们会强制分配 part-2 跟 part-3 去学习细粒度特征，如果把特征尺寸做得大一点，相当于信息更多一点，更利于网络学到更细节的特征。 网络结构从左到右，先是两个人的图片输入，这边有 3 个模块。3 个模块的意思是表示 3 个分支共享网络，前三层这三个分支是共享的，到第四层时分成三个支路，第一个支路是 global 的分支，第二个是 part-2 的分支，第三个是 part-3 的分支。在 global 的地方有两块，右边这个方块比左边的方块大概缩小了一倍，因为做了个下采样，下面两个分支没有做下采样，所以第四层和第五层特征图是一样大小的。 接下来我们对 part-2 跟 part-3 做一个从上到下的纵向分割，part-2 在第五层特征图谱分成两块，part-3 对特征图谱从上到下分成三块。在分割完成后，我们做一个 pooling，相当于求一个最值，我们用的是 Max-pooling，得到一个 2048 的向量，这个是长条形的、横向的、黄色区域这个地方。 但是 part-2 跟 part-3 的操作跟 global 是不一样的，part-2 有两个 pooling，第一个是蓝色的，两个 part 合在一起做一个 global-pooling，我们强制 part-2 去学习细节的联合信息，part-2 有两个细的长条形，就是我们刚才引导它去学细节型的信息。淡蓝色这个地方变成小方体一样，是做降维，从 2048 维做成 256 维，这个主要方便特征计算，因为可以降维，更快更有效。我们在测试的时候会在淡蓝色的地方，小方块从上到下应该是 8 个，我们把这 8 个 256 维的特征串连一个 2048 的特征，用这个特征替代前面输入的图片。 （三）多粒度网络（MGN）——Loss 设计 Loss 说简单也简单，说复杂也复杂，为什么？简单是因为整个模型里只用了两种 Loss，是机器学习里最常见的，一个是 SoftmaxLoss 一个是 TripletLoss。复杂是因为分支比较多，包括 global 的，包括刚才 local 的分支，而且在各个分支的 Loss 设计上不是完全均等的。我们当时做了些实验和思考去想 Loss 的设计。现在这个方案，第一，从实践上证明是比较好的，第二，从理解上也是容易理解的。 首先，看一下 global 分支。上面第一块的 Loss 设计。这个地方对 2048 维做了 SoftmaxLoss，对 256 维做了一个 TripletLoss，这是对 global 信息通用的方法。下面两个部分 global 的处理方式也是一样的，都是对 2048 做一个 SoftmaxLoss，对 256 维做一个 TripletLoss。中间 part-2 地方有一个全局信息，有 global 特征，做 SoftmaxLoss+TripletLoss。 但是，下面两个 Local 特征看不到 TripletLoss，只用了 SoftmaxLoss，这个在文章里也有讨论，我们当时做了实验，如果对细节当和分支做 TripletLoss，效果会变差。为什么效果会变差？ 一张图片分成从上到下两部分的时候，最完美的情况当然是上面部分是上半身，下面部分是下半身，但是在实际的图片中，有可能整个人都在上半部分，下半部分全是背景，这种情况用上、下部分来区分，假设下半部分都是背景，把这个背景放到 TripletLoss 三元损失里去算这个 Loss，就会使得这个模型学到莫名其妙的特征。 比如背景图是个树，另外一张图是某个人的下半身，比如一个女生的下半身是一个裙子，你让裙子跟另外图的树去算距离，无论是同类还是不同类，算出来的距离是没有任何物理意义或实际意义的。从模型的角度来讲，它属于污点数据，这个污点数据会引导整个模型崩溃掉或者学到错误信息，使得预测的时候引起错误。所以以后有同学想复现我们方法的时候要注意一下， Part-2、part-3 的 Local 特征千万不要加 TripletLoss。 （四）多粒度网络（MGN）—— 实验参数 图片展示的是一些实验参数，因为很多同学对复现我们的方案有一定兴趣，也好奇到底这个东西为什么可以做那么好。其实我们在文章里把很多参数说得非常透，大家可以按照我们的参数去尝试一下。 我们当时用的框架是 Pytorch。TripletLoss 复现是怎么选择的？我们这个 batch 是选 P=16，K=4，16×4，64 张图作为 batch，是随机选择 16 个人，每个人随机选择 4 张图。 然后用 SGD 去训练，我们的参数用的是 0.9。另外，我们做了 weight decay，参数是万分之五。像 Market1501 是训练 80epochs，是基于 Resnet50 微调了。我们之前实验过，如果不基于 Resnet50，用随机初始化去训练的话效果很差，很感谢 Resnet50 的作者，对这个模型训练得 非常有意义。 初始学习率是百分之一，到 40 个 epoch 降为千分之一，60 个 epoch 时降为万分之一。我们评估时会对评估图片做左右翻转后提取两个特征，这两个特征求一个平均值，代表这张图片的特征。刚才有问题到我们用了什么硬件，我们用了 2 张的 TITAN 的 GPU。 在 Market1501 上训练 80 epoch 的时间大概差不多是 2 小时左右，这个时间是可以接受的，一天训练得快一点可以做出 5-10 组实验。 （五）多粒度网络（MGN）—— 实验结果 我们发表成果时，这个结果是属于三个数据集上最好的。 1.Market1501：我们不做 ReRank 的时候，原始的 Rank1 是 95.7%，mAP 是 86.9%，跟刚才讲的业内比较好的 PCB 那个文章相比，我们的 Rank1 提高差不多 1.9 个点，mAP 整整提高 5.3 个点，得到非常大的提升。 2.RK：Rank1 达到 96.6%，mAP 是 94.2%。RK 是 ReRank 重新排序的简称， ReID 有一篇文章是专门讲 ReRank 技术的，不是从事 ReID 的同学对 ReRank 的技术可能有一定迷惑，大家就理解为这是某种技术，这种技术是用在测试结果重新排列的结果，它会用到测试集本身的信息。因为在现实意义中很有可能这个测试集是开放的，没有办法用到测试集信息，就没有办法做 ReRank，前面那个原始的 Rank1 和 mAP 比较有用。 但是对一些已知道测试集数据分布情况下，可以用 ReRank 技术把这个指标有很大的提高，特别是 mAP，像我们方案里从 86.9% 提升到 94.2%，这其中差不多 7.3% 的提升，是非常显著的。 3.DukeMTMC-reID 和 CUHKO3 这两个结果在我们公布研究成果时算是最好的，我们是 4 月份公布的成果，现在是 6 月份了，最近 2 个月 CEPR 对关于 ReID 的文章出了差不多 30 几篇，我们也在关注结果。现在除了我们以外最好的成果，原始 Rank1 在 93.5%-94% 之间，mAP 在 83.5%-84% 之间，很少看到 mAP 超过 84% 或者 85% 的关于。 （六）多粒度网络（MGN）—— 有趣的对比实验 因为网络结构很复杂，这么复杂的事情能说得清楚吗？里面各个分支到底有没有效？我们在文章里做了几组比较有意思的实验，这里跟大家对比一下。 第一个对比，对比 MGN 跟 Resnet50，这倒数第二行，就是那个 MGN w/o TP，跟第一行对比，发现我们的多粒度网络比 Resnet50 水平，Rank1 提高了 7.8%，mAP 提高了 14.8%，整体效果是不错的。 第二个对比，因为我们的网络有三个分支，里面参数量肯定会增加，增加的幅度跟 Resnet101 的水平差不多，是不是我们网络成果来自于参数增加？我们做了一组实验，第二行有一个 Resnet101，它的 rank1 是 90.4%，mAP 是 78%，这个比 Resnet50 确实好了很多，但是跟我们的工作成果有差距，说明我们的网络也不是纯粹堆参数堆出来的结果，应该是有网络设计的合理性在。 第三个对比，表格第二个大块，搞了三个分支，把这三个分支做成三个独立的网络，同时独立训练，然后把结果结合在一起，是不是效果跟我们差不多，或者比我们好？我们做了实验，最后的结果是 “G+P2+P3（single）”，Rank1 有 94.4%，mAP85.2%，效果也不错，但跟我们三个网络联合的网络结构比起来，还是我们的结构更合理。我们的解释是不同分支在学习的时候，会互相去督促或者互相共享有价值的信息，使得大家即使在独立运作时也会更好。 （七）多粒度网络（MGN）—— 多粒度网络效果示例 这是排序图片的呈现效果，左图是排序位置，4 个人的检索结果，前 2 个人可以看到我们的模型是很强的，无论这个人是侧身、背身还是模糊的，都能够检测出来。尤其是第 3 个人，这张图是非常模糊的，整个人是比较黑的，但是我们这个模型根据他的绿色衣服、白色包的信息，还是能够找出来，尽管在第 9 位有一个判断失误。第 4 个人用了一张背面的图，背个包去检索，可以发现结果里正脸照基本被搜出来了。 右边是我们的网络注意力模型，比较有意思的一个结果，左边是原图，右边从左到右有三列，是 global、part2、part3 的特征组，可以看到 global 的时候分布是比较均匀的，说明它没有特别看细节。 越到右边的时候，发现亮点越小，越关注在局部点上，并不是完整的整个人的识别。第 4 个人我用红圈圈出来了，这个人左胸有一个 LOGO，看 part3 右边这张图的时候，整个人只有在 LOGO 地方有一个亮点或者亮点最明显，说明我们网络在 part3 专门针对这个 LOGO 学到非常强的信息，检索结果里肯定是有这个 LOGO 的人排列位置比较靠前。 四、应用场景与技术展望 （一）ReID 的应用场景 第一个，与人脸识别结合。 之前人脸识别技术比较成熟，但是人脸识别技术有一个明显的要求，就是必须看到相对清晰的人脸照，如果是一个背面照，完全没有人脸的情况下，人脸识别技术是失效的。 但 ReID 技术和人脸的技术可以做一个补充，当能看到人脸的时候用人脸的技术去识别，当看不到人脸的时候用 ReID 技术去识别，可以延长行人在摄像头连续跟踪的时空延续性。右边位置 2、位置 3、位置 4 的地方可以用 ReID 技术去持续跟踪。跟人脸识别结合是大的 ReID 的应用方向，不是具象的应用场景。 第二个，智能安防。 它的应用场景是这样子的，比如我已经知道某个嫌疑犯的照片，警察想知道嫌疑犯在监控视频里的照片，但监控视频是 24 小时不间断在监控，所以数据量非常大，监控摄像头非常多，比如有几百个、几十个摄像头，但人来对摄像头每秒每秒去看的话非常费时，这时可以用 ReID 技术。 ReID 根据嫌疑犯照片，去监控视频库里去收集嫌疑犯出现的视频段。这样可以把嫌疑犯在各个摄像头的轨迹串连起来，这个轨迹一旦串连起来之后，相信对警察的破案刑侦有非常大的帮助。这是在智能安防的具象应用场景。 第三个，智能寻人系统。 比如大型公共场所，像迪斯尼乐园，爸爸妈妈带着小朋友去玩，小朋友在玩的过程中不小心与爸爸妈妈走散了，现在走散时是在广播里播一下 “某某小朋友，你爸爸妈妈在找你”，但小朋友也不是非常懂，父母非常着急。 这时可以用 ReID 技术，爸爸妈妈提供一张小朋友拍的照片，因为游乐园里肯定拍了小朋友拍的照片，比如今天穿得什么衣服、背得什么包，把这个照片输入到 ReID 系统里，实时的在所有监控摄像头寻找这个小朋友的照片，ReID 有这个技术能力，它可以很快的找到跟爸爸妈妈提供的照片最相似的人，相信对立马找到这个小朋友有非常大的帮助。 这种大型公共场所还有更多，比如超市、火车站、展览馆，人流密度比较大的公共场所。智能寻人系统也是比较具象的 ReID 应用场景。 第四个，智能商业 - 大型商场。 想通过了解用户在商场里的行为轨迹，通过行为轨迹了解用户的兴趣，以便优化用户体验。ReID 可以根据行人外观的照片，实时动态跟踪用户轨迹，把轨迹转化成管理员能够理解的信息，以帮助大家去优化商业体验。 这个过程中会涉及到用户隐私之类的，但从 ReID 的角度来讲，我们比较提倡数据源来自于哪个商场，那就应用到哪个商场。因为 ReID 的数据很复杂，数据的迁移能力是比较弱的，这个上场的数据不见得在另外一个商场里能用，所以我们提倡 ReID 的数据应用在本商场。 第五个，智能商业 - 无人超市。 无人超市也有类似的需求，无人超市不只是体验优化，它还要了解用户的购物行为，因为如果只基于人脸来做，很多时候是拍不到客户的正面，ReID 这个技术在无人超市的场景下有非常大的应用帮助。 第六个，相册聚类。 现在拍照时，可以把相同人的照片聚在一起，方便大家去管理，这也是一个具象的应用场景。 第七个，家庭机器人。 家庭机器人通过衣着或者姿态去认知主人，做一些智能跟随等动作，因为家庭机器人很难实时看到主人的人脸，用人脸识别的技术去做跟踪的话，我觉得还是有一些局限性的。但是整个人体的照片比较容易获得，比如家里有一个小的机器人，它能够看到主人的照片，无论是上半年还是下半年，ReID 可以基于背影或者局部服饰去识别。 （二）ReID 的技术展望 第一个，ReID 的数据比较难获取，如果用应用无监督学习去提高 ReID 效果，可以降低数据采集的依赖性，这也是一个研究方向。右边可以看到，GAN 生成数据来帮助 ReID 数据增强，现在也是一个很大的分支，但这只是应用无监督学习的一个方向。 第二个，基于视频的 ReID。因为刚才几个数据集是基于对视频切好的单个图片而已，但实际应用场景中还存在着视频的连续帧，连续帧可以获取更多信息，跟实际应用更贴近，很多研究者也在进行基于视频 ReID 的技术。 第三个，跨模态的 ReID。刚才讲到白天和黑夜的问题，黑夜时可以用红外的摄像头拍出来的跟白色采样摄像头做匹配。 第四个，跨场景的迁移学习。就是在一个场景比如 market1501 上学到的 ReID，怎样在 Duke 数据集上提高效果。 第五个，应用系统设计。相当于设计一套系统让 ReID 这个技术实际应用到行人检索等技术上去。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"ReID","slug":"ReID","permalink":"https://leezhao415.github.io/tags/ReID/"}]},{"title":"常见距离度量总结以及优缺点概述","slug":"常见距离度量总结以及优缺点概述","date":"2021-11-21T14:16:54.000Z","updated":"2021-11-21T14:29:14.129Z","comments":true,"path":"2021/11/21/常见距离度量总结以及优缺点概述/","link":"","permalink":"https://leezhao415.github.io/2021/11/21/%E5%B8%B8%E8%A7%81%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%80%BB%E7%BB%93%E4%BB%A5%E5%8F%8A%E4%BC%98%E7%BC%BA%E7%82%B9%E6%A6%82%E8%BF%B0/","excerpt":"","text":"文章目录 许多算法，无论是监督或非监督，都使用距离度量。这些度量，如欧几里得距离或余弦相似度，经常可以在 k-NN、UMAP、HDBSCAN 等算法中找到。 理解距离测量域比你可能意识到的更重要。以 k-NN 为例，这是一种经常用于监督学习的技术。作为默认值，它通常使用欧几里得距离。它本身就是一个很大的距离。 但是，如果您的数据是高维的呢？那么欧几里得距离还有效吗？或者，如果您的数据包含地理空间信息呢？也许 haversine 距离是更好的选择！ 知道何时使用哪种距离量度可以帮助您从分类不正确的模型转变为准确的模型。 在本文中，我们将研究许多距离度量方法，并探讨如何以及何时最佳地使用它们。 最重要的是，我将谈论它们的缺点，以便您可以识别何时避开某些措施。 注意：对于大多数距离度量，可以并且已经针对其用例，优点和缺点编写了详尽的论文。 我会尽量覆盖，但可能会有所欠缺！ 因此，请将本文视为这些方法的概述。 欧氏距离 Euclidean Distance 我们从最常见的距离度量开始，即欧几里得距离。 最好将距离量度解释为连接两个点的线段的长度。 该公式非常简单，因为使用勾股定理从这些点的笛卡尔坐标计算距离。 缺点 尽管这是一种常用的距离度量，但欧几里德距离并不是比例不变的，这意味着所计算的距离可能会根据要素的单位而发生偏斜。 通常，在使用此距离度量之前，需要对数据进行标准化。 此外，随着数据维数的增加，欧氏距离的用处也就越小。 这与维数的诅咒有关，维数的诅咒与高维空间不能像期望的二维或 3 维空间那样起作用。 用例 当您拥有低维数据并且向量的大小非常重要时，欧几里得距离的效果非常好。 如果在低维数据上使用欧几里得距离，则 kNN 和 HDBSCAN 之类的方法将显示出出色的结果。 尽管已开发出许多其他措施来解决欧几里得距离的缺点，但出于充分的原因，它仍然是最常用的距离之一。 它使用起来非常直观，易于实现，并且在许多用例中都显示出了极好的效果。 余弦相似度 Cosine Similarity 余弦相似度经常被用作解决高维数欧几里德距离问题的方法。 余弦相似度就是两个向量夹角的余弦。 如果将向量归一化为长度均为 1，则向量的内积也相同。 两个方向完全相同的向量的余弦相似度为 1，而两个彼此相对的向量的相似度为 - 1。 注意，它们的大小并不重要，因为这是方向的度量。 缺点 余弦相似度的一个主要缺点是没有考虑向量的大小，而只考虑它们的方向。在实践中，这意味着没有充分考虑价值的差异。以一个推荐系统为例，余弦相似度没有考虑到不同用户之间评分尺度的差异。 用例 当我们对拥有的高维数据向量的大小不关注时，通常会使用余弦相似度。 对于文本分析，当数据由字数表示时，此度量非常常用。 例如，当一个单词在一个文档中比另一个单词更频繁出现时，这并不一定意味着一个文档与该单词更相关。 可能是文件长度不均匀，计数的重要性不太重要。 然后，我们最好使用忽略幅度的余弦相似度。。 汉明距离 Hamming Distance 汉明距离是两个向量之间不同值的个数。它通常用于比较两个相同长度的二进制字符串。它还可以用于字符串，通过计算不同字符的数量来比较它们之间的相似程度。 缺点 如您所料，当两个向量的长度不相等时，很难使用汉明距离。为了了解哪些位置不匹配，您可能希望比较相同长度的向量。 此外，只要它们不同或相等，就不会考虑实际值。因此，当幅度是重要指标时，建议不要使用此距离指标。 用例 典型的用例包括数据通过计算机网络传输时的错误纠正 / 检测。它可以用来确定二进制字中失真的数目，作为估计误差的一种方法。 此外，您还可以使用汉明距离来度量分类变量之间的距离。 曼哈顿距离 Manhattan Distance 曼哈顿距离，通常称为出租车距离或城市街区距离，计算实值向量之间的距离。想象描述均匀网格 (如棋盘) 上物体的向量。曼哈顿距离是指两个矢量之间的距离，如果它们只能移动直角。在计算距离时不涉及对角线移动。 缺点 尽管曼哈顿距离在高维数据中似乎可以工作，但它比欧几里得距离更不直观，尤其是在高维数据中使用时。 此外，由于它不是可能的最短路径，它比欧几里得距离更有可能给出一个更高的距离值。虽然这并不一定会带来问题，但这是你应该考虑的。 用例 当数据集具有离散和 / 或二进制属性时，Manhattan 似乎工作得很好，因为它考虑了在这些属性的值中实际可以采用的路径。以欧几里得距离为例，它会在两个向量之间形成一条直线，但实际上这是不可能的。 切比雪夫距离 Chebyshev Distance 切比雪夫距离定义为两个向量在任意坐标维度上的最大差值。换句话说，它就是沿着一个轴的最大距离。由于其本质，它通常被称为棋盘距离，因为国际象棋的国王从一个方格到另一个方格的最小步数等于切比雪夫距离。 缺点 切比雪夫通常用于非常特定的用例，这使得它很难像欧氏距离或余弦相似度那样作通用的距离度量，因此，建议您只在绝对确定它适合您的用例时才使用它。 用例 如前所述，切比雪夫距离可用于提取从一个正方形移动到另一个正方形所需的最小移动次数。 此外，在允许无限制八向移动的游戏中，这可能是有用的方法。 在实践中，切比雪夫距离经常用于仓库物流，因为它非常类似于起重机移动一个物体的时间。 可夫斯基距离（闵氏距离）Minkowski Minkowski 距离比大多数距离更复杂。 它是在范数向量空间（n 维实数空间）中使用的度量，这意味着它可以在任何距离可以表示为具有长度的向量的空间中使用。 该措施具有三个要求： 零向量 — 零向量的长度为零，而每个其他向量的长度为正。 例如，如果我们从一个地方到另一个地方旅行，那么该距离始终为正。 但是，如果我们从一个地方到自己的地方旅行，则该距离为零。标量因数 — 当向量与正数相乘时，其长度会更改，同时保持其方向。 例如，如果我们在一个方向上走了一定距离并添加了相同的距离，则方向不会改变。三角形不等式 — 两点之间的最短距离是一条直线。Minkowski 距离的公式如下所示： 关于这个距离度量最有趣的是参数 p 的使用。我们可以使用这个参数来操纵距离度量，使其与其他度量非常相似。 常见的 p 值有: p=1 - 曼哈顿距离 p=2 - 欧氏距离 p=∞- 切比雪夫距离 缺点 Minkowski 与它们所代表的距离度量具有相同的缺点，因此，良好地理解曼哈顿距离、欧几里得距离和切比雪夫距离等度量标准是非常重要的。 此外，使用参数 p 实际上可能很麻烦，因为根据您的用例，查找正确的值在计算上可能非常低效。 用例 p 的好处是可以迭代它，并找到最适合用例的距离度量。它允许您在距离度量上有很大的灵活性，如果您非常熟悉 p 和许多距离度量，这将是一个巨大的好处。 Jaccard 指数 Jaccard 指数 (交并比 IOU) 是一个用于计算样本集的相似性和多样性的度量。它是交集的大小除以样本集的并集的大小。 实际上，它是集合之间相似实体的总数除以实体的总数。例如，如果两个集合有 1 个共同的实体，而总共有 5 个不同的实体，那么 Jaccard 索引将是 1/5 = 0.2。 要计算 Jaccard 距离，我们只需从 1 中减去 Jaccard 指数： 缺点 Jaccard 指数的主要缺点是它受到数据大小的很大影响。 大型数据集可能会对指数产生很大影响，因为数据量大的话可能显著增加并集，同时保持交集不变。 用例 Jaccard 索引通常用于使用二进制或二进制数据的应用程序中。 当您拥有一个预测图像片段（例如汽车）的深度学习模型时，可以使用 Jaccard 索引来计算给定真实标签的预测片段的准确性。 同样，它也可以用于文本相似度分析，以衡量文档之间的选词重叠程度。因此，它可以用来比较模式集。 半正矢距离 (haversine) Haversine 距离是指球面上两个点之间的经度和纬度。 它与欧几里得距离非常相似，因为它可以计算两点之间的最短线。 主要区别在于不可能有直线，因为这里的假设是两个点都在一个球面上。 缺点 这种距离测量的一个缺点是，假定这些点位于一个球体上。 实际上，这种情况很少出现，例如，地球不是完美的圆形，在某些情况下可能会使计算变得困难。 取而代之的是，将目光转向假定椭圆形的 Vincenty 距离。 用例 如您所料，Haversine 距离通常用于导航。 例如，您可以使用它来计算两个国家之间的飞行距离。 请注意，如果距离本身不那么大，则不太适合。 曲率不会产生太大的影响。 Srensen-Dice 指数 Srensen-Dice 指数与 Jaccard 指数非常相似，它衡量的是样本集的相似性和多样性。尽管它们的计算方法类似，但 Srensen-Dice 索引更直观一些，因为它可以被视为两个集合之间重叠的百分比，这是一个介于 0 和 1 之间的值。 这个指数在距离度量中很重要，因为它允许更好地使用没有 v 的度量 Jaccard 指数是一个用于计算样本集的相似性和多样性的度量。它是交集的大小除以样本集的并集的大小。 实际上，它是集合之间相似实体的总数除以实体的总数。例如，如果两个集合有一个共同的实体，而总共有 5 个不同的实体，那么 DICE 指数将是 1/5 = 0.2。 缺点 就像 Jaccard 指数一样，它们都夸大了很少或没有真值的集合。它可以控制多组平均得分并按相关集合的大小成反比地加权每个项目，而不是平等对待它们。 用例 用例与 Jaccard 指数相似。 您会发现它通常用于图像分割任务或文本相似性分析中。 注意：比这里提到的 9 种距离测量更多。 如果您正在寻找更有趣的指标，我建议您研究以下内容之一：Mahalanobis, Canberra, Braycurtis, and KL-divergence.","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"度量学习","slug":"度量学习","permalink":"https://leezhao415.github.io/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"}]},{"title":"人工智能数据集汇总","slug":"人工智能数据集汇总","date":"2021-10-28T14:32:39.000Z","updated":"2021-10-28T14:55:27.895Z","comments":true,"path":"2021/10/28/人工智能数据集汇总/","link":"","permalink":"https://leezhao415.github.io/2021/10/28/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B1%87%E6%80%BB/","excerpt":"","text":"文章目录 （1）金融 （2）交通 （3）商业 （4）推荐系统 （5）医疗健康 （6）图像数据 （7）视频数据 （8）音频数据 （9）自然语言处理 （10）社会数据 （11）处理后的科研和竞赛数据 （12）运动想象，情绪识别 （13）其他 （1）金融 美国劳工部统计局官方发布数据 上证 A 股日线数据，1999.12.09 至 2016.06.08，前复权，1095 支股票 深证 A 股日线数据，1999.12.09 至 2016.06.08，前复权，1766 支股票 深证创业板日线数据，1999.12.09 至 2016.06.08，前复权，510 支股票 MT4 平台外汇交易历史数据 Forex 平台外汇交易历史数据 几组外汇交易逐笔（Ticks）数据 美国股票新闻数据【Kaggle 数据】 美国医疗保险市场数据【Kaggle 数据】 美国金融客户投诉数据【Kaggle 数据】 Lending Club 网贷违约数据【Kaggle 数据】 信用卡欺诈数据【Kaggle 数据】 某个金融产品实时交易数据【Kaggle 数据】 美国股票数据 XBRL【Kaggle 数据】 纽约股票交易所数据【Kaggle 数据】 （2）交通 2013 年纽约出租车行驶数据 Udacity 自动驾驶数据 纽约 Uber 接客数据 【Kaggle 数据】 英国车祸数据（2005-2015）【Kaagle 数据】 芝加哥汽车超速数据【Kaggle 数据】 KITTI 自动驾驶任务数据【数据太大仅有一部分】 Cityscapes 场景标注数据【数据太大仅有介绍】 （3）商业 Amazon 食品评论数据【Kaggle 数据】 Amazon 无锁手机评论数据【Kaggle 数据】 美国视频游戏销售和评价数据【Kaggle 数据】 Kaggle 各项竞赛情况数据【Kaggle 数据】 Airbnb 开放的民宿信息和住客评论数据 （4）推荐系统 Netflix 电影评价数据 MovieLens 20m 电影推荐数据集 WikiLens Jester HetRec2011 Book Crossing Large Movie Review Retailrocket 商品评论和推荐数据 （5）医疗健康 人识别物体时大脑核磁共振影像数据 人理解单词时大脑核磁共振影像数据 心脏病心房图像及标注数据 细胞病理识别 FIRE 视网膜眼底病变图像数据 食物营养成分数据 【Kaggle 数据】 EGG 大脑电波形状数据【Kaggle 数据】 某人基因序列数据【Kaggle 数据】 癌症 CT 影像数据【Kaggle 数据】 软组织肉瘤 CT 图像数据【Kaggle 数据】 美国国家健康与服务部 - 国家癌症研究所发起的癌症数据仓库介绍【仅有介绍】 Data Science Bowl 2017 肺癌识别竞赛数据【数据太大仅有介绍】 TCGA-LUAD 肺癌 CT 图像数据 RAID 肺癌 CT 图像数据 （6）图像数据 6.1 综合图像 Visual Genome 图像数据 Visual7w 图像数据 COCO 图像数据 SUFR 图像数据 ILSVRC 2014 训练数据（ImageNet 的一部分） PASCAL Visual Object Classes 2012 图像数据 PASCAL Visual Object Classes 2011 图像数据 PASCAL Visual Object Classes 2010 图像数据 80 Million Tiny Image 图像数据【数据太大仅有介绍】 ImageNet【数据太大仅有介绍】 Google Open Images【数据太大仅有介绍】 6.2 场景图像 Street Scences 图像数据 Places2 场景图像数据 UCF Google Street View 图像数据 SUN 场景图像数据 The Celebrity in Places 图像数据 6.3 Web 标签图像 HARRISON 社交标签图像 NUS-WIDE 标签图像 Visual Synset 标签图像 Animals With Attributes 标签图像 6.4 人形轮廓图像 MPII Human Shape 人体轮廓数据 Biwi Kinect Head Pose 头部姿势数据 上半身人像数据 INRIA Person 数据集 6.5 视觉文字识别图像 Street View House Number 门牌号图像数据 MNIST 手写数字识别图像数据 3D MNIST 数字识别图像数据【Kaggle 数据】 MediaTeam Document 文档影印和内容数据 Text Recognition 文字图像数据 NIST Handprinted Forms and Characters 手写英文字符数据 NIST Structured Forms Reference Set of Binary Images (SFRS) 图像数据 NIST Structured Forms Reference Set of Binary Images (SFRS) II 图像数据 6.6 特定一类事物图像 著名的猫图像标注数据 Caltech-UCSD Birds200 鸟类图像数据 Stanford Car 汽车图像数据 Cars 汽车图像数据 MIT Cars 汽车图像数据 Stanford Cars 汽车图像数据 Food-101 美食图像数据 17_Category_Flower 图像数据 102_Category_Flower 图像数据 UCI Folio Leaf 图像数据 Labeled Fishes in the Wild 鱼类图像 美国 Yelp 点评网站酒店照片 CMU-Oxford Sculpture 塑像雕像图像 Oxford-IIIT Pet 宠物图像数据 Nature Conservancy Fisheries Monitoring 过度捕捞监控图像数据【Kaggle 数据】 6.7 材质纹理图像 CURET 纹理材质图像数据 ETHZ Synthesizability 纹理图像数据 KTH-TIPS 纹理材质图像数据 Describable Textures 纹理图像数据 6.8 物体分类图像 COIL-20 图像数据 COIL-100 图像数据 Caltech-101 图像数据 Caltech-256 图像数据 CIFAR-10 图像数据 CIFAR-100 图像数据 STL-10 图像数据 LabelMe_12_50k 图像数据 NORB v1.0 图像数据 NEC Toy Animal 图像数据 iCubWorld 图像分类数据 Multi-class 图像分类数据 GRAZ 图像分类数据 6.9 人脸图像 IMDB-WIKI 500k+ 人脸图像、年龄性别数据 Labeled Faces in the Wild 人脸数据 Extended Yale Face Database B 人脸数据 Bao Face 人脸数据 DC-IGN 论文人脸数据 300 Face in Wild 图像数据 BioID Face 人脸数据 CMU Frontal Face Images FDDB_Face Detection Data Set and Benchmark NIST Mugshot Identification Database Faces in the Wild 人脸数据 CelebA 名人人脸图像数据 VGG Face 人脸图像数据 Caltech 10k Web Faces 人脸图像数据 6.10 姿势动作图像 HMDB_a large human motion database Human Actions and Scenes Dataset Buffy Stickmen V3 人体轮廓识别图像数据 Human Pose Evaluator 人体轮廓识别图像数据 Buffy pose 人类姿势图像数据 VGG Human Pose Estimation 姿势图像标注数据 6.11 指纹识别 NIST FIGS 指纹识别数据 NIST Supplemental Fingerprint Card Data (SFCD) 指纹识别数据 NIST Plain and Rolled Images from Paired Fingerprint Cards in 500 pixels per inch 指纹识别数据 NIST Plain and Rolled Images from Paired Fingerprint Cards 1000 pixels per inch 指纹识别数据 6.12 其它图像数据 Visual Question Answering V1.0 图像数据 Visual Question Answering V2.0 图像数据 （7）视频数据 7.1 综合视频 DAVIS_Densely Annotated Video Segmentation 数据 YouTube-8M 视频数据集【数据太大仅有介绍】 YouTube 网站视频备份【数据太大仅有介绍】 7.2 人类动作视频 Microsoft Research Action 人类动作视频数据 UCF50 Action Recognition 动作识别数据 UCF101 Action Recognition 动作识别数据 UT-Interaction 人类动作视频数据 UCF iPhone 运动中传感器数据 UCF YouTube 人类动作视频数据 UCF Sport 人类动作视频数据 UCF-ARG 人类动作视频数据 HMDB 人类动作视频 HOLLYWOOD2 人类行为动作视频数据 Recognition of human actions 动作视频数据 Motion Capture 动作捕捉视频数据 SBU Kinect Interaction 肢体动作视频数据 7.3 目标检测视频 UCSD Pedestrian 行人视频数据 Caltech Pedestrian 行人视频数据 ETH 行人视频数据 INRIA 行人视频数据 TudBrussels 行人视频数据 Daimler 行人视频数据 ALOV++ 物体追踪视频数据 7.4 密集人群视频 Crowd Counting 高密度人群图像 Crowd Segmentation 高密度人群视频数据 Tracking in High Density Crowds 高密度人群视频 7.5 其它视频 Fire Detection 视频数据 （8）音频数据 8.1 综合音频 Google Audioset 音频数据【数据太大仅有介绍】 8.2 语音识别 Sinhala TTS 英语语音识别 TIMIT 美式英语语音识别数据 LibriSpeech ASR corpus 语音数据 Room Impulse Response and Noise 语音数据 ALFFA 非洲语音数据 THUYG-20 维吾尔语语音数据 AMI Corpus 语音识别 （9）自然语言处理 RCV1 英语新闻数据 20news 英语新闻数据 First Quora Release Question Pairs 问答数据 JRC Names 各国语言专有实体名称 Multi-Domain Sentiment V2.0 LETOR 信息检索数据 Yale Youtube Vedio Text 斯坦福问答数据【Kaggle 数据】 美国假新闻数据【Kaggle 数据】 NIPS 会议文章信息数据（1987-2016）【Kaggle 数据】 2016 年美国总统选举辩论数据【Kaggle 数据】 WikiLinks 跨文档指代语料 European Parliament Proceedings Parallel Corpus 机器翻译数据 WikiText 英语语义词库数据 WMT 2011 News Crawl 机器翻译数据 Stanford Sentiment Treebank 词汇数据 （10）社会数据 希拉里邮件门泄露邮件 波士顿 Airbnb 公开数据【Kaggle 数据】 世界各国经济发展数据【Kaagle 数据】 世界大学排名芝加哥犯罪数据（2001-2017）【Kaagle 数据】 世界范围显著地震数据（1965-2016）【Kaagle 数据】 美国婴儿姓名数据【Kaagle 数据】 全世界鲨鱼袭击人类数据【Kaagle 数据】 1908 年以来空难数据【Kaagle 数据】 2016 年美国总统大选数据【Kaagle 数据】 2013 年美国社区统计数据【Kaagle 数据】 2014 年美国社区统计数据【Kaagle 数据】 2015 年美国社区统计数据【Kaagle 数据】 欧洲足球运动员赛事表现数据【Kaagle 数据】 美国环境污染数据【Kaagle 数据】 美国 H1-B 签证申请数据【Kaggle 数据】 IMDB 五千部电影数据【Kaggle 数据】 2015 年航班延误和取消数据【Kaggle 数据】 凶杀案报告数据【Kaggle 数据】 人力资源分析数据【Kaggle 数据】 美国费城犯罪数据【Kaggle 数据】 安然公司邮件数据【Kaggle 数据】 历史棒球数据【Kaggle 数据】 美联航 Twitter 用户评论数据【Kaggle 数据】 波士顿 Airbnb 公开数据【Kaggle 数据】 （11）处理后的科研和竞赛数据 NIPS 2003 属性选择竞赛数据 台湾大学林智仁教授处理为 LibSVM 格式的分类建模数据 Large-scale 分类建模数据 几个 UCI 中 large-scale 分类建模数据 Social Computing Data Repository 社交网络数据 （12）运动想象，情绪识别 运动影像数据 Left/Right Hand MI: http://gigadb.org/dataset/100295 Motor Movement/Imagery Dataset: https://www.physionet.org/physiobank/database/eegmmidb/ Grasp and Lift EEG Challenge: https://www.kaggle.com/c/grasp-and-lift-eeg-detection/data The largest SCP data of Motor-Imagery: https://doi.org/10.6084/m9.figshare.c.3917698 BCI Competition IV-1: http://www.bbci.de/competition/iv/#dataset1 BCI Competition IV-2a: http://www.bbci.de/competition/iv/#dataset2a BCI Competition IV-2b: http://www.bbci.de/competition/iv/#dataset2b High-Gamma Dataset: https://github.com/robintibor/high-gamma-dataset Left/Right Hand 1D/2D movements: https://sites.google.com/site/projectbci/ Imagination of Right-hand Thumb Movement: https://archive.ics.uci.edu/ml/datasets/Planning+Relax 情绪识别数据 DEAP: http://www.eecs.qmul.ac.uk/mmv/datasets/deap/ Enterface’06: http://www.enterface.net/results/ Imagined Emotion: http://headit.ucsd.edu/studies/3316f70e-35ff-11e3-a2a9-0050563f2612 NeuroMarketing: https://drive.google.com/open?id=0B2T1rQUvyyWcSGVVaHZBZzRtTms SEED: http://bcmi.sjtu.edu.cn/~seed/seed.html SEED-IV: http://bcmi.sjtu.edu.cn/~seed/seed-iv.html SEED-VIG: http://bcmi.sjtu.edu.cn/~seed/seed-vig.html HCI-Tagging: https://mahnob-db.eu/hci-tagging/ REGULATION OF AROUSAL: https://ieee-dataport.org/open-access/regulation-arousal-online-neurofeedback-improves-human-performance-demanding-sensory 误差相关电位 (ErrP) BCI-NER Challenge: https://www.kaggle.com/c/inria-bci-challenge Monitoring ErrP in a target selection task: http://bnci-horizon-2020.eu/database/data-sets ErrPs during continuous feedback: https://www-ti.informatik.uni-tuebingen.de/~spueler/eeg_data/contErrP_description.pdf): 10 subjects with 28 EEG electrodes, playing a video game to study execution and outcome error. Dataset Part-1]: https://www-ti.informatik.uni-tuebingen.de/~spueler/eeg_data/Continous_ErrP_dataset_Part1.rar) Dataset Part-2]: https://www-ti.informatik.uni-tuebingen.de/~spueler/eeg_data/Continous_ErrP_dataset_Part2.rar) HCI-Tagging: https://mahnob-db.eu/hci-tagging/ 视觉诱发电位 (VEPs) c-VEP BCI: https://www-ti.informatik.uni-tuebingen.de/~spueler/eeg_data/cVEP_dataset.rar c-VEP BCI with dry electrodes: https://www-ti.informatik.uni-tuebingen.de/~spueler/eeg_data/dry_cVEP_dataset.rar SSVEP - Visual Search/Discrimination and Handshake: https://archive.ics.uci.edu/ml/datasets/EEG+Steady-State+Visual+Evoked+Potential+SignalsMore Dataset: Dataset 2: http://www2.hu-berlin.de/eyetracking-eeg/testdata.html 事件相关电位 (ERPs) Pattern Visual Evoked Potentials: https://www2.le.ac.uk/departments/engineering/research/bioengineering/neuroengineering-lab/software Face vs. House Discrimination: https://purl.stanford.edu/xd109qh3109 休息状态 Resting State EEG Data: https://dataverse.tdl.org/dataverse/txstatecogelectro EID-M, EID-S: https://drive.google.com/drive/folders/1t6tL434ZOESb06ZvA4Bw1p9chzxzbRbj 音乐与 EEG Music Imagery Information Retrieval: https://github.com/sstober/openmiir 眨眼 / 动作 Involuntary Eye Movements during Face Perception: http://www2.hu-berlin.de/eyetracking-eeg/testdata.html Voluntary-Involuntary Eye-Blinks: https://drive.google.com/file/d/0By5iwWd39NblS2tRWmVTdmRzZUU/view?usp=sharing EEG-eye state: https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State EEG-IO: http://gnan.ece.gatech.edu/eeg-eyeblinks/ EEG-VV, EEG-VR: http://gnan.ece.gatech.edu/eeg-eyeblinks/ 其他一些数据集 MNIST Brain Digits: http://mindbigdata.com/opendb/index.html Imagenet Brain: http://www.mindbigdata.com/opendb/imagenet.html Working Memory: https://github.com/pbashivan/EEGLearn/tree/master/Sample data Deep Sleep Slow Osciallation: https://challengedata.ens.fr/challenges/10 Genetic Predisposition to Alcoholism: https://archive.ics.uci.edu/ml/datasets/EEG+Database 临床脑电图 TUH EEG Resources: https://www.isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml （13）其他 https://sccn.ucsd.edu/~arno/fam2data/publicly_available_EEG_data.html - http://headit.ucsd.edu/studies https://www2.le.ac.uk/departments/engineering/research/bioengineering/neuroengineering-lab/software https://github.com/pbashivan/EEGLearn/tree/master/Sample data Section 2: https://arxiv.org/pdf/1611.08024.pdf EEG Databases for Emotion Recognition, NTU https://engineuring.wordpress.com/2009/07/08/downloadable-eeg-data/ http://www.brainsignals.de/ http://www.fil.ion.ucl.ac.uk/spm/data/ http://www.brainliner.jp/search/showall/1 http://bnci-horizon-2020.eu/database/data-sets http://archive.ics.uci.edu/ml/datasets/EEG+Database https://www.physionet.org/physiobank/database/#neuro http://www.physionet.org/pn6/chbmit/ https://sites.google.com/site/iitrcsepradeep7/resume http://memory.psych.upenn.edu/RAM http://fcon_1000.projects.nitrc.org/indi/cmi_eeg/ https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8616018 https://arxiv.org/pdf/1805.06427.pdf http://www.gtec.at/Research/Biosignal-Data-Sets/content/Biosignal-Data-Sets http://studycatalog.org/ http://predict.cs.unm.edu/ https://datadryad.org/resource/doi:10.5061/dryad.070jc https://ieee-dataport.org/data-competitions The Australian EEG Database https://aed.newcastle.edu.au/AED/login.jsp","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"数据集","slug":"数据集","permalink":"https://leezhao415.github.io/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"}]},{"title":"人脸属性识别算法_性别+种族+年龄+表情","slug":"人脸属性识别算法-性别-种族-年龄-表情","date":"2021-10-28T14:32:15.000Z","updated":"2021-10-31T01:17:07.528Z","comments":true,"path":"2021/10/28/人脸属性识别算法-性别-种族-年龄-表情/","link":"","permalink":"https://leezhao415.github.io/2021/10/28/%E4%BA%BA%E8%84%B8%E5%B1%9E%E6%80%A7%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95-%E6%80%A7%E5%88%AB-%E7%A7%8D%E6%97%8F-%E5%B9%B4%E9%BE%84-%E8%A1%A8%E6%83%85/","excerpt":"","text":"文章目录 内容概要 一、人脸属性识别与算法 二、性别识别 2.1 基于特征脸的性别识别算法 2.2 基于 Fisher 准则的性别识别方法 2.3 基于 Adaboost+SVM 的人脸性别分类算法 三、种族识别 3.1 基于 Adaboost 和 SVM 的人脸种族识别算法 四、年龄估计 4.1 融合 LBP 和 HOG 特征的人脸年龄估计算法 五、表情识别 5.1 融合 LBP 和局部稀疏表示的人脸表情识别算法 智芯原动人脸属性识别 内容概要 随着社会的发展，快速有效的自动身份验证在安防领域变的越来越迫切。由于生物特性是人的内在属性，具有很强的自身稳定性和个体差异性，是身份验证的最理想依据。利用人脸属性进行身份验证又是最自然直接的手段，相比其它人体生物特性，它具有直接、友好、方便的特点，更容易被用户所接受且不易察觉。 一、人脸属性识别与算法 人脸是一种非常重要的生物特征，具有结构复杂、细节变化多等特点，同时也蕴含了大量的信息，比如性别、种族、年龄、表情等。一个正常的成年人可以轻易的理解人脸的信息，但将同样的能力赋予给计算机，并让其代替人类进行类脑思考成为研究学者亟待攻克的科学课题！ 人类可以通过使用相机等图像采集装置和计算机组建一套与人体类似的系统，相机等图像采集装置是 “眼睛”，计算机是 “大脑”。但是问题来了，这些单纯的硬件设施并不足以让机器完成理解人脸信息的任务，这其中还需要载有思考能力，也就是我们平时所说的算法。 目前主流的人脸属性识别算法主要包括：性别识别、种族识别、年龄估计、表情识别等。 二、性别识别 性别识别是利用计算机视觉来辨别和分析图像中的人脸性别属性。多年来，人脸性别分类因其在人类身份认证、人机接口、视频检索以及机器人视觉中的潜在应用而备受关注。 性别分类是一个复杂的大规模二次模式分类问题，分类器将数据录入并划分男性和女性。目前最主要的性别识别方法主要有： 基于特征脸的性别识别算法 、 基于Fisher准则的性别识别方法 和 基于Adaboost+SVM的人脸性别分类算法 三大类。 2.1 基于特征脸的性别识别算法 基于特征脸（EigenFace）的性别识别算法主要是使用 PCA（主成分分析）。在计算过程中通过消除数据中的相关性，将高维图像降低到低维空间，而训练集中的样本则被映射成低维空间中的一点。当需要判断测试图片的性别时，就需要先将测试图片映射到低维空间中，然后计算离测试图片最近样本点是哪一个，将最近样本点的性别赋值给测试图片即可。 2.2 基于 Fisher 准则的性别识别方法 基于 Fisher 准则的性别识别方法主要利用 LDA（线性投影分析）的思想。它是通过将样本空间中的男女样本投影到过原点的一条直线上，并确保样本在该线上的投影类内距离最小，类间距离最大，从而分离出识别男女的分界线。 2.3 基于 Adaboost+SVM 的人脸性别分类算法 基于 Adaboost+SVM 的人脸性别分类算法主要分为两个阶段： 2.3.1 训练阶段 通过对样本图像进行预处理，提取图像的 Gabor 小波特征，通过 Adaboost 分类器进行特征降维，最后对 SVM 分类器进行训练； 2.3.2 测试阶段 通过对样本图像进行预处理，提取图像的 Gabor 小波特征，通过 Adaboost 分类器进行特征降维，最后用训练好的 SVM 分类器进行识别，输出识别结果。 三、种族识别 准确的种族分类不仅可以有效地获取人脸数据中的人脸特性，还可以获取更多的人脸语义理解信息。其难点就在于：如何准确的描述人脸数据的种族特性以及如何在特征空间的基础上实现准确的分类。基于 Adaboost 和 SVM 的人脸识别算法为我们提供了一臂之力。 3.1 基于 Adaboost 和 SVM 的人脸种族识别算法 基于 Adaboost 和 SVM 的人脸种族识别算法通过提取人脸的肤色信息和 Gabor 特征，并通过 Adaboost 级联分类器进行特征学习，最后根据 SVM 分类器进行特征分类。 四、年龄估计 说到年龄估计的问题，定义并不明确。它既可以是分类问题，亦可是回归问题。如果将年龄分成几类，比如：少年、青年、中年和老年时，年龄估计就是分类问题；如果精确的估计具体年龄时，年龄估计就是回归问题。 说到底，年龄估计是一个比性别识别更为复杂的问题。原因在于：人的年龄特征在外表上很难准确地被观察出来，即使是人眼也很难准确地判断出一个人的年龄。再看人脸的年龄特征，它通常表现在皮肤纹理、皮肤颜色、光亮程度和皱纹纹理等方面，而这些因素通常与个人的遗传基因、生活习惯、性别和性格特征和工作环境等方面相关。所以说，我们很难用一个统一的模型去定义人脸图像的年龄。若想要较好地估出人的年龄层，则需要通过大量样本的学习，比如说年龄估计开始。 年龄估计大致分为预估和详细评估两个阶段。 预估阶段： 提取出照片中人脸的肌肤纹理特征，对年龄范围做一个大致的评估，得出一个特定的年龄段； 详细评估阶段： 通过支持向量机的方法，建立了对应于多个年龄段的多个模型分类器，并选择合适的模型进行匹配。这其中，以一项融合 LBP 和 HOG 特征的人脸年龄估计算法最为人们所熟知。 4.1 融合 LBP 和 HOG 特征的人脸年龄估计算法 融合 LBP 和 HOG 特征的人脸年龄估计算法提取与年龄变化关系紧密的人脸的局部统计特征。LBP（局部二值化模式）特征和 HOG（梯度直方图）特征，并用 CCA（典型相关分析）的方法融合，最后通过 SVR（支持向量机回归）的方法对人脸库进行训练和测试。 五、表情识别 人脸表情是情绪状态和心理状态表现出来的一种重要形式。心理学家研究表明，只有 7% 的信息通过语言来表达，有 38% 按辅助语言来传达，如节奏、语音、语调等，而占比重最大的是人脸表情 —— 达到总量的 55%。也就是说，我们通过人脸表情可以得到很多有价值的信息，比如人的意识和心理活动等，这也就是我们常说的人脸表情识别。 人脸表情识别是指研究一个自动、高效、准确的系统来识别人脸表情的状态，进而通过人脸表情信息了解人的情绪，比如高兴、悲伤、愤怒、恐惧、惊讶、厌恶等。在算法识别中，融合 LBP 和局部稀疏表示的人脸表情识别算法最为著名。 5.1 融合 LBP 和局部稀疏表示的人脸表情识别算法 融合 LBP 和局部稀疏表示的人脸表情识别算法包括： 首先，对规格化后的训练集人脸图像进行特征分区，对于没个人脸分区计算该区域的 LBP 特征，并采用直方图统计方法整合该区域特征向量，形成由特定人脸的局部特征组成的训练集局部特征库； 其次，对于测试人脸，同样进行人脸图像规格化、人脸分区、局部 LBP 特征计算和局部直方图统计操作； 最后，对于测试人脸的局部直方图统计特征，利用训练集特征库进行局部稀疏重构表示，并采用局部稀疏重构残差加权方法进行最终人脸表情分类识别。 智芯原动人脸属性识别 智芯原动的人脸属性识别算法中使用了具有深度学习的卷积神经网络。卷积神经网络是一种特殊的深层的神经网络模型，它是将人工神经网络和深度学习技术相结合而产生的一种新型人工神经网络方法，具有局部感受区域、层次结构化、特征提取和分类过程结合的全局训练的特点，在图像识别领域获得了广泛的应用。 卷积神经网络的特殊性体现在两个方面。 1、它的神经元之间的连接是非全连接的； 2、同一层中某些神经元之间的连接的权重是共享的。 这种非全连接和权值共享的网络结构降低了网络模型的复杂度，减少了权值的数量，这种网络结构对平移、旋转、倾斜、比例缩放等具有高度不变性。 优势：智芯原动的人脸属性识别算法很大程度上解决了装饰物、姿态、表情、光源带来的难点，并对图像质量、环境复杂度的要求较低，可以准确识别人脸的性别、种族、年龄、表情和佩戴的饰品（例如眼镜、耳环等），其人脸属性识别准确率 96% 以上。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"人脸识别","slug":"人脸识别","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"}]},{"title":"表面缺陷检测数据集汇总及其相关项目推荐","slug":"表面缺陷检测数据集汇总及其相关项目推荐","date":"2021-10-28T14:31:40.000Z","updated":"2021-10-28T15:30:28.313Z","comments":true,"path":"2021/10/28/表面缺陷检测数据集汇总及其相关项目推荐/","link":"","permalink":"https://leezhao415.github.io/2021/10/28/%E8%A1%A8%E9%9D%A2%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B1%87%E6%80%BB%E5%8F%8A%E5%85%B6%E7%9B%B8%E5%85%B3%E9%A1%B9%E7%9B%AE%E6%8E%A8%E8%8D%90/","excerpt":"","text":"文章目录 1 表面缺陷检测关键问题 1、小样本问题 1）数据扩增、生成 2）网络预训练与迁移学习 3）合理的网络结构设计 4）无监督或半监督 2、实时性问题 2 工业表面缺陷检测常用数据集 1）钢材表面：NEU-CLS（可用于分类、定位任务） 2）太阳能板：elpv-dataset 3）金属表面：KolektorSDD 4）PCB 板检测：DeepPCB 5）AITEX 数据集（面料缺陷） 6）天池布匹缺陷数据（竞赛） 7）天池铝型材表面瑕疵数据集（竞赛） 8）弱监督学习下的工业光学检测（DAGM 2007） 9）基建表面裂纹检测数据 3 项目推荐 一、Defect Eye 如何使用 DEye 应用 二、缺陷检测文献记录 三、项目地址： 参考资料 最近，有许多朋友都在关注缺陷检测领域，今天来看看缺陷检测。 目前，基于机器视觉的表面缺陷装备已经在各工业领域广泛替代人工肉眼检测，包括 3C、汽车、家电、机械制造、半导体及电子、化工、医药、航空航天、轻工等行业。传统的基于机器视觉的表面缺陷检测方法，往往采用常规图像处理 算法或人工设计特征加分类器方式。一般来说，通常利用被检表面或缺陷的不同性质进行成像方案的设计，合理的成像方案有助于获得光照均匀的图像，并将物体表面缺陷明显的体现出来。近年来，不少基于深度学习的缺陷检测方法也被广泛应用在各种工业场景中。 对比计算机视觉中明确的分 类、检测和分割任务，缺陷检测的需求非常笼统。实 际上，其需求可以划分为三个不同的层次: “缺陷是什么”（分类）、“缺陷在哪里”（定位）和 “缺陷是多少”（分割）。 1 表面缺陷检测关键问题 1、小样本问题 目前深度学习方法广泛应用在各种计算机视觉 任务中，表面缺陷检测一般被看作是其在工业领域的具体应用。在传统的认识中，深度学习方法无法直接应用在表面缺陷检测中的原因是因为在真实的 工业环境中，所能提供的工业缺陷样本太少。 相比于 ImageNet 数据集中 1400 多万张样本数据，表面缺陷检测中面临的最关键的问题是小样本问题，在很多真实的工业场景下甚至只有几张或几十张缺陷图片。 实际上，针对于工业表面缺陷检测中关键问题之 一的小样本问题，目前有 4 种不同的解决方式: 1）数据扩增、生成 最常用的缺陷图像扩 增方法是对原始缺陷样本采用镜像、旋转、平移、扭曲、滤波、对比度调整等多种图像处理操作来获取 更多的样本。另外一种较为 常见方法是数据合成，常常将单独缺陷融合叠加到 正常 (无缺陷) 样本上构成缺陷样本。 2）网络预训练与迁移学习 一般来说，，采用小样本来训练深度学习网络很容易导致过拟合，因此 基于预训练网络或迁移学习的方法是目前针对样 本中最常用的方法之一。 3）合理的网络结构设计 通过设计合理的网络 结构也可以大大减少样本的需求。基于压缩采样定理来压缩和扩充小样本数据，使 用 CNN 直接对压缩采样的数据特征进行分类。相比 于原始的图像输入，通过对输入进行压缩采样能大 大降低网络对样本的需求。此外，基于孪生网络的表 面缺陷检测方法也可以看作是一种特殊的网络设计，能够大幅减少样本需求。 4）无监督或半监督 在无监督模型中，只利用正 常样本进行训练，因此不需要缺陷样本。半监督方法 可以利用没有标注的样本来解决小样本情况下的网络训练难题。 2、实时性问题 基于深度学习的缺陷检测方法在工业应用中包括三个主要环节：数据标注、模型训练与模型推断。在实际工业应用中的实时性更关注模型推断这一环节。目前大多数缺陷检测方法都集中在分类或 识别的准确性上，而很少关注模型推断的效率。有不少方法用于加速模型，例如模型权重量化和模型剪枝等。另外，虽然现有深度学习模型 使用 GPU 作为通用计算单元，但随着技术发展，相信 FPGA 会成为一个具有吸引力的替代方案。 2 工业表面缺陷检测常用数据集 1）钢材表面：NEU-CLS（可用于分类、定位任务） 项目地址： http://faculty.neu.edu.cn/yunhyan/NEU_surface_defect_database.html 数据集简介：由东北大学（NEU）发布的表面缺陷数据库，收集了热轧钢带的六种典型表面缺陷，即轧制氧化皮（RS），斑块（Pa），开裂（Cr），点蚀表面（ PS），内含物（In）和划痕（Sc）。该数据库包括 1,800 个灰度图像：六种不同类型的典型表面缺陷，每一类缺陷包含 300 个样本。对于缺陷检测任务，数据集提供了注释，指示每个图像中缺陷的类别和位置。对于每个缺陷，黄色框是指示其位置的边框，绿色标签是类别分数。 2）太阳能板：elpv-dataset 地址：https://github.com/zae-bayern/elpv-dataset 3）金属表面：KolektorSDD 地址：http://www.vicos.si/Downloads/KolektorSDD 4）PCB 板检测：DeepPCB 地址：https://github.com/tangsanli5201/DeepPCB 5）AITEX 数据集（面料缺陷） 数据集下载链接： https://pan.baidu.com/s/1cfC4Ll5QlnwN5RTuSZ6b7w 提取码：b9uy 该数据库由七个不同织物结构的 245 张 4096 x 256 像素图像组成。数据库中有 140 个无缺陷图像，每种类型的织物 20 个，除此之外，有 105 幅纺织行业中常见的不同类型的织物缺陷（12 种缺陷）图像。图像的大尺寸允许用户使用不同的窗口尺寸，从而增加了样本数量。Internet 上的数据库还包含所有具有缺陷的图像的分割 mask，使得白色像素表示缺陷区域，其余像素为黑色。 6）天池布匹缺陷数据（竞赛） 数据下载链接：https://pan.baidu.com/s/1LMbujxvr5iB3SwjFGYHspA 提取码：gat2 数据集简介：在布匹的实际生产过程中，由于各方面因素的影响，会产生污渍、破洞、毛粒等瑕疵，为保证产品质量，需要对布匹进行瑕疵检测。布匹疵点检验是纺织行业生产和质量管理的重要环节，目前人工检测易受主观因素影响，缺乏一致性；并且检测人员在强光下长时间工作对视力影响极大。由于布匹疵点种类繁多、形态变化多样、观察识别难道大，导致布匹疵点智能检测是困扰行业多年的技术瓶颈。本数据涵盖了纺织业中布匹的各类重要瑕疵，每张图片含一个或多种瑕疵。数据包括包括素色布和花色布两类，其中，素色布数据约 8000 张，用于初赛；花色布数据约 12000 张，用于复赛。 7）天池铝型材表面瑕疵数据集（竞赛） 数据集下载链接：https://tianchi.aliyun.com/competition/entrance/231682/information 数据介绍：在铝型材的实际生产过程中，由于各方面因素的影响，铝型材表面会产生裂纹、起皮、划伤等瑕疵，这些瑕疵会严重影响铝型材的质量。为保证产品质量，需要人工进行肉眼目测。然而，铝型材的表面自身会含有纹路，与瑕疵的区分度不高。传统人工肉眼检查十分费力，不能及时准确的判断出表面瑕疵，质检的效率难以把控。近年来，深度学习在图像识别等领域取得了突飞猛进的成果。铝型材制造商迫切希望采用最新的 AI 技术来革新现有质检流程，自动完成质检任务，减少漏检发生率，提高产品的质量，使铝型材产品的生产管理者彻底摆脱了无法全面掌握产品表面质量的状态。大赛数据集里有 1 万份来自实际生产中有瑕疵的铝型材监测影像数据，每个影像包含一个或多种瑕疵。供机器学习的样图会明确标识影像中所包含的瑕疵类型。 8）弱监督学习下的工业光学检测（DAGM 2007） 数据下载链接：https://hci.iwr.uni-heidelberg.de/node/3616 数据集介绍： 主要针对纹理背景上的杂项缺陷。 较弱监督的训练数据。 包含是个数据集，前六个为训练数据集，后四个为测试数据集。 每个数据集均包含以灰度 8 位 PNG 格式保存的 1000 个 “无缺陷” 图像和 150 个 “有缺陷” 图像。每个数据集由不同的纹理模型和缺陷模型生成。 “无缺陷” 图像显示的背景纹理没有缺陷，“无缺陷” 图像的背景纹理上恰好有一个标记的缺陷。 所有数据集已随机分为大小相等的训练和测试子数据集。 弱标签以椭圆形表示，大致表示缺陷区域。 9）基建表面裂纹检测数据 数据集是 github 上的一个项目，主要是一些基建（水泥表面裂纹）。 项目地址：https://github.com/cuilimeng/CrackForest-dataset 或者链接：https://pan.baidu.com/s/1108j5QbDr7T3XQvDxAzVpg 提取码：jajn 更多数据集可参考： https://github.com/abin24/Surface-Inspection-defect-detection-dataset 3 项目推荐 一、Defect Eye 项目地址: https://github.com/sundyCoder/DEye Defect Eye 是一个基于 tensorflow1.4 的开源软件库，专注于表面缺陷检测。应用领域涵盖制造环境中的所有良率应用，包括进入来料加工工具认证，晶圆认证，玻璃表面认证，光罩鉴定，研究和开发以及工具，工艺和生产线监控。图案化和未图案化的晶圆缺陷检查和鉴定工具在晶圆的前表面，后表面和边缘上发现颗粒和图案缺陷，使工程师能够检测和监控关键的屈服偏移。此外，它可用于医学图像的注射，包括肺 PET / CT，乳房 MRI，CT Colongraphy，数字胸部 X 射线图像。 使用 CMake，VisualStudio 2015，CUDA8.0，cudnn6.0 编译 tensorflow-r1.4 GPU 版本。 tensorflow.dll，tensorflow.lib，libprotobuf.lib 百度云盘链接：https://pan.baidu.com/s/1o9tv1n8 密码：ekec Google 云端硬盘链接： https://drive.google.com/open?id=1kANDNErMNLU9wNR3rKhUTz--ltwPNPUv 如何使用 DEye 安装 VisualStudio Community 2015、安装 NVIDIA CUDA 8.0 git clone https://github.com/sundyCoder/DEye 下载 tensorflow.dll，将其置于 DEye /bin 下 下载 tensorflow.lib 和 libprotobuf.lib，在 DEye /extra/tensorflow-r1.4 / 下放置主题 下载 inception_v3_2016_08_28_frozen.pb，将其置于 DEye /data 下 打开 Visual Studio 解决方案 “DEye.sln”，它应该在 DEye /build/vc14 下，解决方案配置选项选择 “Release”，Soluton Platform 选项选择 “x64”。 构建并运行 GUI 项目，您可以为您的检查案例进行模型训练。 应用 （1）IC 芯片缺陷检查 （2）公路道路裂缝损坏检查 （3）编织物瑕疵点检查 （4）盖玻片检查 （5）民用基础设施缺陷检测 （6）电力线裂纹检测 二、缺陷检测文献记录 项目地址：https://github.com/Eatzhy/surface-defect-detection 三、项目地址： 项目地址：https://github.com/Wslsdx/Deep-Learning-Approach-for-Surface-Defect-Detection 参考资料 [1] 《Github 项目推荐丨深度学习缺陷检测 https://www.yanxishe.com/blogDetail/13979 [2] 《基于深度学习的表面缺陷检测方法综述》陶显等，中科院自动化所 [3] 《汇总丨缺陷检测数据集》","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"表面缺陷检测","slug":"表面缺陷检测","permalink":"https://leezhao415.github.io/tags/%E8%A1%A8%E9%9D%A2%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B/"}]},{"title":"【照片光照调整算法】portrait relighting算法","slug":"【照片光照调整算法】portrait-relighting算法","date":"2021-10-28T14:20:19.000Z","updated":"2021-10-28T14:27:41.022Z","comments":true,"path":"2021/10/28/【照片光照调整算法】portrait-relighting算法/","link":"","permalink":"https://leezhao415.github.io/2021/10/28/%E3%80%90%E7%85%A7%E7%89%87%E5%85%89%E7%85%A7%E8%B0%83%E6%95%B4%E7%AE%97%E6%B3%95%E3%80%91portrait-relighting%E7%AE%97%E6%B3%95/","excerpt":"","text":"文章目录 在拍照时候最大的困难之一就是无法控制环境光的影响。在逆光、测光、暗光的影响下无法拍出满意的照片。很多时候不禁感叹，和摄影大师也许就仅仅隔着一盏摄影灯的距离！如何让手机也能拍出专业摄影工作室灯效下的人像呢？加州大学圣迭戈分校和谷歌的研究人员给出了他们的答案。 在最新提出的 portrait relighti 算法中，只需要单张 RGB 人像模型就可以重建出重新调整光照后的完美照片，画质不输专业摄影装备，而你需要的仅仅是一个可以拍照的智能手机，不用打光不用等候，轻轻一拍算法处理后就能拿到期待的光照效果。 上面的图中最左侧是输入的照片，右边三个是模型重建出在不同环境光照下的效果，照片的光照被完全修改为期待的样子了。光照是肖像摄影最重要的部分之一，不同光照可以呈现出千差万别的效果。所以摄影也被称为光与影的艺术。专业的摄影工作室除了昂贵的相机外最引人瞩目的就是那一排排炫目的灯光设备了。 但对于大多数使用手机的消费者来说并不具备控制环境光照的条件，用手机自拍的小伙伴们没有滤镜、没有散射片、没有反光板，手头最多只有一个相机自带的闪光灯，而且闪光灯的强度还极其有限。 如何打破硬件设备的限制为重新为照片进行光照渲染一直是图形学、手机厂商的研究热点。使用后处理的方法改变照片的光照将会给手机摄影、特别是自拍照片的质量带来较大的提升。先前的方法主要基于特定的硬件或者多张图像的 HDR、或者需要已知环境光照，已知目标对象精确的几何形貌和反射率的方法来对照片的光照进行调整。研究人员充分考虑了 HDR 方法、光度立体视觉、单图像表面重建、人脸重建和重光照以及最近的深度学习方法，提出了一种基于自编码器的新型架构，在无需环境光照先验和物体外形及反射率的情况下重建出调整光照后的图像。模型主要结构由编码器 - 解码器构成，在使用时只需要输入原图像并在编码空间中注入希望的环境光照，即可生成出期望的光照效果。 同时这一网络也可以预测输入图像的环境光照，并可以在编码空间（网络图中的瓶颈位置）进行调整，随后解码成期待光照下的图像。例如下图中用户对编码器恢复的环境光进行操作，解码后得到了新光照条件下的图像，随着光源的右移照片中人脸的右侧明显变亮了。 和传统方法或先前基于学习的方场景推断、人脸重建法不同的是，这一方法并没有任何显式的步骤用于估计物体的几何外形与反射率。像朗伯体反射和球均匀光照等模型限制方法的表达能力。而这篇文章中提出的方法直接利用单个网络通过原始图像和目标光照恢复出重光照后的图像，并将所有需要的几何与反射等物理模型涵盖到网络内部，通过充分的训练模型可以处理绝大多数的人脸表情和面部细节，阴暗、散射、高光和阴影都可以有效处理。这一网络模型可以由下面的映射公式表示，通过输入源图像 Is 和目标光照，估计出目标图像 It 和源光照 Ls。 下面是网络架构的细节图，输入图通过不断的卷积进行编码，并利用加权平均预测出了输入图像的环境光。随后目标光照注入网络并与编码器各层的特征共同解码最后得到期待光照下的图像。模型的损失主要来自于目标图像与输出图像，预测光照与真实环境光照之间的损失。 为了训练这一网络，研究人员们首先建立了 (one-light-at-a-time,OLAT) 数据集，首先构建了一个由七个相机 304 个 LED 光源组成的图像收集装置，其中每个相机相隔中心相机 20 度，并与拍摄主题相隔 1.7m 进行拍摄，随后将这些图像进行线性加权并结合起来得到训练的基准图像。实验中共邀请了 22 个人，每个人 3-5 个表情，共进行了 98 次拍摄（每次拍摄 304 个灯以此亮起）。通过数据处理和渲染共得到了 226800 对肖像数据用于训练。 此外，为了在自然环境光下渲染图像，研究人员利用了多个 HDR 数据集来得到环境光图，主要包括了来自 Laval Indoor HDR Dataset，室外 Eisklotz, HDRI Haven, HDRI Skies, HDRLabs, HDRMAPS, NoEmotion HDRs, and Openfootage 等多个数据集的共计 3094 张不同的环境光照。下图中可以看到模型的效果，与先前方法相比，重光照图像非常自然，不会出现各种奇怪的人工痕迹： 对于自然环境下的图像，在估计出的光照图上调整后重新渲染得到的效果也相当不错： 这种方法将有可能为手机照相软件的后处理提供强大的光照后处理能力，用户可以为照片像添加滤镜一样添加不同的环境光来实现丰富的照片效果，海滩、焰火、夕阳、雪山… 你想要的都可以。也可以通过调整源图像的环境光来修正图像的光照，重新渲染出最靓的你！","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"CV算法","slug":"CV算法","permalink":"https://leezhao415.github.io/tags/CV%E7%AE%97%E6%B3%95/"}]},{"title":"【精华】算法工程师技术学习路线","slug":"【精华】算法工程师技术学习路线","date":"2021-09-12T03:19:33.000Z","updated":"2021-09-12T03:30:44.215Z","comments":true,"path":"2021/09/12/【精华】算法工程师技术学习路线/","link":"","permalink":"https://leezhao415.github.io/2021/09/12/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/","excerpt":"","text":"文章目录 2021 算法工程师超实用技术路线图 1 阅读建议 2 工程基础 2.1 编程语言 2.2 操作系统 2.3 软件工程 3 算法基础 3.1 数据分析 3.2 机器学习基础 3.3 算法框架 4 算法工程交叉 4.1 大规模算法运行 4.2 MLOps 5 工程深入方向 5.1 数据库 5.2 云计算 6 算法深入方向 6.1 AutoML 6.2 模型解释 总结 2021 算法工程师超实用技术路线图 [Datawhale] 2020-09-05 作者：周远，来源：机器之心 这是一份写给公司算法组同事们的技术路线图，其目的主要是为大家在技术路线的成长方面提供一些方向指引，配套一些自我考核项，可以带着实践进行学习，加深理解和掌握。 内容上有一定的通用性，所以也分享到知乎上。欢迎大家给出建议，如有错误纰漏，还望不吝指正 😃 工程师能力层级概览 对于不同级别的算法工程师技能要求，我们大致可以分成以下几个层级： 初级：可以在一些指导和协助下独立完成开发任务。具体到算法方面，需要你对于工具框架，建模技术，业务特性等方面有一定的了解，可以独立实现一些算法项目上的需求。 中级：可以基本独立完成一个项目的开发与交付。在初级工程师的基础上，对于深入了解技术原理的要求会更高，并且能够应对项目中各种复杂多变的挑战，对于已有技术和工具进行改造适配。在整体工程化交付方面，对于代码质量，架构设计，甚至项目管理方面的要求会开始显现。另外从业务出发来评估技术选型和方案也变得尤为重要。 高级：可以独立负责一条产品线的运作。在中级工程师的基础上，需要更广阔的技术视野与开拓创新能力，定义整个产品线的前进方向。解决问题已经不是关键，更重要的是提出和定义问题，能够打造出在业界具有领先性和差异性的产品，为公司创造更大的价值。 事实上对于不同层级的工程师，非技术部分的要求都有一定占比。本文主要聚焦在技术路线图上，对于其他方面的学习进阶路线不会做覆盖。 1 阅读建议 以下内容分工程基础，算法基础，算法工程交叉，工程深入方向，算法深入方向几个部分，在各个部分内部会进一步区分一些主题。在各个主题内部，也是有深入程度的区别的，不过限于篇幅没有进行详细的说明。建议学习路线可以先把两个基础部分与工作中较为相关的内容做一个整体基础的夯实，然后可以在后续交叉和深入方向的主题中选择感兴趣的进行深入了解和学习，过程中发现基础部分欠缺的，可以再回到基础部分查漏补缺，迭代前行。 2 工程基础 2.1 编程语言 2.1.1 Python Python 是算法工程师日常工作中最常用的语言，应该作为必须掌握的一门技术。大致的学习路线如下： 学习掌握 Python 的基本语法，可以通过各类入门教程来看，个人推荐《Learn Python the Hard Way》。 自我考核：能够读懂大多数的内部项目及一些开源项目代码的基本模块，例如 pandas, sklearn 等。 学习 Python 的编程风格，建议学习观远内部的 Python 代码规范。 自我考核：编写的代码符合编码规范，能够通过各类 lint 检查。 Python 进阶，这方面有一本非常著名的书《Fluent Python》，深入介绍了 Python 内部的很多工作原理，读完之后对于各类疑难问题的理解排查，以及语言高级特性的应用方面会很有帮助。另外动态语言元编程这块，《Ruby 元编程》也是一本非常值得推荐的书。 自我考核：能够读懂一些复杂的 Python 项目，例如 sqlalchemy 中就大量使用了元编程技巧。在实际工程项目中，能够找到一些应用高级技巧的点进行实践，例如基于 Cython 的性能优化等。 领域应用，Python 的应用相当广泛，在各个领域深入下去都有很多可以学习的内容，比如 Web 开发，爬虫，运维工具，数据处理，机器学习等。这块主要就看大家各自的兴趣来做自由选择了，个人推荐熟悉了解一下 Python web 开发，测试开发相关的内容，开拓视野。 自我考核：以 Web 开发和测试开发为例，尝试写一个简单的 model serving http 服务，并编写相应的自动化测试。 2.1.2 Scala/Java Java 目前是企业级开发中最常用的软件，包括在大数据领域，也是应用最广泛的语言，例如当年的 Hadoop 生态基本都是基于 Java 开发的。Scala 由于其函数式编程的特性，在做数据处理方面提供了非常方便的 API，也因为 Spark 等项目的火热，形成了一定的流行度。在进行企业级的软件开发，高性能，大规模数据处理等方面，JVM 上的这两门语言有很大的实用价值，值得学习。 顺带一提，Scala 本身是一门非常有意思的语言，其中函数式编程的思想与设计模式又是非常大的一块内容，对于拓宽视野，陶冶情操都是挺不错的选择。 考虑到算法工程师的工作内容属性，这边给出一个 Scala 的学习路线： 学习掌握 Scala 的基本语法，开发环境配置，项目编译运行等基础知识。这里推荐 Coursera 上 Martin Odersky 的课程，《快学 Scala》或《Programming in Scala》两本书也可以搭配着浏览参考。 自我考核：能使用 Scala 来实现一些简单算法问题，例如 DFS/BFS。或者使用 Scala 来处理一些日常数据工作，例如读取日志文件，提取一些关键信息等。 学习使用 Scala 来开发 Spark 应用，推荐 edX 上的《Big Data Analytics Using Spark》或者 Coursera 上的《Big Data Analytics with Scala and Spark》，另外有些相关书籍也可以参考，比如《Spark 快速大数据分析》等。 自我考核：能够使用 Spark 的 Scala API 来进行大规模的数据分析及处理，完成 lag feature 之类的特征工程处理。 JVM 的原理学习，Scala/Java 都是 JVM 上运行的优秀语言，其背后是一个非常大的生态，包括在 Web，Android，数据基础架构等方面有广泛的应用。JVM 相比 Python 虚拟机，发展更加成熟，有一套非常完善的 JDK 工具链及衍生的各类项目，便于开发者 debug，调优应用。这方面推荐学习周志明的《深入理解 Java 虚拟机》。 自我考核：理解 JVM GC 原理，能通过 JDK 中相关工具或者优秀的第三方工具如 arthas 等，排查分析 Spark 数据应用的资源使用情况，GC profiling，hot method profiling 等，进而进行参数优化。 计算机语言理论。Programming Language 作为计算机科学的一个重要分支，包含了很多值得深入研究的主题，例如类型论，程序分析，泛型，元编程，DSL，编译原理等。这方面的很多话题，在机器学习方面也有很多实际应用，比如 TVM 这类工作，涉及到大量编译原理的应用，知乎大佬 “蓝色” 也作为这个领域的专家在从事深度学习框架相关的工作。llvm, clang 作者 Chris Lattner 也加入 Google 主导了 Swift for Tensorflow 等工作。Scala 作为一门学术范非常强的语言，拥有极佳的 FP，元编程等能力支持，强大的类型系统包括自动推理，泛型等等高级语言特性，相对来说是一门非常 “值得” 学习的新语言，也是一个进入 PL 领域深入学习的 “gateway drug” 😃 对这个方面有兴趣的同学，可以考虑阅读《Scala 函数式编程》，《冒号课堂》，以及 Coursera 上《Programming Languages》也是一门非常好的课程。另外只想做科普级了解的同学，也可以读一读著名的《黑客与画家》感受一下。 2.1.3 C/C++/Rust 当前流行的算法框架，例如 TensorFlow, PyTorch, LightGBM 等，底层都是基于 C++ 为主要语言进行实现的。但是 C++ 本身过于复杂，使用场景也比较有限制，建议只需要达到能够读懂一些基础的 C++ 代码逻辑即可。在系统级开发领域，目前有一门新语言逐渐崛起，连续几年被 StackOverflow 投票评选为程序员最喜爱的语言：Rust。从设计理念和一些业界应用（例如 TiKV）来看还是非常不错的，但是我也没有深入学习了解过，就不做具体推荐了。这方面建议的学习内容包括经典的《The C Programming Language》以及 Rust 官方的：https://github.com/rust-lang/rustlings 自我考核：能够读懂 LightGBM 里对于 tweedie loss 的相关定义代码。 2.2 操作系统 2.2.1 基本概念 我们所编写的算法应用，都是通过操作系统的环境运行在物理硬件之上的。在实际运作过程中，会碰到不少相关的问题，例如为什么程序报了资源不足的错误，为什么 notebook 在浏览器里打不开，为什么进程 hang 住了没有响应等等，都需要一些操作系统的知识来帮助理解和分析问题，最终排查解决。操作系统涵盖的内容比较多，建议一开始只需要了解一些主要概念（例如硬件结构，CPU 调度，进程，线程，内存管理，文件系统，IO，网络等），对于整体图景有一些感觉即可。后续碰到了实际问题，可以再在各个部分深入学习展开。优秀的学习资料也有很多，基本都是大部头，重点推荐《深入理解计算机系统》，《Operating Systems: Three Easy Pieces》，以及《现代操作系统》。 自我考核：能够基本明确运行一个模型训练任务过程中，底层使用到的硬件，操作系统组件，及其交互运作的方式是如何的。 2.2.2 Linux 基础 平时工作中最常用的两个操作系统 CentOS 和 macOS，都是 Unix/Linux 系的，因此学习掌握相关的基础知识非常重要。一些必须掌握的知识点包括：Shell 与命令行工具，软件包管理，用户及权限，系统进程管理，文件系统基础等。这方面的入门学习资料推荐《鸟哥的 Linux 私房菜》，基本涵盖了 Linux 系统管理员需要掌握知识的方方面面。进阶可以阅读《Unix 环境高级编程》，对于各种系统调用的讲解非常深入，可以为后续性能调优等高级应用打下基础。 自我考核：开发一个 shell 小工具，实现一些日常工作需求，例如定时自动清理数据文件夹中超过一定年龄的数据文件，自动清理内存占用较大且运行时间较久的 jupyter notebook 进程等。 2.2.3 深入应用 工作中碰到的疑难问题排查，性能分析与优化，系统运维及稳定性工程等方面，都需要较为深入的计算机体系和操作系统知识，感兴趣的同学可以针对性的进行深入学习。以性能优化为例，可以学习经典的《性能之巅》，了解其中的原理及高级工具链。像其中的系统调用追踪 (strace)，动态追踪 (systemtap, DTrace, perf, eBPF) 等技术，对于操作系统相关的问题排查都会很有帮助。 自我考核：能够分析定位出 LightGBM 训练过程中的性能瓶颈，精确到函数调用甚至代码行号的级别。 2.3 软件工程 2.3.1 算法与数据结构 暂时先把这块放到软件工程模块下。这里指的算法是计算机科学中的经典算法，例如递归，排序，搜索，动态规划等，有别于我们常说的机器学习算法。这块的学习资料网上有非常多，个人当年是通过普林斯顿的算法课 (需要有 Java 基础) 入门，后来又上了斯坦福的算法分析与设计，开拓了一些视野。书籍方面推荐新手从《算法图解》入门，然后可以考虑阅读 Jeff Erickson 的《Algorithms》，或者选择上面提到的网课。另外像《编程珠玑》，《编程之美》等也可以参阅，里面有不少问题的巧妙解法。除了从书本中学习，还可以直接去 LeetCode 等网站进行实战操作进行练习提高。 自我考核：能够设计相关的数据结构，实现一个类似 airflow 中点击任意节点向后运行的功能。 2.3.2 代码规范 从初级程序员到中高级程序员，其中比较大的一个差异就是代码编写习惯上，从一开始写计算机能理解，能够运行成功的代码，逐渐演化到写人能够理解，易于修改与维护的代码。在这条学习路径上，首先需要建立起这方面的意识，然后需要在实战中反复思考和打磨自己的代码，评判和学习其它优秀的项目代码，才能逐渐精进。推荐的学习书籍有《编写可读代码的艺术》，一本非常短小精悍的入门书籍，后续可以再慢慢阅读那些经典大部头，例如《Clean Code》，《Code Complete》，《The Pragmatic Programmer》等。这方面 Python 也有一本比较针对性的书籍《Effective Python》，值得一读。 自我考核：审视自己写的项目代码，能发现并修正至少三处不符合最佳编码实践的问题。 2.3.3 设计模式 在代码架构方面，设计模式是一个重要的话题，对于日常工作中出现的许多典型场景，给出了一些解决方案的 “套路”。这方面最著名的书当属 GoF 的《设计模式》，不过个人并不十分推荐，尤其是以 Python 作为主要工作语言的话，其中很大部分的设计模式可能并不需要。入门可以浏览一下这个网站掌握一些基本概念：https://refactoringguru.cn/design-patterns/python ，后续可以考虑阅读《Clean Architecture》，《重构》等相关数据，理解掌握在优化代码架构过程中思考的核心点，并加以运用。Python 相关的设计模式应用，还可以参考《Python in Practice》。 自我考核：在项目中，找到一处可以应用设计模式的地方，进行重构改进。 2.3.4 质量保障 对于需要实际上线运行的软件工程，质量保障是非常重要的一个环节，能够确保整个产品按照期望的方式进行运作。在机器学习项目中，由于引入了数据这个因素，相比传统的软件测试会有更高的难度，也是业界还在摸索前进的方向。建议可以先阅读《单元测试的艺术》或《Google 软件测试之道》，大致理解软件测试的一些基本概念和运作方式，在此基础上可以进一步阅读 Martin Fowler 对于机器学习领域提出的 CD4ML 中相关的测试环节，学习 sklearn，LightGBM 等开源库的测试开发方式，掌握机器学习相关的质量保障技术能力。 自我考核：在项目中，实现基础的数据输入测试，预测输出测试。 2.3.5 项目管理 软件工程推进过程中，项目管理相关的技能方法与工具运用也非常的关键。其中各种研发流程与规范，例如敏捷开发，设计评审，代码评审，版本管控，任务看板管理等，都是实际项目推进中非常重要的知识技能点。这方面推荐学习一本经典的软件工程教材《构建之法》，了解软件项目管理的方方面面。进一步来说广义的项目管理上的很多知识点也是后续深入学习的方向，可以参考极客时间上的课程《项目管理实战 20 讲》。 自我考核：在某个负责项目中运用项目管理方法，完成一个实际的需求评估，项目规划，设计与评审，开发执行，项目上线，监控维护流程，并对整个过程做复盘总结。 2.3.6 高级话题 软件工程师在技能方向成长的一条路线就是成为软件架构师，在这个方向上对于技能点会有非常高的综合性要求，其中也有不少高级话题需要深入学习和了解，例如技术选型与系统架构设计，架构设计原则与模式，宽广的研发知识视野，高性能，高可用，可扩展性，安全性等等。有兴趣的同学可以了解一下极客时间的《从 0 开始学架构》这门课，逐渐培养这方面的视野与能力。另外如《微服务架构设计模式》还有领域驱动设计方面的一系列书籍也值得参考学习。 自我考核：设计一个算法项目 Docker 镜像自动打包系统。 3 算法基础 3.1 数据分析 3.3.1 数学基础 在进行算法建模时，深入了解数据情况，做各类探索性分析，统计建模等工作非常重要，这方面对一些数学基础知识有一定的要求，例如概率论，统计学等。这方面除了经典的数学教材，也可以参考更程序员向的《统计思维》，《贝叶斯方法》，《程序员的数学 2》等书籍。 自我考核：理解实际项目中的数据分布情况，并使用统计建模手段，推断预测值的置信区间。 3.3.2 可视化 在进行数据分析时，可视化是一个非常重要的手段，有助于我们快速理解数据情况，发掘数据规律，并排查异常点。对于各种不同类型的数据，会对应不同的可视化最佳实践，如选择不同的图表类型，板式设计，分析思路编排，人机交互方式等等。另一方面，可视化与数据报告也是我们与不同角色人群沟通数据 insights 的一个重要途径，需要从业务角度出发去思考可视化与沟通方式。这方面可以参考《Storytelling with Data》，《The Visual Display of Quantitative Information》等经典数据，同时也需要培养自己的商业背景 sense，提升沟通能力。 自我考核：对内沟通方面，能使用可视化技术，分析模型的 bad case 情况，并确定优化改进方向。对外沟通方面，能独立完成项目的数据分析沟通报告。 3.3.3 误差分析与调优 在做算法模型调优改进中，需要从数据分析的基础上出发来决定实验方向，这么做有几个好处： 从分析出发指导调优更有方向性，而不是凭经验加个特征，改个参数碰运气。哪怕是业务方提供的信息，也最好是有数据分析为前提再做尝试，而不是当成一个既定事实。 由分析发现的根源问题，对于结果验证也更有帮助。尤其在预测的数据量极大情况下，加一个单一特征很可能总体只有千分位准确率的提升，无法确定是天然波动还是真实的提升。但如果有分析的前提，那么我们可以有针对性的看对于这个已知问题，我们的调优策略是否生效，而不是只看一个总体准确率。 对于问题的彻底排查解决也更有帮助，有时候结果没有提升，不一定是特征没用，也可能是特征代码有 bug 之类的问题。带着数据分析的目标去看为什么这个特征没有效果，是模型没学到还是特征没有区分度等，有没有改进方案，对于我们评判调优尝试是否成功的原因也更能彻查到底。 数据分析会帮助我们发现一些额外的问题点，比如销量数据清洗处理是不是有问题，是不是业务本身有异常，需要剔除数据等。 这方面在业界有一些关于误差分析的探索研究，不过大多数都是基于分类问题的，例如《Identifying Unknown Unknowns in the Open World》，《A Characterization of Prediction Errors》等。可以在了解这些研究的基础上，结合具体的业务情况，深入思考总结误差分析的思路与方法论。 自我考核：在项目中形成一套可以重复使用的误差分析方案，能够快速从预测输出中定位到目前模型最重要的误差类别，并一定程度上寻找到根本原因。 3.2 机器学习基础 3.2.1 传统机器学习 这块大家应该都非常熟悉了，初阶的学习路线可以参考周志华老师的《机器学习》，涵盖了机器学习基础，常用机器学习方法，和一些进阶话题如学习理论，强化学习等。如果希望深化理论基础，可以参考经典的《PRML》，《ESL》和《统计学习方法》。在实战中，需要综合业务知识，算法原理，及数据分析等手段，逐渐积累形成建模调优的方法论，提高整体实验迭代的效率和成功率。 自我考核：结合实际业务和机器学习理论知识，挖掘项目中算法表现不够好的问题，并通过算法改造进行提升或解决。 3.2.2 深度学习 近些年兴起的深度学习，已经成为机器学习领域一个非常重要的分支，在各个应用方向发挥了很大的作用。相对于传统机器学习，对于特征工程要求的降低成了其核心优势。另一方面，深度学习对于大数据量，大规模算力的应用能力很强，也一定程度上提升了整体的产出效果。由于理论方面的研究稍显落后，深度学习在实际应用中对于使用者的经验技能要求相对比较高，需要有大量的实战经验才能达到比较理想的效果。这方面的学习资料推荐 Keras 作者的《Deep Learning with Python》，以及《Hands-on Machine Learning with Scikit-Learn and TensorFlow》，而在理论方面推荐著名的 “花书”《Deep Learning》。在学习理论原理的基础上，尤其要注意在实际算法应用中，能够通过观察各种指标与数据分析，找到提升模型的操作改进方向。 自我考核：能够在实际项目中，使用深度学习模型，达到接近甚至超过传统 GBDT 模型的精确度效果，或者通过 ensemble，embedding 特征方式，提升已有模型的精度。 3.2.3 领域建模 目前我们的业务领域在时间序列预测，自然语言处理，推荐等方面，其它类似图像，搜索，广告等领域也都有各自的一些领域建模方法。在时间序列领域，包括了传统时序模型，如 ARIMA, Prophet，机器学习模型，如划动窗口特征构建方法结合 LightGBM，及深度学习模型，例如 LSTM，seq2seq，transformer 等。这方面可以参考 Kaggle 上相关比赛的方案分享，以及 Amazon，Uber，天猫等有类似业务场景公司的分享资料。其它领域也是类似，通过了解历史技术演进，相关比赛，业界的方案分享与开源项目，会议论文来逐渐掌握学习建模方法，结合实际业务进行实践尝试，积累起更加体系性的个人知识技能。 自我考核：在项目中复现一个 Kaggle 获胜方案，检验其效果，分析模型表现背后的原因，并尝试进行改进。 3.3 算法框架 3.3.1 数据处理框架 在项目实施过程中，会需要各类复杂的数据处理操作，因此熟练掌握此类框架就显得尤为重要。目前行业的标准基本上会参照 Pandas DataFrame 的定义，在数据量较大的情况下，也有许多类似的框架，如 Spark，Dask，Modin，Mars 等支持分布式运行的 DataFrame，以及 cuDF，Vaex 等提升单机性能的改进实现。这方面经典的书籍可以参考 Wes McKinney 的《Python for Data Analysis》，在掌握基础数据操作的基础上，可以进而了解窗口函数，向量化性能优化等高级话题。另外 SQL 也可以做非常复杂的数据处理工作，有不少公司例如阿里会以 SQL 为主来构建数据处理流程，感兴趣的同学也可以学习一下 SQL 中各种高级计算的使用及优化方法。 自我考核：在已有项目中，能把至少三个使用 apply 方法的 pandas 处理修改成向量化运行，并测试性能提升。使用 window function 或其它方案来实现 lag 特征，减少 join 次数。 3.3.2 机器学习框架 机器学习方面的新框架层出不穷，一方面我们需要掌握经典框架的使用方式，理解其模块构成，接口规范的设计，一定程度上来说其它新框架也都需要遵循这些业界标准框架的模块与接口定义。另一方面对于新框架或特定领域框架，我们需要掌握快速评估，上手使用，并且做一定改造适配的能力。一些比较经典的框架有： 通用机器学习：scikit-learn，Spark ML，LightGBM 通用深度学习：Keras/TensorFlow，PyTorch 特征工程：tsfresh, Featuretools，Feast AutoML：hyperopt，SMAC3，nni，autogluon 可解释机器学习：shap，aix360，eli5，interpret 异常检测：pyod，egads 可视化：pyecharts，seaborn 数据质量：cerberus，pandas_profiling，Deequ 时间序列：fbprophet，sktime，pyts 大规模机器学习：Horovod，BigDL，mmlspark Pipeline：MLflow, metaflow，KubeFlow，Hopsworks 一般的学习路径主要是阅读这些框架的官方文档和 tutorial，在自己的项目中进行尝试使用。对于一些核心接口，也可以阅读一下相关的源代码，深入理解其背后的原理。 自我考核：在 LightGBM 框架下，实现一个自定义的损失函数，并跑通训练与预测流程。 3.3.3 其它框架 其它比较常见且与算法工程师日常工作会有一些联系的有 Web 框架，爬虫框架等，最具有代表性的当属 Flask 和 scrapy。这两者背后各自又是很大一块领域，尤其 web 开发更是保罗万象。感兴趣的同学还可以了解一下一些新兴的基于 Python3 的框架，例如 FastAPI，其背后借鉴的许多现代框架的思想设计，包括数据验证，序列化，自动文档，异步高性能等，开拓一下知识面。 自我考核：实现一个简单的 model serving http 服务。 4 算法工程交叉 4.1 大规模算法运行 4.1.1 分布式训练 在很多项目中，数据量达到十亿级以上的情况下，单机训练会难以支撑。因此分布式训练也是实际工程落地中非常重要的一个主题。分布式训练涉及到多机的通讯协同方式，优化算法的改造，数据及模型的并行与聚合，以及框架的选择和运维等话题，具体可以参考《分布式机器学习》。另外对于分布式系统，也可以参阅《数据密集型应用系统设计》这本神作，了解其背后原理。 自我考核：能够在多机上进行亿级数据的 GBDT 模型训练与预测。 4.1.2 高性能计算 在做大规模的数据训练与推理时，近些年涌现出许多高性能计算优化的方法，例如从硬件方面，有各种超线程技术，向量化指令集，GPGPU，TPU 的应用等，从软件方面，有针对数值计算场景的 OpenBLAS，有自动并行化的 OpenMP，有各种 codegen，JIT 技术下的运行时优化等。这方面可以学习的方向也很多，从基础的并行编程，编译原理及优化的知识开始，到 CUDA，OpenMP 的应用（例如 Nvidia 的 cuDNN，还有 LightGBM 中也用到了 OpenMP），Codegen，JIT 等技术在 Spark，TVM 等项目中的使用等，建议有深度性能优化需求时可以往这些方向做调研和学习。 自我考核：能够通过 LLVM JIT 来优化实现 Spark window function 的执行性能。 4.1.3 模型加速领域 这个方向分两个部分，一块是模型训练方面，能够做到加速，例如使用大 batch size，迁移学习，持续的在线 / 增量学习等手段，另一块在模型预测方面，也有很多加速需求，比如模型参数量优化，模型压缩，混合精度，知识蒸馏等技术手段，都是为了做到更高性能，更低资源消耗的模型预测推理。这方面业界有各个方向的文章和技术实现可以参考，比如经典的《Training ImageNet in 1 Hour》，MobileNet，TensorRT，二值网络等。 自我考核：在典型的销量预测场景中实现增量训练与预测。 4.2 MLOps 4.2.1 编排调度 包含各类 pipeline 的编排与调度能力的支持，包括数据 pipeline，训练 pipeline 和 serving pipeline 等。这方面比较常用的框架工具有 Airflow，DolphinScheduler，Cadence 等，需要掌握其基本的工作原理和使用方式，并能够应用于离线实验与线上运行。 自我考核：使用 Airflow 完成一个标准的项目 pipeline 搭建与运行。 4.2.2 数据集成 相对于传统的 DevOps，机器学习项目最大的区别在于数据方面的依赖会更加显著与重要。这方面的话题包括数据血缘，数据质量保障，数据版本控制等，有各类工具可以借鉴使用，例如数据版本管理方面的 DVC，数据质量方面的 TFX Data Validation，Cerberus，Deequ 等。在方法论层面，《The ML Test Score》中给出了不少数据相关的具体测试方法，值得参考学习。 自我考核：在项目中实现输入数据的分布测试，特征工程测试及特征重要性准入测试。 4.2.3 实验管理 这部分也是 ML 项目的独特之处，在开发过程中有大量的实验及相应的结果输出需要记录，以指导后续调整优化的方向，并选择最优结果来进行上线部署。这方面可以参考的项目有 MLflow，fitlog，wandb 等。当然对于单独的项目来说，可能 online Excel 就能满足需求了 😃 自我考核：在实际项目中实行一套标准的实验记录手段，并能从中找出各类实验尝试带来的精度提升的 top 5 分别是哪些操作。 4.2.4 Serving 目前我们的 serving 大多数是离线 batch 预计算的形式，所以主要依赖的技术手段是各类离线 inference 的方法，例如直接使用 model predict 接口，使用 mmlspark 等做大规模并行 inference 等。如果涉及到在线 serving，情况会更加复杂，例如在线 pipeline 的运行，实时特征获取，low latency/high throughput 的 serving 服务等，可以参考 TF Serving，MLeap，H2O，PredictionIO，PMML/PFA/ONNX 等开发标准模型格式等。 自我考核：部署一个实时预测服务，能够根据用户输入产生相应的预测结果。 4.2.5 CI/CD 软件工程中的持续集成，持续部署已经成为一种标准实践，在算法项目中，额外引入了数据这个维度的复杂性，带来了一些新的挑战。在这个方向上，几个主要话题包括自动化测试，pipeline 打包部署，持续监控运维等，可以参考 Martin Fowler 关于 CD4ML 的文章。工具系统层面，可以学习传统的 Jenkins，也有一些新选择例如 CircleCI，GoCD，VerCD（Uber）等。 自我考核：通过 Jenkins 实现 pipeline 自动测试，打包，上线流程。 4.2.6 系统监控 在整个项目上线后，需要对系统的各个环节进行监控，并对各种异常情况作出响应。例如输入数据的监控，判别测试数据与训练数据的分布是否有偏移，整个运行 pipeline 的监控，判别是否有运行失败抛出异常的情况，对于预测输出的监控，确保没有异常的预测输出值，也包括对于系统计算资源等方面的监控，确保不会因为资源不足导致业务受到影响等。在监控信息收集，基础上，还需要配套一系列的自动告警通知，日志追踪排查等。这方面的工具框架包括 TF data validation 这类专门针对算法项目的新产品，也有 elasicsearch + kibana 这类传统产品。 自我考核：将三个项目中做过的问题排查改造成常规监控手段，支持自动的问题发现，告警通知，如有可能，提供自动化或半自动化的问题排查解决方案。 4.2.7 MLOps 系统 MLOps 整体是一个比较大的话题，在这方面有很多产品和系统设计方面的实践可以参考学习。例如 Uber 的 Michelangelo 系列文章，Facebook 的 FBLearner，neptune.ai，dataiku，domino 等，虽然没有开源，但是其背后的很多设计理念，演进思考，白皮书等都非常值得我们学习。在开源界也有很多可以参考的项目，例如 MLflow，Kubeflow，Metaflow，TFX 等，可以学习他们的设计理念，Roadmap，以及实现细节等。 自我考核：总结各个 MLOps 产品的功能模块矩阵对比，能够根据项目需求来进行产品选型与使用。 5 工程深入方向 5.1 数据库 5.1.1 数据库原理 在平时工作中，我们有大量的场景需要用到数据库。从客户数据的对接，数据集的管理和使用，到各种业务系统的数据表设计及优化等，都需要对数据库的运作原理，适用场景，运维使用，性能优化等方面有一定的了解。常见的需要掌握的概念有 OLTP vs OLAP，事务，索引，隔离级别，ACID 与 CAP 理论，数据同步，数据分片，SQL 语法，ORM 等。从底层原理看，会涉及到数据，索引，及日志等存储引擎方面，以及各种计算查询引擎，包括分布式系统的设计与实现。这方面推荐的学习资料有《数据库系统内幕》及《数据密集型应用系统设计》。 自我考核：能够理解 SQL 执行计划，并能够根据执行计划来做索引或查询调优。 5.1.2 关系型数据库 目前常用的关系型数据库主要是 MySQL 和 PostgreSQL，主要需要掌握的是日常的一些 SQL 操作，例如 DML（增删改查），DDL（创建表，修改索引等），DCL（权限相关）。在此基础上还可以进一步了解一些如数据类型，高级计算，存储引擎，部署运维，范式概念与表结构设计等方面的话题。对于高级话题这块，推荐《高性能 MySQL》与《高可用 MySQL》。 自我考核：在 MySQL 中设计相关表结构，存储实际项目中的一系列中间数据集。 5.1.3 NoSQL 数据库 常用的 NoSQL 数据库有几类，KV 存储（Redis），文档数据库（MongoDB），Wide-column 存储（Cassandra，HBase）以及图数据库（Neo4j）。在目前我们的算法项目中，比较有可能会用到的主要是 Redis 这类 KV 存储（也可能把 Cassandra 之类当泛 KV 来用），或者更新一点的类似 Delta Lake 的存储系统。建议学习了解一下这类 KV 存储，以及分布式数据库的常见操作方式，以及基础的运维排查，性能优化方法。 自我考核：考虑一个线上模型服务的场景，用户输入作为基础特征，使用类似 Redis 的 KV 系统，实现实时获取其它特征，并进行模型预测。 5.2 云计算 5.2.1 基础架构 IT 系统总体的发展趋势在往云计算方向演进，即使是自建的基础设施，也会采用云计算的一套构建方式，让开发者不用过多的关注底层计算存储资源的部署运维。对于应用开发者来说，需要了解一些基础架构方面的知识，例如各类虚拟化及容器技术，配置管理，容器编排等，便于在日常工作中使用相关技术来管理和发布应用。从工具层面看，Docker 与 k8s 等技术发展速度较快，主要还是根据官方文档来学习为主。浙大之前出版的《Docker - 容器与容器云》一书中有一些更深入的话题的探讨，另外《Kubernetes in Action》中也值得一读。从方法论层面看，《Infrastructure as Code》和《Site Reiliability Engineering》是两本非常不错的学习资料。与算法应用结合的虚拟化，运维，持续集成等都是比较新的领域，需要我们探索出一条可行路线。 自我考核：对于已有的算法项目，总结制定一套开发，测试，发布，运维的标准流程，且尽可能自动化执行。 5.2.2 分布式存储 前些年最流行的分布式存储是脱胎于 Google 经典的 GFS 论文实现的 HDFS，不过随着硬件技术的发展，计算存储分离思想的逐渐兴起，不但灵活性更高，成本更低，且各自架构的复杂度也大大降低了。因此目前更建议学习简单的 object store 形式的分布式存储，例如 s3，minio 等。在此基础上的一些存储系统，例如 Delta Lake，提供了事务，高效的 upsert，time travel 等功能，也值得关注与学习。原理方面，还是推荐《数据密集型应用设计》这本。 自我考核：在项目中实现不同机器能够访问同一个 s3 路径的文件，并进行正常的数据读写，模型文件读写等功能。 5.2.3 分布式计算 大数据时代的分布式计算的鼻祖来自于 Google 经典的 MapReduce 论文，后续在 Hadoop 系统中做了开源实现，在前几年是非常火热的一项技术。目前业界的主流是 Spark 和 Flink，前者在批处理计算中处于霸者地位，后者是流处理领域的领先者。目前我们的业务应用中，Spark 是比较常用的分布式计算引擎，其基本操作相关内容比较简单，参考官方文档或者《Spark 快速大数据分析》即可。后续的主要难点会有大数据量下的问题排查与性能调优，执行复杂计算或与 Python 相关 UDF 的交互配合方式等。这方面需要对 Spark 的系统架构，内部原理有一定了解，例如 master，worker，driver，executor 等之间的关系，lazy evaluation，DAG 的 lineage 与 stage 概念，shuffle 优化，wholestage codegen 等技术细节。这方面暂时没有找到比较好的资料，主要还是依赖实际问题解决的经验积累。 自我考核：用 Spark 来实现项目中的特征工程，并在一定数据量情况下取得比单机 Pandas 更好的性能效果。 5.2.4 其它话题 其它云服务基础设施还包括分布式数据库，消息队列，zk/raft 分布式协作系统，虚拟网络，负载均衡等。这些话题离算法应用方面会比较远一些，基本上达到遇到需求时会使用的能力即可，在这里不做展开。 6 算法深入方向 6.1 AutoML 6.1.1 超参优化 自动化机器学习中比较传统的一块是超参数优化，进而可以推广到整个 pipeline 的超参优化，包括数据预处理，特征工程，特征选择，模型选择，模型调优，后处理等部分。目前业界应用比较广泛的技术手段主要是随机搜索，贝叶斯优化，进化算法，Hyperband/BOHB 等，在特征工程方面有 Featuretools，tsfresh，AutoCrossing 等自动化特征工程工具。学术界有一些进一步的探索研究，包括 multi-fidelity 优化，多任务优化，HPO 结合 ensemble learning，pipeline planning，data diff 自动数据分布探测等方面。可以参考 http://automl.org 上的各类参考资料与书籍进行学习了解。主要难点包括 automl 算法的泛化能力，scalability，整体 pipeline 组合的搜索与生成，针对不同学习算法的自动优化手段等。 自我考核：了解超参优化的基础概念，能够在项目中应用框架工具来实现模型超参的贝叶斯优化流程。 6.1.2 元学习 Meta learning 是近年来非常活跃的一个新兴领域，其主要思路是希望能通过元学习模型方法，去积累建模调优的先验知识，跨任务推断模型效果并 warm start 新的训练任务，或者指导学习算法来进行更高效的具体任务的训练过程。这方面在工业界的主要应用基本上集中在建模调优先验知识的积累方面，比如通过一系列公开数据集搜索寻找出表现较好的起始参数，用于指导在新任务上做超参优化的起始搜索点。学术研究中除了 configuration space 的研究，还包括从 learning curve 中进行学习推断，元特征提取与建模，HTN planning 在 pipeline 构建中的应用，以及 MAML 等 few-shot learning 方向的探索。这方面推荐 Lilian Weng 的一系列文章（https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html），以及 http://automl.org 网站上的资料。 自我考核：设计一系列 meta feature 与 meta learning 手段，实现对新任务的参数选择的初始化。 6.1.3 NAS AutoML 领域比较火，但也是比较特别的一个方向，目前需要大量的计算资源投入才能做这方面的研究与尝试，因此主要建议了解一下这个方向的一些工作即可，不做深入探索学习。 6.1.4 AutoML 系统 自动化机器学习相关的框架工具也非常多，比较有代表性的框架有 auto-sklearn (来自 http://automl.org 团队)，nni (microsoft)，auto-gluon (amazon)，H2O，ray tune 等，在工具级别也有如 hyperopt，SMAC3，featuretools 等。可以通过学习这些工具框架，了解 AutoML 系统的架构与实现方式，并应用到实际项目中。 自我考核：使用一种 AutoML 系统来进行项目的模型自动优化，并与手工优化的结果进行比较，看是否有所提升，及寻找背后的原因。 6.2 模型解释 6.2.1 模型解释技术 主要有三个方面，一是模型本身的解释性，例如线性回归，决策树等，模型结构简单，根据其原理，可以直接对预测结果，特征使用等方面给出解释。另外一些复杂模型，例如 EBM，神经网络，Bayesian rule lists，SLIMs 等，也可以利用一些本身的特性给出一些解释，例如 GradCAM 方法等。二是模型无关的解释方法，包括经典的 PDP，ICE 等特征图，LIME 等 surrogate model 方法，以及基于博弈论的 Shapley 方法。三是基于 sample 的解释方法，例如 conterfactual explanations，adversarial examples，prototypes，influential instances，kNN 等，不过看起来这类方法对于计算的开销一般都会比较大，不太容易在工程中实现落地。这方面的资料可以学习《Interpretable Machine Learning》和《Explainable AI》（关于深度学习的内容会更多）。另外学术界也有很多前沿探索，比如针对模型解释的降维工作，自动的时间序列分析及报告生成，因果模型，模型公平性及社会影响等方面，可以保持关注。 自我考核：理解 LIME，Shapley 的运作原理，并分析其局限性，尝试提出改进方案。 6.2.2 模型解释应用 从工具框架方面，有许多可以使用的开源项目，例如微软的 interpret，eli5，shap，AIX360 等。另外也有一些非传统意义上的模型解释，例如 manifold，tensorboard 这类模型 debugging 工具，自动化的误差分析与模型改进方案，因果模型框架，模型公平性评估与纠正工具等，都可以涵盖在广义的模型解释领域中。在工具基础上，如何结合业务领域知识，给出更有针对性的解释方案，也是值得思考深挖的方向。 自我考核：使用 shap，eli5 等工具来进行模型解释，并在此基础上形成面向开发者的模型 debug，误差分析及改进方案，或形成面向业务的 what-if 分析看板。 总结 目前机器学习应用领域还在高速发展与演进过程中，除了上述提到的技能方向，后续很可能会不断有新的主题引入进来，需要练就快速学习并应用落地的能力。在掌握前面编程，软件工程，机器学习的基础上，后半部分的研究方向，大家可以根据个人兴趣，选择几个进行深入探索与实践。仅阅读相关书籍和文章，只能对知识内容有一个初步的认识，必须要通过深入的动手实践，反复试错思考和修正，才能逐渐内化为自己的技能，并构建起较为坚实的知识体系。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"Deep SORT多目标跟踪算法代码解析","slug":"Deep-SORT多目标跟踪算法代码解析","date":"2021-08-28T12:46:21.000Z","updated":"2021-08-28T12:57:03.221Z","comments":true,"path":"2021/08/28/Deep-SORT多目标跟踪算法代码解析/","link":"","permalink":"https://leezhao415.github.io/2021/08/28/Deep-SORT%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"文章目录 Deep SORT 多目标跟踪算法代码解析 1. MOT 主要步骤 2. SORT 3. Deep SORT 4. Deep SORT 代码解析 4.1 类图 4.2 核心模块 5. 流程解析 6. 总结 Deep SORT 多目标跟踪算法代码解析 Deep SORT 是多目标跟踪 (Multi-Object Tracking) 中常用到的一种算法，是一个 Detection Based Tracking 的方法。这个算法工业界关注度非常高，在知乎上有很多文章都是使用了 Deep SORT 进行工程部署。笔者将参考前辈的博客，结合自己的实践 (理论 &amp; 代码) 对 Deep SORT 算法进行代码层面的解析。 在之前笔者写的一篇 Deep SORT 论文阅读总结中，总结了 DeepSORT 论文中提到的核心观点，如果对 Deep SORT 不是很熟悉，可以先理解一下，然后再来看解读代码的部分。 由于知乎对文章篇幅有限制，所以分上下篇发。 上篇将梳理 SORT、Deep SORT，以类图为主，讲解 DeepSORT 代码部分的各个模块。 下篇主要是梳理运行的流程，对照流程图进行代码层面理解。以及最后的总结 + 代码推荐。 1. MOT 主要步骤 在《DEEP LEARNING IN VIDEO MULTI-OBJECT TRACKING: A SURVEY》这篇基于深度学习的多目标跟踪的综述中，描述了 MOT 问题中四个主要步骤： 给定视频原始帧。 运行目标检测器如 Faster R-CNN、YOLOv3、SSD 等进行检测，获取目标检测框。 将所有目标框中对应的目标抠出来，进行特征提取（包括表观特征或者运动特征）。 进行相似度计算，计算前后两帧目标之间的匹配程度（前后属于同一个目标的之间的距离比较小，不同目标的距离比较大） 数据关联，为每个对象分配目标的 ID。 以上就是四个核心步骤，其中核心是检测，SORT 论文的摘要中提到，仅仅换一个更好的检测器，就可以将目标跟踪表现提升 18.9%。 2. SORT Deep SORT 算法的前身是 SORT, 全称是 Simple Online and Realtime Tracking。简单介绍一下，SORT 最大特点是基于 Faster R-CNN 的目标检测方法，并利用卡尔曼滤波算法 + 匈牙利算法，极大提高了多目标跟踪的速度，同时达到了 SOTA 的准确率。 这个算法确实是在实际应用中使用较为广泛的一个算法，核心就是两个算法：卡尔曼滤波和匈牙利算法。 卡尔曼滤波算法分为两个过程，预测和更新。该算法将目标的运动状态定义为 8 个正态分布的向量。 预测：当目标经过移动，通过上一帧的目标框和速度等参数，预测出当前帧的目标框位置和速度等参数。 更新：预测值和观测值，两个正态分布的状态进行线性加权，得到目前系统预测的状态。 匈牙利算法：解决的是一个分配问题，在 MOT 主要步骤中的计算相似度的，得到了前后两帧的相似度矩阵。匈牙利算法就是通过求解这个相似度矩阵，从而解决前后两帧真正匹配的目标。这部分 sklearn 库有对应的函数 linear_assignment 来进行求解。 SORT 算法中是通过前后两帧 IOU 来构建相似度矩阵，所以 SORT 计算速度非常快。 下图是一张 SORT 核心算法流程图： Detections 是通过目标检测器得到的目标框，Tracks 是一段轨迹。核心是匹配的过程与卡尔曼滤波的预测和更新过程。 流程如下：目标检测器得到目标框 Detections，同时卡尔曼滤波器预测当前的帧的 Tracks, 然后将 Detections 和 Tracks 进行 IOU 匹配，最终得到的结果分为： Unmatched Tracks，这部分被认为是失配，Detection 和 Track 无法匹配，如果失配持续了 次，该目标 ID 将从图片中删除。 Unmatched Detections, 这部分说明没有任意一个 Track 能匹配 Detection, 所以要为这个 detection 分配一个新的 track。 Matched Track，这部分说明得到了匹配。 卡尔曼滤波可以根据 Tracks 状态预测下一帧的目标框状态。 卡尔曼滤波更新是对观测值 (匹配上的 Track) 和估计值更新所有 track 的状态。 3. Deep SORT DeepSort 中最大的特点是加入外观信息，借用了 ReID 领域模型来提取特征，减少了 ID switch 的次数。整体流程图如下： 可以看出，Deep SORT 算法在 SORT 算法的基础上增加了级联匹配 (Matching Cascade)+ 新轨迹的确认 (confirmed)。总体流程就是： 卡尔曼滤波器预测轨迹 Tracks 使用匈牙利算法将预测得到的轨迹 Tracks 和当前帧中的 detections 进行匹配 (级联匹配和 IOU 匹配) 卡尔曼滤波更新。 其中上图中的级联匹配展开如下： 上图非常清晰地解释了如何进行级联匹配，上图由虚线划分为两部分： 上半部分中计算相似度矩阵的方法使用到了外观模型 (ReID) 和运动模型 (马氏距离) 来计算相似度，得到代价矩阵，另外一个则是门控矩阵，用于限制代价矩阵中过大的值。 下半部分中是是级联匹配的数据关联步骤，匹配过程是一个循环 (max age 个迭代，默认为 70)，也就是从 missing age=0 到 missing age=70 的轨迹和 Detections 进行匹配，没有丢失过的轨迹优先匹配，丢失较为久远的就靠后匹配。通过这部分处理，可以重新将被遮挡目标找回，降低被遮挡然后再出现的目标发生的 ID Switch 次数。 将 Detection 和 Track 进行匹配，所以出现几种情况 Detection 和 Track 匹配，也就是 Matched Tracks。普通连续跟踪的目标都属于这种情况，前后两帧都有目标，能够匹配上。 Detection 没有找到匹配的 Track，也就是 Unmatched Detections。图像中突然出现新的目标的时候，Detection 无法在之前的 Track 找到匹配的目标。 Track 没有找到匹配的 Detection，也就是 Unmatched Tracks。连续追踪的目标超出图像区域，Track 无法与当前任意一个 Detection 匹配。 以上没有涉及一种特殊的情况，就是两个目标遮挡的情况。刚刚被遮挡的目标的 Track 也无法匹配 Detection，目标暂时从图像中消失。之后被遮挡目标再次出现的时候，应该尽量让被遮挡目标分配的 ID 不发生变动，减少 ID Switch 出现的次数，这就需要用到级联匹配了。 4. Deep SORT 代码解析 论文中提供的代码是如下地址: https://github.com/nwojke/deep_sort 上图是 Github 库中有关 Deep SORT 的核心代码，不包括 Faster R-CNN 检测部分，所以主要将讲解这部分的几个文件，笔者也对其中核心代码进行了部分注释，地址在: https://github.com/pprp/deep_sort_yolov3_pytorch , 将其中的目标检测器换成了 U 版的 yolov3, 将 deep_sort 文件中的核心进行了调用。 4.1 类图 下图是笔者总结的这几个类调用的类图 (不是特别严谨，但是能大概展示各个模块的关系)： DeepSort 是核心类，调用其他模块，大体上可以分为三个模块： ReID 模块，用于提取表观特征，原论文中是生成了 128 维的 embedding。 Track 模块，轨迹类，用于保存一个 Track 的状态信息，是一个基本单位。 Tracker 模块，Tracker 模块掌握最核心的算法，卡尔曼滤波和匈牙利算法都是通过调用这个模块来完成的。 DeepSort 类对外接口非常简单： 12self.deepsort = DeepSort(args.deepsort_checkpoint)#实例化outputs = self.deepsort.update(bbox_xcycwh, cls_conf, im)#通过接收目标检测结果进行更新 在外部调用的时候只需要以上两步即可，非常简单。 通过类图，对整体模块有了框架上理解，下面深入理解一下这些模块。 4.2 核心模块 Detection 类 1234567891011121314151617181920212223class Detection(object): &quot;&quot;&quot; This class represents a bounding box detection in a single image. &quot;&quot;&quot; def __init__(self, tlwh, confidence, feature): self.tlwh = np.asarray(tlwh, dtype=np.float) self.confidence = float(confidence) self.feature = np.asarray(feature, dtype=np.float32) def to_tlbr(self): &quot;&quot;&quot;Convert bounding box to format `(min x, min y, max x, max y)`, i.e., `(top left, bottom right)`. &quot;&quot;&quot; ret = self.tlwh.copy() ret[2:] += ret[:2] return ret def to_xyah(self): &quot;&quot;&quot;Convert bounding box to format `(center x, center y, aspect ratio, height)`, where the aspect ratio is `width / height`. &quot;&quot;&quot; ret = self.tlwh.copy() ret[:2] += ret[2:] / 2 ret[2] /= ret[3] return ret Detection 类用于保存通过目标检测器得到的一个检测框，包含 top left 坐标 + 框的宽和高，以及该 bbox 的置信度还有通过 reid 获取得到的对应的 embedding。除此以外提供了不同 bbox 位置格式的转换方法： tlwh: 代表左上角坐标 + 宽高 tlbr: 代表左上角坐标 + 右下角坐标 xyah: 代表中心坐标 + 宽高比 + 高 Track 类 12345678910111213141516171819202122232425262728293031class Track: # 一个轨迹的信息，包含(x,y,a,h) &amp; v &quot;&quot;&quot; A single target track with state space `(x, y, a, h)` and associated velocities, where `(x, y)` is the center of the bounding box, `a` is the aspect ratio and `h` is the height. &quot;&quot;&quot; def __init__(self, mean, covariance, track_id, n_init, max_age, feature=None): # max age是一个存活期限，默认为70帧,在 self.mean = mean self.covariance = covariance self.track_id = track_id self.hits = 1 # hits和n_init进行比较 # hits每次update的时候进行一次更新（只有match的时候才进行update） # hits代表匹配上了多少次，匹配次数超过n_init就会设置为confirmed状态 self.age = 1 # 没有用到，和time_since_update功能重复 self.time_since_update = 0 # 每次调用predict函数的时候就会+1 # 每次调用update函数的时候就会设置为0 self.state = TrackState.Tentative self.features = [] # 每个track对应多个features, 每次更新都将最新的feature添加到列表中 if feature is not None: self.features.append(feature) self._n_init = n_init # 如果连续n_init帧都没有出现失配，设置为deleted状态 self._max_age = max_age # 上限 Track 类主要存储的是轨迹信息，mean 和 covariance 是保存的框的位置和速度信息，track_id 代表分配给这个轨迹的 ID。state 代表框的状态，有三种： Tentative: 不确定态，这种状态会在初始化一个 Track 的时候分配，并且只有在连续匹配上 n_init 帧才会转变为确定态。如果在处于不确定态的情况下没有匹配上任何 detection，那将转变为删除态。 Confirmed: 确定态，代表该 Track 确实处于匹配状态。如果当前 Track 属于确定态，但是失配连续达到 max age 次数的时候，就会被转变为删除态。 Deleted: 删除态，说明该 Track 已经失效。 max_age 代表一个 Track 存活期限，他需要和 time_since_update 变量进行比对。time_since_update 是每次轨迹调用 predict 函数的时候就会 + 1，每次调用 predict 的时候就会重置为 0，也就是说如果一个轨迹长时间没有 update (没有匹配上) 的时候，就会不断增加，直到 time_since_update 超过 max age (默认 70)，将这个 Track 从 Tracker 中的列表删除。 hits 代表连续确认多少次，用在从不确定态转为确定态的时候。每次 Track 进行 update 的时候，hits 就会 + 1, 如果 hits&gt;n_init (默认为 3)，也就是连续三帧的该轨迹都得到了匹配，这时候才将不确定态转为确定态。 需要说明的是每个轨迹还有一个重要的变量，features 列表，存储该轨迹在不同帧对应位置通过 ReID 提取到的特征。为何要保存这个列表，而不是将其更新为当前最新的特征呢？这是为了解决目标被遮挡后再次出现的问题，需要从以往帧对应的特征进行匹配。另外，如果特征过多会严重拖慢计算速度，所以有一个参数 budget 用来控制特征列表的长度，取最新的 budget 个 features, 将旧的删除掉。 ReID 特征提取部分 ReID 网络是独立于目标检测和跟踪器的模块，功能是提取对应 bounding box 中的 feature, 得到一个固定维度的 embedding 作为该 bbox 的代表，供计算相似度时使用。 12345678910111213141516171819202122232425262728293031323334353637383940class Extractor(object): def __init__(self, model_name, model_path, use_cuda=True): self.net = build_model(name=model_name, num_classes=96) self.device = &quot;cuda&quot; if torch.cuda.is_available( ) and use_cuda else &quot;cpu&quot; state_dict = torch.load(model_path)[&#x27;net_dict&#x27;] self.net.load_state_dict(state_dict) print(&quot;Loading weights from &#123;&#125;... Done!&quot;.format(model_path)) self.net.to(self.device) self.size = (128,128) self.norm = transforms.Compose([ transforms.ToTensor(), transforms.Normalize([0.3568, 0.3141, 0.2781], [0.1752, 0.1857, 0.1879]) ]) def _preprocess(self, im_crops): &quot;&quot;&quot; TODO: 1. to float with scale from 0 to 1 2. resize to (64, 128) as Market1501 dataset did 3. concatenate to a numpy array 3. to torch Tensor 4. normalize &quot;&quot;&quot; def _resize(im, size): return cv2.resize(im.astype(np.float32) / 255., size) im_batch = torch.cat([ self.norm(_resize(im, self.size)).unsqueeze(0) for im in im_crops ],dim=0).float() return im_batch def __call__(self, im_crops): im_batch = self._preprocess(im_crops) with torch.no_grad(): im_batch = im_batch.to(self.device) features = self.net(im_batch) return features.cpu().numpy() 模型训练是按照传统 ReID 的方法进行，使用 Extractor 类的时候输入为一个 list 的图片，得到图片对应的特征。 NearestNeighborDistanceMetric 类 这个类中用到了两个计算距离的函数： 计算欧氏距离 123456789101112131415def _pdist(a, b): # 用于计算成对的平方距离 # a NxM 代表N个对象，每个对象有M个数值作为embedding进行比较 # b LxM 代表L个对象，每个对象有M个数值作为embedding进行比较 # 返回的是NxL的矩阵，比如dist[i][j]代表a[i]和b[j]之间的平方和距离 # 实现见：https://blog.csdn.net/frankzd/article/details/80251042 a, b = np.asarray(a), np.asarray(b) # 拷贝一份数据 if len(a) == 0 or len(b) == 0: return np.zeros((len(a), len(b))) a2, b2 = np.square(a).sum(axis=1), np.square( b).sum(axis=1) # 求每个embedding的平方和 # sum(N) + sum(L) -2 x [NxM]x[MxL] = [NxL] r2 = -2. * np.dot(a, b.T) + a2[:, None] + b2[None, :] r2 = np.clip(r2, 0., float(np.inf)) return r2 计算余弦距离 1234567891011def _cosine_distance(a, b, data_is_normalized=False): # a和b之间的余弦距离 # a : [NxM] b : [LxM] # 余弦距离 = 1 - 余弦相似度 # https://blog.csdn.net/u013749540/article/details/51813922 if not data_is_normalized: # 需要将余弦相似度转化成类似欧氏距离的余弦距离。 a = np.asarray(a) / np.linalg.norm(a, axis=1, keepdims=True) # np.linalg.norm 操作是求向量的范式，默认是L2范式，等同于求向量的欧式距离。 b = np.asarray(b) / np.linalg.norm(b, axis=1, keepdims=True) return 1. - np.dot(a, b.T) 以上代码对应公式，注意余弦距离 = 1 - 余弦相似度。 最近邻距离度量类 12345678910111213141516171819202122232425262728293031323334353637383940414243class NearestNeighborDistanceMetric(object): # 对于每个目标，返回一个最近的距离 def __init__(self, metric, matching_threshold, budget=None): # 默认matching_threshold = 0.2 budge = 100 if metric == &quot;euclidean&quot;: # 使用最近邻欧氏距离 self._metric = _nn_euclidean_distance elif metric == &quot;cosine&quot;: # 使用最近邻余弦距离 self._metric = _nn_cosine_distance else: raise ValueError(&quot;Invalid metric; must be either &#x27;euclidean&#x27; or &#x27;cosine&#x27;&quot;) self.matching_threshold = matching_threshold # 在级联匹配的函数中调用 self.budget = budget # budge 预算，控制feature的多少 self.samples = &#123;&#125; # samples是一个字典&#123;id-&gt;feature list&#125; def partial_fit(self, features, targets, active_targets): # 作用：部分拟合，用新的数据更新测量距离 # 调用：在特征集更新模块部分调用，tracker.update()中 for feature, target in zip(features, targets): self.samples.setdefault(target, []).append(feature) # 对应目标下添加新的feature，更新feature集合 # 目标id : feature list if self.budget is not None: self.samples[target] = self.samples[target][-self.budget:] # 设置预算，每个类最多多少个目标，超过直接忽略 # 筛选激活的目标 self.samples = &#123;k: self.samples[k] for k in active_targets&#125; def distance(self, features, targets): # 作用：比较feature和targets之间的距离，返回一个代价矩阵 # 调用：在匹配阶段，将distance封装为gated_metric, # 进行外观信息(reid得到的深度特征)+ # 运动信息(马氏距离用于度量两个分布相似程度) cost_matrix = np.zeros((len(targets), len(features))) for i, target in enumerate(targets): cost_matrix[i, :] = self._metric(self.samples[target], features) return cost_matrix Tracker 类 Tracker 类是最核心的类，Tracker 中保存了所有的轨迹信息，负责初始化第一帧的轨迹、卡尔曼滤波的预测和更新、负责级联匹配、IOU 匹配等等核心工作。 123456789101112131415161718192021222324252627282930class Tracker: # 是一个多目标tracker，保存了很多个track轨迹 # 负责调用卡尔曼滤波来预测track的新状态+进行匹配工作+初始化第一帧 # Tracker调用update或predict的时候，其中的每个track也会各自调用自己的update或predict &quot;&quot;&quot; This is the multi-target tracker. &quot;&quot;&quot; def __init__(self, metric, max_iou_distance=0.7, max_age=70, n_init=3): # 调用的时候，后边的参数全部是默认的 self.metric = metric # metric是一个类，用于计算距离(余弦距离或马氏距离) self.max_iou_distance = max_iou_distance # 最大iou，iou匹配的时候使用 self.max_age = max_age # 直接指定级联匹配的cascade_depth参数 self.n_init = n_init # n_init代表需要n_init次数的update才会将track状态设置为confirmed self.kf = kalman_filter.KalmanFilter()# 卡尔曼滤波器 self.tracks = [] # 保存一系列轨迹 self._next_id = 1 # 下一个分配的轨迹id def predict(self): # 遍历每个track都进行一次预测 &quot;&quot;&quot;Propagate track state distributions one time step forward. This function should be called once every time step, before `update`. &quot;&quot;&quot; for track in self.tracks: track.predict(self.kf) 然后来看最核心的 update 函数和 match 函数，可以对照下面的流程图一起看： update 函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def update(self, detections): # 进行测量的更新和轨迹管理 &quot;&quot;&quot;Perform measurement update and track management. Parameters ---------- detections : List[deep_sort.detection.Detection] A list of detections at the current time step. &quot;&quot;&quot; # Run matching cascade. matches, unmatched_tracks, unmatched_detections = \\ self._match(detections) # Update track set. # 1. 针对匹配上的结果 for track_idx, detection_idx in matches: # track更新对应的detection self.tracks[track_idx].update(self.kf, detections[detection_idx]) # 2. 针对未匹配的tracker,调用mark_missed标记 # track失配，若待定则删除，若update时间很久也删除 # max age是一个存活期限，默认为70帧 for track_idx in unmatched_tracks: self.tracks[track_idx].mark_missed() # 3. 针对未匹配的detection， detection失配，进行初始化 for detection_idx in unmatched_detections: self._initiate_track(detections[detection_idx]) # 得到最新的tracks列表，保存的是标记为confirmed和Tentative的track self.tracks = [t for t in self.tracks if not t.is_deleted()] # Update distance metric. active_targets = [t.track_id for t in self.tracks if t.is_confirmed()] # 获取所有confirmed状态的track id features, targets = [], [] for track in self.tracks: if not track.is_confirmed(): continue features += track.features # 将tracks列表拼接到features列表 # 获取每个feature对应的track id targets += [track.track_id for _ in track.features] track.features = [] # 距离度量中的 特征集更新 self.metric.partial_fit(np.asarray(features), np.asarray(targets), active_targets) match 函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778def _match(self, detections): # 主要功能是进行匹配，找到匹配的，未匹配的部分 def gated_metric(tracks, dets, track_indices, detection_indices): # 功能： 用于计算track和detection之间的距离，代价函数 # 需要使用在KM算法之前 # 调用： # cost_matrix = distance_metric(tracks, detections, # track_indices, detection_indices) features = np.array([dets[i].feature for i in detection_indices]) targets = np.array([tracks[i].track_id for i in track_indices]) # 1. 通过最近邻计算出代价矩阵 cosine distance cost_matrix = self.metric.distance(features, targets) # 2. 计算马氏距离,得到新的状态矩阵 cost_matrix = linear_assignment.gate_cost_matrix( self.kf, cost_matrix, tracks, dets, track_indices, detection_indices) return cost_matrix # Split track set into confirmed and unconfirmed tracks. # 划分不同轨迹的状态 confirmed_tracks = [ i for i, t in enumerate(self.tracks) if t.is_confirmed() ] unconfirmed_tracks = [ i for i, t in enumerate(self.tracks) if not t.is_confirmed() ] # 进行级联匹配，得到匹配的track、不匹配的track、不匹配的detection &#x27;&#x27;&#x27; !!!!!!!!!!! 级联匹配 !!!!!!!!!!! &#x27;&#x27;&#x27; # gated_metric-&gt;cosine distance # 仅仅对确定态的轨迹进行级联匹配 matches_a, unmatched_tracks_a, unmatched_detections = \\ linear_assignment.matching_cascade( gated_metric, self.metric.matching_threshold, self.max_age, self.tracks, detections, confirmed_tracks) # 将所有状态为未确定态的轨迹和刚刚没有匹配上的轨迹组合为iou_track_candidates， # 进行IoU的匹配 iou_track_candidates = unconfirmed_tracks + [ k for k in unmatched_tracks_a if self.tracks[k].time_since_update == 1 # 刚刚没有匹配上 ] # 未匹配 unmatched_tracks_a = [ k for k in unmatched_tracks_a if self.tracks[k].time_since_update != 1 # 已经很久没有匹配上 ] &#x27;&#x27;&#x27; !!!!!!!!!!! IOU 匹配 对级联匹配中还没有匹配成功的目标再进行IoU匹配 !!!!!!!!!!! &#x27;&#x27;&#x27; # 虽然和级联匹配中使用的都是min_cost_matching作为核心， # 这里使用的metric是iou cost和以上不同 matches_b, unmatched_tracks_b, unmatched_detections = \\ linear_assignment.min_cost_matching( iou_matching.iou_cost, self.max_iou_distance, self.tracks, detections, iou_track_candidates, unmatched_detections) matches = matches_a + matches_b # 组合两部分match得到的结果 unmatched_tracks = list(set(unmatched_tracks_a + unmatched_tracks_b)) return matches, unmatched_tracks, unmatched_detections 以上两部分结合注释和以下流程图可以更容易理解。 级联匹配 下边是论文中给出的级联匹配的伪代码： 以下代码是伪代码对应的实现 1234567891011121314151617181920212223242526272829# 1. 分配track_indices和detection_indicesif track_indices is None: track_indices = list(range(len(tracks)))if detection_indices is None: detection_indices = list(range(len(detections)))unmatched_detections = detection_indicesmatches = []# cascade depth = max age 默认为70for level in range(cascade_depth): if len(unmatched_detections) == 0: # No detections left break track_indices_l = [ k for k in track_indices if tracks[k].time_since_update == 1 + level ] if len(track_indices_l) == 0: # Nothing to match at this level continue # 2. 级联匹配核心内容就是这个函数 matches_l, _, unmatched_detections = \\ min_cost_matching( # max_distance=0.2 distance_metric, max_distance, tracks, detections, track_indices_l, unmatched_detections) matches += matches_lunmatched_tracks = list(set(track_indices) - set(k for k, _ in matches)) 门控矩阵 门控矩阵的作用就是通过计算卡尔曼滤波的状态分布和测量值之间的距离对代价矩阵进行限制。 代价矩阵中的距离是 Track 和 Detection 之间的表观相似度，假如一个轨迹要去匹配两个表观特征非常相似的 Detection，这样就很容易出错，但是这个时候分别让两个 Detection 计算与这个轨迹的马氏距离，并使用一个阈值 gating_threshold 进行限制，所以就可以将马氏距离较远的那个 Detection 区分开，可以降低错误的匹配。 12345678910111213141516def gate_cost_matrix( kf, cost_matrix, tracks, detections, track_indices, detection_indices, gated_cost=INFTY_COST, only_position=False): # 根据通过卡尔曼滤波获得的状态分布，使成本矩阵中的不可行条目无效。 gating_dim = 2 if only_position else 4 gating_threshold = kalman_filter.chi2inv95[gating_dim] # 9.4877 measurements = np.asarray([detections[i].to_xyah() for i in detection_indices]) for row, track_idx in enumerate(track_indices): track = tracks[track_idx] gating_distance = kf.gating_distance( track.mean, track.covariance, measurements, only_position) cost_matrix[row, gating_distance &gt; gating_threshold] = gated_cost # 设置为inf return cost_matrix 卡尔曼滤波器 在 Deep SORT 中，需要估计 Track 的以下状态： 均值：用 8 维向量（x, y, a, h, vx, vy, va, vh）表示。(x,y) 是框的中心坐标，宽高比是 a, 高度 h 以及对应的速度，所有的速度都将初始化为 0。 协方差：表示目标位置信息的不确定程度，用 8x8 的对角矩阵来表示，矩阵对应的值越大，代表不确定程度越高。 下图代表卡尔曼滤波器主要过程： 卡尔曼滤波首先根据当前帧 (time=t) 的状态进行预测，得到预测下一帧的状态 (time=t+1) 得到测量结果，在 Deep SORT 中对应的测量就是 Detection，即目标检测器提供的检测框。 将预测结果和测量结果进行更新。 下面这部分主要参考： https://zhuanlan.zhihu.com/p/90835266 如果对卡尔曼滤波算法有较为深入的了解，可以结合卡尔曼滤波算法和代码进行理解。 预测分两个公式： 第一个公式： 其中 F 是状态转移矩阵，如下图： 第二个公式： P 是当前帧 (time=t) 的协方差，Q 是卡尔曼滤波器的运动估计误差，代表不确定程度。 123456789101112131415161718192021222324252627def predict(self, mean, covariance): # 相当于得到t时刻估计值 # Q 预测过程中噪声协方差 std_pos = [ self._std_weight_position * mean[3], self._std_weight_position * mean[3], 1e-2, self._std_weight_position * mean[3]] std_vel = [ self._std_weight_velocity * mean[3], self._std_weight_velocity * mean[3], 1e-5, self._std_weight_velocity * mean[3]] # np.r_ 按列连接两个矩阵 # 初始化噪声矩阵Q motion_cov = np.diag(np.square(np.r_[std_pos, std_vel])) # x&#x27; = Fx mean = np.dot(self._motion_mat, mean) # P&#x27; = FPF^T+Q covariance = np.linalg.multi_dot(( self._motion_mat, covariance, self._motion_mat.T)) + motion_cov return mean, covariance 更新的公式 123456789101112131415161718192021222324252627282930313233343536373839404142434445def project(self, mean, covariance): # R 测量过程中噪声的协方差 std = [ self._std_weight_position * mean[3], self._std_weight_position * mean[3], 1e-1, self._std_weight_position * mean[3]] # 初始化噪声矩阵R innovation_cov = np.diag(np.square(std)) # 将均值向量映射到检测空间，即Hx&#x27; mean = np.dot(self._update_mat, mean) # 将协方差矩阵映射到检测空间，即HP&#x27;H^T covariance = np.linalg.multi_dot(( self._update_mat, covariance, self._update_mat.T)) return mean, covariance + innovation_covdef update(self, mean, covariance, measurement): # 通过估计值和观测值估计最新结果 # 将均值和协方差映射到检测空间，得到 Hx&#x27; 和 S projected_mean, projected_cov = self.project(mean, covariance) # 矩阵分解 chol_factor, lower = scipy.linalg.cho_factor( projected_cov, lower=True, check_finite=False) # 计算卡尔曼增益K kalman_gain = scipy.linalg.cho_solve( (chol_factor, lower), np.dot(covariance, self._update_mat.T).T, check_finite=False).T # z - Hx&#x27; innovation = measurement - projected_mean # x = x&#x27; + Ky new_mean = mean + np.dot(innovation, kalman_gain.T) # P = (I - KH)P&#x27; new_covariance = covariance - np.linalg.multi_dot(( kalman_gain, projected_cov, kalman_gain.T)) return new_mean, new_covariance 这个公式中，z 是 Detection 的 mean，不包含变化值，状态为 [cx,cy,a,h]。H 是测量矩阵，将 Track 的均值向量 映射到检测空间。计算的 y 是 Detection 和 Track 的均值误差。 R 是目标检测器的噪声矩阵，是一个 4x4 的对角矩阵。 对角线上的值分别为中心点两个坐标以及宽高的噪声。 计算的是卡尔曼增益，是作用于衡量估计误差的权重。 更新后的均值向量 x。 更新后的协方差矩阵。 卡尔曼滤波笔者理解也不是很深入，没有推导过公式，对这部分感兴趣的推荐几个博客： 卡尔曼滤波 + python 写的 demo: https://zhuanlan.zhihu.com/p/113685503?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=801414067897135104 详解 + 推导： https://blog.csdn.net/honyniu/a 5. 流程解析 流程部分主要按照以下流程图来走一遍： 感谢知乎 @猫弟总结的流程图，讲解非常地清晰，如果单纯看代码，非常容易混淆。比如说代价矩阵的计算这部分，连续套了三个函数，才被真正调用。上图将整体流程总结地非常棒。笔者将参考以上流程结合代码进行梳理： 分析 detector 类中的 Deep SORT 调用： 12345678910111213141516171819class Detector(object): def __init__(self, args): self.args = args if args.display: cv2.namedWindow(&quot;test&quot;, cv2.WINDOW_NORMAL) cv2.resizeWindow(&quot;test&quot;, args.display_width, args.display_height) device = torch.device( &#x27;cuda&#x27;) if torch.cuda.is_available() else torch.device(&#x27;cpu&#x27;) self.vdo = cv2.VideoCapture() self.yolo3 = InferYOLOv3(args.yolo_cfg, args.img_size, args.yolo_weights, args.data_cfg, device, conf_thres=args.conf_thresh, nms_thres=args.nms_thresh) self.deepsort = DeepSort(args.deepsort_checkpoint) 初始化 DeepSORT 对象，更新部分接收目标检测得到的框的位置，置信度和图片： 1outputs = self.deepsort.update(bbox_xcycwh, cls_conf, im) 顺着 DeepSORT 类的 update 函数看 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class DeepSort(object): def __init__(self, model_path, max_dist=0.2): self.min_confidence = 0.3 # yolov3中检测结果置信度阈值，筛选置信度小于0.3的detection。 self.nms_max_overlap = 1.0 # 非极大抑制阈值，设置为1代表不进行抑制 # 用于提取图片的embedding,返回的是一个batch图片对应的特征 self.extractor = Extractor(&quot;resnet18&quot;, model_path, use_cuda=True) max_cosine_distance = max_dist # 用在级联匹配的地方，如果大于改阈值，就直接忽略 nn_budget = 100 # 预算，每个类别最多的样本个数，如果超过，删除旧的 # 第一个参数可选&#x27;cosine&#x27; or &#x27;euclidean&#x27; metric = NearestNeighborDistanceMetric(&quot;cosine&quot;, max_cosine_distance, nn_budget) self.tracker = Tracker(metric) def update(self, bbox_xywh, confidences, ori_img): self.height, self.width = ori_img.shape[:2] # generate detections features = self._get_features(bbox_xywh, ori_img) # 从原图中crop bbox对应图片并计算得到embedding bbox_tlwh = self._xywh_to_tlwh(bbox_xywh) detections = [ Detection(bbox_tlwh[i], conf, features[i]) for i, conf in enumerate(confidences) if conf &gt; self.min_confidence ] # 筛选小于min_confidence的目标，并构造一个Detection对象构成的列表 # Detection是一个存储图中一个bbox结果 # 需要：1. bbox(tlwh形式) 2. 对应置信度 3. 对应embedding # run on non-maximum supression boxes = np.array([d.tlwh for d in detections]) scores = np.array([d.confidence for d in detections]) # 使用非极大抑制 # 默认nms_thres=1的时候开启也没有用，实际上并没有进行非极大抑制 indices = non_max_suppression(boxes, self.nms_max_overlap, scores) detections = [detections[i] for i in indices] # update tracker # tracker给出一个预测结果，然后将detection传入，进行卡尔曼滤波操作 self.tracker.predict() self.tracker.update(detections) # output bbox identities # 存储结果以及可视化 outputs = [] for track in self.tracker.tracks: if not track.is_confirmed() or track.time_since_update &gt; 1: continue box = track.to_tlwh() x1, y1, x2, y2 = self._tlwh_to_xyxy(box) track_id = track.track_id outputs.append(np.array([x1, y1, x2, y2, track_id], dtype=np.int)) if len(outputs) &gt; 0: outputs = np.stack(outputs, axis=0) return np.array(outputs) 从这里开始对照以上流程图会更加清晰。在 Deep SORT 初始化的过程中有一个核心 metric，NearestNeighborDistanceMetric 类会在匹配和特征集更新的时候用到。 梳理 DeepSORT 的 update 流程： 根据传入的参数（bbox_xywh, conf, img）使用 ReID 模型提取对应 bbox 的表观特征。 构建 detections 的列表，列表中的内容就是 Detection 类，在此处限制了 bbox 的最小置信度。 使用非极大抑制算法，由于默认 nms_thres=1，实际上并没有用。 Tracker 类进行一次预测，然后将 detections 传入，进行更新。 最后将 Tracker 中保存的轨迹中状态属于确认态的轨迹返回。 以上核心在 Tracker 的 predict 和 update 函数，接着梳理。 Tracker 的 predict 函数 Tracker 是一个多目标跟踪器，保存了很多个 track 轨迹，负责调用卡尔曼滤波来预测 track 的新状态 + 进行匹配工作 + 初始化第一帧。Tracker 调用 update 或 predict 的时候，其中的每个 track 也会各自调用自己的 update 或 predict 12345678910111213141516171819202122class Tracker: def __init__(self, metric, max_iou_distance=0.7, max_age=70, n_init=3): # 调用的时候，后边的参数全部是默认的 self.metric = metric self.max_iou_distance = max_iou_distance # 最大iou，iou匹配的时候使用 self.max_age = max_age # 直接指定级联匹配的cascade_depth参数 self.n_init = n_init # n_init代表需要n_init次数的update才会将track状态设置为confirmed self.kf = kalman_filter.KalmanFilter() # 卡尔曼滤波器 self.tracks = [] # 保存一系列轨迹 self._next_id = 1 # 下一个分配的轨迹id def predict(self): # 遍历每个track都进行一次预测 &quot;&quot;&quot;Propagate track state distributions one time step forward. This function should be called once every time step, before `update`. &quot;&quot;&quot; for track in self.tracks: track.predict(self.kf) predict 主要是对轨迹列表中所有的轨迹使用卡尔曼滤波算法进行状态的预测。 Tracker 的更新 Tracker 的更新属于最核心的部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def update(self, detections): # 进行测量的更新和轨迹管理 &quot;&quot;&quot;Perform measurement update and track management. Parameters ---------- detections : List[deep_sort.detection.Detection] A list of detections at the current time step. &quot;&quot;&quot; # Run matching cascade. matches, unmatched_tracks, unmatched_detections = \\ self._match(detections) # Update track set. # 1. 针对匹配上的结果 for track_idx, detection_idx in matches: # track更新对应的detection self.tracks[track_idx].update(self.kf, detections[detection_idx]) # 2. 针对未匹配的tracker,调用mark_missed标记 # track失配，若待定则删除，若update时间很久也删除 # max age是一个存活期限，默认为70帧 for track_idx in unmatched_tracks: self.tracks[track_idx].mark_missed() # 3. 针对未匹配的detection， detection失配，进行初始化 for detection_idx in unmatched_detections: self._initiate_track(detections[detection_idx]) # 得到最新的tracks列表，保存的是标记为confirmed和Tentative的track self.tracks = [t for t in self.tracks if not t.is_deleted()] # Update distance metric. active_targets = [t.track_id for t in self.tracks if t.is_confirmed()] # 获取所有confirmed状态的track id features, targets = [], [] for track in self.tracks: if not track.is_confirmed(): continue features += track.features # 将tracks列表拼接到features列表 # 获取每个feature对应的track id targets += [track.track_id for _ in track.features] track.features = [] # 距离度量中的 特征集更新 self.metric.partial_fit(np.asarray(features), np.asarray(targets),active_targets) 这部分注释已经很详细了，主要是一些后处理代码，需要关注的是对匹配上的，未匹配的 Detection，未匹配的 Track 三者进行的处理以及最后进行特征集更新部分，可以对照流程图梳理。 Tracker 的 update 函数的核心函数是 match 函数，描述如何进行匹配的流程： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879def _match(self, detections): # 主要功能是进行匹配，找到匹配的，未匹配的部分 def gated_metric(tracks, dets, track_indices, detection_indices): # 功能： 用于计算track和detection之间的距离，代价函数 # 需要使用在KM算法之前 # 调用： # cost_matrix = distance_metric(tracks, detections, # track_indices, detection_indices) features = np.array([dets[i].feature for i in detection_indices]) targets = np.array([tracks[i].track_id for i in track_indices]) # 1. 通过最近邻计算出代价矩阵 cosine distance cost_matrix = self.metric.distance(features, targets) # 2. 计算马氏距离,得到新的状态矩阵 cost_matrix = linear_assignment.gate_cost_matrix( self.kf, cost_matrix, tracks, dets, track_indices, detection_indices) return cost_matrix # Split track set into confirmed and unconfirmed tracks. # 划分不同轨迹的状态 confirmed_tracks = [ i for i, t in enumerate(self.tracks) if t.is_confirmed() ] unconfirmed_tracks = [ i for i, t in enumerate(self.tracks) if not t.is_confirmed() ] # 进行级联匹配，得到匹配的track、不匹配的track、不匹配的detection &#x27;&#x27;&#x27; !!!!!!!!!!! 级联匹配 !!!!!!!!!!! &#x27;&#x27;&#x27; # gated_metric-&gt;cosine distance # 仅仅对确定态的轨迹进行级联匹配 matches_a, unmatched_tracks_a, unmatched_detections = \\ linear_assignment.matching_cascade( gated_metric, self.metric.matching_threshold, self.max_age, self.tracks, detections, confirmed_tracks) # 将所有状态为未确定态的轨迹和刚刚没有匹配上的轨迹组合为iou_track_candidates， # 进行IoU的匹配 iou_track_candidates = unconfirmed_tracks + [ k for k in unmatched_tracks_a if self.tracks[k].time_since_update == 1 # 刚刚没有匹配上 ] # 未匹配 unmatched_tracks_a = [ k for k in unmatched_tracks_a if self.tracks[k].time_since_update != 1 # 已经很久没有匹配上 ] &#x27;&#x27;&#x27; !!!!!!!!!!! IOU 匹配 对级联匹配中还没有匹配成功的目标再进行IoU匹配 !!!!!!!!!!! &#x27;&#x27;&#x27; # 虽然和级联匹配中使用的都是min_cost_matching作为核心， # 这里使用的metric是iou cost和以上不同 matches_b, unmatched_tracks_b, unmatched_detections = \\ linear_assignment.min_cost_matching( iou_matching.iou_cost, self.max_iou_distance, self.tracks, detections, iou_track_candidates, unmatched_detections) matches = matches_a + matches_b # 组合两部分match得到的结果 unmatched_tracks = list(set(unmatched_tracks_a + unmatched_tracks_b)) return matches, unmatched_tracks, unmatched_detections 对照下图来看会顺畅很多： 可以看到，匹配函数的核心是级联匹配 + IOU 匹配，先来看看级联匹配： 调用在这里： 1234567matches_a, unmatched_tracks_a, unmatched_detections = linear_assignment.matching_cascade( gated_metric, self.metric.matching_threshold, self.max_age, self.tracks, detections, confirmed_tracks) 级联匹配函数展开： 1234567891011121314151617181920212223242526272829303132333435def matching_cascade( distance_metric, max_distance, cascade_depth, tracks, detections, track_indices=None, detection_indices=None): # 级联匹配 # 1. 分配track_indices和detection_indices if track_indices is None: track_indices = list(range(len(tracks))) if detection_indices is None: detection_indices = list(range(len(detections))) unmatched_detections = detection_indices matches = [] # cascade depth = max age 默认为70 for level in range(cascade_depth): if len(unmatched_detections) == 0: # No detections left break track_indices_l = [ k for k in track_indices if tracks[k].time_since_update == 1 + level ] if len(track_indices_l) == 0: # Nothing to match at this level continue # 2. 级联匹配核心内容就是这个函数 matches_l, _, unmatched_detections = \\ min_cost_matching( # max_distance=0.2 distance_metric, max_distance, tracks, detections, track_indices_l, unmatched_detections) matches += matches_l unmatched_tracks = list(set(track_indices) - set(k for k, _ in matches)) return matches, unmatched_tracks, unmatched_detections 可以看到和伪代码是一致的，文章上半部分也有提到这部分代码。这部分代码中还有一个核心的函数 min_cost_matching，这个函数可以接收不同的 distance_metric，在级联匹配和 IoU 匹配中都有用到。 min_cost_matching 函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def min_cost_matching( distance_metric, max_distance, tracks, detections, track_indices=None, detection_indices=None): if track_indices is None: track_indices = np.arange(len(tracks)) if detection_indices is None: detection_indices = np.arange(len(detections)) if len(detection_indices) == 0 or len(track_indices) == 0: return [], track_indices, detection_indices # Nothing to match. # ----------------------------------------- # Gated_distance——&gt; # 1. cosine distance # 2. 马氏距离 # 得到代价矩阵 # ----------------------------------------- # iou_cost——&gt; # 仅仅计算track和detection之间的iou距离 # ----------------------------------------- cost_matrix = distance_metric( tracks, detections, track_indices, detection_indices) # ----------------------------------------- # gated_distance中设置距离中最高上限， # 这里最远距离实际是在deep sort类中的max_dist参数设置的 # 默认max_dist=0.2， 距离越小越好 # ----------------------------------------- # iou_cost情况下，max_distance的设置对应tracker中的max_iou_distance, # 默认值为max_iou_distance=0.7 # 注意结果是1-iou，所以越小越好 # ----------------------------------------- cost_matrix[cost_matrix &gt; max_distance] = max_distance + 1e-5 # 匈牙利算法或者KM算法 row_indices, col_indices = linear_assignment(cost_matrix) matches, unmatched_tracks, unmatched_detections = [], [], [] # 这几个for循环用于对匹配结果进行筛选，得到匹配和未匹配的结果 for col, detection_idx in enumerate(detection_indices): if col not in col_indices: unmatched_detections.append(detection_idx) for row, track_idx in enumerate(track_indices): if row not in row_indices: unmatched_tracks.append(track_idx) for row, col in zip(row_indices, col_indices): track_idx = track_indices[row] detection_idx = detection_indices[col] if cost_matrix[row, col] &gt; max_distance: unmatched_tracks.append(track_idx) unmatched_detections.append(detection_idx) else: matches.append((track_idx, detection_idx)) # 得到匹配，未匹配轨迹，未匹配检测 return matches, unmatched_tracks, unmatched_detections 注释中提到 distance_metric 是有两个的： 第一个是级联匹配中传入的 distance_metric 是 gated_metric, 其内部核心是计算的表观特征的级联匹配。 1234567891011121314151617def gated_metric(tracks, dets, track_indices, detection_indices): # 功能： 用于计算track和detection之间的距离，代价函数 # 需要使用在KM算法之前 # 调用： # cost_matrix = distance_metric(tracks, detections, # track_indices, detection_indices) features = np.array([dets[i].feature for i in detection_indices]) targets = np.array([tracks[i].track_id for i in track_indices]) # 1. 通过最近邻计算出代价矩阵 cosine distance cost_matrix = self.metric.distance(features, targets) # 2. 计算马氏距离,得到新的状态矩阵 cost_matrix = linear_assignment.gate_cost_matrix( self.kf, cost_matrix, tracks, dets, track_indices, detection_indices) return cost_matrix 对应下图进行理解 (下图上半部分就是对应的 gated_metric 函数)： 第二个是 IOU 匹配中的 iou_matching.iou_cost: 12345678910# 虽然和级联匹配中使用的都是min_cost_matching作为核心，# 这里使用的metric是iou cost和以上不同matches_b, unmatched_tracks_b, unmatched_detections = \\ linear_assignment.min_cost_matching( iou_matching.iou_cost, self.max_iou_distance, self.tracks, detections, iou_track_candidates, unmatched_detections) iou_cost 代价很容易理解，用于计算 Track 和 Detection 之间的 IOU 距离矩阵。 1234567891011121314151617181920def iou_cost(tracks, detections, track_indices=None, detection_indices=None): # 计算track和detection之间的iou距离矩阵 if track_indices is None: track_indices = np.arange(len(tracks)) if detection_indices is None: detection_indices = np.arange(len(detections)) cost_matrix = np.zeros((len(track_indices), len(detection_indices))) for row, track_idx in enumerate(track_indices): if tracks[track_idx].time_since_update &gt; 1: cost_matrix[row, :] = linear_assignment.INFTY_COST continue bbox = tracks[track_idx].to_tlwh() candidates = np.asarray( [detections[i].tlwh for i in detection_indices]) cost_matrix[row, :] = 1. - iou(bbox, candidates) return cost_matrix 6. 总结 以上就是 Deep SORT 算法代码部分的解析，核心在于类图和流程图，理解 Deep SORT 实现的过程。 如果第一次接触到多目标跟踪算法领域的，可以到知乎上看这篇文章以及其系列，对新手非常友好： https://zhuanlan.zhihu.com/p/62827974 笔者也收集了一些多目标跟踪领域中认可度比较高、常见的库，在这里分享给大家： SORT 官方代码： https://github.com/abewley/sort DeepSORT 官方代码： https://github.com/nwojke/deep_sort 奇点大佬 keras 实现 DeepSORT: https://github.com/Qidian213/deep_sort_yolov3 CenterNet 作检测器的 DeepSORT: https://github.com/xingyizhou/CenterTrack 和 https://github.com/kimyoon-young/centerNet-deep-sort JDE Github 地址: https://github.com/Zhongdao/Towards-Realtime-MOT FairMOT Github 地址: https://github.com/ifzhang/FairMOT 笔者修改的代码： https://github.com/pprp/deep_sort_yolov3_pytorch 笔者也是最近一段时间接触目标跟踪领域，数学水平非常有限 (卡尔曼滤波只能肤浅了解大概过程，但是还不会推导)。本文目标就是帮助新入门多目标跟踪的新人快速了解 Deep SORT 流程，由于自身水平有限，也欢迎大佬对文中不足之处进行指点一二。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"MOT","slug":"MOT","permalink":"https://leezhao415.github.io/tags/MOT/"}]},{"title":"【详解】IoU、GIoU、DIoU、CIoU损失函数","slug":"【详解】IoU、GIoU、DIoU、CIoU损失函数","date":"2021-08-28T08:45:30.000Z","updated":"2021-08-28T08:50:22.362Z","comments":true,"path":"2021/08/28/【详解】IoU、GIoU、DIoU、CIoU损失函数/","link":"","permalink":"https://leezhao415.github.io/2021/08/28/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91IoU%E3%80%81GIoU%E3%80%81DIoU%E3%80%81CIoU%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","excerpt":"","text":"文章目录 一、IOU (Intersection over Union) 二、GIOU (Generalized Intersection over Union) 三、DIoU (Distance-IoU) 四、CIoU (Complete-IoU) 五、损失函数在 YOLOv3 上的性能 (论文效果) 一、IOU (Intersection over Union) 1. 特性 (优点) IoU 就是我们所说的交并比，是目标检测中最常用的指标，在 anchor-based 的方法中，他的作用不仅用来确定正样本和负样本，还可以用来评价输出框（predict box）和 ground-truth 的距离。 可以说它可以反映预测检测框与真实检测框的检测效果。 还有一个很好的特性就是尺度不变性，也就是对尺度不敏感（scale invariant）， 在 regression 任务中，判断 predict box 和 gt 的距离最直接的指标就是 IoU。(满足非负性；同一性；对称性；三角不等性) 12345678910111213141516171819202122import numpy as npdef Iou(box1, box2, wh=False): if wh == False: xmin1, ymin1, xmax1, ymax1 = box1 xmin2, ymin2, xmax2, ymax2 = box2 else: xmin1, ymin1 = int(box1[0]-box1[2]/2.0), int(box1[1]-box1[3]/2.0) xmax1, ymax1 = int(box1[0]+box1[2]/2.0), int(box1[1]+box1[3]/2.0) xmin2, ymin2 = int(box2[0]-box2[2]/2.0), int(box2[1]-box2[3]/2.0) xmax2, ymax2 = int(box2[0]+box2[2]/2.0), int(box2[1]+box2[3]/2.0) # 获取矩形框交集对应的左上角和右下角的坐标（intersection） xx1 = np.max([xmin1, xmin2]) yy1 = np.max([ymin1, ymin2]) xx2 = np.min([xmax1, xmax2]) yy2 = np.min([ymax1, ymax2]) # 计算两个矩形框面积 area1 = (xmax1-xmin1) * (ymax1-ymin1) area2 = (xmax2-xmin2) * (ymax2-ymin2) inter_area = (np.max([0, xx2-xx1])) * (np.max([0, yy2-yy1])) #计算交集面积 iou = inter_area / (area1+area2-inter_area+1e-6) #计算交并比 return iou 2. 作为损失函数会出现的问题 (缺点) 如果两个框没有相交，根据定义，IoU=0，不能反映两者的距离大小（重合度）。同时因为 loss=0，没有梯度回传，无法进行学习训练。 IoU 无法精确的反映两者的重合度大小。如下图所示，三种情况 IoU 都相等，但看得出来他们的重合度是不一样的，左边的图回归的效果最好，右边的最差。 二、GIOU (Generalized Intersection over Union) 1、来源 在 CVPR2019 中，论文 《Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression》arxiv.org/abs/1902.09630 的提出了 GIoU 的思想。由于 IoU 是比值的概念，对目标物体的 scale 是不敏感的。然而检测任务中的 BBox 的回归损失 (MSE loss, l1-smooth loss 等）优化和 IoU 优化不是完全等价的，而且 Ln 范数对物体的 scale 也比较敏感，IoU 无法直接优化没有重叠的部分。 这篇论文提出可以直接把 IoU 设为回归的 loss。 GIoU 对 scale 不敏感 GIoU 是 IoU 的下界，在两个框无限重合的情况下，IoU=GIoU=1 IoU 取值 [0,1]，但 GIoU 有对称区间，取值范围 [-1,1]。在两者重合的时候取最大值 1，在两者无交集且无限远的时候取最小值 - 1，因此 GIoU 是一个非常好的距离度量指标。 与 IoU 只关注重叠区域不同，GIoU 不仅关注重叠区域，还关注其他的非重合区域，能更好的反映两者的重合度。 12345678910111213141516171819202122def Giou(rec1,rec2): #分别是第一个矩形左右上下的坐标 x1,x2,y1,y2 = rec1 x3,x4,y3,y4 = rec2 iou = Iou(rec1,rec2) area_C = (max(x1,x2,x3,x4)-min(x1,x2,x3,x4))*(max(y1,y2,y3,y4)-min(y1,y2,y3,y4)) area_1 = (x2-x1)*(y1-y2) area_2 = (x4-x3)*(y3-y4) sum_area = area_1 + area_2 w1 = x2 - x1 #第一个矩形的宽 w2 = x4 - x3 #第二个矩形的宽 h1 = y1 - y2 h2 = y3 - y4 W = min(x1,x2,x3,x4)+w1+w2-max(x1,x2,x3,x4) #交叉部分的宽 H = min(y1,y2,y3,y4)+h1+h2-max(y1,y2,y3,y4) #交叉部分的高 Area = W*H #交叉的面积 add_area = sum_area - Area #两矩形并集的面积 end_area = (area_C - add_area)/area_C #闭包区域中不属于两个框的区域占闭包区域的比重 giou = iou - end_area return giou 三、DIoU (Distance-IoU) 1、来源 DIoU 要比 GIou 更加符合目标框回归的机制，将目标与 anchor 之间的距离，重叠率以及尺度都考虑进去，使得目标框回归变得更加稳定，不会像 IoU 和 GIoU 一样出现训练过程中发散等问题。论文中 Distance-IoUarxiv.org/pdf/1911.08287.pdf 基于 IoU 和 GIoU 存在的问题，作者提出了两个问题： 1. 直接最小化 anchor 框与目标框之间的归一化距离是否可行，以达到更快的收敛速度？ 2. 如何使回归在与目标框有重叠甚至包含时更准确、更快？ 其中，b, bgt 分别代表了预测框和真实框的中心点，且 ρ 代表的是计算两个中心点间的欧式距离。 c 代表的是能够同时包含预测框和真实框的最小闭包区域的对角线距离。 DIoU 中对 anchor 框和目标框之间的归一化距离进行了建模 附： YOLOV3 DIoU GitHub 项目地址：github.com/Zzh-tju/DIoU-darknet 2、优点 DIoU loss 可以直接最小化两个目标框的距离，因此比 GIoU loss 收敛快得多。 对于包含两个框在水平方向和垂直方向上这种情况，DIoU 损失可以使回归非常快，而 GIoU 损失几乎退化为 IoU 损失。 DIoU 还可以替换普通的 IoU 评价策略，应用于 NMS 中，使得 NMS 得到的结果更加合理和有效。 实现代码： 1234567891011121314151617181920212223242526272829303132333435363738394041def Diou(bboxes1, bboxes2): rows = bboxes1.shape[0] cols = bboxes2.shape[0] dious = torch.zeros((rows, cols)) if rows * cols == 0:# return dious exchange = False if bboxes1.shape[0] &gt; bboxes2.shape[0]: bboxes1, bboxes2 = bboxes2, bboxes1 dious = torch.zeros((cols, rows)) exchange = True # #xmin,ymin,xmax,ymax-&gt;[:,0],[:,1],[:,2],[:,3] w1 = bboxes1[:, 2] - bboxes1[:, 0] h1 = bboxes1[:, 3] - bboxes1[:, 1] w2 = bboxes2[:, 2] - bboxes2[:, 0] h2 = bboxes2[:, 3] - bboxes2[:, 1] area1 = w1 * h1 area2 = w2 * h2 center_x1 = (bboxes1[:, 2] + bboxes1[:, 0]) / 2 center_y1 = (bboxes1[:, 3] + bboxes1[:, 1]) / 2 center_x2 = (bboxes2[:, 2] + bboxes2[:, 0]) / 2 center_y2 = (bboxes2[:, 3] + bboxes2[:, 1]) / 2 inter_max_xy = torch.min(bboxes1[:, 2:],bboxes2[:, 2:]) inter_min_xy = torch.max(bboxes1[:, :2],bboxes2[:, :2]) out_max_xy = torch.max(bboxes1[:, 2:],bboxes2[:, 2:]) out_min_xy = torch.min(bboxes1[:, :2],bboxes2[:, :2]) inter = torch.clamp((inter_max_xy - inter_min_xy), min=0) inter_area = inter[:, 0] * inter[:, 1] inter_diag = (center_x2 - center_x1)**2 + (center_y2 - center_y1)**2 outer = torch.clamp((out_max_xy - out_min_xy), min=0) outer_diag = (outer[:, 0] ** 2) + (outer[:, 1] ** 2) union = area1+area2-inter_area dious = inter_area / union - (inter_diag) / outer_diag dious = torch.clamp(dious,min=-1.0,max = 1.0) if exchange: dious = dious.T return dious 四、CIoU (Complete-IoU) 实现代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def bbox_overlaps_ciou(bboxes1, bboxes2): rows = bboxes1.shape[0] cols = bboxes2.shape[0] cious = torch.zeros((rows, cols)) if rows * cols == 0: return cious exchange = False if bboxes1.shape[0] &gt; bboxes2.shape[0]: bboxes1, bboxes2 = bboxes2, bboxes1 cious = torch.zeros((cols, rows)) exchange = True w1 = bboxes1[:, 2] - bboxes1[:, 0] h1 = bboxes1[:, 3] - bboxes1[:, 1] w2 = bboxes2[:, 2] - bboxes2[:, 0] h2 = bboxes2[:, 3] - bboxes2[:, 1] area1 = w1 * h1 area2 = w2 * h2 center_x1 = (bboxes1[:, 2] + bboxes1[:, 0]) / 2 center_y1 = (bboxes1[:, 3] + bboxes1[:, 1]) / 2 center_x2 = (bboxes2[:, 2] + bboxes2[:, 0]) / 2 center_y2 = (bboxes2[:, 3] + bboxes2[:, 1]) / 2 inter_max_xy = torch.min(bboxes1[:, 2:],bboxes2[:, 2:]) inter_min_xy = torch.max(bboxes1[:, :2],bboxes2[:, :2]) out_max_xy = torch.max(bboxes1[:, 2:],bboxes2[:, 2:]) out_min_xy = torch.min(bboxes1[:, :2],bboxes2[:, :2]) inter = torch.clamp((inter_max_xy - inter_min_xy), min=0) inter_area = inter[:, 0] * inter[:, 1] inter_diag = (center_x2 - center_x1)**2 + (center_y2 - center_y1)**2 outer = torch.clamp((out_max_xy - out_min_xy), min=0) outer_diag = (outer[:, 0] ** 2) + (outer[:, 1] ** 2) union = area1+area2-inter_area u = (inter_diag) / outer_diag iou = inter_area / union with torch.no_grad(): arctan = torch.atan(w2 / h2) - torch.atan(w1 / h1) v = (4 / (math.pi ** 2)) * torch.pow((torch.atan(w2 / h2) - torch.atan(w1 / h1)), 2) S = 1 - iou alpha = v / (S + v) w_temp = 2 * w1 ar = (8 / (math.pi ** 2)) * arctan * ((w1 - w_temp) * h1) cious = iou - (u + alpha * ar) cious = torch.clamp(cious,min=-1.0,max = 1.0) if exchange: cious = cious.T return cious 五、损失函数在 YOLOv3 上的性能 (论文效果) 目标检测算法之 AAAI 2020 DIoU Loss 已开源 (YOLOV3 涨近 3 个点)：cloud.tencent.com/developer/article/1558533","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"IOU","slug":"IOU","permalink":"https://leezhao415.github.io/tags/IOU/"}]},{"title":"Python 实现的十大经典排序算法","slug":"Python-实现的十大经典排序算法","date":"2021-08-28T07:15:08.000Z","updated":"2021-08-28T07:19:41.342Z","comments":true,"path":"2021/08/28/Python-实现的十大经典排序算法/","link":"","permalink":"https://leezhao415.github.io/2021/08/28/Python-%E5%AE%9E%E7%8E%B0%E7%9A%84%E5%8D%81%E5%A4%A7%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","excerpt":"","text":"文章目录 Python 实现的十大经典排序算法 需求 划分方法 常见排序方法 选择排序（Selection Sort） 冒泡排序（Bubble Sort） 插入排序（Insertion Sort） 希尔排序（Shell Sort） 归并排序（Merge Sort） 快速排序（Quick Sort） 堆排序（Heap Sort） 计数排序（Counting Sort） 桶排序（Bucket Sort） 基数排序（Radix Sort） 算法总结 Python 实现的十大经典排序算法 参考链接：https://blog.csdn.net/MobiusStrip/article/details/83785159?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task 需求 对一个无序数组，根据某个关键字排序。 划分方法 排序算法划分方法有：稳定性，内外排序，时空复杂度 按照稳定性划分，稳定排序，如果 a 原本在 b 前面，而 a=b ，排序之后 a 仍然在 b 的前面；而不稳定可能出现在 b 之后。 按照内外排序划分，内排序，所有排序操作都在内存中完成；外排序 ：由于数据太大，因此把数据放在磁盘中，而排序通过磁盘和内存的数据传输才能进行； 按照时空复杂度划分，时间复杂度是指运行时间，空间复杂度运行完一个程序所需内存的大小。 常见排序方法 选择排序（Selection Sort） 这应该最符合人类思维的排序方法，工作原理，首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 稳定性：用数组实现的选择排序是不稳定的，用链表实现的选择排序是稳定的；内排序； 1234567def selection_sort(nums): n = len(nums) for i in range(n): for j in range(i, n): if nums[i] &gt; nums[j]: nums[i], nums[j] = nums[j], nums[i] return nums 算法分析： 时间复杂度：O (n2) ， n 是数组长度 冒泡排序（Bubble Sort） 冒泡排序时针对相邻元素之间的比较，可以将大的数慢慢 “沉底”(数组尾部) 12345678def bubble_sort(nums): n = len(nums) # 进行多次循环 for c in range(n): for i in range(1, n - c): if nums[i - 1] &gt; nums[i]: nums[i - 1], nums[i] = nums[i], nums[i - 1] return nums 算法分析： 稳定排序，内排序，时间复杂度：O (n2) 插入排序（Insertion Sort） 插入排序是前面已排序数组找到插入的位置 1234567def insertion_sort(nums): n = len(nums) for i in range(1, n): while i &gt; 0 and nums[i - 1] &gt; nums[i]: nums[i - 1], nums[i] = nums[i], nums[i - 1] i -= 1 return nums 算法分析： 稳定排序，内排序，时间复杂度：O (n2) 希尔排序（Shell Sort） 插入排序进阶版， 算法描述： 我们来看下希尔排序的基本步骤，在此我们选择增量 gap=length/2 ，缩小增量继续以 gap = gap/2 的方式，这种增量选择我们可以用一个序列来表示， &#123;n/2,(n/2)/2…1&#125; ，称为增量序列。希尔排序的增量序列的选择与证明是个数学难题，我们选择的这个增量序列是比较常用的，也是希尔建议的增量，称为希尔增量，但其实这个增量序列不是最优的。 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 步骤 1：选择一个增量序列 t1，t2，…，tk，其中 ti&gt;tj，tk=1； 步骤 2：按增量序列个数 k，对序列进行 k 趟排序； 步骤 3：每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 12345678910def shell_sort(nums): n = len(nums) gap = n // 2 while gap: for i in range(gap, n): while i - gap &gt;= 0 and nums[i - gap] &gt; nums[i]: nums[i - gap], nums[i] = nums[i], nums[i - gap] i -= gap gap //= 2 return nums 算法分析： 非稳定排序，内排序； 希尔排序的时间复杂度和增量序列是相关的。 &#123;1,2,4,8,...&#125; 这种序列并不是很好的增量序列，使用这个增量序列的时间复杂度（最坏情形）是 O (n2)； Hibbard 提出了另一个增量序列 1,3,7，…,2k-1，这种序列的时间复杂度 (最坏情形) 为 O (n1.5)； Sedgewick 提出了几种增量序列，其最坏情形运行时间为 O (n1.3)，其中最好的一个序列是 &#123;1,5,19,41,109,...&#125; ； 对于不同增量的复杂度感兴趣可以参考《数据结构与算法分析》一书或其他相关论文。 归并排序（Merge Sort） 归并排序，采用是分治法，先将数组分成子序列，让子序列有序，再将子序列间有序，合并成有序数组。 算法描述： 把长度为 n 的输入序列分成长度 n/2 的子序列； 对两个子序列采用归并排序； 合并所有子序列。 12345678910111213141516171819202122232425def merge_sort(nums): if len(nums) &lt;= 1: return nums mid = len(nums) // 2 # 分 left = merge_sort(nums[:mid]) right = merge_sort(nums[mid:]) # 合并 return merge(left, right)def merge(left, right): res = [] i = 0 j = 0 while i &lt; len(left) and j &lt; len(right): if left[i] &lt;= right[j]: res.append(left[i]) i += 1 else: res.append(right[j]) j += 1 res += left[i:] res += right[j:] return res 算法分析： 稳定排序，外排序（占用额外内存），时间复杂度 O (nlogn)。 快速排序（Quick Sort） 快速排序是选取一个 “哨兵”( pivot )，将小于 pivot 放在左边，把大于 pivot 放在右边，分割成两部分，并且可以固定 pivot 在数组的位置，在对左右两部分继续进行排序。 快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 步骤 1：从数列中挑出一个元素，称为 “基准”（pivot ）； 步骤 2：重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 步骤 3：递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 123456789101112131415161718192021def quick_sort(nums): n = len(nums) def quick(left, right): if left &gt;= right: return nums pivot = left i = left j = right while i &lt; j: while i &lt; j and nums[j] &gt; nums[pivot]: j -= 1 while i &lt; j and nums[i] &lt;= nums[pivot]: i += 1 nums[i], nums[j] = nums[j], nums[i] nums[pivot], nums[j] = nums[j], nums[pivot] quick(left, j - 1) quick(j + 1, right) return nums return quick(0, n - 1) 算法分析： 不稳定排序，内排序，时间复杂度度 O (nlogn)。 堆排序（Heap Sort） 堆排序是利用堆这个数据结构设计的排序算法。 算法描述： 建堆，从底向上调整堆，使得父亲节点比孩子节点值大，构成大顶堆； 交换堆顶和最后一个元素，重新调整堆。 调整堆方法写了递归和迭代，都很好理解！ 12345678910111213141516171819202122232425262728293031323334353637383940def heap_sort(nums): # 调整堆 # 迭代写法 def adjust_heap(nums, startpos, endpos): newitem = nums[startpos] pos = startpos childpos = pos * 2 + 1 while childpos &lt; endpos: rightpos = childpos + 1 if rightpos &lt; endpos and nums[rightpos] &gt;= nums[childpos]: childpos = rightpos if newitem &lt; nums[childpos]: nums[pos] = nums[childpos] pos = childpos childpos = pos * 2 + 1 else: break nums[pos] = newitem # 递归写法 def adjust_heap(nums, startpos, endpos): pos = startpos chilidpos = pos * 2 + 1 if chilidpos &lt; endpos: rightpos = chilidpos + 1 if rightpos &lt; endpos and nums[rightpos] &gt; nums[chilidpos]: chilidpos = rightpos if nums[chilidpos] &gt; nums[pos]: nums[pos], nums[chilidpos] = nums[chilidpos], nums[pos] adjust_heap(nums, pos, endpos) n = len(nums) # 建堆 for i in reversed(range(n // 2)): adjust_heap(nums, i, n) # 调整堆 for i in range(n - 1, -1, -1): nums[0], nums[i] = nums[i], nums[0] adjust_heap(nums, 0, i) return nums 算法分析： 不稳定排序，内排序，时间复杂度为 O (nlogn)。 计数排序（Counting Sort） 计数排序是典型的空间换时间算法，开辟额外数据空间存储用索引号记录数组的值和数组值个数 算法描述： 找出待排序的数组的最大值和最小值 统计数组值的个数 反向填充目标数组 123456789101112131415def counting_sort(nums): if not nums: return [] n = len(nums) _min = min(nums) _max = max(nums) tmp_arr = [0] * (_max - _min + 1) for num in nums: tmp_arr[num - _min] += 1 j = 0 for i in range(n): while tmp_arr[j] == 0: j += 1 nums[i] = j + _min tmp_arr[j] -= 1 return nums 算法分析： 稳定排序，外排序，时间复杂度 O (n + k)，但是对于数据范围很大的数组，需要大量时间和内存。 桶排序（Bucket Sort） 桶排序是计数排序的升级版，原理是：输入数据服从均匀分布的，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的算法或是以递归方式继续使用桶排序，此文编码采用递归方式） 算法描述： 人为设置一个桶的 BucketSize ，作为每个桶放置多少个不同数值（意思就是 BucketSize = 5 ，可以放 5 个不同数字比如 [1, 2, 3,4,5] 也可以放 100000 个 3 ，只是表示该桶能存几个不同的数值） 遍历待排序数据，并且把数据一个一个放到对应的桶里去 对每个不是桶进行排序，可以使用其他排序方法，也递归排序 不是空的桶里数据拼接起来 1def bucket_sort(nums, bucketSize): if len(nums) &lt; 2: return nums _min = min(nums) _max = max(nums) # 需要桶个数 bucketNum = (_max - _min) // bucketSize + 1 buckets = [[] for _ in range(bucketNum)] for num in nums: # 放入相应的桶中 buckets[(num - _min) // bucketSize].append(num) res = [] for bucket in buckets: if not bucket: continue if bucketSize == 1: res.extend(bucket) else: # 当都装在一个桶里,说明桶容量大了 if bucketNum == 1: bucketSize -= 1 res.extend(bucket_sort(bucket, bucketSize)) return res 算法分析： 稳定排序，外排序，时间复杂度 O (n + k)， k 为桶的个数。 基数排序（Radix Sort） 基数排序是对数字每一位进行排序，从最低位开始排序 算法描述： 找到数组最大值，得最大位数； 从最低位开始取每个位组成 radix 数组； 对 radix 进行计数排序（计数排序适用于小范围的特点）。 1def Radix_sort(nums): if not nums: return [] _max = max(nums) # 最大位数 maxDigit = len(str(_max)) bucketList = [[] for _ in range(10)] # 从低位开始排序 div, mod = 1, 10 for i in range(maxDigit): for num in nums: bucketList[num % mod // div].append(num) div *= 10 mod *= 10 idx = 0 for j in range(10): for item in bucketList[j]: nums[idx] = item idx += 1 bucketList[j] = [] return nums 算法分析： 稳定排序，外排序，时间复杂度 posCount * (n + n) ，其中 posCount 为数组中最大元素的最高位数；简化下得：$O (k *n) $；其中 k 为常数，n 为元素个数。 算法总结 图片名词解释： n: 数据规模 k: “桶” 的个数 In-place: 占用常数内存，不占用额外内存 Out-place: 占用额外内存","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://leezhao415.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}]},{"title":"Hadoop详解","slug":"Hadoop详解","date":"2021-08-09T05:32:12.000Z","updated":"2021-08-09T06:01:55.072Z","comments":true,"path":"2021/08/09/Hadoop详解/","link":"","permalink":"https://leezhao415.github.io/2021/08/09/Hadoop%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"文章目录 作者：动力节点在线 链接：https://www.zhihu.com/question/333417513/answer/742465814 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 1、Hadoop 是什么 1.1、小故事版本的解释 小明接到一个任务：计算一个 100M 的文本文件中的单词的个数，这个文本文件有若干行，每行有若干个单词，每行的单词与单词之间都是以空格键分开的。对于处理这种 100M 量级数据的计算任务，小明感觉很轻松。他首先把这个 100M 的文件拷贝到自己的电脑上，然后写了个计算程序在他的计算机上执行后顺利输出了结果。 后来，小明接到了另外一个任务，计算一个 1T（1024G）的文本文件中的单词的个数。再后来，小明又接到一个任务，计算一个 1P (1024T) 的文本文件中的单词的个数…… 面对这样大规模的数据，小明的那一台计算机已经存储不下了，也计算不了这样大的数据文件中到底有多少个单词了。机智的小明上网百度了一下，他在百度的输入框中写下了：大数据存储和计算怎么办？按下回车键之后，出现了有关 Hadoop 的网页。 看了很多网页之后，小明总结一句话：Hadoop 就是存储海量数据和分析海量数据的工具。 1.2、稍专业点的解释 Hadoop 是由 java 语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架，其核心部件是 HDFS 与 MapReduce。 ​ HDFS 是一个分布式文件系统：引入存放文件元数据信息的服务器 Namenode 和实际存放数据的服务器 Datanode，对数据进行分布式储存和读取。 MapReduce 是一个计算框架：MapReduce 的核心思想是把计算任务分配给集群内的服务器里执行。通过对计算任务的拆分（Map 计算 / Reduce 计算）再根据任务调度器（JobTracker）对任务进行分布式计算。 1.3、记住下面的话 ​ Hadoop 的框架最核心的设计就是：HDFS 和 MapReduce。HDFS 为海量的数据提供了存储，则 MapReduce 为海量的数据提供了计算。 ​ 把 HDFS 理解为一个分布式的，有冗余备份的，可以动态扩展的用来存储大规模数据的大硬盘。 ​ 把 MapReduce 理解成为一个计算引擎，按照 MapReduce 的规则编写 Map 计算 / Reduce 计算的程序，可以完成计算任务。 2、Hadoop 能干什么 大数据存储：分布式存储 日志处理：擅长日志分析 ETL: 数据抽取到 oracle、mysql、DB2、mongdb 及主流数据库 机器学习：比如 Apache Mahout 项目 搜索引擎：Hadoop + lucene 实现 数据挖掘：目前比较流行的广告推荐，个性化广告推荐 Hadoop 是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式。 实际应用： （1）Flume+Logstash+Kafka+Spark Streaming 进行实时日志处理分析 （2）酷狗音乐的大数据平台 3、怎么使用 Hadoop 3.1、Hadoop 集群的搭建 无论是在 windows 上装几台虚拟机玩 Hadoop，还是真实的服务器来玩，说简单点就是把 Hadoop 的安装包放在每一台服务器上，改改配置，启动就完成了 Hadoop 集群的搭建。 3.2、上传文件到 Hadoop 集群 Hadoop 集群搭建好以后，可以通过 web 页面查看集群的情况，还可以通过 Hadoop 命令来上传文件到 hdfs 集群，通过 Hadoop 命令在 hdfs 集群上建立目录，通过 Hadoop 命令删除集群上的文件等等。 3.3、编写 map/reduce 程序 通过集成开发工具（例如 eclipse）导入 Hadoop 相关的 jar 包，编写 map/reduce 程序，将程序打成 jar 包扔在集群上执行，运行后出计算结果。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"大数据框架","slug":"大数据框架","permalink":"https://leezhao415.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"}]},{"title":"深度学习目标检测工具箱mmdetection","slug":"深度学习目标检测工具箱mmdetection","date":"2021-08-08T09:42:19.000Z","updated":"2021-08-08T10:34:27.513Z","comments":true,"path":"2021/08/08/深度学习目标检测工具箱mmdetection/","link":"","permalink":"https://leezhao415.github.io/2021/08/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E7%AE%B1mmdetection/","excerpt":"","text":"文章目录 MMDetection 详解 简介 1. 安装文档 — 手把手教你如何搭建 MMDetection 框架 2. 入门文档 — 教你如何快速上手 MMDetection 3. 基准和模型汇总 — 展示 MMDetection 上各个模型的准确率和使用方法 4. 技术细节 — 手把手教你如何实现一个新的网络 5. 更新日志 —MMDetection 版本升级改动 6. 目标检测比赛中的 tricks 7.mmdetection 的 configs 中的各项参数具体解释 MMDetection 详解 简介 商汤科技（2018 COCO 目标检测挑战赛冠军）和香港中文大学最近开源了一个基于 Pytorch 实现的深度学习目标检测工具箱 mmdetection，支持 Faster-RCNN，Mask-RCNN，Fast-RCNN 等主流的目标检测框架，后续会加入 Cascade-RCNN 以及其他一系列目标检测框架。 相比于 Facebook 开源的 Detectron 框架，作者声称 mmdetection 有三点优势：performance 稍高、训练速度稍快、所需显存稍小。、 以增添 mmdetection 的 configs 中的各项参数具体解释，以及目标检测比赛的一些小技巧（比赛专用）。 1. 安装文档 — 手把手教你如何搭建 MMDetection 框架 初识 CV：MMDetection 中文文档 —1. 安装 2. 入门文档 — 教你如何快速上手 MMDetection 初识 CV：MMDetection 中文文档 —2. 入门 3. 基准和模型汇总 — 展示 MMDetection 上各个模型的准确率和使用方法 初识 CV：MMDetection 中文文档 —3. 基准和模型动物园 4. 技术细节 — 手把手教你如何实现一个新的网络 初识 CV：MMDetection 中文文档 —4. 技术细节 5. 更新日志 —MMDetection 版本升级改动 初识 CV：MMDetection 中文文档 —5. 更新日志 6. 目标检测比赛中的 tricks 初识 CV：目标检测比赛中的 tricks（已更新更多代码解析） 7.mmdetection 的 configs 中的各项参数具体解释 初识 CV：mmdetection 的 configs 中的各项参数具体解释","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"CV/目标检测工具箱","slug":"CV-目标检测工具箱","permalink":"https://leezhao415.github.io/tags/CV-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E7%AE%B1/"}]},{"title":"常见的NLG评估指标","slug":"常见的NLG评估指标","date":"2021-08-08T09:41:58.000Z","updated":"2021-08-08T10:33:28.957Z","comments":true,"path":"2021/08/08/常见的NLG评估指标/","link":"","permalink":"https://leezhao415.github.io/2021/08/08/%E5%B8%B8%E8%A7%81%E7%9A%84NLG%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/","excerpt":"","text":"文章目录 常见的 NLG 评估指标 引言 BLEU 评估法（机器翻译） ROUGE 评估法（自动摘要） Rouge-N Rouge-L Rouge-W Rouge-S METEOR 评估法（机器翻译、自动文摘） CIDEr 评价方法 常见的 NLG 评估指标 引言 如何判定训练出来的模型好与坏呢？关键是要有一个比较好的模型评估方法，那么今天作者就给大家汇总一下自然语言生成（NLG）中经常见到的无监督自评估方法（BLEU、METEOR、ROUGE、CIDEr）（含评估代码）。 BLEU 评估法（机器翻译） Bleu 全称为 Bilingual Evaluation Understudy（双语评估研究） ，意为双语评估替换，是 IBM 在 2002 年提出的用于机器翻译的一个评测指标，是衡量一个有多个正确输出结果的模型的精确度的评估指标。 BLEU 的设计思想与评判机器翻译好坏的思想是一致的：机器翻译结果越接近专业人工翻译的结果，则越好。BLEU 算法实际上在做的事：判断两个句子的相似程度。我想知道一个句子翻译前后的表示是否意思一致，显然没法直接比较，那我就拿这个句子的标准人工翻译与我的机器翻译的结果作比较，如果它们是很相似的，说明我的翻译很成功。因此，BLUE 去做判断：一句机器翻译的话与其相对应的几个参考翻译作比较，算出一个综合分数。这个分数越高说明机器翻译得越好。 举个例子：下面拿中英机器翻译做例子： 中文：垫上有一只老鼠。 参考翻译 1：The cat is on the mat. 参考翻译 1：There is a cat on the mat. MT (机器翻译):the cat the cat on the mat. bleu 的得分有一元组，二元组，三元组等等，这里做了 1-3 元组的例子，如下： 下面先计算 BELU 一元组得分，即先把 MT 输出的句子拆分成 the,cat,on,mat，频数分别为 3,2,1,1: 上面的 Count (clip) 叫截取计数，是取每个单词在所有参考翻译句子中，出现最多的次数，the 在参考翻译 1 中出现 2 次，在参考翻译 2 中出现 1 次，所以 the 的 Count (clip) 取最大值就是 2，剩下的单词依次类推。 所以 BLEU 的一元组上的得分为： p1 = Count (clip)/Count=（2+1+1+1）/(3+2+1+1) =5/7 下面再计算 BLEU 的二元组得分： 参考翻译 1：The cat is on the mat. 参考翻译 1：There is a cat on the mat. MT (机器翻译):the cat the cat on the mat. 所以 bleu 的二元组的得分为：p2 = Count (clip)/Count=（1+0+1+1+1）/(2+1+1+1+1) =4/6=2/3 同理 BELU 的三元组得分： 参考翻译 1：The cat is on the mat. 参考翻译 1：There is a cat on the mat. MT (机器翻译):the cat the cat on the mat. 所以 bleu 的三元组的得分为：p3 = Count (clip)/Count= 2/5；最后加所有元组的 bleu 得分都加起来然后取平均数得: bleu（avg） = （p1+p2+p3）/3 = (5/7+2/3+2/5)/3 = 0.594 最后再乘上一个 “简短惩罚” BP（brevity penalty），即最后的 bleu 得分为：Bleu (total)=BP * bleu (avg)。 这里为什么要乘以 BP：如果 MT 输出了一个非常短的翻译，那么会更容易得到一个高精度的 bleu，因为输出的大部分词都会出现在参考翻译中，所以我们并不想要特别短的翻译结果，所以加入 BP 这么一个调整因子： 上式中，r 为参考翻译的句子长度，c 为 MT 的输出句子长度，若 c&lt;=r , 则 0&lt;exp (1-r/c)&lt;=1, 得分 bleu (avg) 就会乘以小于 1 的系数 ，从而被 “惩罚”。 那么最后：Bleu (total)=BP*bleu (avg) ROUGE 评估法（自动摘要） Rouge (recall-oriented understanding for gisting evaluation) 是评估自动文摘以及机器翻译的一组指标。 论文链接地址：http://citeseer.ist.psu.edu/viewdoc/download 该方法的主要是思想是：由多个专家分别生成人工摘要，构成标准摘要集。将系统生成的自动摘要与人工生成的标准摘要相对比，通过统计二者之间重叠的基本单元（n 元语法、词序列和词对）的数目，来评价摘要的质量。通过多专家人工摘要的对比，提高评价系统的稳定性和健壮性。该方法现在已经成为摘要评价技术的通用标准之一。关于该算法演变评价标准有：Rouge-N、Rouge-L、Rouge-S、Rouge-W、Rouge-SU。 Rouge-N 其中，n 表示 n-gram 的长度，{Reference Summaries} 表示参考摘要，即事先获得的标准摘要，Countmatch (gramn) 表示候选摘要和参考摘要中同时出现 n-gram 的个数，Count (gramn) 则表示参考摘要中出现的 n-gram 个数。不难看出，ROUGE 公式是由召回率的计算公式演变而来的，分子可以看作 “检出的相关文档数目”，即系统生成摘要与标准摘要相匹配的 N-gram 个数，分母可以看作 “相关文档数目”，即标准摘要中所有的 N-gram 个数。具体计算方式具体如下： 通过上面可以看到其实 ROUGE-N 和 BLEU 几乎一模一样，区别是 BLEU 只计算准确率，而 ROUGE 只计算召回率。 优点：直观，简介，能反映词序。 缺点：区分度不高，且当 N&gt;3 时，ROUGE-N 值通常很小。 应用场景：ROUGE-1：短摘要评估，多文档摘要（去停用词条件）;ROUGE-2: 单文档摘要，多文档摘要（去停用词条件）; Rouge-L 子序列：一个给定序列的子序列就是该给定序列中去掉零个或者多个元素。 公共子序列：给定两个序列 X 和 Y，如果 Z 既是 X 的一个子序列又是 Y 的一个子序列，则序列 Z 是 X 和 Y 的一个公共子序列。 LCS（最长公共子序列）：给定两个序列 X 和 Y，使得公共子序列长度最大的序列是 X 和 Y 的最长公共子序列。其计算公式为： 其中 X 为参考摘要，长度为 m，Y 为候选摘要，长度为 n，用 F 值来衡量摘要 X 与 Y 的相似度，在 DUC 测评中，由于 β—&gt;+∞，所以只考虑 Rlcs 。具体计算例子如下： 优点：不要求词的连续匹配，只要求按词的出现顺序匹配即可，能够像 n-gram 一样反映句子级的词序。自动匹配最长公共子序列，不需要预先定义 n-gram 的长度。 缺点：只计算一个最长子序列，最终的值忽略了其他备选的最长子序列及较短子序列的影响。 应用场景：单文档摘要；短摘要评估。 将 LCS 应用到摘要级数相时，对参考摘要中的每一个句子 *ri 与候选摘要中的所有句子比对，以 union LCS 作为摘要句 ri * 的匹配结果。计算公式： 其中 R 为参考摘要，包含 u 个句子，m 个词，C 为候选摘要，包含 v 个句子，n 个词，长度为 n，LCSU(ri，C) 是句子 ri 和候选摘要 C 的 union LCS。 Rouge-W 为使连续匹配比不连续匹配赋予更大的权重，公式描述如下： 例如 f(k) = kα，α&gt;1 ，同时为了归一化最终的 Rouge-W 的值，通常选择函数与反函数具有相似形式的函数。例如： f(k) = k2, f-1 = k1/2 ，具体计算公式如下所示： 举个例子如下： 优点：同一 LCS 下，对连续匹配词数多的句子赋予更高权重，比 LCS 区分度更高。 缺点：同 ROUGE-L，只计算一个最长子序列，最终的值忽略了其他备选的最长子序列及较短子序列的影响。 应用场景：单文档摘要；短摘要评估。 Rouge-S Skip-Bigram 是按句子顺序中的任何成对词语。计算公式如下： 其中 X 为参考摘要，长度为 m，Y 为候选摘要，长度为 n。SKIP2 (X，Y) 表示候选摘要与参考摘要的 skip-bigram 匹配次数。 Skip-gram 如果不限制跳跃的距离，会出现很多无意义的词对，比如 “the of”、“in the” 等。为了减少无意义词对的出现，可以限制最大跳跃距离 dskip，通常写 ROUGE-S4 表示最大跳跃距离为 4，ROUGE-S9 表示最大跳跃距离为 9，依次类推。如果 dskip 为 0，那么 ROUGE-S0 = ROUGE-2。举个例子如下： 优点：考虑了所有按词序排列的词对，比 n-gram 模型更深入反映句子级词序。 缺点：若不设定最大跳跃词数会出现很多无意义词对。若设定最大跳跃词数，需要指定最大跳跃词数的值。 应用场景：单文档摘要；ROUGE-S4，ROUGE-S9: 多文档摘要（去停用词条件)。 METEOR 评估法（机器翻译、自动文摘） 2004 年，卡内基梅隆大学的 Lavir 提出评价指标中召回率的意义，基于此研究，Banerjee 和 Lavie（Banerjee and Lavie, 2005）发明了基于单精度的加权调和平均数和单字召回率的 METEOR 度量方法，目的是解决 BLEU 标准中的一些固有缺陷。 论文链接地址：http://www.cs.cmu.edu/~alavie/METEOR/pdf/Banerjee-Lavie-2005-METEOR.pdf METEOR 扩展了 BLEU 有关 “共现” 的概念，提出了三个统计共现次数的模块：一是 “绝对” 模块（“exact” module），即统计待测译文与参考译文中绝对一致单词的共现次数；二是 “波特词干” 模块（porter stem module），即基于波特词干算法计算待测译文与参考译文中词干相同的词语 “变体” 的共现次数，如 happy 和 happiness 将在此模块中被认定为共现词；三是 “WN 同义词” 模块（WN synonymy module），即基于 WordNet 词典匹配待测译文与参考译文中的同义词，计入共现次数，如 sunlight 与 sunshine。 同时 METEOR 将词序纳入评估范畴，设立基于词序变化的罚分机制，当待测译文词序与参考译文不同时，进行适当的罚分。最终基于共现次数计算准确率、召回率与 F 值，并考虑罚分最终得到待测译文的 METEOR 值。 该算法首先计算 unigram 情况下的准确率 P 和召回率 R（计算方式与 BLEU、ROUGE 类似），得到调和均值 F 值： 看到这可能还没有什么特别的。Meteor 的特别之处在于，它不希望生成很 “碎” 的译文：比如参考译文是 “A B C D”，模型给出的译文是 “B A D C”，虽然每个 unigram 都对应上了，但是会受到很严重的惩罚。惩罚因子的计算方式为： 上式中的 chunks 表示匹配上的语块个数，如果模型生成的译文很碎的话，语块个数会非常多；unigrams_matched 表示匹配上的 unigram 个数。所以最终的评分为： 用于机器翻译评测时，通常取 α = 3，γ = 0.5，θ = 3。 自从 2004 年以来，该团队也在不断的对 METEOR 评估方法进行优化，具体可见：http://www.cs.cmu.edu/~alavie/METEOR/index.html CIDEr 评价方法 CIDEr（Consensuus-based Image Description Evaluation）评价标准是 Vedantm 在 2015 年计算机视觉与模式识别大会上提出来的针对图像摘要问题的度量标准。 论文链接地址为：https://arxiv.org/pdf/1411.5726.pdf 研究者认为过去的多种评价方法和人类评价具有较强的相关性，但是无法统一到一个度量标准来评价与人的相似性（human-like），为了解决这个问题，从而评价计算机自动生成的句子到底有多像人工书写的，Vedantam 等人提出了基于共识的评价标准（consensus-based protocol），其基本工作原理就是通过度量带测评语句与其他大部分人工描述句之间的相似性来评价相似性。研究者证明 CIDEr 在与人工共识的匹配度上要好于前述其它评价指标。 CIDEr 首先将 n-grams 在参考句子中的出现频率编码进来，n-gram 在数据集所有图片中经常出现的图片的权重应该减少，因为其包含的信息量更少，该权重研究者通过 TF-IDF 计算每个 n-gram 的权重。将句子用 n-gram 表示成向量形式，每个参考句和待评测句之间通过计算 TF-IDF 项链的余玄距离来度量其相似性。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NLP/评估指标","slug":"NLP-评估指标","permalink":"https://leezhao415.github.io/tags/NLP-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"}]},{"title":"NLP数据增强工具—jionlp","slug":"NLP数据增强工具—jionlp","date":"2021-08-08T09:41:41.000Z","updated":"2021-08-08T10:29:44.490Z","comments":true,"path":"2021/08/08/NLP数据增强工具—jionlp/","link":"","permalink":"https://leezhao415.github.io/2021/08/08/NLP%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7%E2%80%94jionlp/","excerpt":"","text":"文章目录 安装 操作说明 数据增强方法对比 回译数据增强 1 BackTranslation 2 BaiduApi 3 XunfeiApi 4 GoogleApi 5 TecentApi 6 YoudaoApi 7 YoudaoFreeApi 邻近汉字换位 swap_char_position 同音词替换 homophone_substitution 随机增删字符 random_add_delete NER 实体替换 ReplaceEntity 安装 1pip install jionlp 该如何使用呢？可以参考 Github 以下项目 https://github.com/dongrixinyu/JioNLPgithub.com/dongrixinyu/JioNLP 操作说明 其中包含了回译、邻近汉字换位、同音词替换、随机增删字符等方法。具体使用样例如下： 12345678910&gt;&gt;&gt; import jionlp as jio&gt;&gt;&gt; text = &#x27;人口危机如果无法得到及时解决，80后、90后们将受到巨大的冲击。&#x27;&gt;&gt;&gt; res1 = jio.swap_char_position(text) # 邻近汉字换位&gt;&gt;&gt; res2 = jio.homophone_substitution(text) # 同音词替换&gt;&gt;&gt; res3 = jio.random_add_delete(text) # 随机增删字符&gt;&gt;&gt; print(res1[0] + &#x27;\\n&#x27; + res2[0] + &#x27;\\n&#x27; + res3[0])# 人口危机如果无法得时及到解决，80后、90后们将受到巨大的冲击。# 人口危机如果无法得到即时解决，80后、90后们将受到巨大的冲击。# 人D口危机如果无法得到及时解决，80后90后们将到巨大的冲击。 数据增强方法对比 文本数据增强的两个前提： 1、不干扰模型标签：文本增强后的语义不干扰模型训练，不会导致样本标签失效；如 “这个小吃真好吃。=&gt; 正面情绪” 增强为 “这个小吃真不好吃。=&gt; 正面情绪”，随机加字影响到了标签的正确性。 2、人可理解：增强后文本，依然保持可读性，达到人可以理解文本的含义；如 “这个小吃真好吃。” 增强为 “斯口吃真好吃。”，其中 “这个” 替换为 “斯”，“小吃” 替换为 “口吃”，已经完全令人无法理解，模型训练也已偏离。此问题在 同义词替换上非常频繁与普遍。 方法 任务类型 效果 回译 文本分类、序列标注、匹配、文本生成 基于机翻效果决定，目前对新闻通用领域效果较好，专项领域视语料决定。长短文本均适合 邻近汉字换位 文本分类、匹配 汉字换位会影响具体实体的含义，在实体含义并不影响整体语义情况下适用。换位汉字占比不宜过大 同音词替换 文本分类、匹配 同音词替换会对局部语义产生影响，造成误差，但对整体语义理解并无干扰。替换词汇的占比不宜过大 随机增删符号 文本分类、匹配 在文本中随机增删不影响语义的额外非中文符号。增加比例不宜过大，若某类字符（数字、字母）对语义有影响，则应该规避此类字符 NER 实体替换 文本分类、匹配、序列标注 在文本中随机替换不影响语义的实体。如人名、地名、机构等实体 同义词替换 - 造成语言连贯性差，语义完全被曲解的概率非常大。此种方法作废，本工具包不支持。具体解释见 jio.random_add_delete.__doc__ 语言模型预测 分类、匹配、文本生成 利用大型的语言模型如 bert 等，预测句子中空缺的词汇。此种方法依赖大型的语言模型参数，本工具暂不支持 回译数据增强 1 BackTranslation 2 BaiduApi 3 XunfeiApi 4 GoogleApi 5 TecentApi 6 YoudaoApi 7 YoudaoFreeApi 给定一段文本，利用各类大厂公开的免费 api，对文本数据做增强。用户可在各大厂的云平台上自行申请密钥，填在接口的参数中。 各厂申请 API 地址如下： 百度 BaiduApi 有道 YoudaoApi 腾讯 TecentApi 讯飞 XunfeiApi 请注意：样例中的 api 参数并非全都可用，请注册账号在各个厂官网申请自己的 api 参数 123456789101112131415161718192021222324252627282930313233343536373839&gt;&gt;&gt; import jionlp as jio&gt;&gt;&gt; xunfei_api = jio.XunfeiApi( [&#123;&quot;appid&quot;: &quot;5f5846b1&quot;, &quot;api_key&quot;: &quot;52465bb3de9a258379e6909c4b1f2b4b&quot;, &quot;secret&quot;: &quot;b21fdc62a7ed0e287f31cdc4bf4ab9a3&quot;&#125;])&gt;&gt;&gt; tencent_api = jio.TencentApi( [&#123;&quot;project_id&quot;: &quot;0&quot;, &quot;secret_id&quot;: &quot;AKID5zGGuInJwmLehbyKyYXGS3NXOXYLE96o&quot;, &quot;secret_key&quot;: &quot;buwiGXXifLt888rKQLwGH3dsfsdmeCX&quot;&#125;, # 错误的 api &#123;&quot;project_id&quot;: &quot;0&quot;, &quot;secret_id&quot;: &quot;AKID5zGGuInJwmLehbyKyYXGS3NXOXYLE&quot;, &quot;secret_key&quot;: &quot;buwiGXXifLt888rKQLwGH3asuhFbmeCX&quot;&#125;]) # 错误的 api&gt;&gt;&gt; youdao_free_api = jio.YoudaoFreeApi()&gt;&gt;&gt; youdao_api = jio.YoudaoApi( [&#123;&#x27;appid&#x27;: &#x27;39856bd56b482cfc&#x27;, &#x27;app_secret&#x27;: &#x27;87XpTE63nBVnrR0b6Hy0aTDWlkoq2l4A&#x27;&#125;])&gt;&gt;&gt; google_api = jio.GoogleApi()&gt;&gt;&gt; baidu_api = jio.BaiduApi( [&#123;&#x27;appid&#x27;: &#x27;20200618000498778&#x27;, &#x27;secretKey&#x27;: &#x27;raHalLakgYitNuzGOoB2&#x27;&#125;, # 错误的密钥 &#123;&#x27;appid&#x27;: &#x27;20200618000498778&#x27;, &#x27;secretKey&#x27;: &#x27;raHalLakgYitNuzGdsoB2&#x27;&#125;, # 错误的密钥 &#123;&#x27;appid&#x27;: &#x27;20200618000498778&#x27;, &#x27;secretKey&#x27;: &#x27;raHalLakgYitNuzGOoBZ&#x27;&#125;], gap_time=0.5)&gt;&gt;&gt; print(baidu_api.__doc__) # 查看接口说明&gt;&gt;&gt; apis = [baidu_api, youdao_api, google_api, youdao_free_api, tencent_api, xunfei_api]&gt;&gt;&gt; back_trans = jio.BackTranslation(mt_apis=apis)&gt;&gt;&gt; text = &#x27;饿了么凌晨发文将推出新功能，用户可选择是否愿意多等外卖员 5 分钟，你愿意多等这 5 分钟吗？&#x27;&gt;&gt;&gt; print(youdao_api(text)) # 使用接口做单次调用&gt;&gt;&gt; result = back_trans(text)&gt;&gt;&gt; print(result)# [&#x27;饿了么将在凌晨推出一项新功能。用户可以选择是否愿意额外等待外卖人员5分钟。您想多等5分钟吗？&#x27;, # &#x27;《饿了么》将在凌晨推出一档新节目。用户可以选择是否愿意等待餐饮人员多花5分钟。您愿意再等五分钟吗？&#x27;, # &#x27;Ele.me将在早晨的最初几个小时启动一个新的功能。用户可以选择是否准备好再等5分钟。你不想再等五分钟吗？&#x27;, # &#x27;Eleme将在清晨推出新的功能。用户可以选择是否愿意再等5分钟工作人员。你想再等五分钟吗？&#x27;] 原理简述：利用公开的大厂 API 对文本数据做回译增强，即完成从 中文 -&gt; 外文 -&gt; 中文 的翻译过程。 该框架考虑了对各 API 的语言种类支持问题；两次调用之间的等待时间问题；等待超时问题；支持在 API 接口中输入多个密钥（appkey_obj）。 每一个 API 类提供了初始化 lang_pool 参数，用于指定翻译的语种。基于此种考虑：某些小语种的模型效果并不如英语理想，如上例 “饿了么” 句子的翻译，小语种的翻译质量不如英汉互译。 该接口框架包括了常用的若干 API（BaiduApi、XunfeiApi、GoogleApi、TecentApi、YoudaoApi、YoudaoFreeApi），也支持 自定义训练的模型 API 接口 。具体见下。 自定义 API 接口接收一个 str 格式文本输入，输出对应的 str 格式翻译文本； 自定义 API 须指定文本的源语言和目标翻译语言，如 (zh, en) 和 (en, zh)； 自定义 API 在请求调用报错后需要提供 raise Exeption 语句的异常抛出。 自定义 API 接口可参考代码中的写法。 API 接口支持多个密钥，即申请若干个某一厂商的 API，混合在一起调用。框架接口自动选择可用密钥，忽略掉无效密钥。如上例中腾讯和百度的多个密钥，以列表形式传入。 您可自己登录对应大厂的云平台，机器翻译服务页面，申请属于自己的 API 的密钥。使用更高效。 若某些 API 接口效果不理想，可以随意选定若干或指定某个厂商的 API。 各厂机翻评价（个人使用体会，不完全客观）： 厂名 翻译质量 可免费调用数量 百度 中上 大 腾讯 较优 小 有道 中上 大 讯飞 中下 小 谷歌 中上 无穷多但有 ip 反爬限制 邻近汉字换位 swap_char_position 随机交换相邻近字符的位置，用以增强文本数据，理论依据为相邻近汉字顺序变动不影响人的阅读理解。 如 “民盟发言人：昂季素山目前情况良好”，“研表究明，汉字的序顺并不定一能影阅响读”。 1234567&gt;&gt;&gt; import jionlp as jio&gt;&gt;&gt; res = jio.swap_char_position(&#x27;民盟发言人：昂山素季目前情况良好&#x27;)&gt;&gt;&gt; print(res)# [&#x27;民盟发言人：昂季素山目前情况良好&#x27;,# &#x27;民盟发言人：昂山季素目前情况良好&#x27;,# &#x27;民盟发言人：素山昂季目前情况良好&#x27;] 随机交换相近字符的位置，且交换位置的距离以正态分布得到，scale 参数为 1，默认比例为相邻字符交换占 76.4%，中间隔 1 个字符占比 21.8%，中间隔两个字符占比为 1.8% augmentation_num(int) 参数控制返回几条增强后的数据 swap_ratio(float) 参数控制对每一个汉字的调整其位置概率 其余参数参考 jio.swap_char_position.__doc__ 同音词替换 homophone_substitution 采用同音词汇进行原文替换，达到数据增强的目的。汉语输入法中，拼音输入法为目前使用最广泛的一种打字法，使用率占比约 97%。 在实际使用中，常常出现同音词的打字错误，例如：原句为 12# 原句：“人口危机如果无法得到及时解决，80后、90后们将受到巨大的冲击”# 拼输：“人口危机如果无法得到即时解决，80后、90后门将受到巨大的冲击”。 从输入的错误来看，完全不影响人的阅读理解。 1234567&gt;&gt;&gt; import jionlp as jio&gt;&gt;&gt; res = jio.homophone_substitution(&#x27;中国驻英记者一向恪守新闻职业道德，为增进中英两国人民之间的了解和沟通发挥了积极作用。&#x27;)&gt;&gt;&gt; print(res)# [&#x27;中国驻英记者一向刻手信问职业道德，为增进中英两国人民之间的了解和沟通发挥了积极作用。&#x27;,# &#x27;中国驻英记者一向恪守新闻职业道德，为增进中英两国人民指尖的了解和沟通发挥了积极作用。&#x27;,# &#x27;中国驻英记者一向恪守新闻职业道德，为增进中英两国人民之间的了解和沟通发挥了积积作用。&#x27;] 不考虑拼音声调，考虑常见方言读音误读，如 zh 与 z 不分，eng 与 en 不分，f 与 h 不分，l 与 n 不分等情况 替换时，优先使用常用词汇（依据词频而定） augmentation_num(int) 参数控制返回几条增强后的数据 homo_ratio(float) 参数控制对每一个汉字的调整其位置概率 allow_mispronounce(bool) 控制是否允许方言读音误读，如 zh 与 z 卷舌不分，默认为 True，允许词汇错音 其余参数参考 jio.homophone_substitution.__doc__ 随机增删字符 random_add_delete 随机在文本中增加、删除某个字符。不影响原意的字符，对文本语义不造成影响。例如： 12# 原句：“23日，山东省监狱管理局原副局长王文杰等5人玩忽职守”# 增删：&quot;2日，山东监狱 管理局、原副局长文杰等5人玩忽职守..&quot; 随机增加的字符的选择，依据对海量文本统计字符分布规律的 char_distribution.json 文件得到，取其中的非中文字符进行添加。 1234567&gt;&gt;&gt; import jionlp as jio&gt;&gt;&gt; res = jio.random_add_delete(&#x27;孙俪晒11年对比照庆领证纪念日，邓超被指沧桑。&#x27;)&gt;&gt;&gt; print(res)# [&#x27;孙俪晒11年对比照庆领证纪念日，邓超被指沧。&#x27;,# &#x27;孙+俪晒11年对比照庆领证纪念日，邓超被指沧桑。&#x27;,# &#x27;孙俪晒 11年对比照庆领证纪念日，邓超被指沧/桑。&#x27;] 对于某些 NLP 任务，如抽取其中时间词汇，则以上方法很容易干扰关键时间信息，故方法失效。待后续优化， 替换时，优先使用常用词汇（依据词频而定） augmentation_num(int) 参数控制返回几条增强后的数据 add_ratio(float) 对每一个位置随机增加字符概率，默认为 0.02 delete_ratio(float) 对每一个汉字随机做删除的概率，默认为 0.02 其余参数参考 jio.random_add_delete.__doc__ NER 实体替换 ReplaceEntity 根据实体词典，随机在文本中替换某个实体，对语义不造成影响。例如： 1# 原句：“坦桑尼亚现任总统马古富力病逝”# 增删：&quot;柬埔寨现任总统张达美病逝&quot; 该方法不仅仅用于实体识别数据增强，也可用于其他相似序列标注任务（如要素抽取等），也可用于文本分类、匹配等任务。 实体词典的获得，可用 jio.ner.collect_dataset_entities 工具使用。 1&gt;&gt;&gt; import jionlp as jio&gt;&gt;&gt; # 从标注语料中获取实体词典&gt;&gt;&gt; dataset_y = [[&#123;&#x27;type&#x27;: &#x27;Person&#x27;, &#x27;text&#x27;: &#x27;马成宇&#x27;, &#x27;offset&#x27;: (0, 3)&#125;, &#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;百度&#x27;, &#x27;offset&#x27;: (10, 12)&#125;, &#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;百度&#x27;, &#x27;offset&#x27;: (20, 22)&#125;], [&#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;国力教育公司&#x27;, &#x27;offset&#x27;: (2, 8)&#125;], [&#123;&#x27;type&#x27;: &#x27;Organization&#x27;, &#x27;text&#x27;: &#x27;延平区人民法院&#x27;, &#x27;offset&#x27;: (0, 7)&#125;, &#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;百度&#x27;, &#x27;offset&#x27;: (10, 12)&#125;, &#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;百度&#x27;, &#x27;offset&#x27;: (20, 22)&#125;]]&gt;&gt;&gt; entity_dict = jio.ner.collect_dataset_entities(dataset_y)&gt;&gt;&gt; print(entity_dict)&gt;&gt;&gt; replace_entity = jio.ReplaceEntity(entity_dict)&gt;&gt;&gt; text = &#x27;腾讯致力于游戏，阿里巴巴致力于电商。小马会玩。&#x27;&gt;&gt;&gt; entities = [&#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;腾讯&#x27;, &#x27;offset&#x27;: (0, 2)&#125;, &#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;阿里巴巴&#x27;, &#x27;offset&#x27;: (8, 12)&#125;, &#123;&#x27;type&#x27;: &#x27;Person&#x27;, &#x27;text&#x27;: &#x27;小马&#x27;, &#x27;offset&#x27;: (18, 20)&#125;]&gt;&gt;&gt; aug_texts, aug_entities = replace_entity(text, entities)&gt;&gt;&gt; print(aug_texts, aug_entities)# entity_dict:# &#123;# &quot;Person&quot;:&#123;# &quot;马成宇&quot;:1# &#125;,# &quot;Company&quot;:&#123;# &quot;百度&quot;:4,# &quot;国力教育公司&quot;:1# &#125;,# &quot;Organization&quot;:&#123;# &quot;延平区人民法院&quot;:1# &#125;# &#125;# # aug_texts:# [&#x27;腾讯致力于解决冲突，国力教育公司致力于玩。小马爱玩。&#x27;, # &#x27;百度致力于解决冲突，阿里巴巴致力于玩。小马爱玩。&#x27;,# &#x27;腾讯致力于解决冲突，阿里巴巴致力于玩。马成宇爱玩。&#x27;]# aug_entities:# [[&#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;腾讯&#x27;, &#x27;offset&#x27;: (0, 2)&#125;, # &#123;&#x27;text&#x27;: &#x27;国力教育公司&#x27;, &#x27;type&#x27;: &#x27;Company&#x27;, &#x27;offset&#x27;: [10, 16]&#125;,# &#123;&#x27;text&#x27;: &#x27;小马&#x27;, &#x27;type&#x27;: &#x27;Person&#x27;, &#x27;offset&#x27;: (21, 23)&#125;],# [&#123;&#x27;text&#x27;: &#x27;百度&#x27;, &#x27;type&#x27;: &#x27;Company&#x27;, &#x27;offset&#x27;: [0, 2]&#125;, # &#123;&#x27;text&#x27;: &#x27;阿里巴巴&#x27;, &#x27;type&#x27;: &#x27;Company&#x27;, &#x27;offset&#x27;: (10, 14)&#125;,# &#123;&#x27;text&#x27;: &#x27;小马&#x27;, &#x27;type&#x27;: &#x27;Person&#x27;, &#x27;offset&#x27;: (19, 21)&#125;],# [&#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;腾讯&#x27;, &#x27;offset&#x27;: (0, 2)&#125;, # &#123;&#x27;type&#x27;: &#x27;Company&#x27;, &#x27;text&#x27;: &#x27;阿里巴巴&#x27;, &#x27;offset&#x27;: (10, 14)&#125;, # &#123;&#x27;text&#x27;: &#x27;马成宇&#x27;, &#x27;type&#x27;: &#x27;Person&#x27;, &#x27;offset&#x27;: [19, 22]&#125;]]) 由此可以看到，该方法不仅仅可以用于序列标注的数据增强，同时可以用于文本分类：使用前须将文本做实体识别、序列标注，将相应的实体词典准备好，进行替换。 augmentation_num(int) 参数控制返回几条增强后的数据 replace_ratio(float) 对每一个实体做替换的概率，默认为 0.1 这个工具包还有很多功能，在 Linux 系统里输入命令 jionlp_help 搜索看看吧！","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NLP/数据增强工具","slug":"NLP-数据增强工具","permalink":"https://leezhao415.github.io/tags/NLP-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7/"}]},{"title":"机器学习中的损失函数详解","slug":"机器学习中的损失函数详解","date":"2021-08-08T09:41:10.000Z","updated":"2021-08-08T10:26:47.335Z","comments":true,"path":"2021/08/08/机器学习中的损失函数详解/","link":"","permalink":"https://leezhao415.github.io/2021/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"文章目录 什么是损失函数？ 一、Zero-one Loss（0-1 损失） 二、Hinge Loss 三、softmax-loss （多类别） 四、Logistic-loss（二分类的交叉熵损失函数） 五、交叉熵，cross entropy（多分类） 六、softmax cross entropy 七、triplet loss 八、均方误差（mean squared error，MSE） 九、平均绝对误差（Mean Absolute Error，MAE） 十、Smooth L1 损失 十一、center loss 什么是损失函数？ 损失函数 （Loss Function） 也可称为代价函数 （Cost Function）或误差函数（Error Function），用于衡量预测值与实际值的偏离程度。一般来说，我们在进行机器学习任务时，使用的每一个算法都有一个目标函数，算法便是对这个目标函数进行优化，特别是在分类或者回归任务中，便是使用损失函数（Loss Function）作为其目标函数。机器学习的目标就是希望预测值与实际值偏离较小，也就是希望损失函数较小，也就是所谓的最小化损失函数。 损失函数是用来评价模型的预测值与真实值的不一致程度，它是一个非负实值函数。通常使用来表示，损失函数越小，模型的性能就越好。 一、Zero-one Loss（0-1 损失） 0-1 loss 是最原始的 loss，它是一种较为简单的损失函数，如果预测值与目标值不相等，那么为 1，否则为 0，即： 0-1 损失可用于分类问题，但是由于该函数是非凸的，在最优化过程中求解不方便，有阶跃，不连续。0-1 loss 无法对 x 进行求导，在依赖于反向传播的深度学习任务中，无法被使用，所以使用不多。 二、Hinge Loss Hinge loss 主要用于支持向量机（SVM）中，它的称呼来源于损失的形状，定义如下： 其中 y=+1 或−1，f (x)=wx+b，当为 SVM 的线性核时。如果分类正确，loss=0，如果错误则为 1-f (x)，所以它是一个分段不光滑的曲线。Hinge loss 被用来解 SVM 中的间隔最大化问题。 三、softmax-loss （多类别） 其中主要是 softmax 函数计算的类别概率。softmax loss 被广泛用于分类问题中，而且发展出了很多的变种，有针对不平衡样本问题的 weighted softmax loss、focal loss，针对蒸馏学习的 soft softmax loss，促进类内更加紧凑的 L-softmax Loss 等一系列改进。 强调一下 ：softmax 函数与 softmax-loss 函数是不一样的，千万，千万别记混了。 softmax 函数最常用作分类器的输出，来表示 个不同类上的概率分布。 softmax 公式如下： 使用 softmax 分类的前提：类别之间都是相互独立的。 softmax 分类的本质：将特征向量做归一化处理（输出总是和为 1），将线性预测值转换为类别概率。 四、Logistic-loss（二分类的交叉熵损失函数） Logistic 不使用平方损失的原因：平方损失会导致损失函数是非凸的，不利于求解，因为非凸函数会存在许多的局部最优解。 五、交叉熵，cross entropy（多分类） cross entropy loss 用于度量两个概率分布之间的相似性。 其中为样本的真实标签，取值只能为 0 或 1；为预测样本属于类别的概率；为类别的数量。 六、softmax cross entropy 其中 Pk,i 表示样本 k 属于类别 i 的概率（真实标签，只能为 0 或 1）；qk,i 表示 softmax 预测的样本 k 属于类别 i 的概率；c 是类别数；n 是样本总数。如果概率是通过 softmax 计算得到的，那么就是 softmax cross entropy。 七、triplet loss triplet-loss 是深度学习中的一种损失函数，用于训练差异性较小的样本，如人脸等。数据包括锚（Anchor）示例、正（Positive）示例、负（Negative）示例，通过优化锚示例与正示例的距离小于锚示例与负示例的距离，实现样本的相似性计算。也就是说通过学习后，使得同类样本的 positive 更靠近 Anchor，而不同类的样本 Negative 则远离 Anchor。 如上图所示，triplet 是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为 Anchor，然后再随机选取一个与 Anchor (记为 x_a) 属于同一类的样本和不同类的样本，这两个样本对应的称为 Positive (记为 x_p) 和 Negative (记为 x_n)，由此构成一个（Anchor，Positive，Negative）三元组。 有了上面的 triplet 的概念， triplet loss 就好理解了。针对三元组中的每个元素（样本），训练一个参数共享或者不共享的网络，得到三个元素（样本）的特征表达，分别记为： 通过 Triplet Loss 的学习后，使得 Positive 和 Anchor（同类）特征表达之间的距离尽可能小，而 Anchor 和 Negative（不同类）特征表达之间的距离尽可能大，并且要让 x_a 与 x_n 之间的距离和 x_a 与 x_p 之间的距离之间有一个最小的间隔。公式表示就是： 其中距离用欧式距离度量，α 也称为 margin（间隔）参数。设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。 对应的目标函数为： 其中 + 表示 [ ] 内的值大于零的时候，取该值为损失；小于零的时候，损失为零。 【triplet loss 梯度推导】 我们将上述目标函数记为 L ，则有： 在训练 Triplet Loss 模型时，只需要输入样本，不需要输入标签，这样避免标签过多、同标签样本过少的问题，模型只关心样本编码，不关心样本类别。Triplet Loss 在相似性计算和检索中的效果较好，可以学习到样本与变换样本之间的关联，检索出与当前样本最相似的其他样本。Triplet Loss 通常应用于个体级别的细粒度识别，比如分类猫与狗等是大类别的识别，但是有些需求要精确至个体级别，比如识别不同种类不同颜色的猫等，所以 Triplet Loss 最主要的应用也是在细粒度检索领域中。 Triplet Loss 的优点： 如果把不同个体作为类别进行分类训练，Softmax 维度可能远大于 Feature 维度，精度无法保证。 Triplet Loss 一般比分类能学习到更好的特征，在度量样本距离时，效果较好； Triplet Loss 支持调整阈值 Margin，控制正负样本的距离，当特征归一化之后，通过调节阈值提升置信度。 八、均方误差（mean squared error，MSE） 也叫平方损失或 L2 损失，常用在最小二乘法中。它的思想是使得各个训练点到最优拟合线的距离最小（平方和最小）。均方误差损失函数也是我们最常见的损失函数了，相信大家都很熟悉了，常用于回归问题中。定义如下： 当预测值与目标值相差很大时，梯度容易爆炸，这既是 L2 loss 的最大问题。 九、平均绝对误差（Mean Absolute Error，MAE） 所有单个观测值与算术平均值的绝对值的平均，也被称为 L1 loss，常用于回归问题中。与平均误差相比，平均绝对误差由于离差被绝对值化，不会出现正负相抵消的情况，因而，平均绝对误差能更好地反映预测值误差的实际情况。 由于 L1 loss 具有稀疏性，为了惩罚较大的值，因此常常将其作为正则项添加到其他 loss 中作为约束。L1 loss 的最大问题是梯度在零点不平滑，导致会跳过极小值。 十、Smooth L1 损失 原始的 L1 loss 和 L2 loss 都有缺陷，比如 L1 loss 的最大问题是梯度不平滑，而 L2 loss 的最大问题是容易梯度爆炸，所以研究者们对其提出了很多的改进。 在 faster rcnn 框架中，使用了 smooth L1 loss 来综合 L1 与 L2 loss 的优点，定义如下： 在比较小时，上式等价于 L2 loss，保持平滑。 在比较大时，上式等价于 L1 loss，可以限制数值的大小。Smooth L1 损失能够解决梯度爆炸问题。 十一、center loss center loss 来自 ECCV2016 的一篇论文：A Discriminative Feature Learning Approach for Deep Face Recognition。 论文链接：http://ydwen.github.io/papers/WenECCV16.pdf 代码链接：https://github.com/pangyupo/mxnet_center_loss 什么是 center loss？一个 batch 中的每个样本的 feature 离 feature 的中心的距离的平方和要越小越好，也就是类内（intra-class）距离要越小越好。这就是 center loss。 其中 m 表示 mini-batch 的大小，Xi 表示第 i 个样本的特征，Cyi 表示第 i 个正确样本的特征中心。 通常在用 CNN 做人脸识别等分类问题时，我们一般采用 softmax loss，在 close-set 测试中模型性能良好，但在遇到 unseen 数据情况下，模型性能会急剧下降。一个直观的感觉是：如果模型学到的特征判别度更高，那么再遇到 unseen 数据时，泛化性能会比较好。为了使得模型学到的特征判别度更高，论文提出了一种新的辅助损失函数，之说以说是辅助损失函数是因为新提出的损失函数需要结合 softmax loss，而非替代后者，在不同数据及上提高了识别准确率。 在结合使用这两种损失函数时，可以认为 softmax loss 负责增加 inter-class 距离，center-loss 负责减小 intra-class 距离，这样学习到的特征判别度会更高。 缺点 ：最麻烦的地方在于如何选择训练样本对。在论文中，作者也提到了，选取合适的样本对对于模型的性能至关重要，论文中采用的方法是每次选择比较难以分类的样本对重新训练，类似于 hard-mining。同时，合适的训练样本还可以加快收敛速度。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"机器学习/损失函数","slug":"机器学习-损失函数","permalink":"https://leezhao415.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"}]},{"title":"科研项目成果合集（持续更新中）","slug":"科研项目成果合集（持续更新中）","date":"2021-08-08T09:40:40.000Z","updated":"2021-08-08T10:25:09.671Z","comments":true,"path":"2021/08/08/科研项目成果合集（持续更新中）/","link":"","permalink":"https://leezhao415.github.io/2021/08/08/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE%E6%88%90%E6%9E%9C%E5%90%88%E9%9B%86%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD%EF%BC%89/","excerpt":"","text":"文章目录 （1）基于改进的 MSRCR 下穿立交监控图像增强算法研究 （2）下穿立交监控图像质量提升算法研究 （3）基于二次模版库的车牌残缺字符识别 （4）Detecting the Trajectory of Moving Object for Single-Pixel Imaging System.International Conference on Electrical Engineering, Control and Robotics (EECR 2018) （5）基于双通路跃层卷积网络的交通标志识别算法 论文标题 次序 主题 （1）基于改进的 MSRCR 下穿立交监控图像增强算法研究 第一作者 图像增强算法 （2）下穿立交监控图像质量提升算法研究 第一作者 图像增强 + 图像滤波 （3）基于二次模版库的车牌残缺字符识别 第四作者 车牌识别 + 残缺字符 （4）Detecting the Trajectory of Moving Object for Single-Pixel Imaging System.International Conference on Electrical Engineering, Control and Robotics (EECR 2018) 第四作者 单像素图像系统 + 移动目标检测 （5）基于双通路跃层卷积网络的交通标志识别算法 第三作者 交通标志识别 + 改进 SSD 算法 （1）基于改进的 MSRCR 下穿立交监控图像增强算法研究 文章链接：https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDLAST2019&amp;filename=AHJG201806014&amp;v=NDAi69LzBpJZYBiZyBSfnttq85jikD0OZA1uvKnP0jzAkmESPDAZvQHLv0FXe%mmd2Bwz 摘要 ：受粉尘、低照度或点光源等因素影响，城市道路下穿立交监控图像存在对比度低、背景噪声强和整体视觉效果不理想等问题。针对这类问题，本文提出了一种改进的带色彩恢复的多尺度视网膜增强（MSRCR）算法，并且详细分析了改进的 MSRCR 算法的原理和实现方法。为验证本文算法的有效性，以实际采集的 4 幅彩色图像为实验样本，分别利用 SSR（单尺度 Retinex）算法、MSRCR 算法和本文算法对其进行了增强处理。实验结果表明，与对比算法相比，本文算法具有更大的优势。 关键词 ：图像增强；下穿立交；Retinex; 多尺度； ​ （2）下穿立交监控图像质量提升算法研究 文章链接：https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD201902&amp;filename=1019602718.nh&amp;v=osFmEM3yWVV%mmd2FEt9hS1OGERKQAg9HWx%mmd2FOOyISXwFfPO8KEBddyAj8GaI0EHrRIE14 摘要 ：视频监控系统作为智慧城市重要的信息采集环节，能够提供实时、直观和全方位的数据信息，已辐射到生产和生活中的各个环节和领域。下穿立交作为城市交通的重要组成部分，一直以来都是智慧城市和平安社区建设中监控系统布控的的重点区域。因此，市政管理部门在下穿立交中布设大量监控设备以实现对下穿立交的全方位、立体化的监测。下穿立交由于所处位置的光照不均匀，大雾、沙尘、雨天等恶劣天气以及采集设备精度和传输线路等的影响，其监控系统不可避免地引入了噪声，使得采集到的视频图像存在对比度低，边缘等细节信息丢失等的问题，影响到真实和原始监控信息的利用效率。因此，为了能够充分发挥下穿立交监控系统的作用，提高监控视频图像的利用效率，对其图像质量提升算法的研究就显得非常必要。针对上述问题，本文主要开展以下的研究工作:（1）在对经典的图像增强算法以及带色彩恢复的多尺度视网膜增强算法分析的基础上，本文结合下穿立交监控图像的特征，提出一种改进的带色彩恢复的多尺度视网膜下穿立交图像增强算法。通过单尺度视网膜增强算法获取更丰富的细节信息，然后再经过多尺度视网膜增强算法来平衡细节信息和全局信息，达到在不影响全局信息量的同时，能够获取到更丰富的细节信息的目的。（2）对多种噪声的特性进行分析建模，本文实现了基于三维块匹配的下穿立交监控视频图像去噪算法。采用非局部块匹配思想，首先在图像中寻找相似块，使用硬阈值线性变换降低欧式距离的复杂度，然后通过相似块域转换，利用联合滤波器降低相似块自身含有的噪声，并在聚集处对相似块进行加权处理，得到降噪后的目标块。（3）结合基于三维块匹配的图像去噪算法和改进的带色彩恢复的多尺度视网膜增强算法来设计联合滤波器，提出了基于联合滤波的下穿立交监控图像质量提升算法。实验结果表明，与上述两种算法相比，经过该算法处理后的图像具有较好的视觉效果和较高的客观质量评价得分，能够在有效提升图像对比度的同时抑制噪声的影响。 关键词 ：下穿立交；图像增强；图像去噪；联合滤波；图像质量提升； （3）基于二次模版库的车牌残缺字符识别 文章链接：https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDLAST2017&amp;filename=AHJG201704008&amp;v=yUvHFsUJxakb0FonScKyGsACK6cnYvjgAgO2NfOHXtmUVP5sLvmCfcyAmJg7PNYE 摘要 ：字符识别在车牌识别中的作用不可或缺，最简便的识别方式为模版匹配法。本文提出改进的模板匹配法，通过细化的字符子库，结合相关匹配算法，提升识别效果，克服了传统模版匹配无法识别残缺字符或匹配识别错误的缺点，实现遮挡或残缺字符的精确识别。 关键词 ：字符识别；二次模板匹配；残缺字符；字符子库；精确识别； （4）Detecting the Trajectory of Moving Object for Single-Pixel Imaging System.International Conference on Electrical Engineering, Control and Robotics (EECR 2018) 文章链接：https://xueshu.baidu.com/usercenter/paper/show?paperid=ab95bf7f27e2c2463d256d31f284def6&amp;site=xueshu_se Abstract . In order to get the trajectory of moving object using single-pixel imaging system, an algorithm is proposed. The same pseudorandom masks are employed to illuminate the different time scene. A time weighted sum of the background correction signals is employed to get the trajectory information using compressed sensing (CS) method. In ideal situation, we can obtain other parameters (e.g., speed, orientation) besides the trajectory. However, the reflective intensity of the object can be change due to the reflective angle change caused by the motion in some situations. This will mislead for achieving the speed, orientation parameters. In order to eliminate this effect, a division method is utilized. At last, the computer simulation results prove the effect validity of the proposed algorithm. （5）基于双通路跃层卷积网络的交通标志识别算法 文章链接：https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDLAST2018&amp;filename=AHJG201801012&amp;v=NDAi69LzBpLYDA8VvVrEQkPCaziaZj7FTVmFQrmPJ1bkdaKe6WbwHcKwi8MYD%mmd2BKo 摘要 ：交通标志识别（Traffic Sign Recognition,TSR）是智能交通系统的重要研究方向之一。因道路交通的环境复杂、交通标志数据库规模大小等因素制约，在设计 TSR 系统可行性方案时必须考虑算法的复杂度、识别率和鲁棒性。针对这一问题，本文提出了一种不同尺度的双通路跃层卷积神经网络算法，在同一通路上交通标志的底层局部特征和高层全局的特征，与不同通路上经过局部响应归一化和池化后的特征在全连接层融合，从而丰富了交通标志分类的特征，最后将特征图输入分类器进行交通标志识别。采用德国交通标志识别标准数据集（German Traffic Sign Recognition Benchmark,GTSRB）进行训练和测试，本文算法的识别率达到 97.96%, 明显优于单一通路的跃层卷积网络算法和人工方法。 关键词 ：卷积神经网络；交通标志识别；双通路跃层；特征融合；深度学习；","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"科研项目成果","slug":"科研项目成果","permalink":"https://leezhao415.github.io/tags/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE%E6%88%90%E6%9E%9C/"}]},{"title":"计算机视觉——顶会、顶刊","slug":"计算机视觉——顶会、顶刊","date":"2021-08-07T10:57:43.000Z","updated":"2021-08-07T11:06:59.247Z","comments":true,"path":"2021/08/07/计算机视觉——顶会、顶刊/","link":"","permalink":"https://leezhao415.github.io/2021/08/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E2%80%94%E2%80%94%E9%A1%B6%E4%BC%9A%E3%80%81%E9%A1%B6%E5%88%8A/","excerpt":"","text":"文章目录 计算机视觉 —— 顶会、顶刊 1、论文 2、出版社 3、数据库 4、学会 一、国际 二、国内 计算机视觉 —— 顶会、顶刊 1、论文 （1）会议：目的 - 交流。某一个领域最好的那个会议称为顶会。 （2）期刊 2、出版社 IEEE （1）出版会议：菜些 。绝大部分的 IEEE 会议并不是由 IEEE 主办，而是由一些学校或者其他机构主办，这些会议主办方提交申请由 IEEE 出版会议论文集，所以 IEEE 只参与后续的出版工作，并没有派遣专家学者参与论文评审。 （2）出版期刊：质量高 3、数据库 science，nature，（最顶尖，科学家级别） 专门会从世界上所有期刊和会议（顶会）里面选择性的收录高质量文章进数据库里面。 （1） SCI （一般 ieee 这个出版社发行的期刊大部分文章，sci 都会收录），分为 1,2,3 区，质量逐渐降低。 （2） EI ：相比 SCI 差些 （3）国内： 知网 ， 万方 ，一般只收录国内的毕业论文，不会去收录那些 ieee 等国际顶尖期刊和顶会的文章。中文核心，一般也只收录国内的一些中文写的高质量论文。 4、学会 CCF 是中国计算机学会，不是出版社（那就不能发行期刊），也不是数据库，也不是某个会议的名字。我们看到他关于论文方面，只是一个高水平论文推荐的作用。 官网能看到他的论文推荐目录，其中推荐的期刊，他划分不同研究领域，A,B,C 类级别的，论文质量逐渐下降。 ACM (Association for Computing Machinery) 中文：国际计算机学会 ACM 所评选的图灵奖（A.M. Turing Award）被公认为世界计算机领域的诺贝尔奖。每年举办 ACM 国际大学生程序设计竞赛。 一、国际 (1) 国际会议 计算机视觉领域的三大顶会： ICCV ， CVPR 和 ECCV ，统称为 ICE 。 1、 ICCV ，International Comference on Computer Vision，国际计算机视觉会议，是公认的三个会议中级别最高的，收录率一般在 20% 左右，由 IEEE 主办。【收录论文的内容：底层视觉与感知，颜色、光照与纹理处理，分割与聚合，运动与跟踪，立体视觉与运动结构重构，基于图像的建模，基于物理的建模，视觉中的统计学习，视频监控，物体、事件和场景的识别，基于视觉的图形学，图片和视频的获取，性能评估，具体应用等。】 2、 CVPR ，Internaltional Conference on Computer Vision and Pattern Recogintion，国际计算机视觉模式识别会议。一年一次，举办地在美国 (除 2002 年)，录取率 25% 左右，由 IEEE 主办。 3、 ECCV ，Europeon Conference on Computer Vision，欧洲计算机视觉会议，两年一次 ，欧洲，收录率一般在 20% 多，由 springer 和一些商业媒体承办。 ACCV ，即亚洲计算机视觉会议（Asian Conference on Computer Vision）是亚洲计算机视觉联盟 (AFCV) 举办。1993 年举办第一届，每两年举办一次。ACCV 为中国计算机学会 CCF 推荐人工智能会议。论文录取率 20%~25%，是仅次于计算机视觉三大顶会的会议，近年学术水平及等级进一步提高。【百度百科】 其他 ICIP ——International Conference on Image Processing. BMVC ——British Machine Vision Conference. IAPR MVA ——IAPR Machine Vision Applications. ICPR ——International Conference on Pattern Recognition. 国际模式识别会议 ACCV ——Asian Conference on Computer Vision. 亚洲计算机视觉会议 SIGGRAPH ——Special Interest Group for Computer GRAPHICS，计算机图形图像特别兴趣小组，更偏重图形方面。 EUROGRAPHICS ——European Association for computer graphics，与 SIGGRAPH 对应，只不过仅在欧洲范围内召开。 IJCAI ——International Joint Conference on Artificial Intelligence. ICSLP —— International Conference on Spoken Language Processing. ICASSP ——International Conference on Acoustics, Speech and Signal Processing. 机器学习领域的两大国际会议： （1） NIPS （NeurIPS），全称神经信息处理系统大会 (Conference and Workshop on Neural Information Processing Systems)，是一个关于机器学习和计算神经科学的国际会议。该会议固定在每年的 12 月举行，由 NIPS 基金会主办。NIPS 是机器学习领域的顶级会议 。在中国计算机学会的国际学术会议排名中，NIPS 为人工智能领域的 A 类会议。 （2） ICML ——International Conference on Machine Learning 国际机器学习大会，由 IMLS 国际机器学习协会支持，始于 1980 年，此后每年夏季举行，2014 年举办地在北京。ICML 会接收到各路机器学习大牛的投稿，录用率只有百分之二十多。第 36 届 ICML 将于 2019 年 6 月 10 日到 15 日在美国长滩举办。 人工智能领域的两个顶尖会议： （1） AAAI ——AAAI Conference on Artificial Intelligence 会议由人工智能促进协会 AAAI（Association for the Advancement of Artificial Intelligence）主办。人工智能促进协会是一个国际化的非营利科学组织，旨在推动人工智能领域的研究和应用，增进大众对人工智能的了解。会议始于 1980 年，既注重理论，也注重应用，还会讨论对人工智能发展有着重要影响的社会、哲学、经济等话题。 （2） IJCAI ——International Joint Conferences on AI IJCAI 是一家 1969 年于美国加州成立的非营利企业，致力于推动科学和教育的发展，由负责会议和期刊的两个部分组成。会议始于 1969 年，每两年举办一次，从 2016 年开始改为一年举办一次。会议选拔要求非常严格，论文录取率几乎不超过 26%，2011 年录取率仅 17%。 (2) 国际期刊 以计算机视觉为主要内容： 1、 PAMI ，IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE 旗下的期刊，是模式识别和机器学习领域最重要的学术性汇刊之一，有很高的影响影子和排名。 2、 IJCV ，International Journal of Computer Vision 3、 CVIU ，Computer Vision and Image Understanding 4、 TIP ，IEEE Transactions on Image Processing 5、 PR ，Pattern Recognition 6、 PR Letters ，Pattern Recognition Letters CVGIP ，Computer Vision, Graphics and Image Processing IJPRAI ，Internatiorial Journat of Pattern Recognition and Artificial Intelligence 二、国内 (1) 国内会议 略 (2) 国内期刊 自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，光学 精密工程等。 ———————————————— 版权声明：本文为 CSDN 博主「ShallowAnomaly」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/shiue_gx/article/details/102482081","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"计算机顶会","slug":"计算机顶会","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%A1%B6%E4%BC%9A/"}]},{"title":"DETR：基于 Transformers 的目标检测","slug":"DETR：基于-Transformers-的目标检测","date":"2021-08-07T10:48:23.000Z","updated":"2021-08-07T11:12:12.517Z","comments":true,"path":"2021/08/07/DETR：基于-Transformers-的目标检测/","link":"","permalink":"https://leezhao415.github.io/2021/08/07/DETR%EF%BC%9A%E5%9F%BA%E4%BA%8E-Transformers-%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/","excerpt":"","text":"文章目录 DETR：基于 Transformers 的目标检测 前言 相关工作 DETR 的实现原理 CNN Transformers encoder-decoder FFN 匈牙利匹配 通过对比 ViT 思考 DETR DETR 还能做分割 实验结果分析 Comparison Study Ablation Study 简易代码 结论 DETR：基于 Transformers 的目标检测 前言 最近可以说是随着 ViT 的大火，几乎可以说是一天就能看到一篇基于 Transformers 的 CV 论文，今天给大家介绍的是另一篇由 Facebook 在 ECCV2020 上发表的一篇基于 Transformers 的目标检测论文，这篇论文也是后续相当多的 Transformers 检测 / 分割的 baseline, 透过这篇论文我们来了解其套路. 相关工作 提到目标检测，我们先来简要回顾一下最基础的一个工作 Faster R-CNN Faster R-CNN 第一步是用 CNN 给图像提特征，再通过非极大值抑制算法提取出候选框，最后预测每个候选框的位置和类别， DETR 的实现原理 DETR 这篇文章就极大的简化了这个过程，他把候选框提取的过程通过一个标准的 Transformers encoder-decoder 架构代替，在 decoder 部分直接预测出来物体的位置和类别. 流程分为三步: CNN 提特征 Transformers 的 encoder-decoder 进行信息的融合 FFN 预测 class 和 box CNN 利用 resnet-50 网络，将输入的图像 3 X W0 X H0 变成尺度为 2048 X W0/32 X H0/32 的特征，再通过一个 1X1 卷积，将 channel 从 2048 变为更小 (通常 512) Transformers encoder-decoder Transformer encoder 部分首先将输入的特征图降维并 flatten 成 d 个 H X W 维的向量，每个向量作为输入的 token, 由于 Self-attention 是置换不变形的，所以为了体现每个 token 在原图中的顺序关系，我们给每个 token 加上一个 positional encodings. 输出这是对应 Decoder 部分的 V 和 K. 比如说我们一开始输入的图片是 512*512, 那么 d 应该是 256. Transformers decoder 部分是输入是 100 个 Object queries, 比如说我们数据集总共有 100 个类别的物体需要预测，那么这 100 object queries 经过 Transformers decoder 之后会预测出若干类别的物体和位置信息. 作者发现在训练过程中在 decoder 中使用 auxiliary losses 很有帮助，特别是有助于模型输出正确数量的每个类的对象。 FFN DETR 在每个解码器层之后添加预测 FFN 和 Hungarian loss，所有预测 FFN 共享其参数。我们使用附加的共享层范数来标准化来自不同解码器层的预测 FFN 的输入，FFN 是一个最简单的多层感知机模块，对 Transformers decoder 的输出预测每个 object query 的类别和位置信息。在实际训练的过程中，通过匈牙利算法匹配预测和标签最小的损失，仅适用配对上的 query 计算 loss 回传梯度. loss 包括 Box loss 和 class loss Box Loss 包括 IOUloss 和 L1loss, 这个原理很简单. where are hyperparameters and is the generalized IoU [38]: class loss 就是最简单的交叉熵了. 匈牙利匹配 匈牙利匹配算法是离散数学中图论部分的一个经典算法，描述的问题是一个二分图的最大匹配。换成人话来说就是这个二分图分成两部分，一部分是我们对 100 种 object query 预测的结果，另一部分是实际的标签，由于我们一开始是不知道这 100 个 object query 输入的时候应该预测那些类别的物体，有可能一开始第一个 token 预测的是 A 物体，第二个 token 预测的是 C 物体，总而言之是无序的，我们就要根据实际的 label, 找到预测结果中和他最接近的计算 loss. 其他没匹配上的则不计算 loss 回传梯度。下面这张图一目了然: 通过对比 ViT 思考 DETR 其实笔者在阅读这篇文章的时候更加重点的是对比 ViT 在一些实现细节上的不同之处， 首先 ViT 是没有使用 CNN 的，而 DETR 是先用 CNN 提取了图像的特征 ViT 只使用了 Transformers-encoder, 在 encoder 的时候额外添加了一个 Class token 来预测图像类型，而 DETR 的 object token 则是通过 Decoder 学习的. DETR 和 VIT 中的 Transformers 在 encoder 部分都使用了 Position Embedding, 但是使用的并不一样，而 VIT 在使用的 Position Embedding 也是笔者一开始阅读文献的疑惑所在. DETR 的 Transformers encoder 使用的 feature 的每一个 pixel 作为 token embeddings 输入，而 ViT 则是直接把图像切成 16*16 个 Patch, 每个 patch 直接拉平作为 token embeddings 相比较 VIT,DETR 更接近原始的 Transformers 架构. DETR 还能做分割 首先检测 box 对每个 box 做分割 为每个像素的类别投票 作者在这篇论文在并没有详细讲实现细节，但是今年 CVPR2021 上发表的 SETR 则是重点讲如何利用 Transformers 做分割，我们下次细讲. 实验结果分析 Comparison Study 对比的是检测领域最经典的 Faster R-CNN, 可以看得出来了在同等参数两的情况下，在大目标物体的检测结果优于 Faster R-CNN, 道理嘛作者说是 Transformers 可以更关注全局信息. Ablation Study Decoder 比 Encoder 重要 Decoder 具有隐含的 “锚”，这对检测至关重要 Encoder 仅帮助聚合同一对象的像素，减轻 decoder 的负担 在位置编码部分，作者对比了可学习的位置编码和基于 Sincos 函数的位置编码方法 (也就是原始 Transformers 的位置编码方法) 可以看得出来效果是 Sincos 的更好，但是都显著好于不加位置编码，因为作者也在原文中 Self-Attention 是并行的，他如果不加位置编码的话是置换不变性的 (这个看 Attention is All you Need 原文) 这个嘛，就是很简单的实验了，验证一下 loss 每个部分的作用，基本上就是格式化的东西. 简易代码 作者最后在附录部分贴上了简易的代码实现细节 1234567891011121314151617181920212223242526272829303132333435363738import torch from torch import nn from torchvision.models import resnet50class DETR(nn.Module): def __init__(self, num_classes, hidden_dim, nheads, num_encoder_layers, num_decoder_layers): super().__init__() # We take only convolutional layers from ResNet-50 model self.backbone=nn.Sequential( *list(resnet50(pretrained=True).children())[:-2]) self.conv = nn.Conv2d(2048, hidden_dim, 1) self.transformer = nn.Transformer(hidden_dim, nheads, num_encoder_layers, num_decoder_layers) self.linear_class = nn.Linear(hidden_dim, num_classes + 1) self.linear_bbox = nn.Linear(hidden_dim, 4) self.query_pos =nn.Parameter(torch.rand(100, hidden_dim)) self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) def forward(self, inputs): x = self.backbone(inputs) h = self.conv(x) H,W=h.shape[-2:] pos = torch.cat([ self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1), self.row_embed[:H].unsqueeze(1).repeat(1, W, 1), ], dim=-1).flatten(0, 1).unsqueeze(1) h = self.transformer(pos + h.flatten(2).permute(2, 0,1), self.query_pos.unsqueeze(1)) return self.linear_class(h), self.linear_bbox(h).sigmoid()detr = DETR(num_classes=91, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6)detr.eval() inputs = torch.randn(1, 3, 800, 1200) logits, bboxes = detr(inputs) 结论 一篇很简单的 Transformers 在目标检测上的应用，也是最近大火的 Transformers 系列必引的一篇论文，我觉得他和 VIT 代表了 CV 对 Transformers 架构的两种看法吧，VIT 是只用 Encoder, 这也是目前最主流的做法，而 DETR 则是运用了 CNN 和 Transformers encoder-decoder 的结合，从 motivation 上来说我个人更喜欢 DETR, 这段时间也基本上把 Transformers 一系列都读完了，会以一个系列调几篇好的论文讲解 (水文实在是太多了)。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能/CV","slug":"人工智能-CV","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-CV/"},{"name":"Transformer/DETR(CV)","slug":"Transformer-DETR-CV","permalink":"https://leezhao415.github.io/tags/Transformer-DETR-CV/"}]},{"title":"【详解】BiLSTM+CRF模型","slug":"【详解】BiLSTM-CRF模型","date":"2021-07-28T06:52:19.000Z","updated":"2021-07-28T08:58:13.827Z","comments":true,"path":"2021/07/28/【详解】BiLSTM-CRF模型/","link":"","permalink":"https://leezhao415.github.io/2021/07/28/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91BiLSTM-CRF%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"文章目录 1 BiLSTM-CRF 模型用途 2 BiLSTM-CRF 模型介绍 2.1 数据标签及模型架构 2.2 BiLSTM 模型 2.3 CRF 模型 2.4 BiLSTM-CRF 模型代码实现 1 BiLSTM-CRF 模型用途 命名实体识别 (Named Entity Recognition，NER) 定义 从一段自然语言文本中找出相关实体，并标注出其位置以及类型。 是信息提取，问答系统，句法分析，机器翻译等应用领域的重要基础工具。 在自然语言处理技术走向实用化的过程中占有重要地位。包含行业，领域专有名词，如人名，地名，公司名，机构名，日期，时间，疾病名，症状名，手术名称，软件名称等。 命名实体识别问题实际上是 序列标注问题 序列标注问题指的是模型的输入是一个序列，包括文字，时间等，输出也是一个序列。针对输入序列的每一个单元，输出一个特定的标签. 以中文分词任务进行举例，例如输入序列是一串文字: “我是中国人”, 输出序列是一串标签: “OOBII”, 其中 &quot;BIO&quot; 组成了一种中文分词的标签体系: B 表示这个字是词的开始，I 表示词的中间到结尾，O 表示其他类型词。因此我们可以根据输出序列 &quot;OOBII&quot; 进行解码，得到分词结果 &quot;我 \\ 是 \\ 中国人&quot;。 序列标注问题涵盖了自然语言处理中的很多任务，包括语音识别，中文分词，机器翻译，命名实体识别等，而常见的序列标注模型包括 HMM, CRF, RNN, LSTM, GRU 等模型。 其中在命名实体识别技术上，目前主流的技术是通过 BiLSTM+CRF 模型进行序列标注。 2 BiLSTM-CRF 模型介绍 2.1 数据标签及模型架构 2.1.1 数据标签 123456789B-Person （人名的开始部分）I- Person （人名的中间部分）B-Organization （组织机构的开始部分）I-Organization （组织机构的中间部分）O （非实体信息） 2.1.2 模型架构 x 是包含了 5 个单词的一句话（W0,W1,W2,W3,W4）。还有，在句子 x 中 [W0,W1] 是人名，[W3] 是组织机构名称，其他都是 “O”。 句中的每个单词是一条包含词嵌入和字嵌入的词向量，词嵌入通常是事先训练好的，字嵌入则是随机初始化的。所有的嵌入都会随着训练的迭代过程被调整。 BiLSTM-CRF 的输入是词嵌入向量，输出是每个单词对应的预测标签。 BiLSTM 层的输入表示该单词对应各个类别的分数。如 W0，BiLSTM 节点的输出是 1.5 (B-Person), 0.9 (I-Person), 0.1 (B-Organization), 0.08 (I-Organization) and 0.05 (O)。这些分数将会是 CRF 层的输入。 所有的经 BiLSTM 层输出的分数将作为 CRF 层的输入，类别序列中分数最高的类别就是我们预测的最终结果。 2.2 BiLSTM 模型 2.2.1 BiLSTM 模型介绍及联系 所谓的 BiLSTM，就是 (Bidirectional LSTM) 双向 LSTM. 单向的 LSTM 模型只能捕捉到从前向后传递的信息，而双向的网络可以同时捕捉正向信息和反向信息，使得对文本信息的利用更全面，效果也更好. 在 BiLSTM 网络最终的输出层后面增加了一个线性层，用来将 BiLSTM 产生的隐藏层输出结果投射到具有某种表达标签特征意义的区间，具体如下图所示： 2.2.2 代码实现细节 BiLSTM 网络结构 设置隐藏层维度的时候，需要将 hidden_size // 2 总共有 3 层需要构建，分别是词嵌入层，双向 LSTM 层，全连接线性层 在代码层面，双向 LSTM 就是将 nn.LSTM () 中的参数 bidirectional 设置为 True BiLSTM 网络的代码实现 构建类 BiLSTM 的初始化函数 添加文本向量化的辅助函数，注意 padding 填充为相同长度的 Tensor 要注意 forward 函数中不同张量的形状约定 2.3 CRF 模型 2.3.1 CRF 模型定义及联系 CRF (全称 Conditional Random Fields), 条件随机场。是给定输入序列的条件下，求解输出序列的条件概率分布模型. 即使没有 CRF 层，我们照样可以训练一个基于 BiLSTM 的命名实体识别模型（因为 BiLSTM 模型的结果是单词对应各类别的分数，我们可以选择分数最高的类别作为预测结果。） 例如 W0，“B-Person” 的分数最高（1.5），那么我们可以选定 “B-Person” 作为预测结果。同样的，W1 是 “I-Person”, W2 是 “O”,W3 是 “B-Organization” ，W4 是 “O”。 但实际情况可能出现下列预测结果 2.3.2 CRF 作用 CRF 层可以加入一些约束来保证最终预测结果是有效的（CRF 层可以学习到句子的约束条件）。这些约束可以在训练数据时被 CRF 层自动学习得到。 可能的约束条件有： 句子的开头应该是 “B-” 或 “O”，而不是 “I-”。 “B-label1 I-label2 I-label3…”，在该模式中，类别 1,2,3 应该是同一种实体类别。比如，“B-Person I-Person” 是正确的，而 “B-Person I-Organization” 则是错误的。 “O I-label” 是错误的，命名实体的开头应该是 “B-” 而不是 “I-”。 有了这些有用的约束，错误的预测序列将会大大减少。 2.3.3 CRF 层的损失函数 1 Emission Score（发射分数 / 状态分数） 发射概率，是指已知当前标签的情况下，对应所出现字符的概率。通俗理解就是当前标签比较可能出现的文字有哪些，及其对应出现的概率. Xi,yj 代表状态分数，i 是单词的位置索引，yj 是类别的索引。根据上表， 表示单词 W1 被预测为 B−Organization 的分数是 0.1。 2 Transition Score （转移分数） 我们用 t (yi,yj) 来表示转移分数。例如，t (B−Person,I−Person)=0.9 表示从类别 B−Person→I−Person 的分数是 0.9。因此，我们有一个所有类别间的转移分数矩阵。 为了使转移分数矩阵更具鲁棒性，我们加上 START 和 END 两类标签。START 代表一个句子的开始（不是句子的第一个单词），END 代表一个句子的结束。 下表是加上 START 和 END 标签的转移分数矩阵。 如上表格所示，转移矩阵已经学习到一些有用的约束条件： 句子的第一个单词应该是 “B-” 或 “O”，而不是 “I”。（从 “START”-&gt;“I-Person 或 I-Organization” 的转移分数很低） “B-label1 I-label2 I-label3…”，在该模式中，类别 1,2,3 应该是同一种实体类别。比如，“B-Person I-Person” 是正确的，而 “B-Person I-Organization” 则是错误的。（“B-Organization” -&gt; “I-Person” 的分数很低） “O I-label” 是错误的，命名实体的开头应该是 “B-” 而不是 “I-”。 要怎样得到这个转移矩阵呢？ 实际上，转移矩阵是 BiLSTM-CRF 模型的一个参数。在训练模型之前，你可以随机初始化转移矩阵的分数。这些分数将随着训练的迭代过程被更新，换句话说，CRF 层可以自己学到这些约束条件。 3 CRF 损失函数 CRF 损失函数由两部分组成，真实路径的分数 和 所有路径的总分数。真实路径的分数应该是所有路径中分数最高的。 例如，我们的数据集中有如下几种类别： 一个包含 5 个单词的句子，可能的类别序列如下： \\1. START B-Person B-Person B-Person B-Person B-Person END \\2. START B-Person I-Person B-Person B-Person B-Person END …… 10. START B-Person I-Person O B-Organization O END N. O O O O O O O 每种可能的路径的分数为 Pi，共有 N 条路径，则路径的总分是 ，e 是常数 e。 如果第十条路径是真实路径，也就是说第十条是正确预测结果，那么第十条路径的分数应该是所有可能路径里得分最高的。 根据如下损失函数，在训练过程中，BiLSTM-CRF 模型的参数值将随着训练过程的迭代不断更新，使得真实路径所占的比值越来越大。 2.4 BiLSTM-CRF 模型代码实现 2.4.1 BiLSTM+CRF 模型的实现 第一步：导入工具包并完成辅助函数 第二步：文本信息张量化 第三步：创建类的初始化函数 第四步：创建获取发射矩阵张量的函数 第五步：计算前向传播分值的函数 第六步：计算句子真实分值的函数 第七步：维特比算法的实现 第八步：完善 BiLSTM_CRF 类的全部功能 2.4.2 模型训练的流程 第一步：熟悉字符到数字编码的码表 第二步：熟悉训练数据集的样式和含义解释 第三步：完成字符到 id 的映射函数 第四步：获取训练数据和验证数据的函数 第五步：完成准确率和召回率的评估代码 第六步：绘制损失曲线和评估曲线图 第七步：完成训练模型的完整代码 第八步：训练集和验证集损失曲线和指标数据曲线的分析","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"自然语言处理NLP","slug":"自然语言处理NLP","permalink":"https://leezhao415.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP/"}]},{"title":"【详解】神经网络梯度更新方法","slug":"【详解】神经网络梯度更新方法","date":"2021-07-17T08:01:31.000Z","updated":"2021-07-17T08:54:06.812Z","comments":true,"path":"2021/07/17/【详解】神经网络梯度更新方法/","link":"","permalink":"https://leezhao415.github.io/2021/07/17/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E6%96%B9%E6%B3%95/","excerpt":"","text":"文章目录 神经网络参数更新方法 1、SGD 2、MBGD 3、Momentum update 4、Nestrevo Momentum update 5、Adagrad 6、AdaDelta 7、RMSprop 8、Adam 神经网络参数更新方法 1、SGD SGD（Stochastic Gradient Descent）就是最常见的随机梯度下降。 向着参数的梯度的负方向改变（梯度方向是增加的方向）。 x+=−learning_rate∗dx\\begin{aligned} x += -learning\\_rate*dx \\end{aligned} x+=−learning_rate∗dx​ 这里的 x 可以是权值 w 也可以是偏置 b。 2、MBGD MBGD（Mini Batch Gradient Descent）小批量梯度下降 对每个批次中的 n 个训练样本，这种方法只执行一次更新。 为了避免 SGD 和标准梯度下降中存在的问题。 使用小批量梯度下降的优点是： 可以减少参数更新的波动，最终得到效果更好和更稳定的收敛。 还可以使用最新的深层学习库中通用的矩阵优化方法，使计算小批量数据的梯度更加高效。 通常来说，小批量样本的大小范围是从 50 到 256，可以根据实际问题而有所不同。 在训练神经网络时，通常都会选择小批量梯度下降算法。 3、Momentum update SGD 方法中的高方差振荡使得网络很难稳定收敛。 研究者提出了一种称为动量（Momentum）的技术，通过优化相关方向的训练和弱化无关方向的振荡，来加速 SGD 训练。 受到物理中的启发：例子的力与势能梯度有相对关系。例子感受到的力，正是损失函数的负梯度。F=ma，负梯度正比于粒的加速度。与普通 SGD 不同，梯度直接作用于位置，这里用物理的角度来看，梯度直接影响速度，速度再影响位置。 x+=−learning_rate∗dxx+=v\\begin{aligned} x +=&amp; -learning\\_rate*dx\\\\ x +=&amp; \\quad v \\end{aligned} x+=x+=​−learning_rate∗dxv​ 4、Nestrevo Momentum update 与 Momentum 稍稍有点不同。对于凸函数具有较强的理论收敛保证，实际中效果比 Mnmentum 稍好。 通过使网络更新与误差函数的斜率相适应，并依次加速 SGD。 也可根据每个参数的重要性来调整和更新对应参数，以执行更大或更小的更新幅度。 当前参数向量在点 x 处，从 Momentum 更新中看 v，忽略第二项 v 变成 mu*v。做一个提前量用 x_ahead=x+mu*v 代替 x。 x_ahead=x+mu∗vv=mu∗v−learning_rate∗dx_aheadx+=v\\begin{aligned} &amp; x\\_ahead = x + mu*v\\\\ &amp; v = mu*v - learning\\_rate*dx\\_ahead\\\\ &amp; x += \\quad v \\end{aligned} ​x_ahead=x+mu∗vv=mu∗v−learning_rate∗dx_aheadx+=v​ 每个参数适应学习率方法 前面的方法对每个参数学习了是固定的，调整学习率是一个耗时的过程。可以使用对每个参数都适应的学习率。 5、Adagrad cache+=(dx)2x+=−learning_rate∗dx(np.sqrt(cache)+eps)\\begin{aligned} &amp; cache += (dx)^2\\\\ &amp;x += -learning\\_rate*\\frac{dx}{(np.sqrt(cache)+eps)} \\end{aligned} ​cache+=(dx)2x+=−learning_rate∗(np.sqrt(cache)+eps)dx​​ Adagrad 方法是通过参数来调整合适的学习率 η，对稀疏参数进行大幅更新和对频繁参数进行小幅更新。因此，Adagrad 方法非常 适合处理稀疏数据 。 对于大的梯度，会使他的有效学习率减小；对于小的梯度会使他的有效学习率增大。这种方法的一个缺点是，在深度学习中，单调的学习率通常证明太激进，会使学习停止过早。 6、AdaDelta AdaDelta 方法是 AdaGrad 的延伸方法，它倾向于解决其学习率衰减的问题。Adadelta 不是累积所有之前的平方梯度，而是将累积之前梯度的窗口限制到某个固定大小 w。 7、RMSprop ​ RMSprop 是一种有效，但目前还未公开发布的自适应学习率方法。RMSprop 改进 Adagrad 方法，减小他的激进，使用单调减小学习率。特别的，它使用了梯度平方移动平均值。 cache=decay_rate∗cache+(1−decay_rate)∗(dx)2x+=−learning_rate∗dx(np.sqrt(cache)+eps)\\begin{aligned} &amp;cache = decay\\_rate*cache+(1-decay\\_rate)*(dx)^2\\\\ &amp;x += -learning\\_rate*\\frac{dx}{(np.sqrt(cache)+eps)} \\end{aligned} ​cache=decay_rate∗cache+(1−decay_rate)∗(dx)2x+=−learning_rate∗(np.sqrt(cache)+eps)dx​​ decay_rate 是一个超参数，典型值为 [0.9, 0.99, 0.999]。 根据梯度尺度来调整学习率是一个有利均衡的效果，但与 Adagrad 不同，更新不是单调小。 8、Adam Adam 算法即自适应时刻估计方法（Adaptive Moment Estimation），能计算每个参数的自适应学习率。这个方法不仅存储了 AdaDelta 先前平方梯度的指数衰减平均值，而且保持了先前梯度 M (t) 的指数衰减平均值，有点像 RMSprop 结合 momentum。 简单的更新如下： m=beltal∗m+(1−belta1)∗dxv=belta2∗v+(1−belta2)∗(dx)2x+=−learning_rate∗dx(np.sqrt(v)+eps)\\begin{aligned} &amp;m =beltal*m + (1-belta1)*dx\\\\ &amp;v = belta2*v+(1-belta2)*(dx)^2\\\\ &amp;x += -learning\\_rate*\\frac{dx}{(np.sqrt(v)+eps)} \\end{aligned} ​m=beltal∗m+(1−belta1)∗dxv=belta2∗v+(1−belta2)∗(dx)2x+=−learning_rate∗(np.sqrt(v)+eps)dx​​ 与 RMSProp 类似，使用 smooth 版本的梯度 m 代替原始梯度 dx。论文中的推荐值 eps=1e-8，belta1=0.9，belta2=0.999。使用推荐值的 Adam 方法效果 RMSprop 稍好。但 SGD 加 Nestrevo Momentum 也值得使用。完整的 Adam 方法还包括一个 bias 校正机制。对前几部 m,v 设置成 0 进行补偿。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"梯度更新","slug":"梯度更新","permalink":"https://leezhao415.github.io/tags/%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0/"}]},{"title":"多模态架构案例-爱奇艺短视频分类技术解析","slug":"多模态架构案例-爱奇艺短视频分类技术解析","date":"2021-07-17T02:26:25.000Z","updated":"2021-07-17T02:27:22.106Z","comments":true,"path":"2021/07/17/多模态架构案例-爱奇艺短视频分类技术解析/","link":"","permalink":"https://leezhao415.github.io/2021/07/17/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%9E%B6%E6%9E%84%E6%A1%88%E4%BE%8B-%E7%88%B1%E5%A5%87%E8%89%BA%E7%9F%AD%E8%A7%86%E9%A2%91%E5%88%86%E7%B1%BB%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/","excerpt":"","text":"文章目录 爱奇艺短视频分类技术解析 简介 技术难点 分类体系复杂 需要文本、图像、生态信息等多模态特征综合判断 解决方案 特征表示模块 01 文本表示 02 图像表示 层次分类模块 后续工作 爱奇艺短视频分类技术解析 简介 近年来，短视频领域一直广受关注，且发展迅速。每天有大量 UGC 短视频被生产、分发和消费，为生产系统带来了巨大的压力，其中的难点之一就是为每个短视频快速、准确地打上标签。为了解决人工编辑的时效和积压问题，自动化标签技术成为各大内容领域公司都非常关注的关键课题。短视频大规模层次分类作为内容理解技术的一个重要方向，为爱奇艺的短视频智能分发业务提供着强力支持，其输出被称为 “类型标签”。 以下是我们对一条爱奇艺短视频的分类效果：（https://www.infoq.cn/article/f49e-Gb1xQxh8DttFDgb） 算法结果：游戏 - 题材 - 角色扮演，与人工结果一致。其实 “漫威”、“蜘蛛侠” 这类 IP 的作品既可能是 “影视” 也可能是 “游戏”，或者其他周边，如果缺乏背景知识，人工也不容易做出准确的分类，但是模型由于见到了足够多的样本，反而比单个人工有更大概率做出正确判断，在一定程度上体现了集体智慧和算法的优势。 类型标签在爱奇艺内部有着广泛的应用。 在短视频生产领域，类型标签从视频的生成、准入、审核、标注等多个方面发挥着重要作用。 标签自动化：部分标签的准确率已经达到 95% 以上，这部分标签已经用算法结果替代人工标注，减少了大量标注人力，提高了视频生产效率； 频道自动化：目前的频道由上传者填写，上传者会投机取巧乱填频道导致频道混乱，影响用户的使用体验，使用类型标签替换频道，提升了频道的分类准确率。 由于准确率很高，短视频生产系统乐高已经部分将自动化标签代替人工标签，并推送到各个业务线，支持着大量业务的智能运营策略。 在个性化推荐领域，已使用算法生成的类型标签全面替代人工标注的频道，成为推荐系统最重要的基础数据之一，在以下的策略中发挥了重要作用。 多样性控制：使用标签完成多样性控制，减少相似内容对用户带来的疲劳，提升播放时长等关键业务指标和多样性等生态指标； 用户画像：基于标签完善用户的长期兴趣和短期兴趣，提升用户画像的完整性、准确性和可解释性； 召回：增强无用户行为的新视频的分发能力，提升用户兴趣探索阶段的泛化性，提升用户的负向兴趣过滤的泛化性，从而提升用户体验； 排序：基于画像的用户兴趣和视频类型标签作为模型的特征，增强排序模型的排序效果。 本文将详细介绍爱奇艺短视频大规模层次分类算法。 技术难点 分类体系复杂 短视频分类体系是一棵人工精心制定的层次结构，体系和规则都比较复杂：层级最少有 3 级，最多有 5 级，总计近 800 个有效类别，类别间有互斥和共同出现的需求。 需要文本、图像、生态信息等多模态特征综合判断 短视频具有标题、描述、封面图、视频、音频等媒体信息。同时，一个短视频也不一定是独立存在的，它可能来自一个影视、综艺片段，它的上传者可能是一个垂直领域的内容贡献者，所以，关联正片、视频来源、上传者等信息对分类也可能有帮助。 解决方案 短视频分类可以分为特征表示 (Feature Representation) 和层次分类 (Hierarchical Classification) 两个模块，前者基于多模态特征建模短视频的整体表达（在我们的模型中通过 Feature Representation 和 Representation Fusion 两个子网络级联建模完成），后者基于前者完成分类任务。我们模型的整体结构如下图： 下文将分别介绍这两个模块。 特征表示模块 短视频的特征种类和形态各异，只有正确使用这些信息才能提升模型效果的天花板，下文将介绍各种特征表示的建模方式以及融合方式。 01 文本表示 短视频一般都有一个代表其视频意义的简短标题和更为详细的描述信息，通过对这些人工抽象出的文本信息进行分类会比直接从视频学习出分类更容易。下文将首先介绍业界常见的文本表建模方式，然后分享在我们任务中采用的方案。 业界常见建模方式： 1.BOW Bag-of-words model 忽略掉文档的语法和语序等要素，将其仅仅看作是若干个词汇的集合，每个单词的出现都是独立的，由一组无序的单词 (words) 来表达。实际操作上可以直接使用线性分类（单层 NN，下左图）或者嵌入到一个词向量空间中进行 AVG 等操作后再进行分类（CBOW，多层 NN，下右图）。由于模型假设文档是一个词袋，忽略了出现的顺序和组合，所以在构建特征时，可以考虑将表示了词组的 ngram 和词共现的组合特征放入模型中，提高模型的效果。 优点：建模容易，性能好，在使用了大量人工构造的特征后也可以达到极佳的效果。 缺点：过渡依赖人工特征的构造，构造的人工特征可能因为过大，在模型训练上带来困难。 2.CNN 利用 CNN 对文本建模表示进行分类是源自图像领域 CNN 取得的巨大成功，但是在文本领域仅用 CNN 进行文本建模效果并不突出。CNN 通过不同大小的 filter 对有序的词向量进行卷积操作，以期望模型能够从中学到不同大小的 ngram 信息，并且通过 pooling 操作（一般是 max-pooling），找到最强的信号，作为该文本的表示。 优点：建模比较容易，性能不差。 缺点：模型效果上限较低，对长距离共现信息建模较差。 3.RNN 利用 RNN（GRU/LSTM）进行文本建模，理论上具有最高的天花板，在实操上效果也介于 CNN 和精选了人工特征的 BOW，以 LSTM 为例，其不仅对词序敏感，并且具有长短记忆功能，能够将短距离的 ngram 信息和长距离的共现信息学习到。 优点：模型效果上限高，效果较好。 缺点：建模和训练较难，运行时间慢，在大数据集训练实用性不高。 4.Attention 使用 Attention 可以对长距离的共现信息进行建模，并且能够识别整个序列中最为关注的部分，该技术可以和上述的 CNN 和 RNN 这种与序列有关的技术配合使用，能够取得更好的效果，下图是典型的基于点积的（多头）注意力机制。 优点：建模难度一般（Attention 实现方式多种多样），几乎总是能够提升模型效果。 缺点：无明显缺点，可以和其他模型共用。 我们的建模方式： 权衡模型的执行效率和效果，最终类型标签采用的是 BOW 和 CNN+Attention 方式完成文本表示的建模。 1.CBOW 与人工特征构造 前面已经提到 BOW 在使用了大量人工构造的特征后也可以达到极佳的效果，所以我们也尝试了很多人工 / 机器构造的特征： (1) 字、词特征，用以提高模型的泛化能力 (2) Ngram 特征，提供片段特征 (3) 词对特征，提供远距离组合特征 (4) 经过 gbdt 学习到的组合特征，更高维的组合特征 (5) 一些 ID 类的离散特征我们也一起和字和词组合到一起 2. 带位置信息的 CNN 普通的 TextCNN 使用的 Max Pooling 是全文进行，忽略了文本表达的顺序信息，我们将 Max Pooling 以一定步长进行，提取出每个位置上的文本表示。 3.Self-Attention 基于 CNN 提取出的带位置信息的文本表示，我们加入 Attention 结构，组合不同位置的文本表示，并且让模型识别应该关注哪个部分。 02 图像表示 短视频数据存在的文不对题、标题描述类型区分力弱的问题，这些问题都对模型的学习带来较大的困难。封面图作为从短视频中精选的一帧，能够在一定程度上代表短视频主题的意义，并且与文本具有互补性，如果能够从其中识别图像表征，补充到类型标签分类任务，应该能够提升模型的分类效果。 表达融合方式： 对图像进行表征，并融合到分类模型中，目前业界非常流行的做法是基于预训练的 ImageNet 模型在训练数据较少的目标任务上进行迁移学习，有 3 种方式： 特征抽取 实现方式：把 ImageNet 预训练的模型作为特征抽取器，将模型的某一层或者某几层特征作为类型标签模型特征提取源。 优点：预训练模型容易获取，不需要训练模型，只需要进行特征抽取，上线速度快。 缺点：模型效果差，需要选择抽取那一层的输出作为抽取的特征，需要保留的特征如果很多的话，特征保存的开销会很大。 FineTune + 特征抽取 实现方式：把 ImageNet 预训练的模型以类型标签为目标进行 FineTune，然后将模型的某一层或者某几层特征作为类型标签模型特征提取源（因训练目标一致，一般选择最后一层即可达到较好的效果）。 优点：模型效果好，输出的特征维度低，容易储存。 缺点：FineTune 耗时较大。 模型融合 实现方式：把 ImageNet 预训练的模型嵌入到类型标签的模型当中，让图像的表示和其他特征的表示同时进行训练。 优点：效果最好，End2End 完成最终的上线模型。 缺点：模型训练调参困难，并且耗时巨大。 基于上述 3 种方式的介绍和分析，我们尝试了 1、2 两种方式，最终采纳了第 2 种方式。 模型选择： 图像模型的好坏直接影响到最终提取的图像特征的效果，需要选择一个效果与效率都很高的模型来完成我们的任务，在项目中我们尝试了 ResNet50 和 Xception 两个模型，并且最终选择后者，后者在我们的场景中训练、预测耗时接近，Accuracy 高 3%。 特征融合： 通过上述不同的特征表达方式，每一种特征都被映射为了一个向量，一种好的特征融合方式可以提升表示的整体效果，为此我们尝试了 3 种方案，并最终采用了 LMF 模型。 1.Concatenate 顾名思义，这种方式就是将每种表达连接到一起后连接全连接学习整体的表达，这种方式简单，并且能够提供一个不错的基线。 2.CentralNet[6] 该模型借助多任务对每个模态的表达进行约束，以期 Fusion 后的表达能够获取更好的泛化能力，相对于 Concatenate 有 1% 的效果提升，模型示例如下： 3.LMF[7] LMF (Low-rank Multimodal Fusion) 通过将 N 个模态的外积运算近似等价为内积和按位相乘的运算实现特征的全组合，相对于 CentralNet 有 0.2% 的效果提升，模型示例如下： 层次分类模块 下文将首先介绍业界常见层次分类建模方式，然后分享在我们任务中采用的方案。 业界常见建模方式： 对于层次分类，业界常见的有 4 大类方法。 1. 弹珠机模型 分类树的每个非叶子节点都有一个独立的模型，利用分类信息做数据的划分。优点是扩展性好，但是由于仅从样本维度使用层次信息，未能共享特征表达，而且模型数量和层次结构体系对应，在我们的应用场景中，需要数量巨大的独立模型，代表论文 [1]。以下图为例，预测过程为： (1) 模型 1 预测为影视 (2) 模型 2 预测为电视剧 (3) 模型 3、模型 4 分别预测为古装和解读 2. 级联策略 低层级模型的输出作为高层级模型的特征，仅从分类结果维度使用层次信息，信息利用率低，实验效果不佳。代表论文 [2],[3]。 3. 正则化约束 通过正则化约束，通过让有上下级关系的分类模型的参数具有符合该正则化约束的相似性，正则化方式通过人工先验知识确定，无法让模型学习，正则化罚项超参也需要人工调整，实验代价大，效果不佳。代表论文 [4]。 4. 多任务 将各层级分类的多个任务合并，以共享模型参数方式学习模型的层次结构，共享样本信息和模型参数，使用合并的 Loss 驱动模型调整参数，完成层次结构信息的使用。代表论文 [5]。 我们的解决方案：DHMCN (Dense Hierarchical Multilabel Classification Network) 结合实际应用场景，经过多次迭代升级，形成了最终的解决方案。 V1：上文提到的多任务模型（HMC）：其核心思想可以简化为采用多任务来分别学习一级、叶子的 global 和 local 表示。 V2：借鉴 DenseNet 的思想，尝试让层级间的连接更加的丰富，让模型更加容易收敛，而不会陷入局部最优解。下图是一个可视化的解释： 下图为我们构建的基于多任务的层次分类网络： 其中： X 是短视频的表达，具体构建方式前文已经介绍 AG1 和 AG2 分别表示 Global 的 1 级和末级分类的隐层表达，PG 表示 Global（所有）的分类概率 AL1 和 AL2 分别表示 Local 的 1 级和末级的分类的隐层表达，PL1 和 PL2 分别表示 1 级和末级分类的概率 训练的 Loss 由 PG，PL1 和 PL2 三者与 GroundTruth 计算交叉熵得出 PF 表示合并了 Local 和 Global 的最终分类概率 V3：借鉴级联策略，用一级表示形成权重去指导叶子节点的分类，这样叶子节点就只用专注在某一级的内部去分类，相当于把其他无关的分类全 mask 掉。 这是一个端到端的自动学习，我们通过可视化权重，发现学习到的 Reweight Vector 符合我们的预期：模型在预测出一级分类为 19 号分类时发现应该提升该分类对应的叶子分类的置信度（如下图）。 后续工作 对于长度较短的短视频，将引入视频和音频特征，保证线上服务性能的情况下提升分类效果 对于样本较少的分类，将引入用户搜索、推荐 Session 行为进行训练获取初始化的短视频表达，然后基于该表达继续训练 更加充分的使用视频之间的关系进行训练（同一专辑、剧集、综艺、UP 主等）","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"多模态","slug":"多模态","permalink":"https://leezhao415.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"}]},{"title":"【详解】多模态MMML介绍","slug":"【详解】多模态MMML介绍","date":"2021-07-17T00:59:06.000Z","updated":"2021-10-28T14:24:27.018Z","comments":true,"path":"2021/07/17/【详解】多模态MMML介绍/","link":"","permalink":"https://leezhao415.github.io/2021/07/17/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E5%A4%9A%E6%A8%A1%E6%80%81MMML%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"文章目录 什么是多模态机器学习？ 多模态学习的分类 1 多模态表示学习 Multimodal Representation 2 转化 Translation / 映射 Mapping 3 对齐 Alignment 4 多模态融合 Multimodal Fusion 5 协同学习 Co-learning 结束语 什么是多模态机器学习？ 首先，什么叫做模态（Modality）呢？ 每一种信息的来源或者形式，都可以称为一种模态。例如，人有触觉，听觉，视觉，嗅觉；信息的媒介，有语音、视频、文字等；多种多样的传感器，如雷达、红外、加速度计等。以上的每一种都可以称为一种模态。 同时，模态也可以有非常广泛的定义，比如我们可以把两种不同的语言当做是两种模态，甚至在两种不同情况下采集到的数据集，亦可认为是两种模态。 因此，多模态机器学习，英文全称 MultiModal Machine Learning (MMML)，旨在通过机器学习的方法实现处理和理解多源模态信息的能力。目前比较热门的研究方向是图像、视频、音频、语义之间的多模态学习。 多模态学习从 1970 年代起步，经历了几个发展阶段，在 2010 后全面步入 Deep Learning 阶段。 人其实是一个多模态学习的总和，所以也有” 砖家 “说了，多模态学习才是真正的人工智能发展方向。 本文将针对多模态学习在深度学习发面的研究方向和应用做相关介绍，主要参考了来自 ACL 2017 的《Tutorial on Multimodal Machine Learning》。 多模态学习的分类 多模态学习可以划分为以下五个研究方向： 多模态表示学习 Multimodal Representation 模态转化 Translation 对齐 Alignment 多模态融合 Multimodal Fusion 协同学习 Co-learning 下面将针对这五大研究方向，逐一进行介绍。 1 多模态表示学习 Multimodal Representation 单模态的表示学习负责将信息表示为计算机可以处理的数值向量或者进一步抽象为更高层的特征向量，而多模态表示学习是指通过利用多模态之间的互补性，剔除模态间的冗余性，从而学习到更好的特征表示。主要包括两大研究方向：联合表示（Joint Representations）和协同表示（Coordinated Representations）。 联合表示将多个模态的信息一起映射到一个统一的多模态向量空间； 协同表示负责将多模态中的每个模态分别映射到各自的表示空间，但映射后的向量之间满足一定的相关性约束（例如线性相关）。 利用多模态表示学习到的特征可以用来做信息检索，也可以用于的分类 / 回归任务。下面列举几个经典的应用。 在来自 NIPS 2012 的 《Multimodal learning with deep boltzmann machines》一文中提出将 deep boltzmann machines（DBM） 结构扩充到多模态领域，通过 Multimodal DBM，可以学习到多模态的联合概率分布。 论文中的实验通过 Bimodal DBM，学习图片和文本的联合概率分布 P (图片，文本)。在应用阶段，输入图片，利用条件概率 P (文本 | 图片)，生成文本特征，可以得到图片相应的文本描述；而输入文本，利用条件概率 P (图片 | 文本)，可以生成图片特征，通过检索出最靠近该特征向量的两个图片实例，可以得到符合文本描述的图片。如下图所示： 协同表示学习一个比较经典且有趣的应用是来自于《Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models 》这篇文章。利用协同学习到的特征向量之间满足加减算数运算这一特性，可以搜索出与给定图片满足 “指定的转换语义” 的图片。例如： 狗的图片特征向量 - 狗的文本特征向量 + 猫的文本特征向量 = 猫的图片特征向量 -&gt; 在特征向量空间，根据最近邻距离，检索得到猫的图片 2 转化 Translation / 映射 Mapping 转化也称为映射，负责将一个模态的信息转换为另一个模态的信息。常见的应用包括： 机器翻译（Machine Translation）：将输入的语言 A（即时）翻译为另一种语言 B。类似的还有唇读（Lip Reading）和语音翻译 （Speech Translation），分别将唇部视觉和语音信息转换为文本信息。 图片描述（Image captioning) 或者视频描述（Video captioning)： 对给定的图片 / 视频形成一段文字描述，以表达图片 / 视频的内容。 语音合成（Speech Synthesis）：根据输入的文本信息，自动合成一段语音信号。 模态间的转换主要有两个难点，一个是 open-ended，即未知结束位，例如实时翻译中，在还未得到句尾的情况下，必须实时的对句子进行翻译；另一个是 subjective，即主观评判性，是指很多模态转换问题的效果没有一个比较客观的评判标准，也就是说目标函数的确定是非常主观的。例如，在图片描述中，形成怎样的一段话才算是对图片好的诠释？也许一千个人心中有一千个哈姆雷特吧。 3 对齐 Alignment 多模态的对齐负责对来自同一个实例的不同模态信息的子分支 / 元素寻找对应关系。这个对应关系可以是时间维度的，比如下图所示的 Temporal sequence alignment，将一组动作对应的视频流同骨骼图片对齐。类似的还有电影画面 - 语音 - 字幕的自动对齐。 对齐又可以是空间维度的，比如图片语义分割 （Image Semantic Segmentation）：尝试将图片的每个像素对应到某一种类型标签，实现视觉 - 词汇对齐。 4 多模态融合 Multimodal Fusion 多模态融合（Multimodal Fusion ）负责联合多个模态的信息，进行目标预测（分类或者回归），属于 MMML 最早的研究方向之一，也是目前应用最广的方向，它还存在其他常见的别名，例如多源信息融合（Multi-source Information Fusion）、多传感器融合（Multi-sensor Fusion)。 按照融合的层次，可以将多模态融合分为 pixel level，feature level 和 decision level 三类，分别对应对原始数据进行融合、对抽象的特征进行融合和对决策结果进行融合。而 feature level 又可以分为 early 和 late 两个大类，代表了融合发生在特征抽取的早期和晚期。当然还有将多种融合层次混合的 hybrid 方法。 常见的机器学习方法都可以应用于多模态融合，下面列举几个比较热门的研究方向。 视觉 - 音频识别（Visual-Audio Recognition）： 综合源自同一个实例的视频信息和音频信息，进行识别工作。 多模态情感分析（Multimodal sentiment analysis）： 综合利用多个模态的数据（例如下图中的文字、面部表情、声音），通过互补，消除歧义和不确定性，得到更加准确的情感类型判断结果。 手机身份认证（Mobile Identity Authentication）： 综合利用手机的多传感器信息，认证手机使用者是否是注册用户。 多模态融合研究的难点主要包括如何判断每个模态的置信水平、如何判断模态间的相关性、如何对多模态的特征信息进行降维以及如何对非同步采集的多模态数据进行配准等。 若想了解传统的机器学习方法在此领域的应用，推荐学习清华大学出版的《多源信息融合》（韩崇昭等著）一书。 5 协同学习 Co-learning 协同学习是指使用一个资源丰富的模态信息来辅助另一个资源相对贫瘠的模态进行学习。 比如迁移学习（Transfer Learning）就是属于这个范畴，绝大多数迈入深度学习的初学者尝试做的一项工作就是将 ImageNet 数据集上学习到的权重，在自己的目标数据集上进行微调。 迁移学习比较常探讨的方面目前集中在领域适应性（Domain Adaptation）问题上，即如何将 train domain 上学习到的模型应用到 application domain。 迁移学习领域著名的还有零样本学习（Zero-Shot Learning）和一样本学习（One-Shot Learning），很多相关的方法也会用到领域适应性的相关知识。 Co-learning 中还有一类工作叫做协同训练（Co-training ），它负责研究如何在多模态数据中将少量的标注进行扩充，得到更多的标注信息。 通过以上应用我们可以发现，协同学习是与需要解决的任务无关的，因此它可以用于辅助多模态映射、融合及对齐等问题的研究。 结束语 到此为止，我们对多模态机器学习领域的研究方向和应用进行了一个大致的梳理，受限于篇幅，还有许多未涉及的研究问题。 有什么读后感吗？ 也许你以前没有听过多模态学习（MMML）这个概念，读了此文发现原来自己做的正是 MMML 一个分支； 也许你以前觉得 CV / NLP / SSP 才是人工智能的正统，读了此文发现多学科交叉的 MMML 一样可以玩 DL 溜得飞起； 也许你目前正苦于找不到研究的方向，读了此文发现 MMML 打开了新的大门，原来有这么多的事情可以做。 多模态学习是一个目前热度逐年递增的研究领域，如果大家感兴趣，欢迎留言反馈，后续我们会考虑推出几个热门 MMML 方向的经典 or 前沿论文、模型解析。 推荐几篇入门综述文献 【1】Atrey P K, Hossain M A, El Saddik A, et al. Multimodal fusion for multimedia analysis: a survey[J]. Multimedia systems, 2010, 16(6): 345-379. 【2】Ramachandram D, Taylor G W. Deep multimodal learning: A survey on recent advances and trends[J]. IEEE Signal Processing Magazine, 2017, 34(6): 96-108. 【3】Baltrušaitis T, Ahuja C, Morency L P. Multimodal machine learning: A survey and taxonomy[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"【详解】激活函数","slug":"【详解】激活函数","date":"2021-07-15T10:52:08.000Z","updated":"2021-07-15T13:06:34.825Z","comments":true,"path":"2021/07/15/【详解】激活函数/","link":"","permalink":"https://leezhao415.github.io/2021/07/15/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/","excerpt":"","text":"文章目录 背景 Sigmoid 函数 tanh 函数 ReLU 函数 Leaky ReLU 函数 ELU (Exponential Linear Units) 函数 小结 深度学习的activation function 背景 深度学习的基本原理是基于人工神经网络，信号从一个神经元进入，经过非线性的 activation function，传入到下一层神经元；再经过该层神经元的 activate，继续往下传递，如此循环往复，直到输出层。正是由于这些非线性函数的反复叠加，才使得神经网络有足够的 capacity 来抓取复杂的 pattern，在各个领域取得 state-of-the-art 的结果。显而易见，activation function 在深度学习中举足轻重，也是很活跃的研究领域之一。目前来讲，选择怎样的 activation function 不在于它能否模拟真正的神经元，而在于能否便于优化整个深度神经网络。下面我们简单聊一下各类函数的特点以及为什么现在优先推荐 ReLU 函数。 Sigmoid 函数 Sigmoid 函数是深度学习领域开始时使用频率最高的 activation function。它是便于求导的平滑函数，其导数为，这是优点。然而，Sigmoid 有三大缺点： 容易出现 gradient vanishing 函数输出并不是 zero-centered 幂运算相对来讲比较耗时 Gradient Vanishing 优化神经网络的方法是 Back Propagation，即导数的后向传递：先计算输出层对应的 loss，然后将 loss 以导数的形式不断向上一层网络传递，修正相应的参数，达到降低 loss 的目的。 Sigmoid 函数在深度网络中常常会导致导数逐渐变为 0，使得参数无法被更新，神经网络无法被优化。原因在于两点：(1) 在上图中容易看出，当 中 x 较大或较小时，导数接近 0，而后向传递的数学依据是微积分求导的链式法则，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近 0 (2) Sigmoid 导数的最大值是 0.25，这意味着导数在每一层至少会被压缩为原来的 1/4，通过两层后被变为 1/16，…，通过 10 层后为 1/1048576。请注意这里是 “至少”，导数达到最大值这种情况还是很少见的。 输出不是 zero-centered Sigmoid 函数的输出值恒大于 0，这会导致模型训练的收敛速度变慢。举例来讲，对，如果所有 * Xi 均为正数或负数，那么其对 Wi * 的导数总是正数或负数，这会导致如下图红色箭头所示的阶梯式更新，这显然并非一个好的优化路径。深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用 zero-centered 数据 (可以经过数据预处理实现) 和 zero-centered 输出。 幂运算相对耗时 相对于前两项，这其实并不是一个大问题，我们目前是具备相应计算能力的，但面对深度学习中庞大的计算量，最好是能省则省。之后我们会看到，在 ReLU 函数中，需要做的仅仅是一个 thresholding，相对于幂运算来讲会快很多。 tanh 函数 tanh 读作 Hyperbolic Tangent，如上图所示，它解决了 zero-centered 的输出问题，然而，gradient vanishing 的问题和幂运算的问题仍然存在。 ReLU 函数 ReLU 函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取 sub-gradient，如上图所示。ReLU 虽然简单，但却是近几年的重要成果，有以下几大优点： 解决了 gradient vanishing 问题 (在正区间) 计算速度非常快，只需要判断输入是否大于 0 收敛速度远快于 sigmoid 和 tanh ReLU 也有几个需要特别注意的问题： ReLU 的输出不是 zero-centered Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate 太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用 Xavier 初始化方法，以及避免将 learning rate 设置太大或使用 adagrad 等自动调节 learning rate 的算法。 尽管存在这两个问题，ReLU 目前仍是最常用的 activation function，在搭建人工神经网络的时候推荐优先尝试！ Leaky ReLU 函数 人们为了解决 Dead ReLU Problem，提出了将 ReLU 的前半段设为 0.01x 而非 0。另外一种直观的想法是基于参数的方法，即 Parametric ReLU:，其中 α 可由 back propagation 学出来。理论上来讲，Leaky ReLU 有 ReLU 的所有优点，外加不会有 Dead ReLU 问题，但是在实际操作当中，并没有完全证明 Leaky ReLU 总是好于 ReLU。 ELU (Exponential Linear Units) 函数 ELU 也是为解决 ReLU 存在的问题而提出，显然，ELU 有 ReLU 的基本所有优点，以及： 不会有 Dead ReLU 问题 输出的均值接近 0，zero-centered 它的一个小问题在于计算量稍大。类似于 Leaky ReLU，理论上虽然好于 ReLU，但在实际使用中目前并没有好的证据 ELU 总是优于 ReLU。 小结 建议使用 ReLU 函数，但是要注意初始化和 learning rate 的设置；可以尝试使用 Leaky ReLU 或 ELU 函数；不建议使用 tanh，尤其是 sigmoid 函数。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"激活函数","slug":"激活函数","permalink":"https://leezhao415.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"}]},{"title":"【详解】OpenCV4.X - DNN模块 Python API","slug":"【详解】OpenCV4-X-DNN模块-Python-API","date":"2021-07-15T10:51:37.000Z","updated":"2021-07-15T13:09:20.340Z","comments":true,"path":"2021/07/15/【详解】OpenCV4-X-DNN模块-Python-API/","link":"","permalink":"https://leezhao415.github.io/2021/07/15/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91OpenCV4-X-DNN%E6%A8%A1%E5%9D%97-Python-API/","excerpt":"","text":"文章目录 1. dnn.blobFromImage 2. dnn.blobFromImages 3. dnn.Net_readFromModelOptimizer 4. dnn.NMSBoxes 5. dnn.NMSBoxesRotated 6. dnn.readNet 7. dnn.readNetFromCaffe 8. dnn.readNetFromDarknet 9. dnn.readNetFromModelOptimizer 10. dnn.readNetFromONNX 11. dnn.readNetFromTensorflow 12. dnn.readNetFromTorch 13. dnn.readTensorFromONNX 14. dnn.resetMyriadDevice 15. dnn.shrinkCaffeModel 16. dnn.writeTextGraph OpenCV 4.X 版本集成了很多直接利用 DNN 模块的 Python API 接口. 安装： 1sudo pip install opencv-python 使用： 1234567891011121314151617181920from cv2 import dnn# Variables with simple valuesDNN_BACKEND_DEFAULT = 0DNN_BACKEND_HALIDE = 1DNN_BACKEND_INFERENCE_ENGINE = 2DNN_BACKEND_OPENCV = 3DNN_BACKEND_VKCOM = 4DNN_TARGET_CPU = 0DNN_TARGET_MYRIAD = 3DNN_TARGET_OPENCL = 1DNN_TARGET_OPENCL_FP16 = 2DNN_TARGET_VULKAN = 4__loader__ = None__spec__ = None 1. dnn.blobFromImage 定义： 12345678def blobFromImage(image, scalefactor=None, size=None, mean=None, swapRB=None, crop=None, ddepth=None): pass 作用： 根据输入图像，创建 NCHW 次序的 4-dim blobs. 参数： [1] - image: cv2.imread 读取的图片数据； [2] - scalefactor: 缩放像素值，如 [0, 255] - [0, 1]. [3] - size: 输出图像的尺寸，如 netInWidth,netInHeight. [4] - mean: 从各通道减均值。如果输入 image 为 BGR 次序，且 swapRB=True，则通道次序为 mean−R,mean−G,mean−B. [5] - swapRB: 交换 3 通道图片的第一个和最后一个通道，如 BGR - RGB. [6] - crop: 图像尺寸 resize 后是否裁剪。如果 crop=True ，则，输入图片的尺寸调整 resize 后，一个边对应与 size 的一个维度，而另一个边的值大于等于 size 的另一个维度；然后从 resize 后的图片中心进行 crop. 如果 crop=False ，则无需 crop，只需保持图片的长宽比. [7] - ddepth: 输出 blob 的 Depth. 可选: CV_32F 或 CV_8U. 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142import cv2from cv2 import dnnimport numpy as np import matplotlib.pyplot as pltimg_cv2 = cv2.imread(&quot;test.jpg&quot;)print(&quot;[INFO]Image shape: &quot;, img_cv2.shape)inWidth = 256inHeight = 256outBlob1 = cv2.dnn.blobFromImage(img_cv2, scalefactor=1.0 / 255, size=(inWidth, inHeight), mean=(0, 0, 0), swapRB=False, crop=False)print(&quot;[INFO]outBlob1 shape: &quot;, outBlob1.shape)outimg1 = np.transpose(outBlob1[0], (1, 2, 0))outBlob2 = cv2.dnn.blobFromImage(img_cv2, scalefactor=1.0 / 255, size=(inWidth, inHeight), mean=(0, 0, 0), swapRB=False, crop=True)print(&quot;[INFO]outBlob2 shape: &quot;, outBlob2.shape)outimg2 = np.transpose(outBlob2[0], (1, 2, 0))plt.figure(figsize=[10, 10])plt.subplot(1, 3, 1)plt.title(&#x27;Input image&#x27;, fontsize=16)plt.imshow(cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB))plt.axis(&quot;off&quot;)plt.subplot(1, 3, 2)plt.title(&#x27;Output image - no crop&#x27;, fontsize=16)plt.imshow(cv2.cvtColor(outimg1, cv2.COLOR_BGR2RGB))plt.axis(&quot;off&quot;)plt.subplot(1, 3, 3)plt.title(&#x27;Output image - crop&#x27;, fontsize=16)plt.imshow(cv2.cvtColor(outimg2, cv2.COLOR_BGR2RGB))plt.axis(&quot;off&quot;)plt.show() 2. dnn.blobFromImages 定义： 1234567def blobFromImages(images, scalefactor=None, size=None, mean=None, swapRB=None, crop=None, ddepth=None): pass 作用： 批量处理图片，创建 4-dim blobs. 其它参数类似于 dnn.blobFromImage . 3. dnn.Net_readFromModelOptimizer 定义： 12def Net_readFromModelOptimizer(xml, bin): pass 作用： 从 Intel’s Model Optimizer intermediate representation 创建网络. 参数： [1] - xml: XML 网络拓扑结果的配置文件. [2] - bin: 训练权重值的二值文件. 4. dnn.NMSBoxes 定义： 1234567def NMSBoxes(bboxes, scores, score_threshold, nms_threshold, eta=None, top_k=None): pass 作用： 根据给定的 boxes 和对应的 scores 进行 NMS 处理. 参数： [1] - boxes: 待处理的边界框 bounding boxes. [2] - scores: 对于于待处理边界框的 scores. [3] - score_threshold: 用于过滤 boxes 的 score 阈值. [4] - nms_threshold: NMS 用到的阈值. [5] - indices: NMS 处理后所保留的边界框的索引值. [6] - eta: 自适应阈值公式中的相关系数：nms_thresholdi+1=eta⋅nms_thresholdi [7] - top_k: 如果 top_k&gt;0，则保留最多 top_k 个边界框索引值. 5. dnn.NMSBoxesRotated 定义： 1234567def NMSBoxesRotated(bboxes, scores, score_threshold, nms_threshold, eta=None, top_k=None): pass 6. dnn.readNet 定义： 12def readNet(model, config=None, framework=None): pass 作用： 从支持的格式中加载深度学习网络和模型参数. 参数： [1] - model: 训练的权重参数的模型二值文件，支持的格式有： *.caffemodel Caffe、 *.pb TensorFlow、 *.t7 或 *.net Torch、 *.weights Darknet、 *.bin DLDT. [2] - config: 包含网络配置的文本文件，支持的格式有： *.prototxt Caffe、 *.pbtxt TensorFlow、 *.cfg Darknet、 *.xml DLDT. [3] - framework: 所支持格式的框架名. 该函数自动检测训练模型所采用的深度框架，然后调用 readNetFromCaffe 、 readNetFromTensorflow 、 readNetFromTorch 或 readNetFromDarknet 中的某个函数. 7. dnn.readNetFromCaffe 定义： 12def readNetFromCaffe(prototxt, caffeModel=None): pass 作用： 加载采用 Caffe 的配置网络和训练的权重参数. 8. dnn.readNetFromDarknet 定义： 12def readNetFromDarknet(cfgFile, darknetModel=None): pass 作用： 加载采用 Darknet 的配置网络和训练的权重参数. 9. dnn.readNetFromModelOptimizer 定义： 12def readNetFromModelOptimizer(xml, bin): pass 作用： 加载采用 Intel’s Model Optimizer intermediate representation 的配置网络和训练的权重参数. 10. dnn.readNetFromONNX 定义： 12def readNetFromONNX(onnxFile): pass 作用： 加载 .onnx 模型网络配置参数和权重参数. 11. dnn.readNetFromTensorflow 定义： 12def readNetFromTensorflow(model, config=None): pass 作用： 加载采用 Tensorflow 的配置网络和训练的权重参数. [1] - model: .pb 文件. [2] - config: .pbtxt 文件. 12. dnn.readNetFromTorch 定义： 12def readNetFromTorch(model, isBinary=None): pass 作用： 加载采用 Torch 的配置网络和训练的权重参数. [1] - model: 采用 torch.save() 函数保存的文件. 所支持的 Torch nn.Module 网络层有： 123456789- nn.Sequential- nn.Parallel- nn.Concat- nn.Linear- nn.SpatialConvolution- nn.SpatialMaxPooling, nn.SpatialAveragePooling- nn.ReLU, nn.TanH, nn.Sigmoid- nn.Reshape- nn.SoftMax, nn.LogSoftMax 13. dnn.readTensorFromONNX 定义： 12def readTensorFromONNX(path): pass 作用： 从 .pb 文件创建 blob. [1] - path: 包含 input tensor 的 .pb 文件. 14. dnn.resetMyriadDevice 定义： 123def resetMyriadDevice(): &quot;&quot;&quot; resetMyriadDevice() -&gt; None . @brief Release a Myriad device is binded by OpenCV. . * . * Single Myriad device cannot be shared across multiple processes which uses . * Inference Engine&#x27;s Myriad plugin. &quot;&quot;&quot; pass 15. dnn.shrinkCaffeModel 定义： 12def shrinkCaffeModel(src, dst, layersTypes=None): pass 作用： 将 Caffe 网络的所有权重转换为半精度浮点数值 halfprecisionfloatingpoint. 参数： [1] - src: Caffe 网路的原始单精度浮点数值权重模型文件 (后缀一般为 .caffemodel ). [2] - dst: 转换后的权重文件. [3] - layersTypes: 待转换参数的网络层类型，默认是只转换卷积层和全连接层的权重参数. 16. dnn.writeTextGraph 定义： 123def writeTextGraph(model, output): #note: To reduce output file size, trained weights are not included. pass 作用： 将以 protocol buffer 格式的二值网络，创建为文本表示 Createatextrepresentationforabinarynetworkstoredinprotocolbufferformat. 参数： [1] - model: 二值网络 binarynetwork 的路径. [2] - output: 创建的输出文件路径.","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"OpenCV之DNN模块","slug":"OpenCV之DNN模块","permalink":"https://leezhao415.github.io/tags/OpenCV%E4%B9%8BDNN%E6%A8%A1%E5%9D%97/"}]},{"title":"在怀疑的时代依然需要信仰","slug":"在怀疑的时代依然需要信仰","date":"2021-07-14T11:13:03.000Z","updated":"2021-07-14T12:24:12.264Z","comments":true,"path":"2021/07/14/在怀疑的时代依然需要信仰/","link":"","permalink":"https://leezhao415.github.io/2021/07/14/%E5%9C%A8%E6%80%80%E7%96%91%E7%9A%84%E6%97%B6%E4%BB%A3%E4%BE%9D%E7%84%B6%E9%9C%80%E8%A6%81%E4%BF%A1%E4%BB%B0/","excerpt":"","text":"文章目录 在怀疑的时代依然需要信仰 ——卢新宁2012年北京大学中文系毕业典礼上的演讲 敬爱的老师和亲爱的同学们： 上午好！ 谢谢你们叫我回家。让我有幸再次聆听老师的教诲，分享我亲爱的学弟学妹们的特殊喜悦。一进家门，光阴倒转，刚才那些美好的视频，同学的发言，老师的讲话，都让我觉得所有年轻的故事都不曾走远。 可是，站在你们面前，亲爱的同学们，我才发现，自己真的老了。1988 年，我本科毕业的时候，你们中的绝大多数人还没有出生。那个时候你们的朗朗部长还是众女生仰慕的帅师兄，你们的渭毅老师正与我的同屋女孩爱得地老天荒。而他们的孩子都该考大学了。 就像刚才那首歌唱的，“记忆中最美的春天，难以再回首的昨天”。如果把生活比作一段将理想 “变现” 的历程，我们只是一叠面额有限的现钞，而你们是即将上市的股票。从一张白纸起步的书写，前程无远弗届，一切皆有可能。面对你们，我甚至缺少一分抒发 “过来人” 心得的勇气。 但我先生力劝我来，我的朋友也劝我来，他们都是 84 级的中文系学长。今天，他们有的仍然是一介文人，清贫淡泊；有的已经主政一方，功成名就；有的发了财做了 “富二代” 的爹，也有的离了婚、生活并不如意，但在网上交流时，听说有今天这样一个机会，他们都无一例外地让我一定要来，代表他们，代表那一代人，向自己的弟弟妹妹说点什么。 是的，跟你们一样，我们曾在中文系就读，甚至读过同一门课程，青涩的背影都曾被燕园的阳光，定格在五院青藤缠满的绿墙上。但那是上个世纪的事了，我们之间横亘着 20 多年的时光。 那个时候我们称为理想的，今天或许你们笑称其为空想；那时的我们流行书生论政，今天的你们要面对诫勉谈话；那时的我们熟悉的热词是民主、自由，今天的你们记住的是 “拼爹”、“躲猫猫”、“打酱油”；那个时候的我们喜欢在三角地游荡，而今天的你们习惯隐形于伟大的互联网。 我们那时的中国依然贫穷却豪情万丈，而今天这个世界第二大经济体，还在苦苦寻找迷失的幸福，无数和你们一样的青年喜欢用 “囧” 形容自己的处境。 20 多年时光，中国到底走了多远？存放我们青春记忆的 “三角地” 早已荡然无存，见证你们少年心绪的 “一塔湖图” 正在创造新的历史。你们这一代人，有着远比我们当年更优越的条件，更广博的见识，更成熟的内心，站在更高的起点。 我们想说的是，站在这样高的起点，由北大中文系出发，你们不缺前辈大师的庇荫，更不少历史文化的熏染。《诗经》《楚辞》的世界，老庄孔孟的思想，李白杜甫的词章，构成了你们生命中最为激荡的青春时光。 我不需要提醒你们，未来将如何以具体琐碎消磨这份浪漫与绚烂；也不需要提醒你们，人生将以怎样的平庸世故，消解你们的万丈雄心；更不需要提醒你们，走入社会，要如何变得务实与现实，因为你们终将以一生浸淫其中。 我唯一的害怕，是你们已经不相信了 —— 不相信规则能战胜潜规则，不相信学场有别于官场，不相信学术不等于权术，不相信风骨远胜于媚骨。 你们或许不相信了，因为追求级别的越来越多，追求真理的越来越少；讲待遇的越来越多，讲理想的越来越少；大官越来越多，大师越来越少。 因此，在你们走向社会之际，我想说的只是，请看护好你曾经的激情和理想。在这个怀疑的时代，我们依然需要信仰。 也许有同学会笑话，大师姐写报社论写多了吧，这么高的调子。可如果我告诉各位，这是我的那些中文系同学，那些不管今天处于怎样的职位，遭遇过怎样的人生的同学共同的想法，你们是否会稍微有些重视？是否会多想一下为什么二十多年过去，他们依然如此？ 我知道，与我们这一代相比，你们这一代人的社会化远在你们踏上社会之前就已经开始了，国家的盛世集中在你们的大学时代，但社会的问题也凸显在你们的青春岁月。你们有我们不曾拥有的机遇，但也有我们不曾经历的挑战。 文学理论无法识别毒奶粉的成分，古典文献挡不住地沟油的泛滥。当利益成为唯一的价值，很多人把信仰、理想、道德都当成交易的筹码，我很担心，“怀疑” 会不会成为我们时代否定一切、解构一切的 “粉碎机”？我们会不会因为心灰意冷而随波逐流，变成钱理群先生所言 “精致利己主义”，世故老到，善于表演，懂得配合？而北大会不会像那个日本年轻人所说的，“有的是人才，却并不培养精英”？ 我有一位清华毕业的同事，从大学开始，就自称是 “北大的跟屁虫”。对北大人甚是敬重。谈到 “大清王朝北大荒” 江湖传言，他特认真地对我说：“这个社会更需要的，不是北大人的适应，而是北大人的坚守。” 这让我想起中文系百年时，陈平原先生的一席话。他提到西南联大时的老照片给自己的感动：一群衣衫褴褛的知识分子，器宇轩昂地屹立于天地间。这应当就是国人眼里北大人的形象。 不管将来的你们身处何处，不管将来的你们从事什么职业，是否都能常常自问，作为北大人，我们是否还存有那种浩然之气？那种精神的魅力，充实的人生，“天地之心、生民之命、往圣绝学”，是否还能在我们心中激起共鸣？ 马克思曾慨叹，法兰西不缺少有智慧的人但缺少有骨气的人。今天的中国，同样不缺少有智慧的人但缺少有信仰的人。 也正因此，中文系给我们的教育，才格外珍贵。从母校的教诲出发，20 多年社会生活给的我最大启示是：当许多同龄人都陷于时代的车轮下，那些能幸免的人，不仅因为坚强，更因为信仰。 不用害怕圆滑的人说你不够成熟，不用在意聪明的人说你不够明智，不要照原样接受别人推荐给你的生活，选择坚守、选择理想，选择倾听内心的呼唤，才能拥有最饱满的人生。 梁漱溟先生写过一本书《这个世界会好吗？》。我很喜欢这个书名，它以朴素的设问提出了人生的大问题。这个世界会好吗？事在人为，未来中国的分量和质量，就在各位的手上。 最后，我想将一位学者的话送给亲爱的学弟学妹 —— 无论中国怎样，请记得：你所站立的地方，就是你的中国；你怎么样，中国便怎么样；你是什么，中国便是什么；你有光明，中国便不会黑暗。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"且读文摘","slug":"且读文摘","permalink":"https://leezhao415.github.io/tags/%E4%B8%94%E8%AF%BB%E6%96%87%E6%91%98/"},{"name":"名人名言","slug":"名人名言","permalink":"https://leezhao415.github.io/tags/%E5%90%8D%E4%BA%BA%E5%90%8D%E8%A8%80/"}]},{"title":"Python数据分析第三方库分析","slug":"Python数据分析第三方库分析","date":"2021-07-09T11:41:03.000Z","updated":"2021-07-09T12:25:54.786Z","comments":true,"path":"2021/07/09/Python数据分析第三方库分析/","link":"","permalink":"https://leezhao415.github.io/2021/07/09/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93%E5%88%86%E6%9E%90/","excerpt":"","text":"文章目录 Python 数据分析第三方库分析 @常用库下载地址 1 Numpy 2 Matplotlib 3 Pandas 4 SciPy 5 Scikit-Learn 6 Keras 7 Gensim 8 Scrapy 总结 Python 数据分析第三方库分析 使用原因：Python 可用于数据分析，但其单纯依赖 Python 本身自带的库进行数据分析还是具有一定的局限性的，需要安装第三方扩展库来增强分析和挖掘能力。 Python 数据分析第三方扩展库： Numpy 、 Pandas 、 SciPy 、 Matplotlib 、 Scikit-Learn 、 Keras 、 Gensim 、 Scrapy 等 @常用库下载地址 1 网络爬虫 requests：https://pypi.org/project/requests/ 简洁且简单的处理 HTTP 请求的第三方库 scrapy：https://scrapy.org/ 快速、高层次的 Web 获取框架 2 数据分析 numpy：http://www.numpy.org/ 开源数值计算扩展第三方库 scipy：https://pypi.org/project/scipy/ 专为科学以及工程计算的第三方库 pandas：http://pandas.pydata.org/ 可高效地操作大型数据集的第三方库 3 文本处理 pdfminer：https://pypi.org/project/pdfminer/ 从 PDF 文档中提取各类信息的第三方库 openpyxl：https://pypi.org/project/openpyxl/ 处理 Microsoft Excel 文档的 Python 第三方库 python-docx：https://pypi.org/project/python-docx/ 处理 Microsoft Word 文档的 Python 第三方库 beautifulsoup4：https://pypi.org/project/beautifulsoup4/ 从 HTML 和 XML 文件中解析出数据的第三方库 4 用户图形界面 PyQt5：https://pypi.org/project/PyQt5/ 成熟的商业级 GUI 第三方库 wxpython：https://pypi.org/project/wxPython/ 优秀的 GUI 图形库 pygtk：https://pypi.org/project/PyGTK/ 轻松创建具有图形用户界面程序的第三方库 5 机器学习 Scikit-learn：https://scikit-learn.org/stable/ 简单且高效的数据挖掘和数据分析工具 Tensorflow：https://pypi.org/project/tensorflow/ 人工智能学习系统 Theano ：http://deeplearning.net/software/theano/ 执行深度学习中大规模神经网络算法的运算 6 Web 开发 Django：https://pypi.org/project/Django/ 最流行的开源 Web 应用框架 Pyramid：https://pypi.org/project/pyramid/ 通用、开源的 Python Web 应用程序开发框架 Flask：https://pypi.org/project/Flask/ 轻量级 Web 应用框架 7 游戏开发 Pygame：https://www.python.org/ 面向游戏开发入门的 Python 第三方库 Panda3D：http://www.panda3d.org/ 开源、跨平台的 3D 渲染和游戏开发库 cocos2d：https://pypi.org/project/cocos2d/ 构建 2D 游戏和图形界面交互式应用的框架 8 数据可视化 Matplotlib：https://matplotlib.org/ 提供数据绘图功能的第三方库，主要进行二维图表数据展示 TVTK：https://www.lfd.uci.edu/~gohlke/pythonlibs/ 图形应用函数库，是专业可编程的三维可视化工具 mayavi：https://pypi.org/project/mayavi/ 方便实用的可视化软件 1 Numpy 提供数组支持以及相应的高效处理函数，是 Python 数据分析的基础，也是 SciPy、Pandas 等数据处理和科学计算库最基本的函数功能库。 2 Matplotlib 强大的数据可视化工具和作图库，是主要用于绘制数据图表的 Python 库，提供了绘制各类可视化图形的命令字库、简单的接口，可以方便用户轻松掌握图形的格式，绘制各类可视化图形。 基于 Numpy 的一套 Python 包，这个包提供了吩咐的数据绘图工具，主要用于绘制一些统计图形。 有一套允许定制各种属性的默认设置，可以控制 Matplotlib 中的每一个默认属性：图像大小、每英寸点数、线宽、色彩和样式、子图、坐标轴、网格属性、文字和文字属性。 3 Pandas 最初被用作金融数据分析工具而开发出来，因此 Pandas 为时间序列分析提供了很好的支持 为了解决数据分析任务而创建的，Pandas 纳入了大量的库和一些标准的数据模型，提供了高效的操作大型数据集所需要的工具。 带有坐标轴的数据结构，支持自动或明确的数据对齐。这能防止由于数据结构没有对齐，以及处理不同来源、采用不同索引的数据而产生的常见错误。 4 SciPy 一组专门解决科学计算中各种标准问题域的包的集合，包含的功能有最优化、线性代数、积分、插值、拟合、特殊函数、快速傅里叶变换、信号处理和图像处理、常微分方程求解和其他科学与工程中常用的计算等 包括统计、优化、整合、线性代数模块、傅里叶变换、信号和图像处理、常微分方程求解器等。Scipy 依赖于 Numpy，并提供许多对用户友好的和有效的数值例程，如数值积分和优化。 5 Scikit-Learn Python 常用的机器学习工具包，提供了完善的机器学习工具箱，支持数据预处理、分类、回归、聚类、预测和模型分析等强大机器学习库，其依赖于 Numpy、Scipy 和 Matplotlib 等。 主要功能分为六个部分，分类、回归、聚类、数据降维、模型选择、数据预处理。 自带一些经典的数据集，比如用于分类的 iris 和 digits 数据集，还有用于回归分析的 boston house prices 数据集。该数据集是一种字典结构，数据存储在.data 成员中，输出标签存储在.target 成员中。 Scikit-Learn 还有一些库，比如：用于自然语言处理的 Nltk、用于网站数据抓取的 Scrappy、用于网络挖掘的 Pattern、用于深度学习的 Theano 等。 6 Keras 深度学习库，人工神经网络和深度学习模型，基于 Theano 之上，依赖于 Numpy 和 Scipy，利用它可以搭建普通的神经网络和各种深度学习模型，如语言处理、图像识别、自编码器、循环神经网络、递归审计网络、卷积神经网络等。 7 Gensim 用来做文本主题模型的库，常用于处理语言方面的任务，支持 TF-IDF、LSA、LDA 和 Word2Vec 在内的多种主题模型算法，支持流式训练，并提供了诸如相似度计算、信息检索等一些常用任务的 API 接口。 8 Scrapy 专门为爬虫而生的工具，具有 URL 读取、HTML 解析、存储数据等功能，可以使用 Twisted 异步网络库来处理网络通讯，架构清晰，且包含了各种中间件接口，可以灵活的完成各种需求。 总结 Python 能直接处理数据，而 Pandas 几乎可以像 SQL 那样对数据进行控制。Matplotlib 能够对数据和记过进行可视化，快速理解数据。Scikit-Learn 提供了机器学习算法的支持，Theano 提供了升读学习框架（还可以使用 CPU 加速）。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://leezhao415.github.io/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}]},{"title":"【详解】BERT的3个Embedding的实现原理","slug":"【详解】BERT的3个Embedding的实现原理","date":"2021-07-09T11:32:40.000Z","updated":"2021-07-14T13:27:07.022Z","comments":true,"path":"2021/07/09/【详解】BERT的3个Embedding的实现原理/","link":"","permalink":"https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91BERT%E7%9A%843%E4%B8%AAEmbedding%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"文章目录 概览 1 Token Embeddings 作用 实现 2 Segment Embeddings 作用 实现 3 Position Embeddings 作用 实现 4 合成表示 本文将阐述 BERT 中嵌入层的实现细节，包括 token embeddings、segment embeddings, 和 position embeddings. 概览 下面这幅来自原论文的图清晰地展示了 BERT 中每一个嵌入层的作用： 和大多数 NLP 深度学习模型一样，BERT 将输入文本中的每一个词（token) 送入 token embedding 层从而将每一个词转换成向量形式。但不同于其他模型的是，BERT 又多了两个嵌入层，即 segment embeddings 和 position embeddings。在阅读完本文之后，你就会明白为何要多加这两个嵌入层了。 1 Token Embeddings 作用 正如前面提到的，token embedding 层是要将各个词转换成固定维度的向量。在 BERT 中，每个词会被转换成 768 维的向量表示。 实现 假设输入文本是 “I like strawberries”。下面这个图展示了 Token Embeddings 层的实现过程: 输入文本在送入 token embeddings 层之前要先进行 tokenization 处理。此外，两个特殊的 token 会被插入到 tokenization 的结果的开头 ([CLS]) 和结尾 ([SEP]) 。它们视为后面的分类任务和划分句子对服务的。 tokenization 使用的方法是 WordPiece tokenization. 这是一个数据驱动式的 tokenization 方法，旨在权衡词典大小和 oov 词的个数。这种方法把例子中的 “strawberries” 切分成了 “straw” 和 “berries”。这种方法的详细内容不在本文的范围内。有兴趣的读者可以参阅 Wu et al. (2016) 和 Schuster &amp; Nakajima (2012)。使用 WordPiece tokenization 让 BERT 在处理英文文本的时候仅需要存储 30,522 个词，而且很少遇到 oov 的词。 Token Embeddings 层会将每一个 wordpiece token 转换成 768 维的向量。这样，例子中的 6 个 token 就被转换成了一个 (6, 768) 的矩阵或者是 (1, 6, 768) 的张量（如果考虑 batch_size 的话）。 2 Segment Embeddings 作用 BERT 能够处理对输入句子对的分类任务。这类任务就像判断两个文本是否是语义相似的。句子对中的两个句子被简单的拼接在一起后送入到模型中。那 BERT 如何去区分一个句子对中的两个句子呢？答案就是 segment embeddings. 实现 假设有这样一对句子 (“I like cats”, “I like dogs”)。下面的图成仙了 segment embeddings 如何帮助 BERT 区分两个句子: Segment Embeddings 层只有两种向量表示。前一个向量是把 0 赋给第一个句子中的各个 token, 后一个向量是把 1 赋给第二个句子中的各个 token。如果输入仅仅只有一个句子，那么它的 segment embedding 就是全 0。 3 Position Embeddings 作用 BERT 包含这一串 Transformers (Vaswani et al. 2017)，而且一般认为，Transformers 无法编码输入的序列的顺序性。 博客更加详细的解释了这一问题。总的来说，加入 position embeddings 会让 BERT 理解下面下面这种情况： I think, therefore I am 第一个 “I” 和第二个 “I” 应该有着不同的向量表示。 实现 BERT 能够处理最长 512 个 token 的输入序列。论文作者通过让 BERT 在各个位置上学习一个向量表示来讲序列顺序的信息编码进来。这意味着 Position Embeddings layer 实际上就是一个大小为 (512, 768) 的 lookup 表，表的第一行是代表第一个序列的第一个位置，第二行代表序列的第二个位置，以此类推。因此，如果有这样两个句子 “Hello world” 和 “Hi there”, “Hello” 和 “Hi” 会由完全相同的 position embeddings，因为他们都是句子的第一个词。同理，“world” 和 “there” 也会有相同的 position embedding。 4 合成表示 我们已经介绍了长度为 n 的输入序列将获得的三种不同的向量表示，分别是： Token Embeddings， (1, n, 768) ，词的向量表示 Segment Embeddings， (1, n, 768)，辅助 BERT 区别句子对中的两个句子的向量表示 Position Embeddings ，(1, n, 768) ，让 BERT 学习到输入的顺序属性 这些表示会被按元素相加，得到一个大小为 (1, n, 768) 的合成表示。这一表示就是 BERT 编码层的输入了。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NLP-BERT","slug":"NLP-BERT","permalink":"https://leezhao415.github.io/tags/NLP-BERT/"}]},{"title":"【详解】模型优化技巧之优化器和学习率调整","slug":"【详解】模型优化技巧之优化器和学习率调整","date":"2021-07-09T11:31:37.000Z","updated":"2021-07-09T12:25:35.488Z","comments":true,"path":"2021/07/09/【详解】模型优化技巧之优化器和学习率调整/","link":"","permalink":"https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/","excerpt":"","text":"文章目录 PyTorch 十大优化器 1 torch.optim.SGD 2 torch.optim.ASGD 3 torch.optim.Rprop 4 torch.optim.Adagrad 5 torch.optim.Adadelta 6 torch.optim.RMSprop 7 torch.optim.Adam(AMSGrad) 8 torch.optim.Adamax 9 torch.optim.SparseAdam 10 torch.optim.LBFGS PyTorch 六大学习率调整方法 1 lr_scheduler.StepLR 2 lr_scheduler.MultiStepLR 3 lr_scheduler.ExponentialLR 4 lr_scheduler.CosineAnnealingLR 5 lr_scheduler.ReduceLROnPlateau 6 lr_scheduler.LambdaLR PyTorch 十大优化器 1 torch.optim.SGD 1class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False) 功能： 可实现 SGD 优化算法，带动量 SGD 优化算法，带 NAG (Nesterov accelerated gradient) 动量 SGD 优化算法，并且均可拥有 weight_decay 项。 参数： params(iterable)- 参数组 (参数组的概念请查看 3.2 优化器基类：Optimizer)，优化器要管理的那部分参数。 lr(float)- 初始学习率，可按需随着训练过程不断调整学习率。 momentum(float)- 动量，通常设置为 0.9，0.8 dampening(float)- dampening for momentum ，暂时不了其功能，在源码中是这样用的：buf.mul_(momentum).add_(1 - dampening, d_p)，值得注意的是，若采用 nesterov，dampening 必须为 0. weight_decay(float)- 权值衰减系数，也就是 L2 正则项的系数 nesterov (bool)- bool 选项，是否使用 NAG (Nesterov accelerated gradient) 注意事项： pytroch 中使用 SGD 十分需要注意的是，更新公式与其他框架略有不同！ pytorch 中是这样的： 123v=ρ∗v+gp=p−lr∗v = p - lr∗ρ∗v - lr∗g12 其他框架： 123v=ρ∗v+lr∗gp=p−v = p - ρ∗v - lr∗g12 ρ 是动量，v 是速率，g 是梯度，p 是参数，其实差别就是在 ρ∗v 这一项，pytorch 中将此项也乘了一个学习率。 2 torch.optim.ASGD 1class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0) 功能： ASGD 也成为 SAG，均表示随机平均梯度下降 (Averaged Stochastic Gradient Descent)，简单地说 ASGD 就是用空间换时间的一种 SGD，详细可参看论文：http://riejohnson.com/rie/stograd_nips.pdf 参数： params(iterable) - 参数组 (参数组的概念请查看 3.1 优化器基类：Optimizer)，优化器要优化的那些参数。 lr(float) - 初始学习率，可按需随着训练过程不断调整学习率。 lambd(float) - 衰减项，默认值 1e-4。 alpha(float) - power for eta update ，默认值 0.75。 t0(float) - point at which to start averaging，默认值 1e6。 weight_decay(float) - 权值衰减系数，也就是 L2 正则项的系数。 3 torch.optim.Rprop 1class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50)) 功能： 实现 Rprop 优化方法 (弹性反向传播)，优化方法原文《Martin Riedmiller und Heinrich Braun: Rprop - A Fast Adaptive Learning Algorithm. Proceedings of the International Symposium on Computer and Information Science VII, 1992》 该优化方法适用于 full-batch，不适用于 mini-batch，因而在 min-batch 大行其道的时代里，很少见到。 4 torch.optim.Adagrad 1class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0) 功能： 实现 Adagrad 优化方法 (Adaptive Gradient)，Adagrad 是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。 详细公式请阅读：Adaptive Subgradient Methods for Online Learning and Stochastic Optimization John Duchi, Elad Hazan, Yoram Singer; 12(Jul):2121−2159, 2011.(http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) 5 torch.optim.Adadelta 1class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0) 功能： 实现 Adadelta 优化方法。Adadelta 是 Adagrad 的改进。Adadelta 分母中采用距离当前时间点比较近的累计项，这可以避免在训练后期，学习率过小。 详细公式请阅读:https://arxiv.org/pdf/1212.5701.pdf 6 torch.optim.RMSprop 1class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) 功能： 实现 RMSprop 优化方法（Hinton 提出），RMS 是均方根（root meam square）的意思。RMSprop 和 Adadelta 一样，也是对 Adagrad 的一种改进。RMSprop 采用均方根作为分母，可缓解 Adagrad 学习率下降较快的问题。并且引入均方根，可以减少摆动，详细了解可读：http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf 7 torch.optim.Adam(AMSGrad) 1class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) 功能： 实现 Adam (Adaptive Moment Estimation)) 优化方法。Adam 是一种自适应学习率的优化方法，Adam 利用梯度的一阶矩估计和二阶矩估计动态的调整学习率。吴老师课上说过，Adam 是结合了 Momentum 和 RMSprop，并进行了偏差修正。 参数： amsgrad- 是否采用 AMSGrad 优化方法，asmgrad 优化方法是针对 Adam 的改进，通过添加额外的约束，使学习率始终为正值。(AMSGrad，ICLR-2018 Best-Pper 之一，《On the convergence of Adam and Beyond》)。 详细了解 Adam 可阅读，Adam: A Method for Stochastic Optimization (https://arxiv.org/abs/1412.6980)。 8 torch.optim.Adamax 1class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) 功能： 实现 Adamax 优化方法。Adamax 是对 Adam 增加了一个学习率上限的概念，所以也称之为 Adamax。 详细了解可阅读，Adam: A Method for Stochastic Optimization (https://arxiv.org/abs/1412.6980)(没错，就是 Adam 论文中提出了 Adamax)。 9 torch.optim.SparseAdam 1class torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08) 功能： 针对稀疏张量的一种 “阉割版” Adam 优化方法。 only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters 10 torch.optim.LBFGS 1class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None) 功能： 实现 L-BFGS（Limited-memory Broyden–Fletcher–Goldfarb–Shanno）优化方法。L-BFGS 属于拟牛顿算法。L-BFGS 是对 BFGS 的改进，特点就是节省内存。 PyTorch 六大学习率调整方法 优化器中最重要的一个参数就是学习率，合理的学习率可以使优化器快速收敛。一般在训练初期给予较大的学习率，随着训练的进行，学习率逐渐减小。学习率什么时候减小，减小多少，这就涉及到学习率调整方法。 PyTorch 中提供了六种方法供大家使用，下面将一一介绍，最后对学习率调整方法进行总结。 1 lr_scheduler.StepLR 1class torch.optim.lr_scheduler.StepLR ( optimizer , step_size , gamma=0.1 , last_epoch=-1 ) 功能： 等间隔调整学习率，调整倍数为 gamma 倍，调整间隔为 step_size。间隔单位是 step。需要注意的是，step 通常是指 epoch，不要弄成 iteration 了。 参数： step_size(int) - 学习率下降间隔数，若为 30，则会在 30、60、90… 个 step 时，将学习率调整为 lr*gamma。 gamma(float) - 学习率调整倍数，默认为 0.1 倍，即下降 10 倍。 last_epoch(int) - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始 值。 2 lr_scheduler.MultiStepLR 1class torch.optim.lr_scheduler.MultiStepLR ( optimizer , milestones , gamma=0.1 , last_epoch=-1 ) 功能： 按设定的间隔调整学习率。这个方法适合后期调试使用，观察 loss 曲线，为每个实验定制学习率调整时机。 参数： milestones(list) - 一个 list，每一个元素代表何时调整学习率，list 元素必须是递增的。如 milestones=[30,80,120] gamma(float) - 学习率调整倍数，默认为 0.1 倍，即下降 10 倍。 last_epoch(int) - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始值。 3 lr_scheduler.ExponentialLR 1class torch.optim.lr_scheduler.ExponentialLR ( optimizer , gamma , last_epoch=-1 ) 功能： 按指数衰减调整学习率，调整公式: lr = lr * gammaepoch 参数： gamma - 学习率调整倍数的底，指数为 epoch，即 gammaepoch last_epoch(int) - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始值。 4 lr_scheduler.CosineAnnealingLR 1class torch.optim.lr_scheduler.CosineAnnealingLR ( optimizer , T_max , eta_min=0 , last_epoch=-1 ) 以余弦函数为周期，并在每个周期最大值时重新设置学习率。具体如下图所示 详细请阅读论文《 SGDR: Stochastic Gradient Descent with Warm Restarts》(ICLR-2017)： https://arxiv.org/abs/1608.03983 参数： T_max(int) - 一次学习率周期的迭代次数，即 T_max 个 epoch 之后重新设置学习率。 eta_min(float) - 最小学习率，即在一个周期中，学习率最小会下降到 eta_min，默认值为 0。 学习率调整公式为： 可以看出是以初始学习率为最大学习率，以 2*Tmax 为周期，在一个周期内先下降，后上升。 5 lr_scheduler.ReduceLROnPlateau 1class torch.optim.lr_scheduler.ReduceLROnPlateau ( optimizer , mode=&#x27;min&#x27; ,factor=0.1 , patience=10 , verbose=False , threshold=0.0001 , threshold_mode=&#x27;rel&#x27; , cooldown=0 , min_lr=0 , eps=1e-08 ) 功能： 当某指标不再变化（下降或升高），调整学习率，这是非常实用的学习率调整策略。 例如，当验证集的 loss 不再下降时，进行学习率调整；或者监测验证集的 accuracy，当 accuracy 不再上升时，则调整学习率。 参数： mode(str) - 模式选择，有 min 和 max 两种模式，min 表示当指标不再降低 (如监测 loss)，max 表示当指标不再升高 (如监测 accuracy)。 factor(float) - 学习率调整倍数 (等同于其它方法的 gamma)，即学习率更新为 lr = lr *factor patience(int) - 直译 ——“耐心”，即忍受该指标多少个 step 不变化，当忍无可忍时，调整学习率。 verbose(bool) - 是否打印学习率信息， print (‘Epoch {:5d}: reducing learning rate’ ’ of group {} to {:.4e}.’.format (epoch, i, new_lr)) threshold(float) - Threshold for measuring the new optimum ，配合 threshold_mode 使用。 threshold_mode(str) - 选择判断指标是否达最优的模式，有两种模式，rel 和 abs。 当 threshold_mode==rel，并且 mode==max 时， dynamic_threshold = best * (1 +threshold) ； 当 threshold_mode==rel，并且 mode==min 时， dynamic_threshold = best * (1 -threshold) ； 当 threshold_mode==abs，并且 mode==max 时， dynamic_threshold = best + threshold ； 当 threshold_mode==abs，并且 mode==min 时， dynamic_threshold = best - threshold cooldown (int)- “ 冷却时间 “ ，当调整学习率之后，让学习率调整策略冷静一下，让模型再训练一段时间，再重启监测模式。 min_lr (float or list)- 学习率下限，可为 float ，或者 list ，当有多个参数组时，可用 list 进行设置。 eps (float)- 学习率衰减的最小值，当学习率变化小于 eps 时，则不调整学习率。 6 lr_scheduler.LambdaLR 1class torch.optim.lr_scheduler.LambdaLR ( optimizer , lr_lambda , last_epoch=-1 ) 功能： 为不同参数组设定不同学习率调整策略。调整规则为，lr = base_lr *lmbda (self.last_epoch) 。 参数： lr_lambda(function or list) - 一个计算学习率调整倍数的函数，输入通常为 step，当有多个参数组时，设为 list。 last_epoch(int) - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始值。 1234567891011121314151617181920212223242526例如： ignored_params = list(map(id, net.fc3.parameters())) base_params = filter(lambda p: id(p) not in ignored_params, net.parameters()) optimizer = optim.SGD([&#123;&#x27;params&#x27;: base_params&#125;,&#123;&#x27;params&#x27;: net.fc3.parameters(), &#x27;lr&#x27;: 0.001*100&#125;], 0.001, momentum=0.9,weight_decay=1e-4)lambda1 = lambda epoch: epoch // 3lambda2 = lambda epoch: 0.95 ** epochscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])for epoch in range(100):scheduler.step()print(&#x27;epoch: &#x27;, i, &#x27;lr: &#x27;, scheduler.get_lr())train(...)validate(...)输出： epoch: 0 lr: [0.0, 0.1]epoch: 1 lr: [0.0, 0.095]epoch: 2 lr: [0.0, 0.09025]epoch: 3 lr: [0.001, 0.0857375]epoch: 4 lr: [0.001, 0.081450625]epoch: 5 lr: [0.001, 0.07737809374999999]epoch: 6 lr: [0.002, 0.07350918906249998]epoch: 7 lr: [0.002, 0.06983372960937498]epoch: 8 lr: [0.002, 0.06634204312890622]epoch: 9 lr: [0.003, 0.0630249409724609] 为什么第一个参数组的学习率会是 0 呢？ 来看看学习率是如何计算的。 第一个参数组的初始学习率设置为 0.001, lambda1 = lambda epoch: epoch // 3, 第 1 个 epoch 时，由 lr = base_lr * lmbda (self.last_epoch)，可知道 lr = 0.001 * (0//3) ，又因为 1//3 等于 0，所以导致学习率为 0。 第二个参数组的学习率变化，就很容易看啦，初始为 0.1，lr = 0.1 * 0.95^epoch ，当 epoch 为 0 时，lr=0.1 ，epoch 为 1 时，lr=0.1*0.95。 学习率调整小结 PyTorch 提供了六种学习率调整方法，可分为三大类，分别是 有序调整； 自适应调整； 自定义调整。 第一类，依一定规律有序进行调整，这一类是最常用的，分别是等间隔下降 (Step)，按需设定下降间隔 (MultiStep)，指数下降 (Exponential) 和 CosineAnnealing。这四种方法的调整时机都是人为可控的，也是训练时常用到的。 第二类，依训练状况伺机调整，这就是 ReduceLROnPlateau 方法。该法通过监测某一指标的变化情况，当该指标不再怎么变化的时候，就是调整学习率的时机，因而属于自适应的调整。 第三类，自定义调整，Lambda。Lambda 方法提供的调整策略十分灵活，我们可以为不同的层设定不同的学习率调整方法，这在 fine-tune 中十分有用，我们不仅可为不同的层设定不同的学习率，还可以为其设定不同的学习率调整策略，简直不能更棒！","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NLP-模型优化","slug":"NLP-模型优化","permalink":"https://leezhao415.github.io/tags/NLP-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"}]},{"title":"自然语言处理中的预训练技术发展史—Word Embedding到Bert模型","slug":"自然语言处理中的预训练技术发展史—Word-Embedding到Bert模型","date":"2021-07-09T11:23:20.000Z","updated":"2021-07-14T12:33:43.759Z","comments":true,"path":"2021/07/09/自然语言处理中的预训练技术发展史—Word-Embedding到Bert模型/","link":"","permalink":"https://leezhao415.github.io/2021/07/09/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2%E2%80%94Word-Embedding%E5%88%B0Bert%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"文章目录 自然语言处理中的预训练技术发展史 —Word Embedding 到 Bert 模型 1 图像领域的预训练 2 Word Embedding 考古史 3 从 Word Embedding 到 ELMO 4 从 Word Embedding 到 GPT 5 Bert 的诞生 自然语言处理中的预训练技术发展史 —Word Embedding 到 Bert 模型 转载自：https://zhuanlan.zhihu.com/p/49271699 Bert 受到较高评价的原因： 效果太好了，基本刷新了很多 NLP 的任务的最好性能，有些任务还被刷爆了，这个才是关键。 Bert 具备广泛的通用性，就是说绝大部分 NLP 任务都可以采用类似的两阶段模式直接去提升效果。 客观的说，把 Bert 当做最近两年 NLP 重大进展的集大成者更符合事实。 本文的主题是自然语言处理中的预训练过程，会大致说下 NLP 中的预训练技术是一步一步如何发展到 Bert 模型的，从中可以很自然地看到 Bert 的思路是如何逐渐形成的，Bert 的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说 Bert 是近年来 NLP 重大进展的集大成者。我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在 Bert 身上。要讲自然语言的预训练，得先从图像领域的预训练说起。 1 图像领域的预训练 自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。 图像领域进行预训练的过程: 训练并保存模型：我们设计好网络结构以后，对于图像来说一般是 CNN 的多层叠加网络结构，可以先用某个训练集合比如训练集合 A 或者训练集合 B 对这个网络进行预先训练，在 A 任务上或者 B 任务上学会网络参数，然后存起来以备后用。 调用预训练模型：假设我们面临第三个任务 C，网络结构采取相同的网络结构，在比较浅的几层 CNN 结构，网络参数初始化的时候可以加载 A 任务或者 B 任务学习好的参数，其它 CNN 高层参数仍然随机初始化。之后我们用 C 任务的训练数据来训练网络，此时有两种做法： 浅层加载的参数在训练 C 任务过程中不动，这种方法被称为 Frozen ； 底层网络参数尽管被初始化了，在 C 任务训练过程中仍然随着训练的进程不断改变，这种一般叫 Fine-Tuning ，顾名思义，就是更好地把参数进行调整使得更适应当前的 C 任务。一般图像或者视频领域要做预训练一般都这么做。 这么做有几个好处，首先，如果手头任务 C 的训练集合数据量较少的话，现阶段的好用的 CNN 比如 Resnet/Densenet/Inception 等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如 ImageNet 预先训练好直接拿来初始化大部分网络结构参数，然后再用 C 任务手头比较可怜的数据量上 Fine-tuning 过程去调整参数让它们更适合解决 C 任务，那事情就好办多了。这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。 那么新的问题来了，为什么这种预训练的思路是可行的？ 目前我们已经知道，对于层级的 CNN 结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用 Fine-tuning 用新数据集合清洗掉高层无关的特征抽取器。 用 ImageNet 来做网络的预训练原因： ImageNet 是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱； ImageNet 有 1000 类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。 分量足的万金油当然老少通吃，人人喜爱。 ” 既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞 NLP 的人比搞 CV 的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然！” 嗯，好问题，其实搞 NLP 的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过 word embedding 吗？2003 年出品，陈年技术，馥郁芳香。word embedding 其实就是 NLP 里的早期预训练技术。当然也不能说 word embedding 不成功，一般加到下游任务里，都能有 1 到 2 个点的性能提升，只是没有那么耀眼的成功而已。 没听过？那下面就把这段陈年老账讲给你听听。 2 Word Embedding 考古史 这块大致讲讲 Word Embedding 的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲 Word Embedding 的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说 Word Embedding 之前，先更粗略地说下语言模型，因为一般 NLP 里面做预训练一般的选择是用语言模型任务来做。 什么是语言模型？ 其实看上面这张 PPT 上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数 P 的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。 假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？ 你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称 “神经网络语言模型”，英文小名 NNLM 的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是 Bengio 在 2003 年发表在 JMLR 上的论文。它生于 2003，火于 2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在 2013 年又被 NLP 考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想 2013 年是什么年头，是深度学习开始渗透 NLP 领域的光辉时刻，万里长征第一步，而 NNLM 可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做 NLP 问题，如果你 10 年前坚持用神经网络做 NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。 上面是闲话，闲言碎语不要讲，我们回来讲一讲 NNLM 的思路。先说训练过程，现在看其实很简单，见过 RNN、LSTM、CNN 后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词 * Wt = “Bert”* 前面句子的 t-1 个单词，要求网络正确预测单词 Bert，即最大化： 前面任意单词 * Wi * 用 Onehot 编码（比如：0001000）作为原始单词输入，之后乘以矩阵 Q 后获得向量 C(Wi) ，每个单词的 C(Wi) 拼接，上接隐层，然后接 softmax 去预测后面应该后续接哪个单词。这个 C(Wi) 是什么？这其实就是单词对应的 Word Embedding 值，那个矩阵 Q 包含 V 行，V 代表词典大小，每一行内容代表对应单词的 Word embedding 值。只不过 Q 的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵 Q，当这个网络训练好之后，矩阵 Q 的内容被正确赋值，每一行代表一个单词对应的 Word embedding 值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵 Q，这就是单词的 Word Embedding 是被如何学会的。 2013 年最火的用语言模型做 Word Embedding 的工具是 Word2Vec，后来又出了 Glove，Word2Vec 是怎么工作的呢？看下图。 Word2Vec 的网络结构其实和 NNLM 是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。 Word2Vec 有两种训练方法: CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词； Skip-gram，和 CBOW 正好反过来，输入某个单词，要求网络预测它的上下文单词。 回头看看，NNLM 是怎么训练的？ 输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么 Word2Vec 这么处理？原因很简单，因为 Word2Vec 和 NNLM 不一样，NNLM 的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而 word embedding 只是无心插柳的一个副产品。但是 Word2Vec 目标不一样，它单纯就是要 word embedding 的，这是主产品，所以它完全可以随性地这么去训练网络。 为什么要讲 Word2Vec 呢？ 这里主要是要引出 CBOW 的训练方法，BERT 其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系 BERT 作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。 使用 Word2Vec 或者 Glove，通过做语言模型任务，就可以获得每个单词的 Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成 Word Embedding 后，很容易找出语义相近的其它词汇。 我们的主题是预训练，那么问题是 Word Embedding 这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会 Word Embedding 后下游任务是怎么用它的。 假设如上图所示，我们有个 NLP 的下游任务，比如 QA，就是问答问题，所谓问答问题，指的是给定一个问题 X，给定另外一个句子 Y, 要判断句子 Y 是否是问题 X 的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的 Word Embedding 的。它的使用方法其实和前面讲的 NNLM 是一样的，句子中每个单词以 Onehot 形式作为输入，然后乘以学好的 Word Embedding 矩阵 Q，就直接取出单词对应的 Word Embedding 了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个 Word Embedding 矩阵 Q 其实就是网络 Onehot 层到 embedding 层映射的网络参数矩阵。所以你看到了，使用 Word Embedding 等价于什么？等价于把 Onehot 层到 embedding 层的网络用预训练好的参数矩阵 Q 初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非 Word Embedding 只能初始化第一层网络参数，再高层的参数就无能为力了。下游 NLP 任务在使用 Word Embedding 的时候也类似图像有两种做法，一种是 Frozen，就是 Word Embedding 那层网络参数固定不动；另外一种是 Fine-Tuning，就是 Word Embedding 这层参数使用新的训练集合训练也需要跟着训练过程更新掉。 上面这种做法就是 18 年之前 NLP 领域里面采用预训练的典型做法，之前说过，Word Embedding 其实对于很多下游 NLP 任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用 Word Embedding 的效果没有期待中那么好呢？答案很简单，因为 Word Embedding 有问题呗。这貌似是个比较弱智的答案，关键是 Word Embedding 存在什么问题？这其实是个好问题。 这片在 Word Embedding 头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对 Word Embedding 来说有什么负面影响？如上图所示，比如多义词 Bank，有两个常用含义，但是 Word Embedding 在对 bank 这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过 word2vec，都是预测相同的单词 bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的 word embedding 空间里去。所以 word embedding 无法区分多义词的不同语义，这就是它的一个比较严重的问题。 你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？ ELMO 提供了一种简洁优雅的解决方案。 3 从 Word Embedding 到 ELMO ELMO 是 “Embedding from Language Models” 的简称，其实这个名字并没有反应它的本质思想，提出 ELMO 的论文题目：“Deep contextualized word representation” 更能体现其精髓，而精髓在哪里？在 deep contextualized 这个短语，一个是 deep，一个是 context，其中 context 更关键。在此之前的 Word Embedding 本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的 Word Embedding 不会跟着上下文场景的变化而改变，所以对于比如 Bank 这个词，它事先学好的 Word Embedding 中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含 money 等词）明显可以看出它代表的是 “银行” 的含义，但是对应的 Word Embedding 内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。 ELMO 的本质思想是：我事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用 Word Embedding 的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的 Word Embedding 表示，这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以 ELMO 本身是个根据当前上下文对 Word Embedding 动态调整的思路。 ELMO 采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的 Word Embedding 作为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向 LSTM，目前语言模型训练的任务目标是根据单词 Wi 的上下文去正确预测单词 Wi，Wi 之前的单词序列 Context-before 称为上文，之后的单词序列 Context-after 称为下文。图中左端的前向双层 LSTM 代表正方向编码器，输入的是从左到右顺序的除了预测单词外 Wi 的上文 Context-before；右端的逆向双层 LSTM 代表反方向编码器，输入的是从右到左的逆序的句子下文 Context-after；每个编码器的深度都是两层 LSTM 叠加。这个网络结构其实在 NLP 中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 Snew，句子中每个单词都能得到对应的三个 Embedding: 最底层是单词的 Word Embedding，往上走是第一层双向 LSTM 中对应单词位置的 Embedding，这层编码单词的句法信息更多一些；再往上走是第二层 LSTM 中对应单词位置的 Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO 的预训练过程不仅仅学会单词的 Word Embedding，还学会了一个双层双向的 LSTM 网络结构，而这两者后面都有用。 上面介绍的是 ELMO 的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是 QA 问题，此时对于问句 X，我们可以先将句子 X 作为预训练好的 ELMO 网络的输入，这样句子 X 中每个单词在 ELMO 网络中都能获得对应的三个 Embedding，之后给予这三个 Embedding 中的每一个 Embedding 一个权重 a，这个权重可以学习得来，根据各自权重累加求和，将三个 Embedding 整合成一个。然后将整合后的这个 Embedding 作为 X 句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务 QA 中的回答句子 Y 来说也是如此处理。因为 ELMO 给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为 “Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。 上面这个图是 TagLM 采用类似 ELMO 的思路做命名实体识别任务的过程，其步骤基本如上述 ELMO 的思路，所以此处不展开说了。TagLM 的论文发表在 2017 年的 ACL 会议上，作者就是 AllenAI 里做 ELMO 的那些人，所以可以将 TagLM 看做 ELMO 的一个前导工作。前几天这个 PPT 发出去后有人质疑说 FastAI 的在 18 年 4 月提出的 ULMFiT 才是抛弃传统 Word Embedding 引入新模式的开山之作，我深不以为然。首先 TagLM 出现的更早而且模式基本就是 ELMO 的思路；另外 ULMFiT 使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；再者，尽管 ULFMiT 实验做了 6 个任务，但是都集中在分类问题相对比较窄，不如 ELMO 验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。所以综合看，尽管 ULFMiT 也是个不错的工作，但是重要性跟 ELMO 比至少还是要差一档，当然这是我个人看法。每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质要么思想特别游离现有框架脑洞开得异常大的工作，所以 ULFMiT 我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看 ELMO 论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得 NAACL2018 最佳论文当之无愧，比 ACL 很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。 前面我们提到静态 Word Embedding 无法解决多义词的问题，那么 ELMO 引入上下文动态调整单词的 embedding 后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。上图给了个例子，对于 Glove 训练出的 Word Embedding 来说，多义词比如 play，根据它的 embedding 找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含 play 的句子中体育领域的数量明显占优导致；而使用 ELMO，根据上下文动态调整后的 embedding 不仅能够找出对应的 “演出” 的相同语义的句子，而且还可以保证找出的句子中的 play 对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层 LSTM 编码了很多句法信息，这在这里起到了重要作用。 ELMO 经过这般操作，效果如何呢？实验效果见上图，6 个 NLP 任务中性能都有幅度不同的提升，最高的提升达到 25% 左右，而且这 6 个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。 那么站在现在这个时间节点看，ELMO 有什么值得改进的缺点呢？首先，一个非常明显的缺点在特征抽取器选择方面，ELMO 使用了 LSTM 而不是新贵 Transformer，Transformer 是谷歌在 17 年做机器翻译任务的 “Attention is all you need” 的论文中提出的，引起了相当大的反响，很多研究已经证明了 Transformer 提取特征的能力是要远强于 LSTM 的。如果 ELMO 采取 Transformer 作为特征提取器，那么估计 Bert 的反响远不如现在的这种火爆场面。另外一点，ELMO 采取双向拼接这种融合特征的能力可能比 Bert 一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。 我们如果把 ELMO 这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。除了以 ELMO 为代表的这种基于特征融合的预训练方法外，NLP 里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为 “基于 Fine-tuning 的模式”，而 GPT 就是这一模式的典型开创者。 4 从 Word Embedding 到 GPT GPT 是 “Generative Pre-Training” 的简称，从名字看其含义是指的生成式的预训练。GPT 也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过 Fine-tuning 的模式解决下游任务。上图展示了 GPT 的预训练过程，其实和 ELMO 是类似的，主要不同在于两点：首先，特征抽取器不是用的 RNN，而是用的 Transformer，上面提到过它的特征抽取能力要强于 RNN，这个选择很明显是很明智的；其次，GPT 的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓 “单向” 的含义是指：语言模型训练的任务目标是根据 Wi 单词的上下文去正确预测单词 Wi，*Wi 之前的单词序列 Context-before 称为上文，之后的单词序列 Context-after 称为下文。ELMO 在做语言模型预训练的时候，预测单词 Wi * 同时使用了上文和下文，而 GPT 则只采用 Context-before 这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到 Word Embedding 中，是很吃亏的，白白丢掉了很多信息。 这里强行插入一段简单提下 Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer 是个叠加的 “自注意力机制（Self Attention）” 构成的深度网络，是目前 NLP 里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到 Transformer 一跃成为踢开 RNN 和 CNN 传统特征提取器，荣升头牌，大红大紫。你问了：什么是注意力机制？这里再插个广告，对注意力不了解的可以参考鄙人 16 年出品 17 年修正的下文：“深度学习中的注意力模型”，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。而介绍 Transformer 比较好的文章可以参考以下两篇文章：一个是 Jay Alammar 可视化地介绍 Transformer 的博客文章 The Illustrated Transformer ，非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学 NLP 研究组写的 “The Annotated Transformer. ”，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解 Transformer 了，所以这里不展开介绍。 其次，我的判断是 Transformer 在未来会逐渐替代掉 RNN 成为主流的 NLP 工具，RNN 一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正 RNN 结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说 CNN，CNN 在 NLP 里一直没有形成主流，CNN 的最大优点是易于做并行计算，所以速度快，但是在捕获 NLP 的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显 Transformer 同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过 RNN 和 CNN。 好了，题外话结束，我们再回到主题，接着说 GPT。上面讲的是 GPT 如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和 ELMO 的方式大有不同。 上图展示了 GPT 在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向 GPT 的网络结构看齐，把任务的网络结构改造成和 GPT 的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化 GPT 的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行 Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。 这里引入了一个新问题：对于 NLP 各种花样的不同任务，怎么改造才能靠近 GPT 的网络结构呢？ GPT 论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如 Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。 GPT 的效果是非常令人惊艳的，在 12 个任务里，9 个达到了最好的效果，有些任务性能提升非常明显。 那么站在现在的时间节点看，GPT 有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有 Bert 太多事了。当然，即使如此 GPT 也是非常非常好的一个工作，跟 Bert 比，其作者炒作能力亟待提升。 5 Bert 的诞生 我们经过跋山涉水，终于到了目的地 Bert 模型了。 Bert 采用和 GPT 完全相同的两阶段模型，首先是语言模型预训练；其次是使用 Fine-Tuning 模式解决下游任务。和 GPT 的最主要不同在于在预训练阶段采用了类似 ELMO 的双向语言模型，当然另外一点是语言模型的数据规模要比 GPT 大。所以这里 Bert 的预训练过程不必多讲了。 第二阶段，Fine-Tuning 阶段，这个阶段的做法和 GPT 是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面 Bert 和 GPT 有些不同，下面简单介绍一下。 在介绍 Bert 如何改造下游任务之前，先大致说下 NLP 的几类问题，说这个是为了强调 Bert 的普适性有多强。通常而言，绝大部分 NLP 问题可以归入上图所示的四类任务中： 序列标注：这是最典型的 NLP 任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。 分类任务：比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。 句子关系判断：比如 Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系； 生成式任务：比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。 对于种类如此繁多而且各具特点的下游 NLP 任务，Bert 如何改造输入输出部分使得大部分 NLP 任务都可以使用 Bert 预训练好的模型参数呢？ 对于 句子关系类任务 ，很简单，和 GPT 类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的 Transformer 最后一层位置上面串接一个 softmax 分类层即可。 对于 分类问题 ，与 GPT 一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造； 对于 序列标注 问题，输入部分和单句分类是一样的，只需要输出部分 Transformer 最后一层每个单词对应位置都进行分类即可。 对于机器翻译或者文本摘要，聊天机器人这种 生成式任务 ，同样可以稍作改造即可引入 Bert 的预训练成果。只需要附着在 S2S 结构上，encoder 部分是个深度 Transformer 结构，decoder 部分也是个深度 Transformer 结构。根据任务选择不同的预训练数据初始化 encoder 和 decoder 即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个 Transformer 结构上加装隐层产生输出也是可以的。 从这里可以看出，上面列出的 NLP 四大任务里面，除了生成类任务外，Bert 其它都覆盖到了，而且改造起来很简单直观。尽管 Bert 论文没有提，但是稍微动动脑子就可以想到。 可以看出，NLP 四大类任务都可以比较方便地改造成 Bert 能够接受的方式。这其实是 Bert 的非常大的优点，这意味着它几乎可以做任何 NLP 的下游任务，具备普适性，这是很强的。 Bert 采用这种两阶段方式解决各种 NLP 任务效果如何？在 11 个各种类型的 NLP 任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。 到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert 其实和 ELMO 及 GPT 存在千丝万缕的关系，比如如果我们把 GPT 预训练阶段换成双向语言模型，那么就得到了 Bert；而如果我们把 ELMO 的特征抽取器换成 Transformer，那么我们也会得到 Bert。所以你可以看出： Bert 最关键两点 特征抽取器采用 Transformer 预训练的时候采用双向语言模型 那么新问题来了：对于 Transformer 来说，怎么才能在这个结构上做双向语言模型任务呢？乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看 ELMO 的网络结构图，只需要把两个 LSTM 替换成两个 Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert 没这么做。那么 Bert 是怎么做的呢？我们前面不是提过 Word2Vec 吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个 CBOW 训练方法，所谓写作时候埋伏笔的 “草蛇灰线，伏脉千里”，大概就是这个意思吧？前面提到了 CBOW 方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文 Context-Before 和下文 Context-after 去预测单词。其实 Bert 怎么做的？Bert 就是这么做的。从这里可以看到方法间的继承关系。当然 Bert 作者没提 Word2Vec 及 CBOW 方法，这是我的判断，Bert 作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过 CBOW 估计是不太可能的。 从这里可以看出，在文章开始我说过 Bert 在模型方面其实没有太大创新，更像一个最近几年 NLP 重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实 Bert 本身的效果好和普适性强才是最大的亮点。 那么 Bert 本身在模型和方法角度有什么创新呢？ Masked 语言模型：本质思想其实是 CBOW Next Sentence Prediction。 Masked 双向语言模型原理： 首先，如果所有参与训练的 token 被 100% 的 [MASK], 那么在 fine-tunning 的时候所有单词都是已知的，不存在 [MASK], 那么模型就只能根据其他 token 的信息和语序结构来预测当前词，而无法利用到这个词本身的信息，因为它们从未出现在训练过程中，等于模型从未接触到它们的信息，等于整个语义空间损失了部分信息。采用 80% 的概率下应用 [MASK], 既可以让模型去学着预测这些单词，又以 20% 的概率保留了语义信息展示给模型。 保留下来的信息如果全部使用原始 token, 那么模型在预训练的时候可能会偷懒，直接照抄当前 token 信息。采用 10% 概率下 random token 来随机替换当前 token, 会让模型不能去死记硬背当前的 token, 而去尽力学习单词周边的语义表达和远距离的信息依赖，尝试建模完整的语言信息。 最后再以 10% 的概率保留原始的 token, 意义就是保留语言本来的面貌，让信息不至于完全被遮掩，使得模型可以 &quot;看清&quot; 真实的语言面貌。 Next Sentence Prediction 原理：做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的 Masked 语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多 NLP 任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是 Bert 的一个创新。 上面这个图给出了一个我们此前利用微博数据和开源的 Bert 做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的 masked 语言模型和下句预测任务。训练数据就长这种样子。 顺带讲解下 Bert 的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个 Embedding: Position Embedding：这是因为 NLP 中单词顺序是很重要的特征，需要在这里对位置信息进行编码； Segment Embedding：这个就是我们之前一直提到的单词 embedding； Token Embedding：因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的 embedding 项对应给每个单词。把单词对应的三个 embedding 叠加，就形成了 Bert 的输入。 至于 Bert 在预训练的输出部分如何组织，可以参考上图的注释。 我们说过 Bert 效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟 GPT 相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。 最后，我讲讲我对 Bert 的评价和看法： Bert 是 NLP 里里程碑式的工作，对于后面 NLP 的研究和工业应用会产生长久的影响，这点毫无疑问。 从上文介绍也可以看出，从模型或者方法角度看，Bert 借鉴了 ELMO，GPT 及 CBOW，主要提出了 Masked 语言模型及 Next Sentence Prediction，但是这里 Next Sentence Prediction 基本不影响大局，而 Masked LM 明显借鉴了 CBOW 的思想。所以说 Bert 的模型没什么大的创新，更像最近几年 NLP 重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议。 总结： &lt;1&gt; 两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务 Fine-tuning 或者做特征集成； &lt;2&gt; 特征抽取要用 Transformer 作为特征提取器而不是 RNN 或者 CNN； &lt;3&gt; 双向语言模型可以采取 CBOW 的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。Bert 最大的亮点在于效果好及普适性强，几乎所有 NLP 任务都可以套用 Bert 这种两阶段解决思路，而且效果应该会有明显提升。 可以预见的是，未来一段时间在 NLP 应用领域，Transformer 将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。 预训练过程本质 通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中。 当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。 如何引入先验的语言学知识其实一直是 NLP 尤其是深度学习场景下的 NLP 的主要目标之一，不过一直没有太好的解决办法，而 ELMO/GPT/Bert 的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。 NLP 的发展方向 更强的特征抽取器，目前看 Transformer 会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器； 如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。 这就是自然语言模型预训练的发展史。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NLP-发展史","slug":"NLP-发展史","permalink":"https://leezhao415.github.io/tags/NLP-%E5%8F%91%E5%B1%95%E5%8F%B2/"}]},{"title":"NLP之Transformer详解","slug":"NLP之Transformer详解","date":"2021-06-24T15:23:24.000Z","updated":"2021-07-14T13:23:47.375Z","comments":true,"path":"2021/06/24/NLP之Transformer详解/","link":"","permalink":"https://leezhao415.github.io/2021/06/24/NLP%E4%B9%8BTransformer%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"Transformer 详解 Attention is all you need 是一篇将 Attention 思想发挥到极致的论文，出自 Google。这篇论文中提出一个全新的模型，叫 Transformer，抛弃了以往深度学习任务里面使用到的 CNN 和 RNN (其实也不完全是，还是用到了一维卷积)。这个模型广泛应用于 NLP 领域，例如机器翻译，问答系统，文本摘要和语音识别等等方向。 参考资料： Transformer 模型的 PyTorch 实现 《Attention is All You Need》浅读（简介 + 代码） 深度学习中的注意力机制 - 云 + 社区 - 腾讯云 Attention? Attention! Building the Mighty Transformer for Sequence Tagging in PyTorch 文章目录 1 Transformer 整体框架 1.1 Encoder 1.2 Decoder 1.3 Attention 1.4 Self-Attention 1.5 Context-Attention 1.6 Scaled Dot-Product Attention 1.7 Scaled Dot-Product Attention 实现 1.8 Multi-head attention 1.9 Multi-head attention 实现 1.10 Layer normalization 1.11 Mask 1.12 Positional Embedding 1.13 Position-wise Feed-Forward network 2 Transformer 的实现 2.1 Encoder 端 2.2 Decoder 端 2.3 Transformer 模型 1 Transformer 整体框架 和经典的 seq2seq 模型一样，Transformer 模型中也采用了 encoer-decoder 架构。上图的左半边用 NX 框出来的，就代表一层 encoder，其中论文里面的 encoder 一共有 6 层这样的结构。上图的右半边用 NX 框出来的，则代表一层 decoder，同样也有 6 层。 定义输入序列首先经过 word embedding，再和 positional encoding 相加后，输入到 encoder 中。输出序列经过的处理和输入序列一样，然后输入到 decoder。 最后，decoder 的输出经过一个线性层，再接 Softmax。 于上便是 Transformer 的整体框架，下面先来介绍 encoder 和 decoder。 1.1 Encoder encoder 由 6 层相同的层组成，每一层分别由两部分组成： 第一部分是 multi-head self-attention 第二部分是 position-wise feed-forward network，是一个全连接层 两个部分，都有一个残差连接 (residual connection)，然后接着一个 Layer Normalization。 1.2 Decoder 和 encoder 类似，decoder 也是由 6 个相同的层组成，每一个层包括以下 3 个部分: 第一个部分是 multi-head self-attention mechanism 第二部分是 multi-head context-attention mechanism 第三部分是一个 position-wise feed-forward network 和 encoder 一样，上面三个部分的每一个部分，都有一个残差连接，后接一个 Layer Normalization。 decoder 和 encoder 不同的地方在 multi-head context-attention mechanism 1.3 Attention 我在以前的文章中讲过，Attention 如果用一句话来描述，那就是 encoder 层的输出经过加权平均后再输入到 decoder 层中。它主要应用在 seq2seq 模型中，这个加权可以用矩阵来表示，也叫 Attention 矩阵。它表示对于某个时刻的输出 y，它在输入 x 上各个部分的注意力。这个注意力就是我们刚才说到的加权。 Attention 又分为很多种，其中两种比较典型的有加性 Attention 和乘性 Attention。加性 Attention 对于输入的隐状态 直接做 concat 操作，乘性 Attention 则是对输入和输出做 dot 操作。 在 Google 这篇论文中，使用的 Attention 模型是乘性 Attention。 我在之前讲 ESIM 模型的文章里面写过一个 soft-align-attention，大家可以参考体会一下。 1.4 Self-Attention 上面我们说 attention 机制的时候，都会说到两个隐状态 hi 和 St，前者是输入序列第 i 个位置产生的隐状态，后者是输出序列在第 t 个位置产生的隐状态。所谓 self-attention 实际上就是，输出序列就是输入序列。因而自己计算自己的 attention 得分。 1.5 Context-Attention context-attention 是 encoder 和 decoder 之间的 attention，是两个不同序列之间的 attention，与来源于自身的 self-attention 相区别。 不管是哪种 attention，我们在计算 attention 权重的时候，可以选择很多方式，常用的方法有 additive attention local-base general dot-product scaled dot-product Transformer 模型采用的是最后一种：scaled dot-product attention。 1.6 Scaled Dot-Product Attention 那么什么是 scaled dot-product attention 呢？ Google 在论文中对 Attention 机制这么来描述： An attention function can be described as a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility of the query with the corresponding key. 通过 query 和 key 的相似性程度来确定 value 的权重分布。论文中的公式长下面这个样子： 看到 Q，K，V 会不会有点晕，没事，后面会解释。 scaled dot-product attention 的结构图如下所示。 现在来说下 K、Q、V 分别代表什么： 目前可能描述有有点抽象，不容易理解。结合一些应用来说，比如，如果是在自动问答任务中的话，Q 可以代表答案的词向量序列，取 K = V 为问题的词向量序列，那么输出就是所谓的 Aligned Question Embedding。 Google 论文的主要贡献之一是它表明了内部注意力在机器翻译 (甚至是一般的 Seq2Seq 任务）的序列编码上是相当重要的，而之前关于 Seq2Seq 的研究基本都只是把注意力机制用在解码端。 1.7 Scaled Dot-Product Attention 实现 123456789101112131415161718192021222324252627282930313233343536373839import torchimport torch.nn as nnimport torch.functional as Fimport numpy as npclass ScaledDotProductAttention(nn.Module): &quot;&quot;&quot;Scaled dot-product attention mechanism.&quot;&quot;&quot; def __init__(self, attention_dropout=0.0): super(ScaledDotProductAttention, self).__init__() self.dropout = nn.Dropout(attention_dropout) self.softmax = nn.Softmax(dim=2) def forward(self, q, k, v, scale=None, attn_mask=None): &quot;&quot;&quot; 前向传播. Args: q: Queries张量，形状为[B, L_q, D_q] k: Keys张量，形状为[B, L_k, D_k] v: Values张量，形状为[B, L_v, D_v]，一般来说就是k scale: 缩放因子，一个浮点标量 attn_mask: Masking张量，形状为[B, L_q, L_k] Returns: 上下文张量和attention张量 &quot;&quot;&quot; attention = torch.bmm(q, k.transpose(1, 2)) if scale: attention = attention * scale if attn_mask: # 给需要 mask 的地方设置一个负无穷 attention = attention.masked_fill_(attn_mask, -np.inf) # 计算softmax attention = self.softmax(attention) # 添加dropout attention = self.dropout(attention) # 和V做点积 context = torch.bmm(attention, v) return context, attention 1.8 Multi-head attention 理解了 Scaled dot-product attention，Multi-head attention 也很容易理解啦。论文提到，他们发现将 Q、K、V 通过一个线性映射之后，分成 h 份，对每一份进行 scaled dot-product attention 效果更好。然后，把各个部分的结果合并起来，再次经过线性映射，得到最终的输出。这就是所谓的 multi-head attention。上面的超参数 h 就是 heads 的数量。论文默认是 8。 multi-head attention 的结构图如下所示。 1.9 Multi-head attention 实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class MultiHeadAttention(nn.Module): def __init__(self, model_dim=512, num_heads=8, dropout=0.0): super(MultiHeadAttention, self).__init__() self.dim_per_head = model_dim // num_heads self.num_heads = num_heads self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads) self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads) self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads) self.dot_product_attention = ScaledDotProductAttention(dropout) self.linear_final = nn.Linear(model_dim, model_dim) self.dropout = nn.Dropout(dropout) # multi-head attention之后需要做layer norm self.layer_norm = nn.LayerNorm(model_dim) def forward(self, key, value, query, attn_mask=None): # 残差连接 residual = query dim_per_head = self.dim_per_head num_heads = self.num_heads batch_size = key.size(0) # linear projection key = self.linear_k(key) value = self.linear_v(value) query = self.linear_q(query) # split by heads key = key.view(batch_size * num_heads, -1, dim_per_head) value = value.view(batch_size * num_heads, -1, dim_per_head) query = query.view(batch_size * num_heads, -1, dim_per_head) if attn_mask: attn_mask = attn_mask.repeat(num_heads, 1, 1) # scaled dot product attention scale = (key.size(-1)) ** -0.5 context, attention = self.dot_product_attention( query, key, value, scale, attn_mask) # concat heads context = context.view(batch_size, -1, dim_per_head * num_heads) # final linear projection output = self.linear_final(context) # dropout output = self.dropout(output) # add residual and norm layer output = self.layer_norm(residual + output) return output, attention 上面代码中出现的 Residual connection 我在之前一篇文章中讲过，这里不再赘述，只解释 Layer normalization。 1.10 Layer normalization Normalization 有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为 0 方差为 1 的数据。我们在把数据送入激活函数之前进行 normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。 说到 normalization，那就肯定得提到 Batch Normalization。 BN 的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。 BN 的具体做法就是对每一小批数据，在批这个方向上做归一化。如下图所示： 可以看到，右半边求均值是沿着数据 batch N 的方向进行的！ Batch normalization 的计算公式如下： 那么什么是 Layer normalization 呢？它也是归一化数据的一种方式，不过 LN 是在每一个样本上计算均值和方差，而不是 BN 那种在批方向计算均值和方差！ 下面是 LN 的示意图： 和上面的 BN 示意图一比较就可以看出二者的区别啦！ 下面看一下 LN 的公式： 1.11 Mask mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。 其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。 Padding Mask 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。因为这些填充的位置，其实是没什么意义的，所以我们的 attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。 具体的做法是，把这些位置的值加上一个非常大的负数 (负无穷)，这样的话，经过 softmax，这些位置的概率就会接近 0！ 而我们的 padding mask 实际上是一个张量，每个值都是一个 Boolean，值为 false 的地方就是我们要进行处理的地方。 实现： 1234567def padding_mask(seq_k, seq_q): # seq_k 和 seq_q 的形状都是 [B,L] len_q = seq_q.size(1) # `PAD` is 0 pad_mask = seq_k.eq(0) pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1) # shape [B, L_q, L_k] return pad_mask Sequence mask 文章前面也提到，sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。 那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为 1，下三角的值权威 0，对角线也是 0。把这个矩阵作用在每一个序列上，就可以达到我们的目的啦。 具体的代码实现如下： 123456def sequence_mask(seq): batch_size, seq_len &#x3D; seq.size() mask &#x3D; torch.triu(torch.ones((seq_len, seq_len), dtype&#x3D;torch.uint8), diagonal&#x3D;1) mask &#x3D; mask.unsqueeze(0).expand(batch_size, -1, -1) # [B, L, L] return mask 效果如下， 对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要 padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个 mask 相加作为 attn_mask。 其他情况，attn_mask 一律等于 padding mask。 1.12 Positional Embedding 现在的 Transformer 架构还没有提取序列顺序的信息，这个信息对于序列而言非常重要，如果缺失了这个信息，可能我们的结果就是：所有词语都对了，但是无法组成有意义的语句。 为了解决这个问题。论文使用了 Positional Embedding：对序列中的词语出现的位置进行编码。 在实现的时候使用正余弦函数。公式如下： 其中，pos 是指词语在序列中的位置。可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码。 上面的位置编码是绝对位置编码。但是词语的相对位置也非常重要。这就是论文为什么要使用三角函数的原因！ 正弦函数能够表达相对位置信息，主要数学依据是以下两个公式： 具体实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class PositionalEncoding(nn.Module): def __init__(self, d_model, max_seq_len): &quot;&quot;&quot;初始化。 Args: d_model: 一个标量。模型的维度，论文默认是512 max_seq_len: 一个标量。文本序列的最大长度 &quot;&quot;&quot; super(PositionalEncoding, self).__init__() # 根据论文给的公式，构造出PE矩阵 position_encoding = np.array([ [pos / np.power(10000, 2.0 * (j // 2) / d_model) for j in range(d_model)] for pos in range(max_seq_len)]) # 偶数列使用sin，奇数列使用cos position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2]) position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2]) # 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding # 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似 # 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐， # 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码 pad_row = torch.zeros([1, d_model]) position_encoding = torch.cat((pad_row, position_encoding)) # 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码， # Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似 self.position_encoding = nn.Embedding(max_seq_len + 1, d_model) self.position_encoding.weight = nn.Parameter(position_encoding, requires_grad=False) def forward(self, input_len): &quot;&quot;&quot;神经网络的前向传播。 Args: input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。 Returns: 返回这一批序列的位置编码，进行了对齐。 &quot;&quot;&quot; # 找出这一批序列的最大长度 max_len = torch.max(input_len) tensor = torch.cuda.LongTensor if input_len.is_cuda else torch.LongTensor # 对每一个序列的位置进行对齐，在原序列位置的后面补上0 # 这里range从1开始也是因为要避开PAD(0)的位置 input_pos = tensor( [list(range(1, len + 1)) + [0] * (max_len - len) for len in input_len]) return self.position_encoding(input_pos) 1.13 Position-wise Feed-Forward network 这是一个全连接网络，包含两个线性变换和一个非线性函数 (实际上就是 ReLU)。公式如下 这个线性变换在不同的位置都表现地一样，并且在不同的层之间使用不同的参数。 这里实现上用到了两个一维卷积。 实现如下: 1234567891011121314151617class PositionalWiseFeedForward(nn.Module): def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0): super(PositionalWiseFeedForward, self).__init__() self.w1 = nn.Conv1d(model_dim, ffn_dim, 1) self.w2 = nn.Conv1d(ffn_dim, model_dim, 1) self.dropout = nn.Dropout(dropout) self.layer_norm = nn.LayerNorm(model_dim) def forward(self, x): output = x.transpose(1, 2) output = self.w2(F.relu(self.w1(output))) output = self.dropout(output.transpose(1, 2)) # add residual and norm layer output = self.layer_norm(x + output) return output 2 Transformer 的实现 现在可以开始完成 Transformer 模型的构建了，encoder 端和 decoder 端分别都有 6 层，实现如下，首先是 2.1 Encoder 端 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class EncoderLayer(nn.Module): &quot;&quot;&quot;Encoder的一层。&quot;&quot;&quot; def __init__(self, model_dim=512, num_heads=8, ffn_dim=2048, dropout=0.0): super(EncoderLayer, self).__init__() self.attention = MultiHeadAttention(model_dim, num_heads, dropout) self.feed_forward = PositionalWiseFeedForward(model_dim, ffn_dim, dropout) def forward(self, inputs, attn_mask=None): # self attention context, attention = self.attention(inputs, inputs, inputs, padding_mask) # feed forward network output = self.feed_forward(context) return output, attentionclass Encoder(nn.Module): &quot;&quot;&quot;多层EncoderLayer组成Encoder。&quot;&quot;&quot; def __init__(self, vocab_size, max_seq_len, num_layers=6, model_dim=512, num_heads=8, ffn_dim=2048, dropout=0.0): super(Encoder, self).__init__() self.encoder_layers = nn.ModuleList( [EncoderLayer(model_dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)]) self.seq_embedding = nn.Embedding(vocab_size + 1, model_dim, padding_idx=0) self.pos_embedding = PositionalEncoding(model_dim, max_seq_len) def forward(self, inputs, inputs_len): output = self.seq_embedding(inputs) output += self.pos_embedding(inputs_len) self_attention_mask = padding_mask(inputs, inputs) attentions = [] for encoder in self.encoder_layers: output, attention = encoder(output, self_attention_mask) attentions.append(attention) return output, attentions 2.2 Decoder 端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class DecoderLayer(nn.Module): def __init__(self, model_dim, num_heads=8, ffn_dim=2048, dropout=0.0): super(DecoderLayer, self).__init__() self.attention = MultiHeadAttention(model_dim, num_heads, dropout) self.feed_forward = PositionalWiseFeedForward(model_dim, ffn_dim, dropout) def forward(self, dec_inputs, enc_outputs, self_attn_mask=None, context_attn_mask=None): # self attention, all inputs are decoder inputs dec_output, self_attention = self.attention( dec_inputs, dec_inputs, dec_inputs, self_attn_mask) # context attention # query is decoder&#x27;s outputs, key and value are encoder&#x27;s inputs dec_output, context_attention = self.attention( enc_outputs, enc_outputs, dec_output, context_attn_mask) # decoder&#x27;s output, or context dec_output = self.feed_forward(dec_output) return dec_output, self_attention, context_attentionclass Decoder(nn.Module): def __init__(self, vocab_size, max_seq_len, num_layers=6, model_dim=512, num_heads=8, ffn_dim=2048, dropout=0.0): super(Decoder, self).__init__() self.num_layers = num_layers self.decoder_layers = nn.ModuleList( [DecoderLayer(model_dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)]) self.seq_embedding = nn.Embedding(vocab_size + 1, model_dim, padding_idx=0) self.pos_embedding = PositionalEncoding(model_dim, max_seq_len) def forward(self, inputs, inputs_len, enc_output, context_attn_mask=None): output = self.seq_embedding(inputs) output += self.pos_embedding(inputs_len) self_attention_padding_mask = padding_mask(inputs, inputs) seq_mask = sequence_mask(inputs) self_attn_mask = torch.gt((self_attention_padding_mask + seq_mask), 0) self_attentions = [] context_attentions = [] for decoder in self.decoder_layers: output, self_attn, context_attn = decoder( output, enc_output, self_attn_mask, context_attn_mask) self_attentions.append(self_attn) context_attentions.append(context_attn) return output, self_attentions, context_attentions 组合一下 2.3 Transformer 模型 class Transformer(nn.Module): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def __init__(self, src_vocab_size, src_max_len, tgt_vocab_size, tgt_max_len, num_layers=6, model_dim=512, num_heads=8, ffn_dim=2048, dropout=0.2): super(Transformer, self).__init__() self.encoder = Encoder(src_vocab_size, src_max_len, num_layers, model_dim, num_heads, ffn_dim, dropout) self.decoder = Decoder(tgt_vocab_size, tgt_max_len, num_layers, model_dim, num_heads, ffn_dim, dropout) self.linear = nn.Linear(model_dim, tgt_vocab_size, bias=False) self.softmax = nn.Softmax(dim=2)def forward(self, src_seq, src_len, tgt_seq, tgt_len): context_attn_mask = padding_mask(tgt_seq, src_seq) output, enc_self_attn = self.encoder(src_seq, src_len) output, dec_self_attn, ctx_attn = self.decoder( tgt_seq, tgt_len, output, context_attn_mask) output = self.linear(output) output = self.softmax(output) return output, enc_self_attn, dec_self_attn, ctx_attn num_heads, ffn_dim, dropout) self.linear = nn.Linear(model_dim, tgt_vocab_size, bias=False) self.softmax = nn.Softmax(dim=2)def forward(self, src_seq, src_len, tgt_seq, tgt_len): context_attn_mask = padding_mask(tgt_seq, src_seq) output, enc_self_attn = self.encoder(src_seq, src_len) output, dec_self_attn, ctx_attn = self.decoder( tgt_seq, tgt_len, output, context_attn_mask) output = self.linear(output) output = self.softmax(output) return output, enc_self_attn, dec_self_attn, ctx_attn","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NLP","slug":"NLP","permalink":"https://leezhao415.github.io/tags/NLP/"}]},{"title":"NLP模型：从transformer到albert","slug":"NLP模型：从transformer到albert","date":"2021-06-24T15:21:14.000Z","updated":"2021-07-14T13:24:58.219Z","comments":true,"path":"2021/06/24/NLP模型：从transformer到albert/","link":"","permalink":"https://leezhao415.github.io/2021/06/24/NLP%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%BB%8Etransformer%E5%88%B0albert/","excerpt":"","text":"NLP 模型：从 transformer 到 albert 文章目录 1 Transformer 1.1 transformer 整体架构 1.2 transformer 结构原理 1.3 transformer 的技术细节 1.4 transformer 的总结 2 bert 2.1 bert 的背景 2.2 bert 的流程 2.3 bert 的技术细节 2.4 bert 的总结 3 xlnet 3.1 xlnet 的背景 3.2 xlnet 的流程 3.3 xlnet 的技术细节 3.4 xlnet 的总结 4 albert 4.1 albert 的背景 4.2 albert 的流程 4.3 albert 的技术细节 4.4 albert 的总结 5. 其他论文 5.1 gpt 5.2 structbert 5.3 roberta 6. 总结 1 Transformer 1.1 transformer 整体架构 1.2 transformer 结构原理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;1&gt; Inputs是经过padding的输入数据，大小是[batch size, max seq length]。&lt;2&gt; 初始化embedding matrix，通过embedding lookup将Inputs映射成token embedding，大小是[batch size, max seq length, embedding size]，然后乘以embedding size的开方。&lt;3&gt; 通过sin和cos函数创建positional encoding，表示一个token的绝对位置信息，并加入到token embedding中，然后dropout。&lt;4&gt; multi-head attention&lt;4.1&gt; 输入token embedding，通过Dense生成Q，K，V，大小是[batch size, max seq length, embedding size]，然后按第2维split成num heads份并按第0维concat，生成新的Q，K，V，大小是[num heads*batch size, max seq length, embedding size/num heads]，完成multi-head的操作。&lt;4.2&gt; 将K的第1维和第2维进行转置，然后Q和转置后的K的进行点积，结果的大小是[num heads*batch size, max seq length, max seq length]。&lt;4.3&gt; 将&lt;4.2&gt;的结果除以hidden size的开方(在transformer中，hidden size=embedding size)，完成scale的操作。&lt;4.4&gt; 将&lt;4.3&gt;中padding的点积结果置成一个很小的数(-2**32+1)，完成mask操作，后续softmax对padding的结果就可以忽略不计了。&lt;4.5&gt; 将经过mask的结果进行softmax操作。&lt;4.6&gt; 将softmax的结果和V进行点积，得到attention的结果，大小是[num heads*batch size, max seq length, hidden size/num heads]。&lt;4.7&gt; 将attention的结果按第0维split成num heads份并按第2维concat，生成multi-head attention的结果，大小是[batch size, max seq length, hidden size]。Figure 2上concat之后还有一个linear的操作，但是代码里并没有。&lt;5&gt; 将token embedding和multi-head attention的结果相加，并进行Layer Normalization。&lt;6&gt; 将&lt;5&gt;的结果经过2层Dense，其中第1层的activation=relu，第2层activation=None。&lt;7&gt; 功能和&lt;5&gt;一样。&lt;8&gt; Outputs是经过padding的输出数据，与Inputs不同的是，Outputs的需要在序列前面加上一个起始符号“&lt;s&gt;”，用来表示序列生成的开始，而Inputs不需要。&lt;9&gt; 功能和&lt;2&gt;一样。&lt;10&gt; 功能和&lt;3&gt;一样。&lt;11&gt; 功能和&lt;4&gt;类似，唯一不同的一点在于mask，&lt;11&gt;中的mask不仅将padding的点积结果置成一个很小的数，而且将当前token与之后的token的点积结果也置成一个很小的数。&lt;12&gt; 功能和&lt;5&gt;一样。&lt;13&gt; 功能和&lt;4&gt;类似，唯一不同的一点在于Q，K，V的输入，&lt;13&gt;的Q的输入来自于Outputs 的token embedding，&lt;13&gt;的K，V来自于&lt;7&gt;的结果。&lt;14&gt; 功能和&lt;5&gt;一样。&lt;15&gt; 功能和&lt;6&gt;一样。&lt;16&gt; 功能和&lt;7&gt;一样，结果的大小是[batch size, max seq length, hidden size]。&lt;17&gt; 将&lt;16&gt;的结果的后2维和embedding matrix的转置进行点积，生成的结果的大小是[batch size, max seq length, vocab size]。&lt;18&gt; 将&lt;17&gt;的结果进行softmax操作，生成的结果就表示当前时刻预测的下一个token在vocab上的概率分布。&lt;19&gt; 计算&lt;18&gt;得到的下一个token在vocab上的概率分布和真实的下一个token的one-hot形式的cross entropy，然后sum非padding的token的cross entropy当作loss，利用adam进行训练。 1.3 transformer 的技术细节 transformer 中的 self-attention 是从普通的点积 attention 中演化出来的 1.3.1 为什么 &lt;2&gt; 要乘以 embedding size 的开方？ 论文并没有讲为什么这么做，我看了代码，猜测是因为 embedding matrix 的初始化方式是 xavier init，这种方式的方差是 1/embedding size，因此乘以 embedding size 的开方使得 embedding matrix 的方差是 1，在这个 scale 下可能更有利于 embedding matrix 的收敛。 1.3.2 为什么 inputs embedding 要加入 positional encoding？ 因为 self-attention 是位置无关的，无论句子的顺序是什么样的，通过 self-attention 计算的 token 的 hidden embedding 都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个 token 的位置信息，transformer 使用了固定的 positional encoding 来表示 token 在句子中的绝对位置信息。positional encoding 的公式如下： 1.3.3 为什么 &lt;4.2&gt; 的结果要 scale？ 以数组为例，2 个长度是 len，均值是 0，方差是 1 的数组点积会生成长度是 len，均值是 0，方差是 len 的数组。而方差变大会导致 softmax 的输入推向正无穷或负无穷，这时的梯度会无限趋近于 0，不利于训练的收敛。因此除以 len 的开方，可以是数组的方差重新回归到 1，有利于训练的收敛。 1.3.4 为什么 &lt;5&gt; 要将 multi-head attention 的输入和输出相加？ 类似于 resnet 中的残差学习单元，有 ensemble 的思想在里面，解决网络退化问题。 1.3.5 为什么 attention 需要 multi-head，一个大 head 行不行？ multi-head 相当于把一个大空间划分成多个互斥的小空间，然后在小空间内分别计算 attention，虽然单个小空间的 attention 计算结果没有大空间计算得精确，但是多个小空间并行然后 concat 有助于网络捕捉到更丰富的信息，类比 cnn 网络中的 channel。 1.3.6 为什么 multi-head attention 后面要加一个 ffn？ 类比 cnn 网络中，cnn block 和 fc 交替连接，效果更好。相比于单独的 multi-head attention，在后面加一个 ffn，可以提高整个 block 的非线性变换的能力。 1.3.7 为什么 &lt;11&gt; 要 mask 当前时刻的 token 与后续 token 的点积结果？ 自然语言生成 (例如机器翻译，文本摘要) 是 auto-regressive 的，在推理的时候只能依据之前的 token 生成当前时刻的 token，正因为生成当前时刻的 token 的时候并不知道后续的 token 长什么样，所以为了保持训练和推理的一致性，训练的时候也不能利用后续的 token 来生成当前时刻的 token。这种方式也符合人类在自然语言生成中的思维方式。 1.4 transformer 的总结 transformer 刚发表的时候，我刚好在百度 nlp 部实习，当时觉得 transformer 噱头更多一些，在小模型上 self-attention 并不比 rnn，lstm 好。直到大力出奇迹的 bert 出现，深深地打了我的脸，当模型变得越来越大，样本数越来越多的时候，self-attention 无论是并行化带来的训练提速，还是在长距离上的建模，都是要比传统的 rnn，lstm 好很多。transformer 现在已经各种具有代表性的 nlp 预训练模型的基础，bert 系列使用了 transformer 的 encoder，gpt 系列 transformer 的 decoder。在推荐领域，transformer 的 multi-head attention 也应用得很广泛。 2 bert 2.1 bert 的背景 在 bert 之前，将预训练的 embedding 应用到下游任务的方式大致可以分为 2 种，一种是 feature-based，例如 ELMo 这种将经过预训练的 embedding 作为特征引入到下游任务的网络中；一种是 fine-tuning，例如 GPT 这种将下游任务接到预训练模型上，然后一起训练。然而这 2 种方式都会面临同一个问题，就是无法直接学习到上下文信息，像 ELMo 只是分别学习上文和下文信息，然后 concat 起来表示上下文信息，抑或是 GPT 只能学习上文信息。因此，作者提出一种基于 transformer encoder 的预训练模型，可以直接学习到上下文信息，叫做 bert。bert 使用了 12 个 transformer encoder block，在 13G 的数据上进行了预训练，可谓是 nlp 领域大力出奇迹的代表。 2.2 bert 的流程 bert 是在 transformer encoder 的基础之上进行改进的，因此在整个流程上与 transformer encoder 没有大的差别，只是在 embedding，multi-head attention，loss 上有所差别。 2.2.1 bert 和 transformer 在 embedding 上的差异 bert 预训练 and fine-tune bert 和 transformer 在 embedding 上的差异主要有 3 点： &lt;1&gt; transformer 的 embedding 由 2 部分构成，一个是 token embedding，通过 embedding matrix lookup 到 token_ids 上生成表示 token 的向量；一个是 position embedding，是通过 sin 和 cos 函数创建的定值向量。而 bert 的 embedding 由 3 部分构成，第一个同样是 token embedding，通过 embedding matrix lookup 到 token_ids 上生成表示 token 的向量；第二个是 segment embedding，用来表达当前 token 是来自于第一个 segment，还是第二个 segment，因此 segment vocab size 是 2；第三个是 position embedding，与 transformer 不同的是，bert 创建了一个 position embedding matrix，通过 position embedding matrix lookup 到 token_ids 的位置上生成表示 token 位置的位置向量。 &lt;2&gt; transformer 在 embedding 之后跟了一个 dropout，但是 bert 在 embedding 之后先跟了一个 layer normalization，再跟了一个 dropout。 &lt;3&gt; bert 在 token 序列之前加了一个特定的 token“[cls]”，这个 token 对应的向量后续会用在分类任务上；如果是句子对的任务，那么两个句子间使用特定的 token“[seq]” 来分割。 2.2.2 bert 和 transformer 在 multi-head attention 上的差异 bert 和 transformer 在 multi-head attention 上的差异主要有 1 点： &lt;1&gt; transformer 在 &lt; 4.7 &gt; 之后没有 linear 的操作 (也可能是因为我看的 transformer 代码不是官方 transformer 的缘故)，而 bert 在 transformer 的 &lt; 4.7 &gt; 之后有一个 linear 的操作。 2.2.3 bert 和 transformer 在 loss 上的差异 bert 和 transformer 在 loss 上的差异主要有 2 点： &lt;1&gt; transformer 的 loss 是在 decoder 阶段计算的，loss 的计算方式是 transformer 的 &lt; 19&gt;。bert 预训练的 loss 由 2 部分构成，一部分是 NSP 的 loss，就是 token“[cls]” 经过 1 层 Dense，然后接一个二分类的 loss，其中 0 表示 segment B 是 segment A 的下一句，1 表示 segment A 和 segment B 来自 2 篇不同的文本；另一部分是 MLM 的 loss，segment 中每个 token 都有 15% 的概率被 mask，而被 mask 的 token 有 80% 的概率用 “” 表示，有 10% 的概率随机替换成某一个 token，有 10% 的概率保留原来的 token，被 mask 的 token 经过 encoder 后乘以 embedding matrix 的转置会生成在 vocab 上的分布，然后计算分布和真实的 token 的 one-hot 形式的 cross entropy，最后 sum 起来当作 loss。这两部分 loss 相加起来当作 total loss，利用 adam 进行训练。bert fine-tune 的 loss 会根据任务性质来设计，例如分类任务中就是 token“[cls]” 经过 1 层 Dense，然后接了一个二分类的 loss；例如问题回答任务中会在 paragraph 上的 token 中预测一个起始位置，一个终止位置，然后以起始位置和终止位置的预测分布和真实分布为基础设计 loss；例如序列标注，预测每一个 token 的词性，然后以每一个 token 在词性的预测分布和真实分布为基础设计 loss。 &lt;2&gt; bert 在 encoder 之后，在计算 NSP 和 MLM 的 loss 之前，分别对 NSP 和 MLM 的输入加了一个 Dense 操作，这部分参数只对预训练有用，对 fine-tune 没用。而 transformer 在 decoder 之后就直接计算 loss 了，中间没有 Dense 操作。 2.3 bert 的技术细节 2.3.1 为什么 bert 需要额外的 segment embedding? 因为 bert 预训练的其中一个任务是判断 segment A 和 segment B 之间的关系，这就需要 embedding 中能包含当前 token 属于哪个 segment 的信息，然而无论是 token embedding，还是 position embedding 都无法表示出这种信息，因此额外创建一个 segment embedding matrix 用来表示当前 token 属于哪个 segment 的信息，segment vocab size 就是 2，其中 index=0 表示 token 属于 segment A，index=1 表示 token 属于 segment B。 2.3.2 为什么 transformer 的 embedding 后面接了一个 dropout，而 bert 是先接了一个 layer normalization，再接 dropout? LN 是为了解决梯度消失的问题，dropout 是为了解决过拟合的问题。在 embedding 后面加 LN 有利于 embedding matrix 的收敛。 2.3.3 为什么 token 被 mask 的概率是 15%？为什么被 mask 后，还要分 3 种情况？ 15% 的概率是通过实验得到的最好的概率，xlnet 也是在这个概率附近，说明在这个概率下，既能有充分的 mask 样本可以学习，又不至于让 segment 的信息损失太多，以至于影响 mask 样本上下文信息的表达。然而因为在下游任务中不会出现 token“”，所以预训练和 fine-tune 出现了不一致，为了减弱不一致性给模型带来的影响，被 mask 的 token 有 80% 的概率用 “” 表示，有 10% 的概率随机替换成某一个 token，有 10% 的概率保留原来的 token，这 3 个百分比也是多次实验得到的最佳组合，在这 3 个百分比的情况下，下游任务的 fine-tune 可以达到最佳的实验结果。 2.4 bert 的总结 相比于那些说自己很好，但是在实际场景中然并软的论文，bert 是真正地影响了学术界和工业界。无论是 GLUE，还是 SQUAD，现在榜单上的高分方法都是在 bert 的基础之上进行了改进。在我的工作中，用 bert 落地的业务效果也比我预想的要好一些。bert 在 nlp 领域的地位可以类比 cv 领域的 inception 或者 resnet，cv 领域的算法效果在几年前就已经超过了人类的标注准确率，而 nlp 领域直到 bert 的出现才做到这一点。不过 bert 也并不是万能的，bert 的框架决定了这个模型适合解决自然语言理解的问题，因为没有解码的过程，所以 bert 不适合解决自然语言生成的问题。因此如何将 bert 改造成适用于解决机器翻译，文本摘要问题的框架，是今后值得研究的一个点。 3 xlnet 3.1 xlnet 的背景 目前语言预训练模型的模式主要有 2 种，第一种是像 gpt 这种的 auto-regressive 模型，每个时刻都依据之前所有时刻的 token 来预测下一个 token，auto-regressive 的 loss 的定义如下： auto-regressive 的 loss 第二种是像 bert 这种的 auto-encoder 模型，随机 mask 掉句子中若干个 token，然后依据上下文预测被 mask 掉的 token，auto-encoder 的 loss 的定义如下： auto-encoder 的 loss auto-regressive 模型在训练的过程中只能用到上文的信息，但是不会出现训练和推理的 gap；auto-encoder 模型在训练的过程中能利用到上下文信息，但是会出现训练和推理的 gap，训练过程中的在推理的时候并不会出现。因此，作者就提出一种基于 transformer-xl 的融合了 auto-regressive 模型和 auto-encoder 模型优势的 auto-regressive 模型。 3.2 xlnet 的流程 3.2.1 因子分解序 一个句子的因子分解序就是这个句子的 token 的一种随机排列。为了能融合 auto-regressive 模型和 auto-encoder 模型的优势，xlnet 使用因子分解序将上下文信息引入 auto-regressive 的 loss 中。例如句子 1-&gt;2-&gt;3-&gt;4-&gt;5，在 auto-regressive 的 loss 中，预测 token 2 可以利用 token 1 的信息，但是不能利用 token 2/3/4/5 的信息；在引入了因子分解序之后，假设使用了 1-&gt;4-&gt;2-&gt;3-&gt;5 的因子分解序，那么预测 token 2 可以利用 token 1/4 的信息，但是不能利用 token 3/5 的信息。在使用因子分解序之后，并不会影响句子的输入顺序，只是在 transformer-xl 的 multi-head attention 中计算每一个 token 的 attention 结果时会有所改变，原先的方式是 mask 掉当前 token 以及句子中的后续 token，而现在是 mask 掉当前 token 以及因子分解序中的后续 token。这种方式可以在计算当前 token 的 attention 结果时利用到当前 token 的上下文信息，例如上面这个因子分解序，计算 token 2 的 attention 结果时就是用到了 token 1/4 的信息，在原始句子中，token 1 在 token 2 之前，token 4 在 token 2 之后。 因子分解序的实现方式是在计算 multi-head attention 的时候进行了 proper mask。例如 1-&gt;4-&gt;2-&gt;3-&gt;5 的因子分解序，在输入 token 2 时，由于在因子分解序中 token 2 排在 token 1/4 的后面，所以在计算 token 2 的 attention 结果时将 token 2/3/5 进行了 mask，只计算 token 2 和 token 1/4 的点积结果，然后 softmax 以及加权求和当作 attention 的结果。 3.2.2 双流自注意力机制 xlnet 使用了 transformer-xl 的框架，并在 transformer 的基础之上使用了双流自注意力机制。 双流自注意力机制 相比于普通的 transformer，xlnet 多加了一个 multi-head attention+ffn 的计算。双流自注意力机制分为查询流 g 和内容流 h 2 个流。h 就是和 transformer 一样的 multi-head attention，计算第 t 个时刻的 attention 的结果时用到了因子分解序中前 t 个位置的位置信息和 token 信息，而 g 在 transformer 的 multi-head attention 的基础之上做了修改，计算第 t 个时刻的 attention 的结果时只用到了因子分解序中前 t 个位置的位置信息和前 t-1 个位置的 token 信息。在预训练的过程当中，为了降低优化的难度，xlnet 只会计算因子分解序最后的 1/6 或者 1/7 的 token 的 g，然后把 g 融合到 auto-regressive 的 loss 当中进行训练，顺带着训练 h。在预训练结束之后，放弃 g，使用 h 做下游任务的 fine-tune，fine-tune 的过程就和普通的 transfomer 的 fine-tune 一模一样了。 3.3 xlnet 的技术细节 3.3.1 因子分解序的优势 因子分解序创新地将上下文信息融入到 auto-regressive 的 loss 中，理论上，只要模型的预训练将一个句子的所有因子分解序都训练一遍，那么模型就能准确地 get 到句子中每一个 token 和上下文之间的联系。然而实际情况下，一个句子的因子分解序的数量是随着句子长度指数增长的，因此在实际训练中只是用到了句子的某个因子分解序或者某几个因子分解序而已。即便如此，相比于只能 get 到上文信息的 auto-regressive，加了因子分解序之后可以同时 get 到上下文信息，能够提高模型的推理能力。 3.3.2 为什么自注意力要用双流？ 因为普通的 transformer 无法融合因子分解序和 auto-regressive 的 loss，例如 2 个不同的因子分解序 1-&gt;3-&gt;2-&gt;4-&gt;5 和 1-&gt;3-&gt;2-&gt;5-&gt;4，第 1 个句子的 4 和第 2 个句子的 5 在 auto-regressive 的 loss 下的 attention 结果是一样的，因此第 1 个句子的 4 和第 2 个句子的 5 在 vocab 上的预测概率分布也是一样的，这就不符合常理了。造成这种现象的原因在于，auto-regressive 的 loss 是利用前 t-1 个 token 的 token 信息和位置信息预测第 t 个 token，然而因子分解序的第 t 个 token 在原始句子中的位置是不确定的，因此需要额外的信息表示因子分解序中需要预测的 token 在原始句子中的位置。为了达到目的，xlnet 使用双流的 multi-head attention+ffn，查询流 g 利用因子分解序中前 t 个位置的位置信息和前 t-1 个位置的 token 信息计算第 t 个位置的输出信息，而内容流 h 利用因子分解序中前 t 个位置的位置信息和 token 信息计算第 t 个位置的输出信息。在预训练的过程中，使用 g 计算 auto-regressive 的 loss，然后最小化的 loss 的值，顺带着训练 h。预训练完成之后，放弃 g，使用 h 无缝切换到普通 transformer 的 fine-tune。 3.4 xlnet 的总结 由于我也是只看过论文，并没有在实际工作中用过 xlnet，因此我也只能讲讲 xlnet 的理论。在 bert 之后，有很多论文都对 bert 进行了改进，但是创新点都很有限，xlnet 是在我看过的论文中唯一一篇在 transformer 的框架之下将上下文信息和 auto-regressive 的 loss 融合在一起的论文。但是 xlnet 是否真的比 bert 优秀，这还是一个疑问，xlnet 使用了 126G 的数据进行预训练，相比于 bert 的 13G 数据大了一个数量级，在 xlnet 发布之后不久，bert 的改进版 roberta 使用了 160G 的数据进行预训练，又打败了 xlnet。 4 albert 4.1 albert 的背景 增大预训练模型的大小通常能够提高预训练模型的推理能力，但是当预训练模型增大到一定程度之后，会碰到 GPU/TPU memory 的限制。因此，作者在 bert 中加入了 2 项减少参数的技术，能够缩小 bert 的大小，并且修改了 bert NSP 的 loss，在和 bert 有相同参数量的前提之下，有更强的推理能力。 4.2 albert 的流程 4.2.1 词向量矩阵的分解 在 bert 以及诸多 bert 的改进版中，embedding size 都是等于 hidden size 的，这不一定是最优的。因为 bert 的 token embedding 是上下文无关的，而经过 multi-head attention+ffn 后的 hidden embedding 是上下文相关的，bert 预训练的目的是提供更准确的 hidden embedding，而不是 token embedding，因此 token embedding 没有必要和 hidden embedding 一样大。albert 将 token embedding 进行了分解，首先降低 embedding size 的大小，然后用一个 Dense 操作将低维的 token embedding 映射回 hidden size 的大小。bert 的 embedding size=hidden size，因此词向量的参数量是 vocab size * hidden size，进行分解后的参数量是 vocab size * embedding size + embedding size * hidden size，只要 embedding size &lt;&lt; hidden size，就能起到减少参数的效果。 4.2.2 参数共享 bert 的 12 层 transformer encoder block 是串行在一起的，每个 block 虽然长得一模一样，但是参数是不共享的。albert 将 transformer encoder block 进行了参数共享，这样可以极大地减少整个模型的参数量。 4.2.3 sentence order prediction(SOP) 在 auto-encoder 的 loss 之外，bert 使用了 NSP 的 loss，用来提高 bert 在句对关系推理任务上的推理能力。而 albert 放弃了 NSP 的 loss，使用了 SOP 的 loss。NSP 的 loss 是判断 segment A 和 segment B 之间的关系，其中 0 表示 segment B 是 segment A 的下一句，1 表示 segment A 和 segment B 来自 2 篇不同的文本。SOP 的 loss 是判断 segment A 和 segment B 的的顺序关系，0 表示 segment B 是 segment A 的下一句，1 表示 segment A 是 segment B 的下一句。 4.3 albert 的技术细节 4.3.1 参数减少技术 albert 使用了 2 项参数减少的技术，但是 2 项技术对于参数减少的贡献是不一样的，第 1 项是词向量矩阵的分解，当 embedding size 从 768 降到 64 时，可以节省 21M 的参数量，但是模型的推理能力也会随之下降。第 2 项是 multi-head attention+ffn 的参数共享，在 embedding size=128 时，可以节省 77M 的参数量，模型的推理能力同样会随之下降。虽然参数减少会导致了模型推理能力的下降，但是可以通过增大模型使得参数量变回和 bert 一个量级，这时模型的推理能力就超过了 bert。 现在学术界发论文有 2 种常见的套路，第 1 种是往死里加参数加数据量，然后提高模型的推理能力；第 2 种是减参数，然后使模型的推理能力不怎么降。albert 使用的参数减少技术看似是第 2 种，实则是第 1 种。当 bert 从 large 变到 xlarge 时，虽然模型变大到了 1270M，但是模型出现了退化现象，推理能力下跌了一大截，说明在 bert 的框架下，large 已经是模型推理能力的极限了。albert 使用了参数减少技术，相比于 bert 的 large 是 334M，albert 的 large 只有 18M，虽然推理能力比 bert 差，但是参数减少后的 albert 还有成长空间，将 albert 从 large 变到 xlarge，甚至是 xxlarge 时，模型的推理能力又得到了提高，并且超过了 bert 最好的模型。 4.3.2 loss 在 albert 之前，很多 bert 的改进版都对 NSP 的 loss 提出了质疑。structbert 在 NSP 的 loss 上进行了修改，有 1/3 的概率是 segment B 是 segment A 的下一句，有 1/3 的概率是 segment A 是 segment B 的下一句，有 1/3 的概率是 segment A 和 segment B 来自 2 篇不同的文本。roberta 则是直接放弃了 NSP 的 loss，修改了样本的构造方式，将输入 2 个 segment 修改为从一个文本中连续 sample 句子直到塞满 512 的长度。当到达文本的末尾且未塞满 512 的长度时，先增加一个 “[sep]”，再从另一个文本接着 sample，直到塞满 512 的长度。 albert 在 structbert 的基础之上又抛弃了 segment A 和 segment B 来自 2 篇不同的文本的做法，只剩下 1/2 的概率是 segment B 是 segment A 的下一句，1/2 的概率是 segment A 是 segment B 的下一句。论文中给出了这么做的解释，NSP 的 loss 包含了 2 部分功能：topic prediction 和 coherence prediction，其中 topic prediction 要比 coherence prediction 更容易学习，而 MLM 的 loss 也包含了 topic prediction 的功能，因此 bert 难以学到 coherence prediction 的能力。albert 的 SOP loss 抛弃了 segment A 和 segment B 来自 2 篇不同的文本的做法，让 loss 更关注于 coherence prediction，这样就能提高模型在句对关系推理上的能力。 4.4 albert 的总结 albert 虽然减少参数量，但是并不会减少推理时间，推理的过程只不过是从串行计算 12 个 transformer encoder block 变成了循环计算 transformer encoder block 12 次。albert 最大的贡献在于使模型具备了比原始的 bert 更强的成长性，在模型变向更大的时候，推理能力还能够得到提高。 5. 其他论文 5.1 gpt gpt 在 bert 之前就发表了，使用了 transformer decoder 作为预训练的框架。在看到了 decoder 只能 get 上文信息，不能 get 下文信息的缺点之后，bert 改用了 transformer encoder 作为预训练的框架，能够同时 get 上下文信息，获得了巨大的成功。 5.2 structbert structbert 的创新点主要在 loss 上，除了 MLM 的 loss 外，还有一个重构 token 顺序的 loss 和一个判断 2 个 segment 关系的 loss。重构 token 顺序的 loss 是以一定的概率挑选 segment 中的 token 三元组，然后随机打乱顺序，最后经过 encoder 之后能够纠正被打乱顺序的 token 三元组的顺序。判断 2 个 segment 关系的 loss 是 1/3 的概率是 segment B 是 segment A 的下一句，有 1/3 的概率是 segment A 是 segment B 的下一句，有 1/3 的概率是 segment A 和 segment B 来自 2 篇不同的文本，通过 “[cls]” 预测样本属于这 3 种的某一种。 5.3 roberta 在 xlnet 使用 126G 的数据登顶 GLUE 之后不久，roberta 使用 160G 的数据又打败了 xlnet。roberta 的创新点主要有 4 点：第 1 点是动态 mask，之前 bert 使用的是静态 mask，就是数据预处理的时候完成 mask 操作，之后训练的时候同一个样本都是相同的 mask 结果，动态 mask 就是在训练的时候每输入一个样本都要重新 mask，动态 mask 相比静态 mask 有更多不同 mask 结果的数据用于训练，效果很好。第 2 点是样本的构造方式，roberta 放弃了 NSP 的 loss，修改了样本的构造方式，将输入 2 个 segment 修改为从一个文本中连续 sample 句子直到塞满 512 的长度。当到达文本的末尾且未塞满 512 的长度时，先增加一个 “[sep]”，再从另一个文本接着 sample，直到塞满 512 的长度。第 3 点是增大了 batch size，在训练相同数据量的前提之下，增大 batch size 能够提高模型的推理能力。第 4 点是使用了 subword 的分词方法，类比于中文的字，相比于 full word 的分词方法，subword 的分词方法使得词表的大小从 30k 变成了 50k，虽然实验效果上 subword 的分词方法比 full word 差，但是作者坚信 subword 具备了理论优越性，今后肯定会比 full word 好 (手动黑脸)。 6. 总结 nlp 和 cv 的不同点在于 nlp 是认识学习，而 cv 是感知学习，nlp 在 cv 的基础之上多了一个符号映射的过程，正因如此，nlp 领域发展得比 cv 慢很多，cv 领域有很多比较成功的创业公司，有很多能够达到商用程度的子领域，而 nlp 领域就比较少。不过 nlp 领域在 17 年的 transformer 发布之后开始进入快速迭代的时期，bert 的发表使得 nlp 领域的 benchmark 提高了一大截，产生了不少可以达到商用程度的子领域。到了 19 年，nlp 领域的发展可以说是越来越快了，我在国庆的时候开始执笔写这个技术分享，当时 albert 刚发表 1 个星期，等我写完这个技术分享已经到 11 月了，前几天谷歌又发表了一篇 T5，又把 albert 打败了。T5 的论文据说有 50 页，是 nlp 预训练模型的一个综述，值得花时间一看。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NLP","slug":"NLP","permalink":"https://leezhao415.github.io/tags/NLP/"}]},{"title":"NLP之常用预训练模型详解","slug":"NLP之常用预训练模型详解","date":"2021-06-24T15:20:07.000Z","updated":"2021-06-26T02:41:40.118Z","comments":true,"path":"2021/06/24/NLP之常用预训练模型详解/","link":"","permalink":"https://leezhao415.github.io/2021/06/24/NLP%E4%B9%8B%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"文章目录 NLP 中流行的预训练模型 1 BERT 及其变体 2 GPT 3 GPT-2 及其变体 4 Transformer-XL 5 XLNet 及其变体 6 XLM 7 RoBERTa 及其变体 8 DistilBERT 及其变体 9 ALBERT 10 T5 及其变体 11 XLM-RoBERTa 及其变体 NLP 中流行的预训练模型 BERT GPT GPT-2 Transformer-XL XLNet XLM RoBERTa DistilBERT ALBERT T5 XLM-RoBERTa 1 BERT 及其变体 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 bert-base-uncased 12 768 12 110M 小写英文文本 bert-large-uncased 24 1024 16 340M 小写英文文本 bert-base-cased 12 768 12 110M 不区分大小写的英文文本 bert-large-cased 24 1024 16 340M 不区分大小写的英文文本 bert-base-multilingual-uncased 12 768 12 110M 小写的 102 种语言文本 bert-large-multilingual-uncased 24 1024 16 340M 小写的 102 种语言文本 bert-base-chinese 12 768 12 110M 简体和繁体中文文本 bert-base-uncased : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在小写的英文文本上进行训练而得到. bert-large-uncased : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 340M 参数量，在小写的英文文本上进行训练而得到. bert-base-cased : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在不区分大小写的英文文本上进行训练而得到. bert-large-cased : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 340M 参数量，在不区分大小写的英文文本上进行训练而得到. bert-base-multilingual-uncased : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在小写的 102 种语言文本上进行训练而得到. bert-large-multilingual-uncased : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 340M 参数量，在小写的 102 种语言文本上进行训练而得到. bert-base-chinese : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在简体和繁体中文文本上进行训练而得到. 2 GPT 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 openai-gpt 12 768 12 110M 英文语料 openai-gpt : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，由 OpenAI 在英文语料上进行训练而得到. 3 GPT-2 及其变体 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 gpt2 12 768 12 117M GPT-2 英文语料 gpt2-xl 48 1600 25 1558M GPT-2 英文语料 gpt2 : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 117M 参数量，在 OpenAI GPT-2 英文语料上进行训练而得到. gpt2-xl : 编码器具有 48 个隐层，输出 1600 维张量，25 个自注意力头，共 1558M 参数量，在大型的 OpenAI GPT-2 英文语料上进行训练而得到. 4 Transformer-XL 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 transfo-xl-wt103 18 1024 16 257M wikitext-103 英文语料 transfo-xl-wt103 : 编码器具有 18 个隐层，输出 1024 维张量，16 个自注意力头，共 257M 参数量，在 wikitext-103 英文语料进行训练而得到. 5 XLNet 及其变体 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 xlnet-base-cased 12 768 12 110M 英文语料 xlnet-large-cased 24 1024 16 240M 英文语料 xlnet-base-cased : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在英文语料上进行训练而得到. xlnet-large-cased : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 240 参数量，在英文语料上进行训练而得到. 6 XLM 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 xlm-mlm-en-2048 12 2048 16 / 英文语料 xlm-mlm-en-2048 : 编码器具有 12 个隐层，输出 2048 维张量，16 个自注意力头，在英文文本上进行训练而得到. 7 RoBERTa 及其变体 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 roberta-base 12 768 12 125M 英文文本 roberta-large 24 1024 16 355M 英文文本 roberta-base : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 125M 参数量，在英文文本上进行训练而得到. roberta-large : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 355M 参数量，在英文文本上进行训练而得到. 8 DistilBERT 及其变体 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 distilbert-base-uncased6 6 768 12 66M / distilbert-base-multilingual-cased 6 768 12 66M / distilbert-base-uncased : 基于 bert-base-uncased 的蒸馏 (压缩) 模型，编码器具有 6 个隐层，输出 768 维张量，12 个自注意力头，共 66M 参数量. distilbert-base-multilingual-cased : 基于 bert-base-multilingual-uncased 的蒸馏 (压缩) 模型，编码器具有 6 个隐层，输出 768 维张量，12 个自注意力头，共 66M 参数量. 9 ALBERT 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 albert-base-v1 12 768 12 125M 英文文本 albert-base-v2 12 768 12 125M 英文文本 albert-base-v1 : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 125M 参数量，在英文文本上进行训练而得到. albert-base-v2 : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 125M 参数量，在英文文本上进行训练而得到，相比 v1 使用了更多的数据量，花费更长的训练时间. 10 T5 及其变体 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 t5-small 6 512 8 60M C4 语料 t5-base 12 768 12 220M C4 语料 t5-large 24 1024 16 770M C4 语料 t5-small : 编码器具有 6 个隐层，输出 512 维张量，8 个自注意力头，共 60M 参数量，在 C4 语料上进行训练而得到. t5-base : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 220M 参数量，在 C4 语料上进行训练而得到. t5-large : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 770M 参数量，在 C4 语料上进行训练而得到. 11 XLM-RoBERTa 及其变体 模型名称 隐层数 张量维度 自注意力头数 参数量 训练语料 xlm-roberta-base 12 768 8 125M 2.5TB 的 100 种语言文本 xlm-roberta-large 24 1027 16 355M 2.5TB 的 100 种语言文本 xlm-roberta-base : 编码器具有 12 个隐层，输出 768 维张量，8 个自注意力头，共 125M 参数量，在 2.5TB 的 100 种语言文本上进行训练而得到. xlm-roberta-large : 编码器具有 24 个隐层，输出 1027 维张量，16 个自注意力头，共 355M 参数量，在 2.5TB 的 100 种语言文本上进行训练而得到. 预训练模型说明: 所有上述预训练模型及其变体都是以 transformer 为基础，只是在模型结构如神经元连接方式，编码器隐层数，多头注意力的头数等发生改变，这些改变方式的大部分依据都是由在标准数据集上的表现而定，因此，对于我们使用者而言，不需要从理论上深度探究这些预训练模型的结构设计的优劣，只需要在自己处理的目标数据上，尽量遍历所有可用的模型对比得到最优效果即可.","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NLP","slug":"NLP","permalink":"https://leezhao415.github.io/tags/NLP/"}]},{"title":"NLP之常用数据集详解","slug":"NLP之常用数据集详解","date":"2021-06-24T15:18:54.000Z","updated":"2021-06-26T02:41:28.720Z","comments":true,"path":"2021/06/24/NLP之常用数据集详解/","link":"","permalink":"https://leezhao415.github.io/2021/06/24/NLP%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"GLUE 数据集合的介绍: 自然语言处理（NLP）主要自然语言理解（NLU）和自然语言生成（NLG）。GLUE（General Language Understanding Evaluation）由纽约大学，华盛顿大学，Google 联合推出，涵盖不同 NLP 任务类型，截止至 2020 年 1 月其中包括 11 个子任务数据集，成为衡量 NLP 研究发展的衡量标准. GLUE 九项任务涉及到自然语言推断、文本蕴含、情感分析、语义相似等多个任务。像 BERT、XLNet、RoBERTa、ERINE、T5 等知名模型都会在此基准上进行测试。 文章目录 GLUE 数据集合包含以下数据集 GLUE 数据集合 1 CoLA 数据集 2 SST-2 数据集 3 MRPC 数据集 4 STS-B 数据集 5 QQP 数据集 6 (MNLI/SNLI) 数据集 7 (QNLI/RTE/WNLI) 数据集 GLUE 数据集合包含以下数据集 CoLA 数据集 SST-2 数据集 MRPC 数据集 STS-B 数据集 QQP 数据集 MNLI 数据集 SNLI 数据集 QNLI 数据集 RTE 数据集 WNLI 数据集 diagnostics 数据集 (官方未完善) GLUE 数据集合的下载方式: 下载脚本代码:https://gluebenchmark.com/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123&#x27;&#x27;&#x27; Script for downloading all GLUE data.&#x27;&#x27;&#x27;import osimport sysimport shutilimport argparseimport tempfileimport urllib.requestimport zipfileTASKS = [&quot;CoLA&quot;, &quot;SST&quot;, &quot;MRPC&quot;, &quot;QQP&quot;, &quot;STS&quot;, &quot;MNLI&quot;, &quot;SNLI&quot;, &quot;QNLI&quot;, &quot;RTE&quot;, &quot;WNLI&quot;, &quot;diagnostic&quot;]TASK2PATH = &#123;&quot;CoLA&quot;:&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&amp;token=46d5e637-3411-4188-bc44-5809b5bfb5f4&#x27;, &quot;SST&quot;:&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&amp;token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8&#x27;, &quot;MRPC&quot;:&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&amp;token=ec5c0836-31d5-48f4-b431-7480817f1adc&#x27;, &quot;QQP&quot;:&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&amp;token=700c6acf-160d-4d89-81d1-de4191d02cb5&#x27;, &quot;STS&quot;:&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&amp;token=bddb94a7-8706-4e0d-a694-1109e12273b5&#x27;, &quot;MNLI&quot;:&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&amp;token=50329ea1-e339-40e2-809c-10c40afff3ce&#x27;, &quot;SNLI&quot;:&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&amp;token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df&#x27;, &quot;QNLI&quot;: &#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&amp;token=6fdcf570-0fc5-4631-8456-9505272d1601&#x27;, &quot;RTE&quot;:&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&amp;token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb&#x27;, &quot;WNLI&quot;:&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&amp;token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf&#x27;, &quot;diagnostic&quot;:&#x27;https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&amp;Expires=2498860800&amp;Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D&#x27;&#125;MRPC_TRAIN = &#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt&#x27;MRPC_TEST = &#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt&#x27;def download_and_extract(task, data_dir): print(&quot;Downloading and extracting %s...&quot; % task) data_file = &quot;%s.zip&quot; % task urllib.request.urlretrieve(TASK2PATH[task], data_file) with zipfile.ZipFile(data_file) as zip_ref: zip_ref.extractall(data_dir) os.remove(data_file) print(&quot;\\tCompleted!&quot;)def format_mrpc(data_dir, path_to_data): print(&quot;Processing MRPC...&quot;) mrpc_dir = os.path.join(data_dir, &quot;MRPC&quot;) if not os.path.isdir(mrpc_dir): os.mkdir(mrpc_dir) if path_to_data: mrpc_train_file = os.path.join(path_to_data, &quot;msr_paraphrase_train.txt&quot;) mrpc_test_file = os.path.join(path_to_data, &quot;msr_paraphrase_test.txt&quot;) else: print(&quot;Local MRPC data not specified, downloading data from %s&quot; % MRPC_TRAIN) mrpc_train_file = os.path.join(mrpc_dir, &quot;msr_paraphrase_train.txt&quot;) mrpc_test_file = os.path.join(mrpc_dir, &quot;msr_paraphrase_test.txt&quot;) urllib.request.urlretrieve(MRPC_TRAIN, mrpc_train_file) urllib.request.urlretrieve(MRPC_TEST, mrpc_test_file) assert os.path.isfile(mrpc_train_file), &quot;Train data not found at %s&quot; % mrpc_train_file assert os.path.isfile(mrpc_test_file), &quot;Test data not found at %s&quot; % mrpc_test_file urllib.request.urlretrieve(TASK2PATH[&quot;MRPC&quot;], os.path.join(mrpc_dir, &quot;dev_ids.tsv&quot;)) dev_ids = [] with open(os.path.join(mrpc_dir, &quot;dev_ids.tsv&quot;), encoding=&quot;utf8&quot;) as ids_fh: for row in ids_fh: dev_ids.append(row.strip().split(&#x27;\\t&#x27;)) with open(mrpc_train_file, encoding=&quot;utf8&quot;) as data_fh, \\ open(os.path.join(mrpc_dir, &quot;train.tsv&quot;), &#x27;w&#x27;, encoding=&quot;utf8&quot;) as train_fh, \\ open(os.path.join(mrpc_dir, &quot;dev.tsv&quot;), &#x27;w&#x27;, encoding=&quot;utf8&quot;) as dev_fh: header = data_fh.readline() train_fh.write(header) dev_fh.write(header) for row in data_fh: label, id1, id2, s1, s2 = row.strip().split(&#x27;\\t&#x27;) if [id1, id2] in dev_ids: dev_fh.write(&quot;%s\\t%s\\t%s\\t%s\\t%s\\n&quot; % (label, id1, id2, s1, s2)) else: train_fh.write(&quot;%s\\t%s\\t%s\\t%s\\t%s\\n&quot; % (label, id1, id2, s1, s2)) with open(mrpc_test_file, encoding=&quot;utf8&quot;) as data_fh, \\ open(os.path.join(mrpc_dir, &quot;test.tsv&quot;), &#x27;w&#x27;, encoding=&quot;utf8&quot;) as test_fh: header = data_fh.readline() test_fh.write(&quot;index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n&quot;) for idx, row in enumerate(data_fh): label, id1, id2, s1, s2 = row.strip().split(&#x27;\\t&#x27;) test_fh.write(&quot;%d\\t%s\\t%s\\t%s\\t%s\\n&quot; % (idx, id1, id2, s1, s2)) print(&quot;\\tCompleted!&quot;)def download_diagnostic(data_dir): print(&quot;Downloading and extracting diagnostic...&quot;) if not os.path.isdir(os.path.join(data_dir, &quot;diagnostic&quot;)): os.mkdir(os.path.join(data_dir, &quot;diagnostic&quot;)) data_file = os.path.join(data_dir, &quot;diagnostic&quot;, &quot;diagnostic.tsv&quot;) urllib.request.urlretrieve(TASK2PATH[&quot;diagnostic&quot;], data_file) print(&quot;\\tCompleted!&quot;) returndef get_tasks(task_names): task_names = task_names.split(&#x27;,&#x27;) if &quot;all&quot; in task_names: tasks = TASKS else: tasks = [] for task_name in task_names: assert task_name in TASKS, &quot;Task %s not found!&quot; % task_name tasks.append(task_name) return tasksdef main(arguments): parser = argparse.ArgumentParser() parser.add_argument(&#x27;--data_dir&#x27;, help=&#x27;directory to save data to&#x27;, type=str, default=&#x27;glue_data&#x27;) parser.add_argument(&#x27;--tasks&#x27;, help=&#x27;tasks to download data for as a comma separated string&#x27;, type=str, default=&#x27;all&#x27;) parser.add_argument(&#x27;--path_to_mrpc&#x27;, help=&#x27;path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt&#x27;, type=str, default=&#x27;&#x27;) args = parser.parse_args(arguments) if not os.path.isdir(args.data_dir): os.mkdir(args.data_dir) tasks = get_tasks(args.tasks) for task in tasks: if task == &#x27;MRPC&#x27;: format_mrpc(args.data_dir, args.path_to_mrpc) elif task == &#x27;diagnostic&#x27;: download_diagnostic(args.data_dir) else: download_and_extract(task, args.data_dir)if __name__ == &#x27;__main__&#x27;: sys.exit(main(sys.argv[1:])) 运行脚本下载所有数据集: 123# 假设你已经将以上代码copy到download_glue_data.py文件中# 运行这个python脚本, 你将同目录下得到一个glue文件夹python download_glue_data.py 输出效果: 1234567891011121314151617181920212223Downloading and extracting CoLA... Completed!Downloading and extracting SST... Completed!Processing MRPC...Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt Completed!Downloading and extracting QQP... Completed!Downloading and extracting STS... Completed!Downloading and extracting MNLI... Completed!Downloading and extracting SNLI... Completed!Downloading and extracting QNLI... Completed!Downloading and extracting RTE... Completed!Downloading and extracting WNLI... Completed!Downloading and extracting diagnostic... Completed! GLUE 数据集合 1 CoLA 数据集 CoLA (The Corpus of Linguistic Acceptability，语言可接受性语料库)，单句子分类任务，语料来自语言理论的书籍和期刊，每个句子被标注为是否合乎语法的单词序列。本任务是一个二分类任务，标签共两个，分别是 0 和 1，其中 0 表示不合乎语法，1 表示合乎语法。 样本个数：训练集 8, 551 个，开发集 1, 043 个，测试集 1, 063 个。 任务：可接受程度，合乎语法与不合乎语法二分类。 文件样式 12345- CoLA/ - dev.tsv - original/ - test.tsv - train.tsv 文件样式说明: 在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据. train.tsv 数据样式: 123456789101112...gj04 1 She coughed herself awake as the leaf landed on her nose.gj04 1 The worm wriggled onto the carpet.gj04 1 The chocolate melted onto the carpet.gj04 0 * The ball wriggled itself loose.gj04 1 Bill wriggled himself loose.bc01 1 The sinking of the ship to collect the insurance was very devious.bc01 1 The ship&#x27;s sinking was very devious.bc01 0 * The ship&#x27;s sinking to collect the insurance was very devious.bc01 1 The testing of such drugs on oneself is too risky.bc01 0 * This drug&#x27;s testing on oneself is too risky.... train.tsv 数据样式说明: train.tsv 中的数据内容共分为 4 列，第一列数据，如 gj04, bc01 等代表每条文本数据的来源即出版物代号；第二列数据，0 或 1, 代表每条文本数据的语法是否正确，0 代表不正确，1 代表正确；第三列数据，'’, 是作者最初的正负样本标记，与第二列意义相同，'' 表示不正确；第四列即是被标注的语法使用是否正确的文本句子. test.tsv 数据样式: 123456789101112index sentence0 Bill whistled past the house.1 The car honked its way down the road.2 Bill pushed Harry off the sofa.3 the kittens yawned awake and played.4 I demand that the more John eats, the more he pay.5 If John eats more, keep your mouth shut tighter, OK?6 His expectations are always lower than mine are.7 The sooner you call, the more carefully I will word the letter.8 The more timid he feels, the more people he interviews without asking questions of.9 Once Janet left, Fred became a lot crazier.... test.tsv 数据样式说明: test.tsv 中的数据内容共分为 2 列，第一列数据代表每条文本数据的索引；第二列数据代表用于测试的句子. CoLA 数据集的任务类型: 二分类任务 评估指标为: MCC (马修斯相关系数，在正负样本分布十分不均衡的情况下使用的二分类评估指标) 2 SST-2 数据集 SST-2 (The Stanford Sentiment Treebank，斯坦福情感树库)，单句子分类任务，包含电影评论中的句子和它们情感的人类注释。这项任务是给定句子的情感，类别分为两类正面情感（positive，样本标签对应为 1）和负面情感（negative，样本标签对应为 0），并且只用句子级别的标签。也就是，本任务也是一个二分类任务，针对句子级别，分为正面和负面情感。 样本个数：训练集 67, 350 个，开发集 873 个，测试集 1, 821 个。 任务：情感分类，正面情感和负面情感二分类。 评价准则：accuracy。 文件样式 12345- SST-2/ - dev.tsv - original/ - test.tsv - train.tsv 文件样式说明: 在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据. train.tsv 数据样式: 1234567891011sentence labelhide new secretions from the parental units 0contains no wit , only labored gags 0that loves its characters and communicates something rather beautiful about human nature 1remains utterly satisfied to remain the same throughout 0on the worst revenge-of-the-nerds clichés the filmmakers could dredge up 0that &#x27;s far too tragic to merit such superficial treatment 0demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . 1of saucy 1a depressed fifteen-year-old &#x27;s suicidal poetry 0... train.tsv 数据样式说明: train.tsv 中的数据内容共分为 2 列，第一列数据代表具有感情色彩的评论文本；第二列数据，0 或 1, 代表每条文本数据是积极或者消极的评论，0 代表消极，1 代表积极. test.tsv 数据样式: 123456789101112index sentence0 uneasy mishmash of styles and genres .1 this film &#x27;s relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .2 by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .3 director rob marshall went out gunning to make a great one .4 lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new .5 a well-made and often lovely depiction of the mysteries of friendship .6 none of this violates the letter of behan &#x27;s book , but missing is its spirit , its ribald , full-throated humor .7 although it bangs a very cliched drum at times , this crowd-pleaser &#x27;s fresh dialogue , energetic music , and good-natured spunk are often infectious .8 it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another .9 this is junk food cinema at its greasiest .... test.tsv 数据样式说明: * test.tsv 中的数据内容共分为 2 列，第一列数据代表每条文本数据的索引；第二列数据代表用于测试的句子. SST-2 数据集的任务类型: 二分类任务 评估指标为: ACC 3 MRPC 数据集 MRPC (The Microsoft Research Paraphrase Corpus，微软研究院释义语料库)，相似性和释义任务，是从在线新闻源中自动抽取句子对语料库，并人工注释句子对中的句子是否在语义上等效。类别并不平衡，其中 68% 的正样本，所以遵循常规的做法，报告准确率（accuracy）和 F1 值。 样本个数：训练集 3, 668 个，开发集 408 个，测试集 1, 725 个。 任务：是否释义二分类，是释义，不是释义两类。 评价准则：准确率（accuracy）和 F1 值。 文件样式 1234567- MRPC/ - dev.tsv - test.tsv - train.tsv - dev_ids.tsv - msr_paraphrase_test.txt - msr_paraphrase_train.txt 文件样式说明: 在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据. train.tsv 数据样式: 12345678910Quality #1 ID #2 ID #1 String #2 String1 702876 702977 Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence . Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .0 2108705 2108831 Yucaipa owned Dominick &#x27;s before selling the chain to Safeway in 1998 for $ 2.5 billion . Yucaipa bought Dominick &#x27;s in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .1 1330381 1330521 They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added . On June 10 , the ship &#x27;s owners had published an advertisement on the Internet , offering the explosives for sale .0 3344667 3344648 Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 . Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .1 1236820 1236712 The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange . PG &amp; E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .1 738533 737951 Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier . With the scandal hanging over Stewart &#x27;s company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .0 264589 264502 The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday . The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .1 579975 579810 The DVD-CCA then appealed to the state Supreme Court . The DVD CCA appealed that decision to the U.S. Supreme Court .... train.tsv 数据样式说明: train.tsv 中的数据内容共分为 5 列，第一列数据，0 或 1, 代表每对句子是否具有相同的含义，0 代表含义不相同，1 代表含义相同。第二列和第三列分别代表每对句子的 id, 第四列和第五列分别具有相同 / 不同含义的句子对. test.tsv 数据样式: 12345678910index #1 ID #2 ID #1 String #2 String0 1089874 1089925 PCCW &#x27;s chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So . Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .1 3019446 3019327 The world &#x27;s two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected . Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash .2 1945605 1945824 According to the federal Centers for Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles in the United States in 2002 . The Centers for Disease Control and Prevention said there were 19 reported cases of measles in the United States in 2002 .3 1430402 1430329 A tropical storm rapidly developed in the Gulf of Mexico Sunday and was expected to hit somewhere along the Texas or Louisiana coasts by Monday night . A tropical storm rapidly developed in the Gulf of Mexico on Sunday and could have hurricane-force winds when it hits land somewhere along the Louisiana coast Monday night .4 3354381 3354396 The company didn &#x27;t detail the costs of the replacement and repairs . But company officials expect the costs of the replacement work to run into the millions of dollars .5 1390995 1391183 The settling companies would also assign their possible claims against the underwriters to the investor plaintiffs , he added . Under the agreement , the settling companies will also assign their potential claims against the underwriters to the investors , he added .6 2201401 2201285 Air Commodore Quaife said the Hornets remained on three-minute alert throughout the operation . Air Commodore John Quaife said the security operation was unprecedented .7 2453843 2453998 A Washington County man may have the countys first human case of West Nile virus , the health department said Friday . The countys first and only human case of West Nile this year was confirmed by health officials on Sept . 8 .... test.tsv 数据样式说明: * test.tsv 中的数据内容共分为 5 列，第一列数据代表每条文本数据的索引；其余列的含义与 train.tsv 中相同. MRPC 数据集的任务类型: 句子对二分类任务 评估指标为: ACC 和 F1 4 STS-B 数据集 STSB (The Semantic Textual Similarity Benchmark，语义文本相似性基准测试)，相似性和释义任务，是从新闻标题、视频标题、图像标题以及自然语言推断数据中提取的句子对的集合，每对都是由人类注释的，其相似性评分为 0-5 (大于等于 0 且小于等于 5 的浮点数，原始 paper 里写的是 1-5，可能是作者失误）。任务就是预测这些相似性得分，本质上是一个回归问题，但是依然可以用分类的方法，可以归类为句子对的文本五分类任务。 样本个数：训练集 5, 749 个，开发集 1, 379 个，测试集 1, 377 个。 任务：回归任务，预测为 1-5 之间的相似性得分的浮点数。但是依然可以使用分类的方法，作为五分类。 评价准则：Pearson and Spearman correlation coefficients。 文件样式 1234567- STS-B/ - dev.tsv - test.tsv - train.tsv - LICENSE.txt - readme.txt - original/ 文件样式说明: 在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据. train.tsv 数据样式: 123456789101112index genre filename year old_index source1 source2 sentence1 sentence2 score0 main-captions MSRvid 2012test 0001 none none A plane is taking off. An air plane is taking off. 5.0001 main-captions MSRvid 2012test 0004 none none A man is playing a large flute. A man is playing a flute. 3.8002 main-captions MSRvid 2012test 0005 none none A man is spreading shreded cheese on a pizza. A man is spreading shredded cheese on an uncooked pizza. 3.8003 main-captions MSRvid 2012test 0006 none none Three men are playing chess.Two men are playing chess. 2.6004 main-captions MSRvid 2012test 0009 none none A man is playing the cello.A man seated is playing the cello. 4.2505 main-captions MSRvid 2012test 0011 none none Some men are fighting. Two men are fighting. 4.2506 main-captions MSRvid 2012test 0012 none none A man is smoking. A man is skating. 0.5007 main-captions MSRvid 2012test 0013 none none The man is playing the piano. The man is playing the guitar. 1.6008 main-captions MSRvid 2012test 0014 none none A man is playing on a guitar and singing. A woman is playing an acoustic guitar and singing. 2.2009 main-captions MSRvid 2012test 0016 none none A person is throwing a cat on to the ceiling. A person throws a cat on the ceiling. 5.000... train.tsv 数据样式说明: train.tsv 中的数据内容共分为 10 列，第一列数据是数据索引；第二列代表每对句子的来源，如 main-captions 表示来自字幕；第三列代表来源的具体保存文件名，第四列代表出现时间 (年); 第五列代表原始数据的索引；第六列和第七列分别代表句子对原始来源；第八列和第九列代表相似程度不同的句子对；第十列代表句子对的相似程度由低到高，值域范围是 [0, 5]. test.tsv 数据样式: 12345678910111213index genre filename year old_index source1 source2 sentence1 sentence20 main-captions MSRvid 2012test 0024 none none A girl is styling her hair. A girl is brushing her hair.1 main-captions MSRvid 2012test 0033 none none A group of men play soccer on the beach. A group of boys are playing soccer on the beach.2 main-captions MSRvid 2012test 0045 none none One woman is measuring another woman&#x27;s ankle. A woman measures another woman&#x27;s ankle.3 main-captions MSRvid 2012test 0063 none none A man is cutting up a cucumber. A man is slicing a cucumber.4 main-captions MSRvid 2012test 0066 none none A man is playing a harp. A man is playing a keyboard.5 main-captions MSRvid 2012test 0074 none none A woman is cutting onions. A woman is cutting tofu.6 main-captions MSRvid 2012test 0076 none none A man is riding an electric bicycle. A man is riding a bicycle.7 main-captions MSRvid 2012test 0082 none none A man is playing the drums. A man is playing the guitar.8 main-captions MSRvid 2012test 0092 none none A man is playing guitar. A lady is playing the guitar.9 main-captions MSRvid 2012test 0095 none none A man is playing a guitar. A man is playing a trumpet.10 main-captions MSRvid 2012test 0096 none none A man is playing a guitar. A man is playing a trumpet.... test.tsv 数据样式说明: test.tsv 中的数据内容共分为 9 列，含义与 train.tsv 前 9 列相同. STS-B 数据集的任务类型: 句子对多分类任务 / 句子对回归任务 评估指标为: Pearson-Spearman Corr 5 QQP 数据集 QQP (The Quora Question Pairs, Quora 问题对数集)，相似性和释义任务，是社区问答网站 Quora 中问题对的集合。任务是确定一对问题在语义上是否等效。与 MRPC 一样，QQP 也是正负样本不均衡的，不同是的 QQP 负样本占 63%，正样本是 37%，所以我们也是报告准确率和 F1 值。我们使用标准测试集，为此我们从作者那里获得了专用标签。我们观察到测试集与训练集分布不同。 样本个数：训练集 363, 870 个，开发集 40, 431 个，测试集 390, 965 个。 任务：判定句子对是否等效，等效、不等效两种情况，二分类任务。 评价准则：准确率（accuracy）和 F1 值。 文件样式 12345- QQP/ - dev.tsv - original/ - test.tsv - train.tsv 文件样式说明: 在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据. train.tsv 数据样式: 12345678910id qid1 qid2 question1 question2 is_duplicate133273 213221 213222 How is the life of a math student? Could you describe your own experiences?Which level of prepration is enough for the exam jlpt5? 0402555 536040 536041 How do I control my horny emotions? How do you control your horniness? 1360472 364011 490273 What causes stool color to change to yellow? What can cause stool to come out as little balls? 0150662 155721 7256 What can one do after MBBS? What do i do after my MBBS ? 1183004 279958 279959 Where can I find a power outlet for my laptop at Melbourne Airport? Would a second airport in Sydney, Australia be needed if a high-speed rail link was created between Melbourne and Sydney? 0119056 193387 193388 How not to feel guilty since I am Muslim and I&#x27;m conscious we won&#x27;t have sex together? I don&#x27;t beleive I am bulimic, but I force throw up atleast once a day after I eat something and feel guilty. Should I tell somebody, and if so who? 0356863 422862 96457 How is air traffic controlled? How do you become an air traffic controller?0106969 147570 787 What is the best self help book you have read? Why? How did it change your life? What are the top self help books I should read? 1... train.tsv 数据样式说明: train.tsv 中的数据内容共分为 6 列，第一列代表文本数据索引；第二列和第三列数据分别代表问题 1 和问题 2 的 id; 第四列和第五列代表需要进行’是否重复’判定的句子对；第六列代表上述问题是 / 不是重复性问题的标签，0 代表不重复，1 代表重复. test.tsv 数据样式: 1234567891011id question1 question20 Would the idea of Trump and Putin in bed together scare you, given the geopolitical implications? Do you think that if Donald Trump were elected President, he would be able to restore relations with Putin and Russia as he said he could, based on the rocky relationship Putin had with Obama and Bush?1 What are the top ten Consumer-to-Consumer E-commerce online? What are the top ten Consumer-to-Business E-commerce online?2 Why don&#x27;t people simply &#x27;Google&#x27; instead of asking questions on Quora? Why do people ask Quora questions instead of just searching google?3 Is it safe to invest in social trade biz? Is social trade geniune?4 If the universe is expanding then does matter also expand? If universe and space is expanding? Does that mean anything that occupies space is also expanding?5 What is the plural of hypothesis? What is the plural of thesis?6 What is the application form you need for launching a company? What is the application form you need for launching a company in Austria?7 What is Big Theta? When should I use Big Theta as opposed to big O? Is O(Log n) close to O(n) or O(1)?8 What are the health implications of accidentally eating a small quantity of aluminium foil?What are the implications of not eating vegetables?... test.tsv 数据样式说明: test.tsv 中的数据内容共分为 3 列，第一列数据代表每条文本数据的索引；第二列和第三列数据代表用于测试的问题句子对. QQP 数据集的任务类型: 句子对二分类任务 评估指标为: ACC/F1 6 (MNLI/SNLI) 数据集 MNLI (The Multi-Genre Natural Language Inference Corpus, 多类型自然语言推理数据库)，自然语言推断任务，是通过众包方式对句子对进行文本蕴含标注的集合。给定前提（premise）语句和假设（hypothesis）语句，任务是预测前提语句是否包含假设（蕴含，entailment），与假设矛盾（矛盾，contradiction）或者两者都不（中立，neutral）。前提语句是从数十种不同来源收集的，包括转录的语音，小说和政府报告。 样本个数：训练集 392, 702 个，开发集 dev-matched 9, 815 个，开发集 dev-mismatched9, 832 个，测试集 test-matched 9, 796 个，测试集 test-dismatched9, 847 个。因为 MNLI 是集合了许多不同领域风格的文本，所以又分为了 matched 和 mismatched 两个版本的数据集，matched 指的是训练集和测试集的数据来源一致，mismached 指的是训练集和测试集来源不一致。 任务：句子对，一个前提，一个是假设。前提和假设的关系有三种情况：蕴含（entailment），矛盾（contradiction），中立（neutral）。句子对三分类问题。 评价准则：matched accuracy/mismatched accuracy。 文件样式 1234567- (MNLI/SNLI)/ - dev_matched.tsv - dev_mismatched.tsv - original/ - test_matched.tsv - test_mismatched.tsv - train.tsv 文件样式说明: 在使用中常用到的文件是 train.tsv, dev_matched.tsv, dev_mismatched.tsv, test_matched.tsv, test_mismatched.tsv 分别代表训练集，与训练集一同采集的验证集，与训练集不是一同采集验证集，与训练集一同采集的测试集，与训练集不是一同采集测试集。其中 train.tsv 与 dev_matched.tsv 和 dev_mismatched.tsv 数据样式相同，都是带有标签的数据，其中 test_matched.tsv 与 test_mismatched.tsv 数据样式相同，都是不带有标签的数据. train.tsv 数据样式: 123456index promptID pairID genre sentence1_binary_parse sentence2_binary_parse sentence1_parse sentence2_parse sentence1 sentence2 label1 gold_label0 31193 31193n government ( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) ) ( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) ) (ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .))) (ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .))) Conceptually cream skimming has two basic dimensions - product and geography. Product and geography are what make cream skimming work. neutral neutral1 101457 101457e telephone ( you ( ( know ( during ( ( ( the season ) and ) ( i guess ) ) ) ) ( at ( at ( ( your level ) ( uh ( you ( ( ( lose them ) ( to ( the ( next level ) ) ) ) ( if ( ( if ( they ( decide ( to ( recall ( the ( the ( parent team ) ) ) ) ) ) ) ) ( ( the Braves ) ( decide ( to ( call ( to ( ( recall ( a guy ) ) ( from ( ( triple A ) ( ( ( then ( ( a ( double ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) and ) ( ( a ( single ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ( You ( ( ( ( lose ( the things ) ) ( to ( the ( following level ) ) ) ) ( if ( ( the people ) recall ) ) ) . ) ) (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN during) (NP (NP (DT the) (NN season)) (CC and) (NP (FW i) (FW guess)))) (PP (IN at) (IN at) (NP (NP (PRP$ your) (NN level)) (SBAR (S (INTJ (UH uh)) (NP (PRP you)) (VP (VBP lose) (NP (PRP them)) (PP (TO to) (NP (DT the) (JJ next) (NN level))) (SBAR (IN if) (S (SBAR (IN if) (S (NP (PRP they)) (VP (VBP decide) (S (VP (TO to) (VP (VB recall) (NP (DT the) (DT the) (NN parent) (NN team)))))))) (NP (DT the) (NNPS Braves)) (VP (VBP decide) (S (VP (TO to) (VP (VB call) (S (VP (TO to) (VP (VB recall) (NP (DT a) (NN guy)) (PP (IN from) (NP (NP (RB triple) (DT A)) (SBAR (S (S (ADVP (RB then)) (NP (DT a) (JJ double) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him))))))) (CC and) (S (NP (DT a) (JJ single) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him)))))))))))))))))))))))))))) (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT the) (NNS things)) (PP (TO to) (NP (DT the) (JJ following) (NN level))) (SBAR (IN if) (S (NP (DT the) (NNS people)) (VP (VBP recall))))) (. .))) you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him You lose the things to the following level if the people recall. entailment entailment2 134793 134793e fiction ( ( One ( of ( our number ) ) ) ( ( will ( ( ( carry out ) ( your instructions ) ) minutely ) ) . ) ) ( ( ( A member ) ( of ( my team ) ) ) ( ( will ( ( execute ( your orders ) ) ( with ( immense precision ) ) ) ) . ) ) (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PRP$ our) (NN number)))) (VP (MD will) (VP (VB carry) (PRT (RP out)) (NP (PRP$ your) (NNS instructions)) (ADVP (RB minutely)))) (. .))) (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN of) (NP (PRP$ my) (NN team)))) (VP (MD will) (VP (VB execute) (NP (PRP$ your) (NNS orders)) (PP (IN with) (NP (JJ immense) (NN precision))))) (. .))) One of our number will carry out your instructions minutely. A member of my team will execute your orders with immense precision. entailment entailment3 37397 37397e fiction ( ( How ( ( ( do you ) know ) ? ) ) ( ( All this ) ( ( ( is ( their information ) ) again ) . ) ) ) ( ( This information ) ( ( belongs ( to them ) ) . ) ) (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do) (NP (PRP you)) (VP (VB know))) (. ?)) (NP (PDT All) (DT this)) (VP (VBZ is) (NP (PRP$ their) (NN information)) (ADVP (RB again))) (. .))) (ROOT (S (NP (DT This) (NN information)) (VP (VBZ belongs) (PP (TO to) (NP (PRP them)))) (. .))) How do you know? All this is their information again. This information belongs to them. entailment entailment... train.tsv 数据样式说明: train.tsv 中的数据内容共分为 12 列，第一列代表文本数据索引；第二列和第三列数据分别代表句子对的不同类型 id; 第四列代表句子对的来源；第五列和第六列代表具有句法结构分析的句子对表示；第七列和第八列代表具有句法结构和词性标注的句子对表示，第九列和第十列代表原始的句子对，第十一和第十二列代表不同标准的标注方法产生的标签，在这里，他们始终相同，一共有三种类型的标签，neutral 代表两个句子既不矛盾也不蕴含，entailment 代表两个句子具有蕴含关系，contradiction 代表两个句子观点矛盾. test_matched.tsv 数据样式: 123456index promptID pairID genre sentence1_binary_parse sentence2_binary_parse sentence1_parse sentence2_parse sentence1 sentence20 31493 31493 travel ( ( ( ( ( ( ( ( Hierbas , ) ( ans seco ) ) , ) ( ans dulce ) ) , ) and ) frigola ) ( ( ( are just ) ( ( a ( few names ) ) ( worth ( ( keeping ( a look-out ) ) for ) ) ) ) . ) ) ( Hierbas ( ( is ( ( a name ) ( worth ( ( looking out ) for ) ) ) ) . ) ) (ROOT (S (NP (NP (NNS Hierbas)) (, ,) (NP (NN ans) (NN seco)) (, ,) (NP (NN ans) (NN dulce)) (, ,) (CC and) (NP (NN frigola))) (VP (VBP are) (ADVP (RB just)) (NP (NP (DT a) (JJ few) (NNS names)) (PP (JJ worth) (S (VP (VBG keeping) (NP (DT a) (NN look-out)) (PP (IN for))))))) (. .))) (ROOT (S (NP (NNS Hierbas)) (VP (VBZ is) (NP (NP (DT a) (NN name)) (PP (JJ worth) (S (VP (VBG looking) (PRT (RP out)) (PP (IN for))))))) (. .))) Hierbas, ans seco, ans dulce, and frigola are just a few names worth keeping a look-out for. Hierbas is a name worth looking out for.1 92164 92164 government ( ( ( The extent ) ( of ( the ( behavioral effects ) ) ) ) ( ( would ( ( depend ( in ( part ( on ( ( the structure ) ( of ( ( ( the ( individual ( account program ) ) ) and ) ( any limits ) ) ) ) ) ) ) ) ( on ( accessing ( the funds ) ) ) ) ) . ) ) ( ( Many people ) ( ( would ( be ( very ( unhappy ( to ( ( loose control ) ( over ( their ( own money ) ) ) ) ) ) ) ) ) . ) ) (ROOT (S (NP (NP (DT The) (NN extent)) (PP (IN of) (NP (DT the) (JJ behavioral) (NNS effects)))) (VP (MD would) (VP (VB depend) (PP (IN in) (NP (NP (NN part)) (PP (IN on) (NP (NP (DT the) (NN structure)) (PP (IN of) (NP (NP (DT the) (JJ individual) (NN account) (NN program)) (CC and) (NP (DT any) (NNS limits)))))))) (PP (IN on) (S (VP (VBG accessing) (NP (DT the) (NNS funds))))))) (. .))) (ROOT (S (NP (JJ Many) (NNS people)) (VP (MD would) (VP (VB be) (ADJP (RB very) (JJ unhappy) (PP (TO to) (NP (NP (JJ loose) (NN control)) (PP (IN over) (NP (PRP$ their) (JJ own) (NN money)))))))) (. .))) The extent of the behavioral effects would depend in part on the structure of the individual account program and any limits on accessing the funds. Many people would be very unhappy to loose control over their own money.2 9662 9662 government ( ( ( Timely access ) ( to information ) ) ( ( is ( in ( ( the ( best interests ) ) ( of ( ( ( both GAO ) and ) ( the agencies ) ) ) ) ) ) . ) ) ( It ( ( ( is ( in ( ( everyone &#x27;s ) ( best interest ) ) ) ) ( to ( ( have access ) ( to ( information ( in ( a ( timely manner ) ) ) ) ) ) ) ) . ) ) (ROOT (S (NP (NP (JJ Timely) (NN access)) (PP (TO to) (NP (NN information)))) (VP (VBZ is) (PP (IN in) (NP (NP (DT the) (JJS best) (NNS interests)) (PP (IN of) (NP (NP (DT both) (NNP GAO)) (CC and) (NP (DT the) (NNS agencies))))))) (. .))) (ROOT (S (NP (PRP It)) (VP (VBZ is) (PP (IN in) (NP (NP (NN everyone) (POS &#x27;s)) (JJS best) (NN interest))) (S (VP (TO to) (VP (VB have) (NP (NN access)) (PP (TO to) (NP (NP (NN information)) (PP (IN in) (NP (DT a) (JJ timely) (NN manner))))))))) (. .))) Timely access to information is in the best interests of both GAO and the agencies. It is in everyone&#x27;s best interest to have access to information in a timely manner.3 5991 5991 travel ( ( Based ( in ( ( the ( Auvergnat ( spa town ) ) ) ( of Vichy ) ) ) ) ( , ( ( the ( French government ) ) ( often ( ( ( ( proved ( more zealous ) ) ( than ( its masters ) ) ) ( in ( ( ( suppressing ( civil liberties ) ) and ) ( ( drawing up ) ( anti-Jewish legislation ) ) ) ) ) . ) ) ) ) ) ( ( The ( French government ) ) ( ( passed ( ( anti-Jewish laws ) ( aimed ( at ( helping ( the Nazi ) ) ) ) ) ) . ) ) (ROOT (S (PP (VBN Based) (PP (IN in) (NP (NP (DT the) (NNP Auvergnat) (NN spa) (NN town)) (PP (IN of) (NP (NNP Vichy)))))) (, ,) (NP (DT the) (JJ French) (NN government)) (ADVP (RB often)) (VP (VBD proved) (NP (JJR more) (NNS zealous)) (PP (IN than) (NP (PRP$ its) (NNS masters))) (PP (IN in) (S (VP (VP (VBG suppressing) (NP (JJ civil) (NNS liberties))) (CC and) (VP (VBG drawing) (PRT (RP up)) (NP (JJ anti-Jewish) (NN legislation))))))) (. .))) (ROOT (S (NP (DT The) (JJ French) (NN government)) (VP (VBD passed) (NP (NP (JJ anti-Jewish) (NNS laws)) (VP (VBN aimed) (PP (IN at) (S (VP (VBG helping) (NP (DT the) (JJ Nazi)))))))) (. .))) Based in the Auvergnat spa town of Vichy, the French government often proved more zealous than its masters in suppressing civil liberties and drawing up anti-Jewish legislation. The French government passed anti-Jewish laws aimed at helping the Nazi.... test_matched.tsv 数据样式说明: test_matched.tsv 中的数据内容共分为 10 列，与 train.tsv 的前 10 列含义相同. (MNLI/SNLI) 数据集的任务类型: 句子对多分类任务 评估指标为: ACC 7 (QNLI/RTE/WNLI) 数据集 QNLI (Qusetion-answering NLI，问答自然语言推断)，自然语言推断任务。QNLI 是从另一个数据集 The Stanford Question Answering Dataset (斯坦福问答数据集，SQuAD 1.0)[3] 转换而来的。SQuAD 1.0 是有一个问题 - 段落对组成的问答数据集，其中段落来自维基百科，段落中的一个句子包含问题的答案。这里可以看到有个要素，来自维基百科的段落，问题，段落中的一个句子包含问题的答案。通过将问题和上下文（即维基百科段落）中的每一句话进行组合，并过滤掉词汇重叠比较低的句子对就得到了 QNLI 中的句子对。相比原始 SQuAD 任务，消除了模型选择准确答案的要求；也消除了简化的假设，即答案适中在输入中并且词汇重叠是可靠的提示。 样本个数：训练集 104, 743 个，开发集 5, 463 个，测试集 5, 461 个。 任务：判断问题（question）和句子（sentence，维基百科段落中的一句）是否蕴含，蕴含和不蕴含，二分类。 评价准则：准确率（accuracy）。 RTE (The Recognizing Textual Entailment datasets，识别文本蕴含数据集)，自然语言推断任务，它是将一系列的年度文本蕴含挑战赛的数据集进行整合合并而来的，包含 RTE1 [4]，RTE2，RTE3[5]，RTE5 等，这些数据样本都从新闻和维基百科构建而来。将这些所有数据转换为二分类，对于三分类的数据，为了保持一致性，将中立（neutral）和矛盾（contradiction）转换为不蕴含（not entailment）。 样本个数：训练集 2, 491 个，开发集 277 个，测试集 3, 000 个。 任务：判断句子对是否蕴含，句子 1 和句子 2 是否互为蕴含，二分类任务。 评价准则：准确率（accuracy）。 WNLI (Winograd NLI，Winograd 自然语言推断)，自然语言推断任务，数据集来自于竞赛数据的转换。Winograd Schema Challenge [6]，该竞赛是一项阅读理解任务，其中系统必须读一个带有代词的句子，并从列表中找到代词的指代对象。这些样本都是都是手动创建的，以挫败简单的统计方法：每个样本都取决于句子中单个单词或短语提供的上下文信息。为了将问题转换成句子对分类，方法是通过用每个可能的列表中的每个可能的指代去替换原始句子中的代词。任务是预测两个句子对是否有关（蕴含、不蕴含）。训练集两个类别是均衡的，测试集是不均衡的，65% 是不蕴含。 样本个数：训练集 635 个，开发集 71 个，测试集 146 个。 任务：判断句子对是否相关，蕴含和不蕴含，二分类任务。 评价准则：准确率（accuracy）。 文件样式 1* QNLI, RTE, WNLI三个数据集的样式基本相同. 1234- (QNLI/RTE/WNLI)/ - dev.tsv - test.tsv - train.tsv 文件样式说明: 在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据. QNLI 中的 train.tsv 数据样式: 1234567891011index question sentence label0 When did the third Digimon series begin? Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese. not_entailment1 Which missile batteries often have individual launchers several kilometres from one another? When MANPADS is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs. not_entailment2 What two things does Popper argue Tarski&#x27;s theory involves in an evaluation of truth? He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer. entailment3 What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians? On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854. entailment4 What famous palace is located in London? London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret&#x27;s Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0° longitude, and GMT). not_entailment5 When is the term &#x27;German dialects&#x27; used in regard to the German language? When talking about the German language, the term German dialects is only used for the traditional regional varieties. entailment6 What was the name of the island the English traded to the Dutch in return for New Amsterdam? At the end of the Second Anglo-Dutch War, the English gained New Amsterdam (New York) in North America in exchange for Dutch control of Run, an Indonesian island. entailment7 How were the Portuguese expelled from Myanmar? From the 1720s onward, the kingdom was beset with repeated Meithei raids into Upper Myanmar and a nagging rebellion in Lan Na. not_entailment8 What does the word &#x27;customer&#x27; properly apply to? The bill also required rotation of principal maintenance inspectors and stipulated that the word &quot;customer&quot; properly applies to the flying public, not those entities regulated by the FAA. entailment... RTE 中的 train.tsv 数据样式: 12345678910index sentence1 sentence2 label0 No Weapons of Mass Destruction Found in Iraq Yet. Weapons of Mass Destruction Found in Iraq. not_entailment1 A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.Pope Benedict XVI is the new leader of the Roman Catholic Church. entailment2 Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients. Herceptin can be used to treat breast cancer. entailment3 Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment. The previous name of Ho Chi Minh City was Saigon.entailment4 A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One&#x27;s Crimewatch. Colette Aram, 16, was walking to her boyfriend&#x27;s house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later. Paul Stewart Hutchinson is accused of having stabbed a girl. not_entailment5 Britain said, Friday, that it has barred cleric, Omar Bakri, from returning to the country from Lebanon, where he was released by police after being detained for 24 hours. Bakri was briefly detained, but was released. entailment6 Nearly 4 million children who have at least one parent who entered the U.S. illegally were born in the United States and are U.S. citizens as a result, according to the study conducted by the Pew Hispanic Center. That&#x27;s about three quarters of the estimated 5.5 million children of illegal immigrants inside the United States, according to the study. About 1.8 million children of undocumented immigrants live in poverty, the study found. Three quarters of U.S. illegal immigrants have children. not_entailment7 Like the United States, U.N. officials are also dismayed that Aristide killed a conference called by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the feuding parties together. Aristide had Prime Minister Robert Malval murdered in Port-au-Prince. not_entailment8 WASHINGTON -- A newly declassified narrative of the Bush administration&#x27;s advice to the CIA on harsh interrogations shows that the small group of Justice Department lawyers who wrote memos authorizing controversial interrogation techniques were operating not on their own but with direction from top administration officials, including then-Vice President Dick Cheney and national security adviser Condoleezza Rice. At the same time, the narrative suggests that then-Defense Secretary Donald H. Rumsfeld and then-Secretary of State Colin Powell were largely left out of the decision-making process. Dick Cheney was the Vice President of Bush. entailment WNLI 中的 train.tsv 数据样式: 12345678910index sentence1 sentence2 label0 I stuck a pin through a carrot. When I pulled the pin out, it had a hole. The carrot had a hole. 11 John couldn&#x27;t see the stage with Billy in front of him because he is so short. John is so short. 12 The police arrested all of the gang members. They were trying to stop the drug trade in the neighborhood. The police were trying to stop the drug trade in the neighborhood. 13 Steve follows Fred&#x27;s example in everything. He influences him hugely. Steve influences him hugely. 04 When Tatyana reached the cabin, her mother was sleeping. She was careful not to disturb her, undressing and climbing back into her berth. mother was careful not to disturb her, undressing and climbing back into her berth. 05 George got free tickets to the play, but he gave them to Eric, because he was particularly eager to see it. George was particularly eager to see it. 06 John was jogging through the park when he saw a man juggling watermelons. He was very impressive. John was very impressive. 07 I couldn&#x27;t put the pot on the shelf because it was too tall. The pot was too tall. 18 We had hoped to place copies of our newsletter on all the chairs in the auditorium, but there were simply not enough of them. There were simply not enough copies of the newsletter. 1 (QNLI/RTE/WNLI) 中的 train.tsv 数据样式说明: train.tsv 中的数据内容共分为 4 列，第一列代表文本数据索引；第二列和第三列数据代表需要进行’是否蕴含’判定的句子对；第四列数据代表两个句子是否具有蕴含关系，0/not_entailment 代表不是蕴含关系，1/entailment 代表蕴含关系. QNLI 中的 test.tsv 数据样式: 1234567891011index question sentence0 What organization is devoted to Jihad against Israel? For some decades prior to the First Palestine Intifada in 1987, the Muslim Brotherhood in Palestine took a &quot;quiescent&quot; stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel&#x27;s &quot;indulgence&quot; to build up a network of mosques and charitable organizations.1 In what century was the Yarrow-Schlick-Tweedy balancing system used? In the late 19th century, the Yarrow-Schlick-Tweedy balancing &#x27;system&#x27; was used on some marine triple expansion engines.2 The largest brand of what store in the UK is located in Kingston Park? Close to Newcastle, the largest indoor shopping centre in Europe, the MetroCentre, is located in Gateshead.3 What does the IPCC rely on for research? In principle, this means that any significant new evidence or events that change our understanding of climate science between this deadline and publication of an IPCC report cannot be included.4 What is the principle about relating spin and space variables? Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive.5 Which network broadcasted Super Bowl 50 in the U.S.? CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game.6 What did the museum acquire from the Royal College of Science? To link this to the rest of the museum, a new entrance building was constructed on the site of the former boiler house, the intended site of the Spiral, between 1978 and 1982.7 What is the name of the old north branch of the Rhine? From Wijk bij Duurstede, the old north branch of the Rhine is called Kromme Rijn (&quot;Bent Rhine&quot;) past Utrecht, first Leidse Rijn (&quot;Rhine of Leiden&quot;) and then, Oude Rijn (&quot;Old Rhine&quot;).8 What was one of Luther&#x27;s most personal writings? It remains in use today, along with Luther&#x27;s hymns and his translation of the Bible.... (RTE/WNLI) 中的 test.tsv 数据样式: 1234567891011index sentence1 sentence20 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when Maude and Dora came in sight.1 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the trains came in sight.2 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the puffs came in sight.3 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the roars came in sight.4 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the whistles came in sight.5 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came in sight. Horses ran away when the horses came in sight.6 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming. Maude and Dora saw a train coming.7 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming. The trains saw a train coming.8 Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming. The puffs saw a train coming.... (QNLI/RTE/WNLI) 中的 test.tsv 数据样式说明: test.tsv 中的数据内容共分为 3 列，第一列数据代表每条文本数据的索引；第二列和第三列数据代表需要进行’是否蕴含’判定的句子对. (QNLI/RTE/WNLI) 数据集的任务类型: 句子对二分类任务 评估指标为: ACC","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"NLP","slug":"NLP","permalink":"https://leezhao415.github.io/tags/NLP/"}]},{"title":"计算机视觉之目标分割","slug":"计算机视觉之目标分割","date":"2021-06-24T15:14:56.000Z","updated":"2021-07-14T12:51:48.763Z","comments":true,"path":"2021/06/24/计算机视觉之目标分割/","link":"","permalink":"https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/","excerpt":"","text":"文章目录 目标分割 1 定义 2 任务描述和类型 2.1 任务描述 2.2 任务类型 3 评价指标 3.1 像素精度 3.2 平均像素精度 3.3 平均交并比 4 经典深度学习网络 4.1 FCN 4.2 UNet 目标分割 图像分割是目标检测更进阶的任务，目标检测只需要框出每个目标的包围盒，语义分割需要进一步判断图像中哪些像素属于哪个目标。但是，语义分割不区分属于相同类别的不同实例。如上图所示，当图像中有多个 cube 时，语义分割会将所有立方体整体的所有像素预测为 “cube” 这个类别。与此不同的是，实例分割需要区分出哪些像素属于第一个 cube、哪些像素属于第二个 cube……。 1 定义 在计算机视觉领域，图像分割（Object Segmentation）指的是将数字图像细分为多个图像子区域（像素的集合）的过程，并且同一个子区域内的特征具有一定相似性，不同子区域的特征呈现较为明显的差异。 图像分割的目标就是为图像中的每个像素分类。应用领域非常的广泛：自动驾驶、医疗影像，图像美化、三维重建等等。 自动驾驶（Autonomous vehicles）：汽车需要安装必要的感知系统以了解它们的环境，这样自动驾驶汽车才能够安全地驶入现有的道路 医疗影像诊断（Medical image diagnostics）：机器在分析能力上比放射科医生更强，而且可以大大减少诊断所需时间。 2 任务描述和类型 2.1 任务描述 简单来说，我们的目标是输入一个 RGB 彩色图片（heigh**t×width×3）或者一个灰度图（heigh**t×width×1），然后输出一个包含各个像素类别标签的分割图（heigh**t×width×1）。如下图所示： 与我们处理分类值的方式类似，预测目标可以采用 one-hot 编码，即为每一个可能的类创建一个输出通道。通过取每个像素点在各个 channel 的 argmax 可以得到最终的预测分割图，（如下图所示）： 比如：person 的编码为：10000，而 Grass 的编码为：00100 当将预测结果叠加到单个 channel 时，称这为一个掩膜 mask，它可以给出一张图像中某个特定类的所在区域： 2.2 任务类型 目前的图像分割任务主要有两类： 语义分割和实例分割 我们以下图为例，来介绍这两种分割方式： 语义分割就是把图像中每个像素赋予一个类别标签，如下图我们将图像中的像素分类为人，羊，狗，草地即可。 实例分割，相对于语义分割来讲，不仅要区分不同类别的像素，还需要需要对同一类别的不同个体进行区分。如下图所示，不仅需要进行类别的划分，还要将各个个体划分出来：羊 1，羊 2，羊 3，羊 4，羊 5 等。 目前图像分割的任务主要集中在语义分割，而目前的难点也在于 “语义”，表达某一语义的同一物体并不总是以相同的形象出现，如包含不同的颜色、纹理等，这对精确分割带来了很大的挑战。而且以目前的模型表现来看，在准确率上还有很大的提升空间。而实例分割的思路主要是目标检测 + 语义分割，即用目标检测方法将图像中的不同实例框出，再用语义分割方法在不同检测结果内进行逐像素标记。 3 评价指标 图像分割中通常使用许多标准来衡量算法的精度。这些标准通常是像素精度及 IoU 的变种，以下我们将会介绍常用的几种逐像素标记的精度标准。 为了便于解释，假设如下：共有 k+1 个类（从 L0 到 L**k，其中包含一个背景类），pij 表示本属于类 i 但被预测为类 j 的像素。即 pii 表示预测正确的像素。 3.1 像素精度 Pixel Accuracy (PA，像素精度)：这是最简单的度量，为预测正确的像素占总像素的比例。 对于样本不均衡的情况，例如医学图像分割中，背景与标记样本之间的比例往往严重失衡。因此并不适合使用这种方法进行度量。 3.2 平均像素精度 Mean Pixel Accuracy (MPA，平均像素精度)：是 PA 的一种简单提升，计算每个类内被正确分类像素数的比例，之后求所有类的平均。 3.3 平均交并比 Mean Intersection over Union (MIoU，平均交并比)：为语义分割的标准度量，其计算两个集合的交集和并集之比，在语义分割的问题中，这两个集合为真实值（ground truth）和预测值（predicted segmentation）。交集为预测正确的像素数（intersection），并集为预测或真实值为 i 类的和减去预测正确的像素，在每个类上计算 IoU，之后求平均即可。 那么，如何理解这里的公式呢？如下图所示，红色圆代表真实值，黄色圆代表预测值。橙色部分红色圆与黄色圆的交集，即预测正确的部分，红色部分表示假负（真实值为该类预测错误）的部分，黄色表示假正（预测值为 i 类，真实值为其他）的部分。 MIoU 计算的是计算 A 与 B 的交集（橙色部分）与 A 与 B 的并集（红色 + 橙色 + 黄色）之间的比例，在理想状态下 A 与 B 重合，两者比例为 1 。 在以上所有的度量标准中，MIoU 由于其简洁、代表性强而成为最常用的度量标准，大多数研究人员都使用该标准报告其结果。PA 对于样本不均衡的情况不适用。 4 经典深度学习网络 4.1 FCN FCN（Fully Convolutional Networks） 用于图像语义分割，自从该网络提出后，就成为语义分割的基本框架，后续算法基本都是在该网络框架中改进而来。 对于一般的分类 CNN 网络，如 VGG 和 Resnet，都会在网络的最后加入一些全连接层，经过 softmax 后就可以获得类别概率信息。 但是这个概率只能标识整个图片的类别，不能标识每个像素点的类别，所以这种全连接方法不适用于图像分割。 而 FCN 提出可以把后面几个全连接都换成卷积，这样就可以获得一张 2 维的 feature map，后接 softmax 获得每个像素点的分类信息，从而解决了分割问题，如下图所示： 简而言之，FCN 和 CNN 的区别就是：CNN 卷积层之后连接的是全连接层；FCN 卷积层之后仍连接卷积层，输出的是与输入大小相同的特征图。 1 网络结构 FCN 是一个端到端，像素对像素的全卷积网络，用于进行图像的语义分割。整体的网络结构分为两个部分：全卷积部分和上采样部分。 1.1 全卷积部分 全卷积部分使用经典的 CNN 网络（以 AlexNet 网络为例），并把最后的全连接层换成 [外链图片转存失败，源站可能有防盗链机制，建议将图片保存下来直接上传 (img-OlWEDzDo-1624508143015)(https://math.jianshu.com/math?formula=1%5Ctimes%201)] 卷积，用于提取特征。 在传统的 Alex 结构中，前 5 层是卷积层，第 6 层和第 7 层分别是一个长度为 4096 的一维向量，第 8 层是长度为 1000 的一维向量，分别对应 1000 个不同类别的概率。 FCN 将最后的 3 层转换为卷积层，卷积核的大小 (通道数，宽，高) 分别为 (4096,1,1)、(4096,1,1)、(1000,1,1)，虽然参数数目相同，但是计算方法就不一样了，这时还可使用预训练模型的参数。 CNN 中输入的图像固定成 227x227 大小，第一层 pooling 后为 55x55，第二层 pooling 后图像大小为 27x27，第五层 pooling 后的图像大小为 13x13, 而 FCN 输入的图像是 H*W 大小，第一层 pooling 后变为原图大小的 ½，第二层变为原图大小的 ¼，第五层变为原图大小的 ⅛，第八层变为原图大小的 1/16，如下所示： 经过多次卷积和 pooling 以后，得到的图像越来越小，分辨率越来越低。对最终的特征图进行 upsampling，把图像进行放大到原图像的大小，就得到原图像的分割结果。 1.2 上采样部分 上采样部分将最终得到的特征图上采样得到原图像大小的语义分割结果。 在这里采用的上采样方法是反卷积（Deconvolution），也叫做转置卷积（Transposed Convolution）： 反卷积是一种特殊的正向卷积 通俗的讲，就是输入补 0 + 卷积。先按照一定的比例通过补 0 来扩大输入图像的尺寸，再进行正向卷积即可。 如下图所示：输入图像尺寸为 3x3，卷积核 kernel 为 3x3，步长 strides=2，填充 padding=1 假设反卷积的输入是 n x n ，反卷积的输出为 mxm ，padding=p，stride=s，kernel_size = k。 那么此时反卷积的输出就为： m=s(n−1)+k−2pm=s(n−1)+k−2p 与正向卷积不同的是，要先根据步长 strides 对输入的内部进行填充，这里 strides 可以理解成输入放大的倍数，而不能理解成卷积移动的步长。 这样我们就可以通过反卷积实现上采样。 1.3 跳层连接 如果只利用反卷积对最后一层的特征图进行上采样的到原图大小的分割，由于最后一层的特征图太小，会损失很多细节。因而提出增加 Skips 结构将最后一层的预测（有更富的全局信息）和更浅层（有更多的局部细节）的预测结合起来。 那么： 对于 FCN-32s，直接对 pool5 feature 进行 32 倍上采样获得 32x upsampled feature，再对 32x upsampled feature 每个点做 softmax prediction 获得 32x upsampled feature prediction（即分割图）。 对于 FCN-16s，首先对 pool5 feature 进行 2 倍上采样获得 2x upsampled feature，再把 pool4 feature 和 2x upsampled feature 逐点相加，然后对相加的 feature 进行 16 倍上采样，并 softmax prediction，获得 16x upsampled feature prediction。 对于 FCN-8s，首先进行 pool4+2x upsampled feature 逐点相加，然后又进行 pool3+2x upsampled 逐点相加，即进行更多次特征融合。具体过程与 16s 类似，不再赘述。 下面有一张 32 倍，16 倍和 8 倍上采样得到的结果图对比： 可以看到随着上采样做得越多，分割结果越来越精细。 4.2 UNet Unet 网络是建立在 FCN 网络基础上的，它的网络架构如下图所示，总体来说与 FCN 思路非常类似。 整个网络由编码部分（左） 和 解码部分（右）组成，类似于一个大大的 U 字母，具体介绍如下： 1、编码部分是典型的卷积网络架构： 架构中含有着一种重复结构，每次重复中都有 2 个 3 x 3 卷积层、非线性 ReLU 层和一个 2 x 2 max pooling 层（stride 为 2）。（图中的蓝箭头、红箭头，没画 ReLu） 每一次下采样后我们都把特征通道的数量加倍 2、解码部分也使用了类似的模式： 每一步都首先使用反卷积 (up-convolution)，每次使用反卷积都将特征通道数量减半，特征图大小加倍。（图中绿箭头） 反卷积过后，将反卷积的结果与编码部分中对应步骤的特征图拼接起来。（白 / 蓝块） 编码部分中的特征图尺寸稍大，将其修剪过后进行拼接。（左边深蓝虚线） 对拼接后的 map 再进行 2 次 3 x 3 的卷积。（右侧蓝箭头） 最后一层的卷积核大小为 1 x 1，将 64 通道的特征图转化为特定类别数量（分类数量）的结果。（图中青色箭头） 数，而不能理解成卷积移动的步长。 这样我们就可以通过反卷积实现上采样。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"计算机视觉CV","slug":"计算机视觉CV","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV/"}]},{"title":"计算机视觉之目标检测","slug":"计算机视觉之目标检测","date":"2021-06-24T15:11:09.000Z","updated":"2021-07-14T12:49:16.638Z","comments":true,"path":"2021/06/24/计算机视觉之目标检测/","link":"","permalink":"https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/","excerpt":"","text":"文章目录 目标检测 1 定义 2 常用数据集 2.1 PASCAL VOC 数据集 2.2 MS COCO 数据集 3 常用的评价指标 3.1 IOU 3.2 mAP（Mean Average Precision） 4 NMS（非极大值抑制） 5 目标检测方法分类 5.1 two-stage 的算法 5.2 One-stage 的算法 6 经典深度学习网络 6.1 RCNN 6.2 Fast RCNN 6.3 Faster RCNN 6.4 YOLO 目标检测 1 定义 目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标，并确定它们的类别和位置。 目标检测中能检测出来的物体取决于当前任务（数据集）需要检测的物体有哪些。假设我们的目标检测模型定位是检测动物（牛、羊、猪、狗、猫五种结果），那么模型对任何一张图片输出结果不会输出鸭子、书籍等其它类型结果。 目标检测的位置信息一般由两种格式（以图片左上角为原点 (0,0)）： 1、极坐标表示：(xmin, ymin, xmax, ymax) xmin,ymin:x,y 坐标的最小值 xmin,ymin:x,y 坐标的最大值 2、中心点坐标：(x_center, y_center, w, h) x_center, y_center: 目标检测框的中心点坐标 w,h: 目标检测框的宽、高 2 常用数据集 经典的目标检测数据集有两种，PASCAL VOC 数据集 和 MS COCO 数据集。 2.1 PASCAL VOC 数据集 PASCAL VOC 是目标检测领域的经典数据集。PASCAL VOC 包含约 10,000 张带有边界框的图片用于训练和验证。PASCAL VOC 数据集是目标检测问题的一个基准数据集，很多模型都是在此数据集上得到的，常用的是 VOC2007 和 VOC2012 两个版本数据，共 20 个类别，分别是： 也就是： 1. 人：人 2. 动物：鸟，猫，牛，狗，马，羊 3. 交通工具：飞机，自行车，船，公共汽车，汽车，摩托车，火车 4. 室内：瓶子，椅子，餐桌，盆栽，沙发，电视 / 显示器 下载地址：https://pjreddie.com/projects/pascal-voc-dataset-mirror/ 整个数据的目录结构如下所示： 其中： JPEGImages 存放图片文件 Annotations 下存放的是 xml 文件，描述了图片信息，如下图所示，需要关注的就是节点下的数据，尤其是 bndbox 下的数据.xmin,ymin 构成了 boundingbox 的左上角，xmax,ymax 构成了 boundingbox 的右下角，也就是图像中的目标位置信息 ImageSets 包含以下 4 个文件夹： Action 下存放的是人的动作（例如 running、jumping 等等） Layout 下存放的是具有人体部位的数据（人的 head、hand、feet 等等） Segmentation 下存放的是可用于分割的数据。 Main 下存放的是图像物体识别的数据，总共分为 20 类，这是进行目标检测的重点。该文件夹中的数据对负样本文件进行了描述。 2.2 MS COCO 数据集 MS COCO 的全称是 Microsoft Common Objects in Context，微软于 2014 年出资标注的 Microsoft COCO 数据集，与 ImageNet 竞赛一样，被视为是计算机视觉领域最受关注和最权威的比赛之一。 COCO 数据集是一个大型的、丰富的物体检测，分割和字幕数据集。这个数据集以场景理解为目标，主要从复杂的日常场景中截取，图像中的目标通过精确的分割进行位置的标定。图像包括 91 类目标，328,000 影像和 2,500,000 个 label。目前为止目标检测的最大数据集，提供的类别有 80 类，有超过 33 万张图片，其中 20 万张有标注，整个数据集中个体的数目超过 150 万个。 图像示例： coco 数据集的标签文件标记了每个 segmentation+bounding box 的精确坐标，其精度均为小数点后两位一个目标的标签示意如下： {“segmentation”:[[392.87, 275.77, 402.24, 284.2, 382.54, 342.36, 375.99, 356.43, 372.23, 357.37, 372.23, 397.7, 383.48, 419.27,407.87, 439.91, 427.57, 389.25, 447.26, 346.11, 447.26, 328.29, 468.84, 290.77,472.59, 266.38], [429.44,465.23, 453.83, 473.67, 636.73, 474.61, 636.73, 392.07, 571.07, 364.88, 546.69,363.0]], “area”: 28458.996150000003, “iscrowd”: 0,“image_id”: 503837, “bbox”: [372.23, 266.38, 264.5,208.23], “category_id”: 4, “id”: 151109}, 3 常用的评价指标 3.1 IOU 在目标检测算法中，IoU（intersection over union，交并比）是目标检测算法中用来评价 2 个矩形框之间相似度的指标： IoU = 两个矩形框相交的面积 / 两个矩形框相并的面积， 如下图所示： 通过一个例子看下在目标检测中的应用： 其中上图蓝色框框为检测结果，红色框框为真实标注。 那我们就可以通过预测结果与真实结果之间的交并比来衡量两者之间的相似度。一般情况下对于检测框的判定都会存在一个阈值，也就是 IoU 的阈值，一般可以设置当 IoU 的值大于 0.5 的时候，则可认为检测到目标物体。 实现方法： 12345678910111213141516171819202122232425262728293031import numpy as np# 定义方法计算IOUdef Iou(box1, box2, wh=False): # 判断bbox的表示形式 if wh == False: # 使用极坐标形式表示：直接获取两个bbox的坐标 xmin1, ymin1, xmax1, ymax1 = box1 xmin2, ymin2, xmax2, ymax2 = box2 else: # 使用中心点形式表示： 获取两个两个bbox的极坐标表示形式 # 第一个框左上角坐标 xmin1, ymin1 = int(box1[0]-box1[2]/2.0), int(box1[1]-box1[3]/2.0) # 第一个框右下角坐标 xmax1, ymax1 = int(box1[0]+box1[2]/2.0), int(box1[1]+box1[3]/2.0) # 第二个框左上角坐标 xmin2, ymin2 = int(box2[0]-box2[2]/2.0), int(box2[1]-box2[3]/2.0) # 第二个框右下角坐标 xmax2, ymax2 = int(box2[0]+box2[2]/2.0), int(box2[1]+box2[3]/2.0) # 获取矩形框交集对应的左上角和右下角的坐标（intersection） xx1 = np.max([xmin1, xmin2]) yy1 = np.max([ymin1, ymin2]) xx2 = np.min([xmax1, xmax2]) yy2 = np.min([ymax1, ymax2]) # 计算两个矩形框面积 area1 = (xmax1-xmin1) * (ymax1-ymin1) area2 = (xmax2-xmin2) * (ymax2-ymin2) #计算交集面积 inter_area = (np.max([0, xx2-xx1])) * (np.max([0, yy2-yy1])) #计算交并比 iou = inter_area / (area1+area2-inter_area+1e-6) return iou 假设我们检测结果如下所示，并展示在图像上： 12345678910111213141516import matplotlib.pyplot as pltimport matplotlib.patches as patches# 真实框与预测框True_bbox, predict_bbox = [100, 35, 398, 400], [40, 150, 355, 398]# bbox是bounding box的缩写img = plt.imread(&#x27;dog.jpeg&#x27;)fig = plt.imshow(img)# 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：((左上x, 左上y), 宽, 高)# 真实框绘制fig.axes.add_patch(plt.Rectangle( xy=(True_bbox[0], True_bbox[1]), width=True_bbox[2]-True_bbox[0], height=True_bbox[3]-True_bbox[1], fill=False, edgecolor=&quot;blue&quot;, linewidth=2))# 预测框绘制fig.axes.add_patch(plt.Rectangle( xy=(predict_bbox[0], predict_bbox[1]), width=predict_bbox[2]-predict_bbox[0], height=predict_bbox[3]-predict_bbox[1], fill=False, edgecolor=&quot;red&quot;, linewidth=2)) 计算 IoU： 1Iou(True_bbox,predict_bbox) 结果为： 10.5114435907762924 3.2 mAP（Mean Average Precision） 目标检测问题中的每个图片都可能包含一些不同类别的物体，需要评估模型的物体分类和定位性能。因此，用于图像分类问题的标准指标 precision 不能直接应用于此。 在目标检测中，mAP 是主要的衡量指标。 mAP 是多个分类任务的 AP 的平均值，而 AP（average precision）是 PR 曲线下的面积，所以在介绍 mAP 之前我们要先得到 PR 曲线。 TP、FP、FN、TN 查准率、查全率 查准率（Precision）: TP/(TP + FP) 查全率（Recall）: TP/(TP + FN) 二者绘制的曲线称为 P-R 曲线 先定义两个公式，一个是 Precision，一个是 Recall，与上面的公式相同，扩展开来，用另外一种形式进行展示，其中 all detctions 代表所有预测框的数量， all ground truths 代表所有 GT 的数量。 AP 是计算某一类 P-R 曲线下的面积，mAP 则是计算所有类别 P-R 曲线下面积的平均值。 假设我们有 7 张图片（Images1-Image7），这些图片有 15 个目标（绿色的框，GT 的数量，上文提及的 all ground truths ）以及 24 个预测边框（红色的框，A-Y 编号表示，并且有一个置信度值）： 根据上图以及说明，我们可以列出以下表格，其中 Images 代表图片的编号，Detections 代表预测边框的编号，Confidences 代表预测边框的置信度，TP or FP 代表预测的边框是标记为 TP 还是 FP（认为预测边框与 GT 的 IOU 值大于等于 0.3 就标记为 TP；若一个 GT 有多个预测边框，则认为 IOU 最大且大于等于 0.3 的预测框标记为 TP，其他的标记为 FP，即一个 GT 只能有一个预测框标记为 TP），这里的 0.3 是随机取的一个值。 通过上表，我们可以绘制出 P-R 曲线（因为 AP 就是 P-R 曲线下面的面积），但是在此之前我们需要计算出 P-R 曲线上各个点的坐标，根据置信度从大到小排序所有的预测框，然后就可以计算 Precision 和 Recall 的值，见下表。（需要记住一个叫累加的概念，就是下图的 ACC TP 和 ACC FP） 标号为 1 的 Precision 和 Recall 的计算方式：Precision=TP/(TP+FP)=1/(1+0)=1，Recall=TP/(TP+FN)=TP/( all ground truths )=1/15=0.0666 （ all ground truths 上面有定义过了 ） 标号 2：Precision=TP/(TP+FP)=1/(1+1)=0.5，Recall=TP/(TP+FN)=TP/( all ground truths )=1/15=0.0666 标号 3：Precision=TP/(TP+FP)=2/(2+1)=0.6666，Recall=TP/(TP+FN)=TP/( all ground truths )=2/15=0.1333 其他的依次类推 然后就可以绘制出 P-R 曲线 得到 P-R 曲线就可以计算 AP（P-R 曲线下的面积），要计算 P-R 下方的面积，有两种方法： 在 VOC2010 以前，只需要选取当 Recall &gt;= 0, 0.1, 0.2, …, 1 共 11 个点时的 Precision 最大值，然后 AP 就是这 11 个 Precision 的平均值，取 11 个点 [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] 的插值所得 得到一个类别的 AP 结果如下： 要计算 mAP，就把所有类别的 AP 计算出来，然后求取平均即可。 在 VOC2010 及以后，需要针对每一个不同的 Recall 值（包括 0 和 1），选取其大于等于这些 Recall 值时的 Precision 最大值，如下图所示： 然后计算 PR 曲线下面积作为 AP 值： 计算方法如下所示： 4 NMS（非极大值抑制） 非极大值抑制（Non-Maximum Suppression，NMS），顾名思义就是抑制不是极大值的元素。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到 NMS 来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。 NMS 在计算机视觉领域有着非常重要的应用，如视频目标跟踪、数据挖掘、3D 重建、目标识别以及纹理分析等 。 在目标检测中，NMS 的目的就是要去除冗余的检测框，保留最好的一个，如下图所示： NMS 的原理是对于预测框的列表 B 及其对应的置信度 S, 选择具有最大 score 的检测框 M, 将其从 B 集合中移除并加入到最终的检测结果 D 中。通常将 B 中剩余检测框中与 M 的 IoU 大于阈值 Nt 的框从 B 中移除。重复这个过程，直到 B 为空。 使用流程如下图所示： 首先是检测出一系列的检测框 将检测框按照类别进行分类 对同一类别的检测框应用 NMS 获取最终的检测结果 通过一个例子看些 NMS 的使用方法，假设定位车辆，算法就找出了一系列的矩形框，我们需要判别哪些矩形框是没用的，需要使用 NMS 的方法来实现。 假设现在检测窗口有：A、B、C、D、E 5 个候选框，接下来进行迭代计算： 第一轮：因为 B 是得分最高的，与 B 的 IoU＞0.5 删除。A，CDE 中现在与 B 计算 IoU，DE 结果＞0.5，剔除 DE，B 作为一个预测结果，有个检测框留下 B，放入集合 第二轮：A 的得分最高，与 A 计算 IoU，C 的结果＞0.5，剔除 C，A 作为一个结果 最终结果为在这个 5 个中检测出了两个目标为 A 和 B。 单类别的 NMS 的实现方法如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import numpy as npdef nms(bboxes, confidence_score, threshold): &quot;&quot;&quot;非极大抑制过程 :param bboxes: 同类别候选框坐标 :param confidence: 同类别候选框分数 :param threshold: iou阈值 :return: &quot;&quot;&quot; # 1、传入无候选框返回空 if len(bboxes) == 0: return [], [] # 强转数组 bboxes = np.array(bboxes) score = np.array(confidence_score) # 取出n个的极坐标点 x1 = bboxes[:, 0] y1 = bboxes[:, 1] x2 = bboxes[:, 2] y2 = bboxes[:, 3] # 2、对候选框进行NMS筛选 # 返回的框坐标和分数 picked_boxes = [] picked_score = [] # 对置信度进行排序, 获取排序后的下标序号, argsort默认从小到大排序 order = np.argsort(score) areas = (x2 - x1) * (y2 - y1) while order.size &gt; 0: # 将当前置信度最大的框加入返回值列表中 index = order[-1] #保留该类剩余box中得分最高的一个 picked_boxes.append(bboxes[index]) picked_score.append(confidence_score[index]) # 获取当前置信度最大的候选框与其他任意候选框的相交面积 x11 = np.maximum(x1[index], x1[order[:-1]]) y11 = np.maximum(y1[index], y1[order[:-1]]) x22 = np.minimum(x2[index], x2[order[:-1]]) y22 = np.minimum(y2[index], y2[order[:-1]]) # 计算相交的面积,不重叠时面积为0 w = np.maximum(0.0, x22 - x11) h = np.maximum(0.0, y22 - y11) intersection = w * h # 利用相交的面积和两个框自身的面积计算框的交并比 ratio = intersection / (areas[index] + areas[order[:-1]] - intersection) # 保留IoU小于阈值的box keep_boxes_indics = np.where(ratio &lt; threshold) # 保留剩余的框 order = order[keep_boxes_indics] # 返回NMS后的框及分类结果 return picked_boxes, picked_score 假设有检测结果如下： 1234567bounding = [(187, 82, 337, 317), (150, 67, 305, 282), (246, 121, 368, 304)]confidence_score = [0.9, 0.65, 0.8]threshold = 0.3picked_boxes, picked_score = nms(bounding, confidence_score, threshold)print(&#x27;阈值threshold为:&#x27;, threshold)print(&#x27;NMS后得到的bbox是：&#x27;, picked_boxes)print(&#x27;NMS后得到的bbox的confidences是：&#x27;, picked_score) 返回结果： 123阈值threshold为: 0.3NMS后得到的bbox是： [array([187, 82, 337, 317])]NMS后得到的bbox的confidences是： [0.9] 5 目标检测方法分类 目标检测算法主要分为 two-stage（两阶段）和 one-stage（单阶段）两类： 5.1 two-stage 的算法 先由算法生成一系列作为样本的候选框，再通过卷积神经网络进行样本分类。如下图所示，主要通过一个卷积神经网络来完成目标检测过程，其提取的是 CNN 卷积特征，进行候选区域的筛选和目标检测两部分。网络的准确度高、速度相对较慢。 two-stages 算法的代表是 RCNN 系列：R-CNN 到 Faster R-CNN 网络 5.2 One-stage 的算法 直接通过主干网络给出目标的类别和位置信息，没有使用候选区域的筛选网路，这种算法速度快，但是精度相对 Two-stage 目标检测网络降低了很多。 one-stage 算法的代表是： YOLO 系列：YOLOv1、YOLOv2、YOLOv3、 SSD 等 6 经典深度学习网络 6.1 RCNN 2014 年提出 R-CNN 网络，该网络不再使用暴力穷举的方法，而是使用候选区域方法（region proposal method）创建目标检测的区域来完成目标检测的任务，R-CNN 是以深度神经网络为基础的目标检测的模型 ，以 R-CNN 为基点，后续的 Fast R-CNN、Faster R-CNN 模型都延续了这种目标检测思路。 步骤是： 候选区域生成：使用选择性搜索（Selective Search）的方法找出图片中可能存在目标的侯选区域 CNN 网络提取特征：选取预训练卷积神经网网络（AlexNet 或 VGG）用于进行特征提取。 目标分类：训练支持向量机（SVM）来辨别目标物体和背景，对每个类别，都要训练一个二元 SVM。 目标定位：训练一个线性回归模型，为每个辨识到的物体生成更精确的边界框。 1 候选区域生成 在选择性搜索（SelectiveSearch，SS）中，使用语义分割的方法，它将颜色、边界、纹理等信息作为合并条件，采用多尺度的综合方法，将图像在像素级上划分出一系列的区域，这些区域要远远少于传统的滑动窗口的穷举法产生的候选区域。 SelectiveSearch 在一张图片上提取出来约 2000 个侯选区域，需要注意的是这些候选区域的长宽不固定。 而使用 CNN 提取候选区域的特征向量，需要接受固定长度的输入，所以需要对候选区域做一些尺寸上的修改。 2 CNN 网络提取特征 采用预训练模型 (AlexNet 或 VGG) 在生成的候选区域上进行特征提取，将提取好的特征保存在磁盘中，用于后续步骤的分类和回归。 1. 全连接层的输入数据的尺寸是固定的，因此在将候选区域送入 CNN 网络中时，需进行裁剪或变形为固定的尺寸，在进行特征提取。 2. 预训练模型在 ImageNet 数据集上获得，最后的全连接层是 1000，在这里我们需要将其改为 N+1 (N 为目标类别的数目，例如 VOC 数据集中 N=20，coco 数据集中 N=80，1 是加一个背景) 后，进行微调即可。 3. 利用微调后的 CNN 网络，提取每一个候选区域的特征，获取一个 4096 维的特征，一幅图像就是 2000x4096 维特征存储到磁盘中。 3 目标分类（SVM） 假设我们要检测猫狗两个类别，那我们需要训练猫和狗两个不同类别的 SVM 分类器，然后使用训练好的分类器对一幅图像中 2000 个候选区域的特征向量分别判断一次，这样得出 [2000, 2] 的得分矩阵，如下图所示： 对于 N 个类别的检测任务，需要训练 N（目标类别数目）个 SVM 分类器，对候选区域的特征向量（4096 维）进行二分类，判断其是某一类别的目标，还是背景来完成目标分类。 4 目标定位 通过选择性搜索获取的目标位置不是非常的准确，实验证明，训练一个线性回归模型在给定的候选区域的结果上去预测一个新的检测窗口，能够获得更精确的位置。修正过程如下图所示： 通过训练一个回归器来对候选区域的范围进行一个调整，这些候选区域最开始只是用选择性搜索的方法粗略得到的，通过调整之后得到更精确的位置，如下所示： 5 预测过程 使用选择性搜索的方法从一张图片中提取 2000 个候选区域，将每个区域送入 CNN 网络中进行特征提取，然后送入到 SVM 中进行分类，并使用候选框回归器，计算出每个候选区域的位置。 候选区域较多，有 2000 个，需要剔除掉部分检测结果。 针对每个类，通过计算 IOU, 采取非最大值抑制 NMS 的方法，保留比较好的检测结果。 算法总结 1、训练阶段多，训练耗时： 微调 CNN 网络 + 训练 SVM + 训练边框回归器。 2、预测速度慢：使用 GPU, VGG16 模型处理一张图像需要 47s。 3、占用磁盘空间大：5000 张图像产生几百 G 的特征文件。 4、数据的形状变化：候选区域要经过缩放来固定大小，无法保证目标的不变形 6.2 Fast RCNN 6.2.1 模型特点 相比于 R-CNN, Fast R-CNN 主要在以下三个方面进行了改进： 1、提高训练和预测的速度 R-CNN 首先从测试图中提取 2000 个候选区域，然后将这 2000 个候选区域分别输入到预训练好的 CNN 中提取特征。由于候选区域有大量的重叠，这种提取特征的方法，就会重复的计算重叠区域的特征。在 Fast-RCNN 中，将整张图输入到 CNN 中提取特征，将候选区域映射到特征图上，这样就避免了对图像区域进行重复处理，提高效率减少时间。 2、不需要额外的空间保存 CNN 网络提取的特征向量 RCNN 中需要将提取到的特征保存下来，用于为每个类训练单独的 SVM 分类器和边框回归器。在 Fast-RCNN 中，将类别判断和边框回归统一使用 CNN 实现，不需要在额外的空间存储特征。 3、不在直接对候选区域进行缩放 RCNN 中需要对候选区域进行缩放送入 CNN 中进行特征提取，在 Fast-RCNN 中使用 ROIpooling 的方法进行尺寸的调整。 6.2.2 算法流程 Fast_RCNN 的流程如下图所示： 步骤是： 1、候选区域生成：使用选择性搜索（Selective Search）的方法找出图片中可能存在目标的侯选区域，只需要候选区域的位置信息 2、CNN 网络特征提取：将整张图像输入到 CNN 网络中，得到整副图的特征图，并将上一步获取的候选区域位置从原图映射到该特征图上 3、ROIPooling: 对于每个特征图上候选框，RoI pooling 层从特征图中提取固定长度的特征向量每个特征向量被送入一系列全连接（fc）层中。 4、目标检测：分两部分完成，一个输出各类别加上 1 个背景类别的 Softmax 概率估计，另一个为各类别的每一个类别输出四个实数值，来确定目标的位置信息。 6.3 Faster RCNN 在 R-CNN 和 Fast RCNN 的基础上，在 2016 年提出了 Faster RCNN 网络模型，在结构上，Faster RCNN 已经将候选区域的生成，特征提取，目标分类及目标框的回归都整合在了一个网络中，综合性能有较大提高，在检测速度方面尤为明显。 1、特征提取：将整个图像缩放至固定的大小输入到 CNN 网络中进行特征提取，得到特征图。 2、候选区域提取：输入特征图，使用区域生成网络 RPN，产生一些列的候选区域 3、ROIPooling: 与 Fast RCNN 网络中一样，使用最大池化固定候选区域的尺寸，送入后续网络中进行处理 4、目标分类和回归：与 Fast RCNN 网络中一样，使用两个同级层：K+1 个类别的 SoftMax 分类层和边框的回归层，来完成目标的分类和回归。 Faster R-CNN 的流程与 Fast R-CNN 的区别不是很大，重要的改进是使用 RPN 网络来替代选择性搜索获取候选区域，所以我们可以将 Faster R-CNN 网络看做 RPN 和 Fast R-CNN 网络的结合。 将网络分为四部分： Backbone：Backbone 由 CNN 卷积神经网络构成，常用的是 VGG 和 resnet, 用来提取图像中的特征，获取图像的特征图。该特征图被共享用于后续 RPN 层生成候选区域和 ROIPooling 层中。 RPN 网络：RPN 网络用于生成候选区域，用于后续的目标检测。 Roi Pooling: 该部分收集图像的特征图和 RPN 网络提取的候选区域位置，综合信息后获取固定尺寸的特征，送入后续全连接层判定目标类别和确定目标位置。 目标分类与回归：该部分利用 ROIpooling 输出特征向量计算候选区域的类别，并通过回归获得检测框最终的精确位置。 6.4 YOLO Yolo 意思是 You Only Look Once，它并没有真正的去掉候选区域，而是创造性的将候选区和目标分类合二为一，看一眼图片就能知道有哪些对象以及它们的位置。 Yolo 模型采用预定义预测区域的方法来完成目标检测，具体而言是将原始图像划分为 7x7=49 个网格（grid），每个网格允许预测出 2 个边框（bounding box，包含某个对象的矩形框），总共 49x2=98 个 bounding box。我们将其理解为 98 个预测区，很粗略的覆盖了图片的整个区域，就在这 98 个预测区中进行目标检测。 YOLO 的结构非常简单，就是单纯的卷积、池化最后加了两层全连接，从网络结构上看，与前面介绍的 CNN 分类网络没有本质的区别，最大的差异是输出层用线性函数做激活函数，因为需要预测 bounding box 的位置（数值型），而不仅仅是对象的概率。所以粗略来说，YOLO 的整个结构就是输入图片经过神经网络的变换得到一个输出的张量，如下图所示： 网络结构比较简单，重点是我们要理解网络输入与输出之间的关系。 4.1 网络输入 网络的输入是原始图像，唯一的要求是缩放到 448x448 的大小。主要是因为 Yolo 的网络中，卷积层最后接了两个全连接层，全连接层是要求固定大小的向量作为输入，所以 Yolo 的输入图像的大小固定为 448x448。 4.2 网络输出 网络的输出就是一个 7x7x30 的张量（tensor）。那这个输出结果我们要怎么理解那？ 4.2.1 7X7 网格 根据 YOLO 的设计，输入图像被划分为 7x7 的网格（grid），输出张量中的 7x7 就对应着输入图像的 7x7 网格。或者我们把 7x7x30 的张量看作 7x7=49 个 30 维的向量，也就是输入图像中的每个网格对应输出一个 30 维的向量。如下图所示，比如输入图像左上角的网格对应到输出张量中左上角的向量。 4.2.2 30 维向量 30 维的向量包含：2 个 bbox 的位置和置信度以及该网格属于 20 个类别的概率 2 个 bounding box 的位置 每个 bounding box 需要 4 个数值来表示其位置，(Center_x,Center_y,width,height)，即 (bounding box 的中心点的 x 坐标，y 坐标，bounding box 的宽度，高度)，2 个 bounding box 共需要 8 个数值来表示其位置。 2 个 bounding box 的置信度 bounding box 的置信度 = 该 bounding box 内存在对象的概率 * 该 bounding box 与该对象实际 bounding box 的 IOU，用公式表示就是： Pr (Object) 是 bounding box 内存在对象的概率 20 个对象分类的概率 Yolo 支持识别 20 种不同的对象（人、鸟、猫、汽车、椅子等），所以这里有 20 个值表示该网格位置存在任一种对象的概率.","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"计算机视觉CV","slug":"计算机视觉CV","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV/"}]},{"title":"计算机视觉之图像分类","slug":"计算机视觉之图像分类","date":"2021-06-24T15:08:20.000Z","updated":"2021-07-14T12:44:51.824Z","comments":true,"path":"2021/06/24/计算机视觉之图像分类/","link":"","permalink":"https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/","excerpt":"","text":"文章目录 图像分类 1 定义 2 常用数据集 2.1 mnist 数据集 2.2 CIFAR-10 和 CIFAR-100 2.3 ImageNet 3 经典深度学习网络 3.1 AlexNet 3.2 VGG 3.3 GoogLeNet 3.4 ResNet 4 图像增强方法 4.1 tf.image 进行图像增强 4.2 使用 ImageDataGenerator () 进行图像增强 5 模型微调 5.1 微调 5.2 热狗识别 图像分类 1 定义 从给定的类别集合中为图像分配对应标签的任务 2 常用数据集 2.1 mnist 数据集 该数据集是手写数字 0-9 的集合，共有 60k 训练图像、10k 测试图像、10 个类别、图像大小 28×28×1. 我们可以通过 tf.keras 直接加载该数据集： 123from tensorflow.keras.datasets import mnist# 加载mnist数据集(train_images, train_labels), (test_images, test_labels) = mnist.load_data() 随机选择图像展示结果如下所示： 2.2 CIFAR-10 和 CIFAR-100 CIFAR-10 数据集 5 万张训练图像、1 万张测试图像、10 个类别、每个类别有 6k 个图像，图像大小 32×32×3。下图列举了 10 个类，每一类随机展示了 10 张图片： CIFAR-100 数据集也是有 5 万张训练图像、1 万张测试图像、包含 100 个类别、图像大小 32×32×3。 在 tf.keras 中加载数据集时： 123456import tensorflow as tffrom tensorflow.keras.datasets import cifar10,cifar100# 加载Cifar10数据集(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()# 加载Cifar100数据集(train_images, train_labels), (test_images, test_labels)= cifar100.load_data() 2.3 ImageNet ImageNet 数据集是 ILSVRC 竞赛使用的是数据集，由斯坦福大学李飞飞教授主导，包含了超过 1400 万张全尺寸的有标记图片，大约有 22000 个类别的数据。ILSVRC 全称 ImageNet Large-Scale Visual Recognition Challenge，是视觉领域最受追捧也是最具权威的学术竞赛之一，代表了图像领域的最高水平。从 2010 年开始举办到 2017 年最后一届，使用 ImageNet 数据集的一个子集，总共有 1000 类。 该比赛的获胜者从 2012 年开始都是使用的深度学习的方法： 2012 年冠军是 AlexNet , 由于准确率远超传统方法的第二名（top5 错误率为 15.3%，第二名为 26.2%），引起了很大的轰动。自此之后，CNN 成为在图像识别分类的核心算法模型，带来了深度学习的大爆发。 2013 年冠军是 ZFNet ，结构和 AlexNet 区别不大，分类效果也差不多。 2014 年亚军是 VGG 网络，网络结构十分简单，因此至今 VGG-16 仍在广泛使用。 2014 年的冠军网络是 GooLeNet ，核心模块是 Inception Module。Inception 历经了 V1、V2、V3、V4 等多个版本的发展，不断趋于完善。GoogLeNet 取名中 L 大写是为了向 LeNet 致敬，而 Inception 的名字来源于盗梦空间中的 &quot;we need to go deeper&quot; 梗。 2015 年冠军网络是 ResNet 。核心是带短连接的残差模块，其中主路径有两层卷积核（Res34），短连接把模块的输入信息直接和经过两次卷积之后的信息融合，相当于加了一个恒等变换。短连接是深度学习又一重要思想，除计算机视觉外，短连接思想也被用到了机器翻译、语音识别 / 合成领域 2017 年冠军 SENet 是一个模块，可以和其他的网络架构结合，比如 GoogLeNet、ResNet 等。 3 经典深度学习网络 3.1 AlexNet 2012 年，AlexNet 横空出世，该模型的名字源于论文第一作者的姓名 Alex Krizhevsky 。AlexNet 使用了 8 层卷积神经网络，以很大的优势赢得了 ImageNet 2012 图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的方向。 该网络的特点是： AlexNet 包含 8 层变换，有 5 层卷积和 2 层全连接隐藏层，以及 1 个全连接输出层 AlexNet 第一层中的卷积核形状是 11\\times11。第二层中的卷积核形状减小到 5\\times5，之后全采用 3\\times3。所有的池化层窗口大小为 3\\times3、步幅为 2 的最大池化。 AlexNet 将 sigmoid 激活函数改成了 ReLU 激活函数，使计算更简单，网络更容易训练 AlexNet 通过 dropOut 来控制全连接层的模型复杂度。 AlexNet 引入了大量的图像增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。 在 tf.keras 中实现 AlexNet 模型： 12345678910111213141516171819202122232425262728293031# 构建AlexNet模型net = tf.keras.models.Sequential([ # 卷积层：96个卷积核，卷积核为11*11，步幅为4，激活函数relu tf.keras.layers.Conv2D(filters=96,kernel_size=11,strides=4,activation=&#x27;relu&#x27;), # 池化:窗口大小为3*3、步幅为2 tf.keras.layers.MaxPool2D(pool_size=3, strides=2), # 卷积层：256个卷积核，卷积核为5*5，步幅为1，padding为same，激活函数relu tf.keras.layers.Conv2D(filters=256,kernel_size=5,padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;), # 池化:窗口大小为3*3、步幅为2 tf.keras.layers.MaxPool2D(pool_size=3, strides=2), # 卷积层：384个卷积核，卷积核为3*3，步幅为1，padding为same，激活函数relu tf.keras.layers.Conv2D(filters=384,kernel_size=3,padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;), # 卷积层：384个卷积核，卷积核为3*3，步幅为1，padding为same，激活函数relu tf.keras.layers.Conv2D(filters=384,kernel_size=3,padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;), # 卷积层：256个卷积核，卷积核为3*3，步幅为1，padding为same，激活函数relu tf.keras.layers.Conv2D(filters=256,kernel_size=3,padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;), # 池化:窗口大小为3*3、步幅为2 tf.keras.layers.MaxPool2D(pool_size=3, strides=2), # 伸展为1维向量 tf.keras.layers.Flatten(), # 全连接层:4096个神经元，激活函数relu tf.keras.layers.Dense(4096,activation=&#x27;relu&#x27;), # 随机失活 tf.keras.layers.Dropout(0.5), # 全链接层：4096个神经元，激活函数relu tf.keras.layers.Dense(4096,activation=&#x27;relu&#x27;), # 随机失活 tf.keras.layers.Dropout(0.5), # 输出层：10个神经元，激活函数softmax tf.keras.layers.Dense(10,activation=&#x27;softmax&#x27;)]) 我们构造一个高和宽均为 227 的单通道数据样本来看一下模型的架构： 12345# 构造输入X，并将其送入到net网络中X = tf.random.uniform((1,227,227,1)y = net(X)# 通过net.summay()查看网络的形状net.summay() 网络架构如下： 123456789101112131415161718192021222324252627282930313233343536Model: &quot;sequential&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================conv2d (Conv2D) (1, 55, 55, 96) 11712 _________________________________________________________________max_pooling2d (MaxPooling2D) (1, 27, 27, 96) 0 _________________________________________________________________conv2d_1 (Conv2D) (1, 27, 27, 256) 614656 _________________________________________________________________max_pooling2d_1 (MaxPooling2 (1, 13, 13, 256) 0 _________________________________________________________________conv2d_2 (Conv2D) (1, 13, 13, 384) 885120 _________________________________________________________________conv2d_3 (Conv2D) (1, 13, 13, 384) 1327488 _________________________________________________________________conv2d_4 (Conv2D) (1, 13, 13, 256) 884992 _________________________________________________________________max_pooling2d_2 (MaxPooling2 (1, 6, 6, 256) 0 _________________________________________________________________flatten (Flatten) (1, 9216) 0 _________________________________________________________________dense (Dense) (1, 4096) 37752832 _________________________________________________________________dropout (Dropout) (1, 4096) 0 _________________________________________________________________dense_1 (Dense) (1, 4096) 16781312 _________________________________________________________________dropout_1 (Dropout) (1, 4096) 0 _________________________________________________________________dense_2 (Dense) (1, 10) 40970 =================================================================Total params: 58,299,082Trainable params: 58,299,082Non-trainable params: 0_________________________________________________________________ 3.2 VGG 2014 年，牛津大学计算机视觉组（Visual Geometry Group）和 Google DeepMind 公司的研究员一起研发出了新的深度卷积神经网络：VGGNet，并取得了 ILSVRC2014 比赛分类项目的第二名，主要贡献是使用很小的卷积核 (3×3) 构建卷积神经网络结构，能够取得较好的识别精度，常用来提取图像特征的 VGG-16 和 VGG-19。 VGGNet 使用的全部都是 3x3 的小卷积核和 2x2 的池化核，通过不断加深网络来提升性能。VGG 可以通过重复使用简单的基础块来构建深度模型。 在 tf.keras 中实现 VGG 模型，首先来实现 VGG 块，它的组成规律是：连续使用多个相同的填充为 1、卷积核大小为 3\\times 3 的卷积层后接上一个步幅为 2、窗口形状为 2\\times 2 的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用 vgg_block 函数来实现这个基础的 VGG 块，它可以指定卷积层的数量 num_convs 和每层的卷积核个数 num_filters： 123456789101112# 定义VGG网络中的卷积块：卷积层的个数，卷积层中卷积核的个数def vgg_block(num_convs, num_filters): # 构建序列模型 blk = tf.keras.models.Sequential() # 遍历所有的卷积层 for _ in range(num_convs): # 每个卷积层：num_filter个卷积核，卷积核大小为3*3，padding是same，激活函数是relu blk.add(tf.keras.layers.Conv2D(num_filters,kernel_size=3, padding=&#x27;same&#x27;,activation=&#x27;relu&#x27;)) # 卷积块最后是一个最大池化，窗口大小为2*2，步长为2 blk.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2)) return blk VGG16 网络有 5 个卷积块，前 2 块使用两个卷积层，而后 3 块使用三个卷积层。第一块的输出通道是 64，之后每次对输出通道数翻倍，直到变为 512。 12# 定义5个卷积块，指明每个卷积块中的卷积层个数及相应的卷积核个数conv_arch = ((2, 64), (2, 128), (3, 256), (3, 512), (3, 512)) 因为这个网络使用了 13 个卷积层和 3 个全连接层，所以经常被称为 VGG-16, 通过制定 conv_arch 得到模型架构后构建 VGG16： 123456789101112131415161718192021222324# 定义VGG网络def vgg(conv_arch): # 构建序列模型 net = tf.keras.models.Sequential() # 根据conv_arch生成卷积部分 for (num_convs, num_filters) in conv_arch: net.add(vgg_block(num_convs, num_filters)) # 卷积块序列后添加全连接层 net.add(tf.keras.models.Sequential([ # 将特征图展成一维向量 tf.keras.layers.Flatten(), # 全连接层：4096个神经元，激活函数是relu tf.keras.layers.Dense(4096, activation=&#x27;relu&#x27;), # 随机失活 tf.keras.layers.Dropout(0.5), # 全连接层：4096个神经元，激活函数是relu tf.keras.layers.Dense(4096, activation=&#x27;relu&#x27;), # 随机失活 tf.keras.layers.Dropout(0.5), # 全连接层：10个神经元，激活函数是softmax tf.keras.layers.Dense(10, activation=&#x27;softmax&#x27;)])) return net# 网络实例化net = vgg(conv_arch) 我们构造一个高和宽均为 224 的单通道数据样本来看一下模型的架构： 12345# 构造输入X，并将其送入到net网络中X = tf.random.uniform((1,224,224,1))y = net(X)# 通过net.summay()查看网络的形状net.summay() 网络架构如下： 1234567891011121314151617181920Model: &quot;sequential_15&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================sequential_16 (Sequential) (1, 112, 112, 64) 37568 _________________________________________________________________sequential_17 (Sequential) (1, 56, 56, 128) 221440 _________________________________________________________________sequential_18 (Sequential) (1, 28, 28, 256) 1475328 _________________________________________________________________sequential_19 (Sequential) (1, 14, 14, 512) 5899776 _________________________________________________________________sequential_20 (Sequential) (1, 7, 7, 512) 7079424 _________________________________________________________________sequential_21 (Sequential) (1, 10) 119586826 =================================================================Total params: 134,300,362Trainable params: 134,300,362Non-trainable params: 0__________________________________________________________________ 3.3 GoogLeNet GoogLeNet 的名字不是 GoogleNet，而是 GoogLeNet，这是为了致敬 LeNet。GoogLeNet 和 AlexNet/VGGNet 这类依靠加深网络结构的深度的思想不完全一样。GoogLeNet 在加深度的同时做了结构上的创新，引入了一个叫做 Inception 的结构来代替之前的卷积加激活的经典组件。GoogLeNet 在 ImageNet 分类比赛上的 Top-5 错误率降低到了 6.7%。 整个网络架构我们分为五个模块，每个模块之间使用步幅为 2 的 3×33×3 最大池化层来减小输出高宽。 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273## B1模块# 定义模型的输入inputs = tf.keras.Input(shape=(224,224,3),name = &quot;input&quot;)# b1 模块# 卷积层7*7的卷积核，步长为2，pad是same，激活函数RELUx = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding=&#x27;same&#x27;, activation=&#x27;relu&#x27;)(inputs)# 最大池化：窗口大小为3*3，步长为2，pad是samex = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=&#x27;same&#x27;)(x)## B2模块# 卷积层1*1的卷积核，步长为2，pad是same，激活函数RELUx = tf.keras.layers.Conv2D(64, kernel_size=1, padding=&#x27;same&#x27;, activation=&#x27;relu&#x27;)(x)# 卷积层3*3的卷积核，步长为2，pad是same，激活函数RELUx = tf.keras.layers.Conv2D(192, kernel_size=3, padding=&#x27;same&#x27;, activation=&#x27;relu&#x27;)(x)# 最大池化：窗口大小为3*3，步长为2，pad是samex = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=&#x27;same&#x27;)(x)## B3模块# Inceptionx = Inception(64, (96, 128), (16, 32), 32)(x)# Inceptionx = Inception(128, (128, 192), (32, 96), 64)(x)# 最大池化：窗口大小为3*3，步长为2，pad是samex = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=&#x27;same&#x27;)(x)## B4模块def aux_classifier(x, filter_size): #x:输入数据，filter_size:卷积层卷积核个数，全连接层神经元个数 # 池化层 x = tf.keras.layers.AveragePooling2D( pool_size=5, strides=3, padding=&#x27;same&#x27;)(x) # 1x1 卷积层 x = tf.keras.layers.Conv2D(filters=filter_size[0], kernel_size=1, strides=1, padding=&#x27;valid&#x27;, activation=&#x27;relu&#x27;)(x) # 展平 x = tf.keras.layers.Flatten()(x) # 全连接层1 x = tf.keras.layers.Dense(units=filter_size[1], activation=&#x27;relu&#x27;)(x) # softmax输出层 x = tf.keras.layers.Dense(units=10, activation=&#x27;softmax&#x27;)(x) return x# Inceptionx = Inception(192, (96, 208), (16, 48), 64)(x)# 辅助输出1aux_output_1 = aux_classifier(x, [128, 1024])# Inceptionx = Inception(160, (112, 224), (24, 64), 64)(x)# Inceptionx = Inception(128, (128, 256), (24, 64), 64)(x)# Inceptionx = Inception(112, (144, 288), (32, 64), 64)(x)# 辅助输出2aux_output_2 = aux_classifier(x, [128, 1024])# Inceptionx = Inception(256, (160, 320), (32, 128), 128)(x)# 最大池化x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=&#x27;same&#x27;)(x)## B5模块# Inceptionx = Inception(256, (160, 320), (32, 128), 128)(x)# Inceptionx = Inception(384, (192, 384), (48, 128), 128)(x)# GAPx = tf.keras.layers.GlobalAvgPool2D()(x)# 输出层main_outputs = tf.keras.layers.Dense(10,activation=&#x27;softmax&#x27;)(x)# 使用Model来创建模型，指明输入和输出model = tf.keras.Model(inputs=inputs, outputs=[main_outputs,aux_output_1，aux_output_2]) model.summary() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546Model: &quot;functional_3&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================input (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________conv2d_122 (Conv2D) (None, 112, 112, 64) 9472 _________________________________________________________________max_pooling2d_27 (MaxPooling (None, 56, 56, 64) 0 _________________________________________________________________conv2d_123 (Conv2D) (None, 56, 56, 64) 4160 _________________________________________________________________conv2d_124 (Conv2D) (None, 56, 56, 192) 110784 _________________________________________________________________max_pooling2d_28 (MaxPooling (None, 28, 28, 192) 0 _________________________________________________________________inception_19 (Inception) (None, 28, 28, 256) 163696 _________________________________________________________________inception_20 (Inception) (None, 28, 28, 480) 388736 _________________________________________________________________max_pooling2d_31 (MaxPooling (None, 14, 14, 480) 0 _________________________________________________________________inception_21 (Inception) (None, 14, 14, 512) 376176 _________________________________________________________________inception_22 (Inception) (None, 14, 14, 512) 449160 _________________________________________________________________inception_23 (Inception) (None, 14, 14, 512) 510104 _________________________________________________________________inception_24 (Inception) (None, 14, 14, 528) 605376 _________________________________________________________________inception_25 (Inception) (None, 14, 14, 832) 868352 _________________________________________________________________max_pooling2d_37 (MaxPooling (None, 7, 7, 832) 0 _________________________________________________________________inception_26 (Inception) (None, 7, 7, 832) 1043456 _________________________________________________________________inception_27 (Inception) (None, 7, 7, 1024) 1444080 _________________________________________________________________global_average_pooling2d_2 ( (None, 1024) 0 _________________________________________________________________dense_10 (Dense) (None, 10) 10250 =================================================================Total params: 5,983,802Trainable params: 5,983,802Non-trainable params: 0___________________________________________________________ 3.4 ResNet 网络越深，获取的信息就越多，特征也越丰富。但是在实践中，随着网络的加深，优化效果反而越差，测试数据和训练数据的准确率反而降低了。 针对这一问题，何恺明等人提出了残差网络（ResNet）在 2015 年的 ImageNet 图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。 ResNet 网络中按照残差块的通道数分为不同的模块。第一个模块前使用了步幅为 2 的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。 下面我们来实现这些模块。注意，这里对第一个模块做了特别处理。 123456789101112131415161718192021# ResNet网络中模块的构成class ResnetBlock(tf.keras.layers.Layer): # 网络层的定义：输出通道数（卷积核个数），模块中包含的残差块个数，是否为第一个模块 def __init__(self,num_channels, num_residuals, first_block=False): super(ResnetBlock, self).__init__() # 模块中的网络层 self.listLayers=[] # 遍历模块中所有的层 for i in range(num_residuals): # 若为第一个残差块并且不是第一个模块，则使用1*1卷积，步长为2（目的是减小特征图，并增大通道数） if i == 0 and not first_block: self.listLayers.append(Residual(num_channels, use_1x1conv=True, strides=2)) # 否则不使用1*1卷积，步长为1 else: self.listLayers.append(Residual(num_channels)) # 定义前向传播过程 def call(self, X): # 所有层依次向前传播即可 for layer in self.listLayers.layers: X = layer(X) return X ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样：在输出通道数为 64、步幅为 2 的 7×77×7 卷积层后接步幅为 2 的 3×33×3 的最大池化层。不同之处在于 ResNet 每个卷积层后增加了 BN 层，接着是所有残差模块，最后，与 GoogLeNet 一样，加入全局平均池化层（GAP）后接上全连接层输出。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 构建ResNet网络class ResNet(tf.keras.Model): # 初始化：指定每个模块中的残差快的个数 def __init__(self,num_blocks): super(ResNet, self).__init__() # 输入层：7*7卷积，步长为2 self.conv=layers.Conv2D(64, kernel_size=7, strides=2, padding=&#x27;same&#x27;) # BN层 self.bn=layers.BatchNormalization() # 激活层 self.relu=layers.Activation(&#x27;relu&#x27;) # 最大池化层 self.mp=layers.MaxPool2D(pool_size=3, strides=2, padding=&#x27;same&#x27;) # 第一个block，通道数为64 self.resnet_block1=ResnetBlock(64,num_blocks[0], first_block=True) # 第二个block，通道数为128 self.resnet_block2=ResnetBlock(128,num_blocks[1]) # 第三个block，通道数为256 self.resnet_block3=ResnetBlock(256,num_blocks[2]) # 第四个block，通道数为512 self.resnet_block4=ResnetBlock(512,num_blocks[3]) # 全局平均池化 self.gap=layers.GlobalAvgPool2D() # 全连接层：分类 self.fc=layers.Dense(units=10,activation=tf.keras.activations.softmax) # 前向传播过程 def call(self, x): # 卷积 x=self.conv(x) # BN x=self.bn(x) # 激活 x=self.relu(x) # 最大池化 x=self.mp(x) # 残差模块 x=self.resnet_block1(x) x=self.resnet_block2(x) x=self.resnet_block3(x) x=self.resnet_block4(x) # 全局平均池化 x=self.gap(x) # 全链接层 x=self.fc(x) return x# 模型实例化：指定每个block中的残差块个数 mynet=ResNet([2,2,2,2]) 这里每个模块里有 4 个卷积层（不计算 1×1 卷积层），加上最开始的卷积层和最后的全连接层，共计 18 层。这个模型被称为 ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的 ResNet 模型，例如更深的含 152 层的 ResNet-152。虽然 ResNet 的主体架构跟 GoogLeNet 的类似，但 ResNet 结构更简单，修改也更方便。这些因素都导致了 ResNet 迅速被广泛使用。 在训练 ResNet 之前，我们来观察一下输入形状在 ResNet 的架构： 12345678910111213141516171819202122232425262728293031X = tf.random.uniform(shape=(1, 224, 224 , 1))y = mynet(X)mynet.summary()Model: &quot;res_net&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================conv2d_2 (Conv2D) multiple 3200 _________________________________________________________________batch_normalization_2 (Batch multiple 256 _________________________________________________________________activation (Activation) multiple 0 _________________________________________________________________max_pooling2d (MaxPooling2D) multiple 0 _________________________________________________________________resnet_block (ResnetBlock) multiple 148736 _________________________________________________________________resnet_block_1 (ResnetBlock) multiple 526976 _________________________________________________________________resnet_block_2 (ResnetBlock) multiple 2102528 _________________________________________________________________resnet_block_3 (ResnetBlock) multiple 8399360 _________________________________________________________________global_average_pooling2d (Gl multiple 0 _________________________________________________________________dense (Dense) multiple 5130 =================================================================Total params: 11,186,186Trainable params: 11,178,378Non-trainable params: 7,808_________________________________________________________________ 4 图像增强方法 图像增强（image augmentation）指通过剪切、旋转 / 反射 / 翻转变换、缩放变换、平移变换、尺度变换、对比度变换、噪声扰动、颜色变换等一种或多种组合数据增强变换的方式来增加数据集的大小。图像增强的意义是通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模，而且随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。 常见的图像增强方式可以分为两类：几何变换类和颜色变换类 几何变换类，主要是对图像进行几何变换操作，包括翻转，旋转，裁剪，变形，缩放等。 颜色变换类，指通过模糊、颜色变换、擦除、填充等方式对图像进行处理 实现图像增强可以通过 tf.image 来完成，也可以通过 tf.keras.imageGenerator 来完成。 4.1 tf.image 进行图像增强 导入所需的工具包并读取要处理的图像： 1234567# 导入工具包import tensorflow as tfimport matplotlib.pyplot as pltimport numpy as np# 读取图像并显示cat = plt.imread(&#x27;./cat.jpg&#x27;)plt.imshow(cat) 1 翻转和裁剪 左右翻转图像是最早也是最广泛使用的一种图像增广方法。可以通过 tf.image.random_flip_left_right 来实现图像左右翻转。 123# 左右翻转并显示cat1 = tf.image.random_flip_left_right(cat)plt.imshow(cat1） 创建 tf.image.random_flip_up_down 实例来实现图像的上下翻转，上下翻转使用的较少。 123# 上下翻转cat2 = tf.image.random_flip_up_down(cat)plt.imshow(cat2) 随机裁剪出一块面积为原面积 10%∼100%10%∼100% 的区域，且该区域的宽和高之比随机取自 0.5∼20.5∼2，然后再将该区域的宽和高分别缩放到 200 像素。 123# 随机裁剪cat3 = tf.image.random_crop(cat,(200,200,3))plt.imshow(cat3) 2 颜色变换 另一类增广方法是颜色变换。我们可以从 4 个方面改变图像的颜色：亮度、对比度、饱和度和色调。接下来将图像的亮度随机变化为原图亮度的 50%50%（即 1−0.51−0.5）∼150%∼150%（即 1+0.51+0.5）。 12cat4=tf.image.random_brightness(cat,0.5)plt.imshow(cat4) 类似地，我们也可以随机变化图像的色调 12cat5 = tf.image.random_hue(cat,0.5)plt.imshow(cat5) 4.2 使用 ImageDataGenerator () 进行图像增强 ImageDataGenerator () 是 keras.preprocessing.image 模块中的图片生成器，可以在 batch 中对数据进行增强，扩充数据集大小，增强模型的泛化能力。比如旋转，变形等，如下所示： 1234567891011keras.preprocessing.image.ImageDataGenerator( rotation_range=0, #整数。随机旋转的度数范围。 width_shift_range=0.0, #浮点数、宽度平移 height_shift_range=0.0, #浮点数、高度平移 brightness_range=None, # 亮度调整 shear_range=0.0, # 裁剪 zoom_range=0.0, #浮点数 或 [lower, upper]。随机缩放范围 horizontal_flip=False, # 左右翻转 vertical_flip=False, # 垂直翻转 rescale=None # 尺度调整 ) 来看下水平翻转的结果： 123456789101112131415161718# 获取数据集(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()# 将数据转换为4维的形式x_train = X_train.reshape(X_train.shape[0],28,28,1)x_test = X_test.reshape(X_test.shape[0],28,28,1)# 设置图像增强方式：水平翻转datagen = ImageDataGenerator(horizontal_flip=True)# 查看增强后的结果for X_batch,y_batch in datagen.flow(x_train,y_train,batch_size=9): plt.figure(figsize=(8,8)) # 设定每个图像显示的大小 # 产生一个3*3网格的图像 for i in range(0,9): plt.subplot(330+1+i) plt.title(y_batch[i]) plt.axis(&#x27;off&#x27;) plt.imshow(X_batch[i].reshape(28,28),cmap=&#x27;gray&#x27;) plt.show() break 5 模型微调 5.1 微调 如何在只有 6 万张图像的 MNIST 训练数据集上训练模型。学术界当下使用最广泛的大规模图像数据集 ImageNet，它有超过 1,000 万的图像和 1,000 类的物体。然而，我们平常接触到数据集的规模通常在这两者之间。假设我们想从图像中识别出不同种类的椅子，然后将购买链接推荐给用户。一种可能的方法是先找出 100 种常见的椅子，为每种椅子拍摄 1,000 张不同角度的图像，然后在收集到的图像数据集上训练一个分类模型。另外一种解决办法是应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。例如，虽然 ImageNet 数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。 微调由以下 4 步构成。 在源数据集（如 ImageNet 数据集）上预训练一个神经网络模型，即源模型。 创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。 为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。 在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。 当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。 5.2 热狗识别 接下来我们来实践一个具体的例子：热狗识别。将基于一个小数据集对在 ImageNet 数据集上训练好的 ResNet 模型进行微调。该小数据集含有数千张热狗或者其他事物的图像。我们将使用微调得到的模型来识别一张图像中是否包含热狗。 首先，导入实验所需的工具包。 12import tensorflow as tfimport numpy as np 5.2.1 获取数据集 我们首先将数据集放在路径 hotdog/data 之下: 每个类别文件夹里面是图像文件。 上一节中我们介绍了 ImageDataGenerator 进行图像增强，我们可以通过以下方法读取图像文件，该方法以文件夹路径为参数，生成经过图像增强后的结果，并产生 batch 数据： 12345flow_from_directory(self, directory, target_size=(256, 256), color_mode=&#x27;rgb&#x27;, classes=None, class_mode=&#x27;categorical&#x27;, batch_size=32, shuffle=True, seed=None, save_to_dir=None） 主要参数： directory: 目标文件夹路径，对于每一个类对应一个子文件夹，该子文件夹中任何 JPG、PNG、BNP、PPM 的图片都可以读取。 target_size: 默认为 (256, 256)，图像将被 resize 成该尺寸。 batch_size: batch 数据的大小，默认 32。 shuffle: 是否打乱数据，默认为 True。 我们创建两个 tf.keras.preprocessing.image.ImageDataGenerator 实例来分别读取训练数据集和测试数据集中的所有图像文件。将训练集图片全部处理为高和宽均为 224 像素的输入。此外，我们对 RGB（红、绿、蓝）三个颜色通道的数值做标准化。 1234567891011121314151617181920212223242526# 获取数据集import pathlibtrain_dir = &#x27;transferdata/train&#x27;test_dir = &#x27;transferdata/test&#x27;# 获取训练集数据train_dir = pathlib.Path(train_dir)train_count = len(list(train_dir.glob(&#x27;*/*.jpg&#x27;)))# 获取测试集数据test_dir = pathlib.Path(test_dir)test_count = len(list(test_dir.glob(&#x27;*/*.jpg&#x27;)))# 创建imageDataGenerator进行图像处理image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)# 设置参数BATCH_SIZE = 32IMG_HEIGHT = 224IMG_WIDTH = 224# 获取训练数据train_data_gen = image_generator.flow_from_directory(directory=str(train_dir), batch_size=BATCH_SIZE, target_size=(IMG_HEIGHT, IMG_WIDTH), shuffle=True)# 获取测试数据test_data_gen = image_generator.flow_from_directory(directory=str(test_dir), batch_size=BATCH_SIZE, target_size=(IMG_HEIGHT, IMG_WIDTH), shuffle=True) 下面我们随机取 1 个 batch 的图片然后绘制出来。 123456789101112import matplotlib.pyplot as plt# 显示图像def show_batch(image_batch, label_batch): plt.figure(figsize=(10,10)) for n in range(15): ax = plt.subplot(5,5,n+1) plt.imshow(image_batch[n]） plt.axis(&#x27;off&#x27;)# 随机选择一个batch的图像 image_batch, label_batch = next(train_data_gen)# 图像显示show_batch(image_batch, label_batch) 5.2.2 模型构建与训练 我们使用在 ImageNet 数据集上预训练的 ResNet-50 作为源模型。这里指定 weights='imagenet' 来自动下载并加载预训练的模型参数。在第一次使用时需要联网下载模型参数。 Keras 应用程序（keras.applications）是具有预先训练权值的固定架构，该类封装了很多重量级的网络架构，如下图所示： 实现时实例化模型架构： 1234tf.keras.applications.ResNet50( include_top=True, weights=&#x27;imagenet&#x27;, input_tensor=None, input_shape=None, pooling=None, classes=1000, **kwargs) 主要参数： include_top: 是否包括顶层的全连接层。 weights: None 代表随机初始化， ‘imagenet’ 代表加载在 ImageNet 上预训练的权值。 input_shape: 可选，输入尺寸元组，仅当 include_top=False 时有效，否则输入形状必须是 (224, 224, 3)（channels_last 格式）或 (3, 224, 224)（channels_first 格式）。它必须为 3 个输入通道，且宽高必须不小于 32，比如 (200, 200, 3) 是一个合法的输入尺寸。 在该案例中我们使用 resNet50 预训练模型构建模型： 12345678910111213# 加载预训练模型ResNet50 = tf.keras.applications.ResNet50(weights=&#x27;imagenet&#x27;, input_shape=(224,224,3))# 设置所有层不可训练for layer in ResNet50.layers: layer.trainable = False# 设置模型net = tf.keras.models.Sequential()# 预训练模型net.add(ResNet50)# 展开net.add(tf.keras.layers.Flatten())# 二分类的全连接层net.add(tf.keras.layers.Dense(2, activation=&#x27;softmax&#x27;)) 接下来我们使用之前定义好的 ImageGenerator 将训练集图片送入 ResNet50 进行训练。 123456789101112131415161718# 模型编译：指定优化器，损失函数和评价指标net.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])# 模型训练：指定数据，每一个epoch中只运行10个迭代，指定验证数据集history = net.fit( train_data_gen, steps_per_epoch=10, epochs=3, validation_data=test_data_gen, validation_steps=10 )Epoch 1/310/10 [==============================] - 28s 3s/step - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6930 - val_accuracy: 0.5094Epoch 2/310/10 [==============================] - 29s 3s/step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.4812Epoch 3/310/10 [==============================] - 31s 3s/step - loss: 0.6935 - accuracy: 0.4844 - val_loss: 0.6933 - val_accuracy: 0.4875","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"计算机视觉CV","slug":"计算机视觉CV","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV/"}]},{"title":"计算机视觉算法导读篇","slug":"计算机视觉算法导读篇","date":"2021-06-24T15:05:36.000Z","updated":"2021-07-14T12:52:49.946Z","comments":true,"path":"2021/06/24/计算机视觉算法导读篇/","link":"","permalink":"https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AF%BB%E7%AF%87/","excerpt":"","text":"文章目录 1 深度学习发展史 2 计算机视觉概述 2.1 定义 2.2 任务分解 2.3 应用场景 2.4 计算机视觉发展史 1 深度学习发展史 起源：深度学习所需要的神经网络技术起源于 20 世纪 50 年代，叫做感知机。当时也通常使用单层感知机，尽管结构简单，但是能够解决复杂的问题。后来感知机被证明存在严重的问题，因为只能学习线性可分函数，连简单的异或 (XOR) 等线性不可分问题都无能为力，1969 年 Marvin Minsky 写了一本叫做《Perceptrons》的书，他提出了著名的两个观点：1. 单层感知机没用，我们需要多层感知机来解决复杂问题 2. 没有有效的训练算法。 发展：20 世纪 90 年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机（SVM，Support Vector Machines）、 Boosting、最大熵方法（如 LR，Logistic Regression）等。这些模型的结构基本上可以看成带有一层隐层节点（如 SVM、Boosting），或没有隐层节点（如 LR）。这些模型无论是在理论分析还是应用中都获得了巨大的成功。相比之下，由于理论分析的难度大，训练方法又需要很多经验和技巧，这个时期浅层人工神经网络反而相对沉寂. 2006 年，杰弗里・辛顿以及他的学生鲁斯兰・萨拉赫丁诺夫正式提出了深度学习的概念。他们在世界顶级学术期刊《科学》发表的一篇文章中详细的给出了 “梯度消失” 问题的解决方案 —— 通过无监督的学习方法逐层训练算法，再使用有监督的反向传播算法进行调优。该深度学习方法的提出，立即在学术圈引起了巨大的反响，以斯坦福大学、多伦多大学为代表的众多世界知名高校纷纷投入巨大的人力、财力进行深度学习领域的相关研究。而后又迅速蔓延到工业界中。 2012 年，在著名的 ImageNet 图像识别大赛中，杰弗里・辛顿领导的小组采用深度学习模型 AlexNet 一举夺冠。AlexNet 采用 ReLU 激活函数，从根本上解决了梯度消失问题，并采用 GPU 极大的提高了模型的运算速度。同年，由斯坦福大学著名的吴恩达教授和世界顶尖计算机专家 Jeff Dean 共同主导的深度神经网络 ——DNN 技术在图像识别领域取得了惊人的成绩，在 ImageNet 评测中成功的把错误率从 26％降低到了 15％。深度学习算法在世界大赛的脱颖而出，也再一次吸引了学术界和工业界对于深度学习领域的关注。 2016 年，随着谷歌公司基于深度学习开发的 AlphaGo 以 4:1 的比分战胜了国际顶尖围棋高手李世石，深度学习的热度一时无两。后来，AlphaGo 又接连和众多世界级围棋高手过招，均取得了完胜。这也证明了在围棋界，基于深度学习技术的机器人已经超越了人类。 2017 年，基于强化学习算法的 AlphaGo 升级版 AlphaGo Zero 横空出世。其采用 “从零开始”、“无师自通” 的学习模式，以 100:0 的比分轻而易举打败了之前的 AlphaGo。除了围棋，它还精通国际象棋等其它棋类游戏，可以说是真正的棋类 “天才”。此外在这一年，深度学习的相关算法在医疗、金融、艺术、无人驾驶等多个领域均取得了显著的成果。所以，也有专家把 2017 年看作是深度学习甚至是人工智能发展最为突飞猛进的一年。 2019 年，基于 Transformer 的自然语言模型的持续增长和扩散，这是一种语言建模神经网络模型，可以在几乎所有任务上提高 NLP 的质量。Google 甚至将其用作相关性的主要信号之一，这是多年来最重要的更新 2020 年，深度学习扩展到更多的应用场景，比如积水识别，路面塌陷等，而且疫情期间，在智能外呼系统，人群测温系统，口罩人脸识别等都有深度学习的应用。 2 计算机视觉概述 2.1 定义 计算机视觉是指用摄像机和电脑及其他相关设备，对生物视觉的一种模拟。它的主要任务让计算机理解图片或者视频中的内容，就像人类和许多其他生物每天所做的那样。 2.2 任务分解 主要分为三大经典任务： 图像分类 、 目标检测 、 图像分割 图像分类（Classification）：即是将图像结构化为某一类别的信息，用事先确定好的类别 (category) 来描述图片。 目标检测（Detection）：分类任务关心整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求同时获得这一目标的类别信息和位置信息（classification + localization）。 图像分割（Segmentation）：分割是对图像的像素级描述，它赋予每个像素类别（实例）意义，适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割。 2.3 应用场景 人脸识别 人脸识别技术目前已经广泛应用于金融、司法、军队、公安、边检、政府、航天、电力、工厂、教育、医疗等行业。据业内人士分析，我国的人脸识别产业的需求旺盛，需求推动导致企业敢于投入资金。 代表企业：Face++ 旷视科技、依图科技、商汤科技、深醒科技、云从科技等。 视频监控 人工智能技术可以对结构化的人、车、物等视频内容信息进行快速检索、查询。这项应用使得让公安系统在繁杂的监控视频中搜寻到罪犯的有了可能。在大量人群流动的交通枢纽，该技术也被广泛用于人群分析、防控预警等。 代表企业：SenseTime 商汤科技、DeepGlint 格灵深瞳、依图科技、云天励飞、深网视界等。 图片识别分析 代表企业：Face++ 旷视科技、图普科技、码隆科技、酒咔嚓、YI + 陌上花科技等。 辅助驾驶 随着汽车的普及，汽车已经成为人工智能技术非常大的应用投放方向，但就目前来说，想要完全实现自动驾驶 / 无人驾驶，距离技术成熟还有一段路要走。不过利用人工智能技术，汽车的驾驶辅助的功能及应用越来越多，这些应用多半是基于计算机视觉和图像处理技术来实现。 代表企业：纵目科技、TuSimple 图森科技、驭势科技、MINIEYE 佑驾创新、中天安驰等。 除了上述这些，计算机视觉在三维视觉，三维重建，工业仿真，地理信息系统，工业视觉，医疗影像诊断，文字识别（OCR），图像及视频编辑等领域也有广泛的应用。 2.4 计算机视觉发展史 1963 年， Larry Roberts 发表了 CV 领域的第一篇专业论文，用以对简单几何体进行边缘提取和三维重建。 1966 年，麻省理工学院 (MIT) 发起了一个夏季项目，目标是搭建一个 机器视觉系统 ，完成模式识别 (pattern recognition) 等工作。虽然未成功，但是计算机视觉作为一个科学领域的正式诞生的标志。 1982 年，学者 David Marr 发表的著作《Vision》从严谨又长远的角度给出了 CV 的 发展方向 和一些 基本算法 ，其中不乏现在为人熟知的 “图层” 的概念、边缘提取、三维重建等，标志着计算机视觉成为了一门独立学科。 1999 年 David Lowe 提出了 尺度不变特征变换 （SIFT, Scale-invariant feature transform）目标检测算法，用于匹配不同拍摄方向、纵深、光线等图片中的相同元素。 2009 年，由 Felzenszwalb 教授在提出基于 HOG 的 deformable parts model，可变形零件模型开发，它是深度学习之前最好的最成功的 objectdetection &amp; recognition 算法。 Everingham 等人在 2006 年至 2012 年间搭建了一个大型图片数据库，供机器识别和训练，称为 PASCAL Visual Object Challenge ，该数据库中有 20 种类别的图片，每种图片数量在一千至一万张不等。 2009 年，李飞飞教授等在 CVPR2009 上发表了一篇名为《ImageNet: A Large-Scale Hierarchical Image Database》的论文，发布了 ImageNet数据集 ，这是为了检测计算机视觉能否识别自然万物，回归机器学习，克服过拟合问题。 2012 年，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 创造了一个 “大型的深度卷积神经网络”，也即现在众所周知的 AlexNet ，赢得了当年的 ILSVRC。这是史上第一次有模型在 ImageNet 数据集表现如此出色。自那时起，CNN 才成了家喻户晓的名字。 2014 年，蒙特利尔大学提出 生成对抗网络（GAN） ：拥有两个相互竞争的神经网络可以使机器学习得更快。一个网络尝试模仿真实数据生成假的数据，而另一个网络则试图将假数据区分出来。随着时间的推移，两个网络都会得到训练，生成对抗网络（GAN）被认为是计算机视觉领域的重大突破。 2018 年末，英伟达发布的 视频到视频生成（Video-to-Video synthesis） ，它通过精心设计的发生器、鉴别器网络以及时空对抗物镜，合成高分辨率、照片级真实、时间一致的视频，实现了让 AI 更具物理意识，更强大，并能够推广到新的和看不见的更多场景。 2019，更强大的 GAN， BigGAN ，是拥有了更聪明的学习技巧的 GAN，由它训练生成的图像连它自己都分辨不出真假，因为除非拿显微镜看，否则将无法判断该图像是否有任何问题，因而，它更被誉为史上最强的图像生成器.","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"计算机视觉CV","slug":"计算机视觉CV","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV/"}]},{"title":"经典的图像数据集介绍","slug":"经典的图像数据集介绍","date":"2021-06-07T13:08:29.000Z","updated":"2021-06-26T02:44:16.415Z","comments":true,"path":"2021/06/07/经典的图像数据集介绍/","link":"","permalink":"https://leezhao415.github.io/2021/06/07/%E7%BB%8F%E5%85%B8%E7%9A%84%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"文章目录 1 经典的数据集介绍 1.1 ImageNet 1.2 PASCAL VOC 1.3 Labelme 1.4 COCO 1.5 SUN 1.6 Caltech 1.7 Corel5k 1.8 CIFAR（Canada Institude For Advanced Research） 2 人脸数据库 2.1 AFLW（Annotated Facial Landmarks in the Wild） 2.2 LFW（Labeled Faces in the Wild） 2.3 AFW（Annotated Faces in the Wild） 2.4 FDDB（Face Detection Data Set and Benchmark） 2.5 WIDER FACE 2.6 CMU-MIT 2.7 GENKI 2.8 IJB-A (IARPA JanusBenchmark A) 2.9 MALF (Multi-Attribute Labelled Faces) 2.10 MegaFace 2.11 300W 2.12 IMM Data Sets 2.13 MUCT Data Sets 2.14 ORL (AT&amp;T Dataset) 3 行人检测数据库 3.1 INRIA Person Dataset 3.2 CaltechPedestrian Detection Benchmark 3.3 MIT cbcl (center for biological and computational learning)Pedestrian Data 4 年龄，性别数据库 4.1 Adience 5 车辆数据库 5.1 KITTI（Karlsruhe Institute ofTechnology and Toyota Technological Institute） 6 字符数据库 6.1 MNIST（Mixed National Instituteof Standards and Technology） 1 经典的数据集介绍 1.1 ImageNet ​ ImageNet 是一个计算机视觉系统识别项目，是目前世界上图像识别最大的数据库。是美国斯坦福的计算机科学家李飞飞模拟人类的识别系统建立的。能够从图片识别物体。目前已经包含 14197122 张图像，是已知的最大的图像数据库。每年的 ImageNet 大赛更是魂萦梦牵着国内外各个名校和大型 IT 公司以及网络巨头的心。图像如下图所示，需要注册 ImageNet 帐号才可以下载，下载链接为 http://www.image-net.org/ ​ 1.2 PASCAL VOC ​ PASCALVOC 数据集是视觉对象的分类识别和检测的一个基准测试，提供了检测算法和学习性能的标准图像注释数据集和标准的评估系统。图像如下图所示，包含 VOC2007（430M），VOC2012（1.9G）两个下载版本。下载链接为 http://pjreddie.com/projects/pascal-voc-dataset-mirror/ 1.3 Labelme ​ Labelme 是斯坦福一个学生的母亲利用休息时间帮儿子做的标注，后来便发展为一个数据集。该数据集的主要特点包括 （1）专门为物体分类识别设计，而非仅仅是实例识别 （2）专门为学习嵌入在一个场景中的对象而设计 （3）高质量的像素级别标注，包括多边形框（polygons）和背景标注（segmentation masks） （4）物体类别多样性大，每种物体的差异性，多样性也大。 （5）所有图像都是自己通过相机拍摄，而非 copy （6）公开的，免费的 图像如下图所示，需要通过 matlab 来下载，一种奇特的下载方式，下载链接为 http://labelme2.csail.mit.edu/Release3.0/index.php 1.4 COCO ​ COCO 是一种新的图像识别，分割和加字幕标注的数据集。主要由 Tsung-Yi Lin（Cornell Tech），Genevieve Patterson （Brown），MatteoRuggero Ronchi （Caltech），Yin Cui （Cornell Tech），Michael Maire （TTI Chicago），Serge Belongie （Cornell Tech），Lubomir Bourdev （UC Berkeley），Ross Girshick （Facebook AI), James Hays (Georgia Tech),PietroPerona (Caltech)，Deva Ramanan (CMU），Larry Zitnick （Facebook AI）， Piotr Dollár （Facebook AI）等人收集而成。其主要特征如下 （1）目标分割 （2）通过上下文进行识别 （3）每个图像包含多个目标对象 （4）超过 300000 个图像 （5）超过 2000000 个实例 （6）80 种对象 （7）每个图像包含 5 个字幕 （8）包含 100000 个人的关键点 图像如下图所示，支持 Matlab 和 Python 两种下载方式，下载链接为 http://mscoco.org/ 1.5 SUN ​ SUN 数据集包含 131067 个图像，由 908 个场景类别和 4479 个物体类别组成，其中背景标注的物体有 313884 个。图像如下图所示，下载链接为 http://groups.csail.mit.edu/vision/SUN/ 1.6 Caltech ​ Caltech 是加州理工学院的图像数据库，包含 Caltech101 和 Caltech256 两个数据集。该数据集是由 Fei-FeiLi, Marco Andreetto, Marc 'Aurelio Ranzato 在 2003 年 9 月收集而成的。Caltech101 包含 101 种类别的物体，每种类别大约 40 到 800 个图像，大部分的类别有大约 50 个图像。Caltech256 包含 256 种类别的物体，大约 30607 张图像。图像如下图所示，下载链接为 http://www.vision.caltech.edu/Image_Datasets/Caltech101/ 1.7 Corel5k ​ 这是 Corel5K 图像集，共包含科雷尔（Corel）公司收集整理的 5000 幅图片，故名：Corel5K，可以用于科学图像实验：分类、检索等。Corel5k 数据集是图像实验的事实标准数据集。请勿用于商业用途。私底下学习交流使用。Corel 图像库涵盖多个主题，由若干个 CD 组成，每个 CD 包含 100 张大小相等的图像，可以转换成多种格式。每张 CD 代表一个语义主题，例如有公共汽车、恐龙、海滩等。Corel5k 自从被提出用于图像标注实验后，已经成为图像实验的标准数据集，被广泛应用于标注算法性能的比较。Corel5k 由 50 张 CD 组成，包含 50 个语义主题。 Corel5k 图像库通常被分成三个部分：4000 张图像作为训练集，500 张图像作为验证集用来估计模型参数，其余 500 张作为测试集评价算法性能。使用验证集寻找到最优模型参数后 4000 张训练集和 500 张验证集混合起来组成新的训练集。 该图像库中的每张图片被标注 1~5 个标注词，训练集中总共有 374 个标注词，在测试集中总共使用了 263 个标注词。图像如下图所示，很遗憾本人也未找到官方下载路径，于是 github 上传了一份，下载链接为 https://github.com/watersink/Corel5K 1.8 CIFAR（Canada Institude For Advanced Research） ​ CIFAR 是由加拿大先进技术研究院的 AlexKrizhevsky, Vinod Nair 和 Geoffrey Hinton 收集而成的 80 百万小图片数据集。包含 CIFAR-10 和 CIFAR-100 两个数据集。 Cifar-10 由 60000 张 32*32 的 RGB 彩色图片构成，共 10 个分类。50000 张训练，10000 张测试（交叉验证）。这个数据集最大的特点在于将识别迁移到了普适物体，而且应用于多分类。CIFAR-100 由 60000 张图像构成，包含 100 个类别，每个类别 600 张图像，其中 500 张用于训练，100 张用于测试。其中这 100 个类别又组成了 20 个大的类别，每个图像包含小类别和大类别两个标签。官网提供了 Matlab,C，Python 三个版本的数据格式。图像如下图所示，下载链接为 http://www.cs.toronto.edu/~kriz/cifar.html 2 人脸数据库 2.1 AFLW（Annotated Facial Landmarks in the Wild） ​ AFLW 人脸数据库是一个包括多姿态、多视角的大规模人脸数据库，而且每个人脸都被标注了 21 个特征点。此数据库信息量非常大，包括了各种姿态、表情、光照、种族等因素影响的图片。AFLW 人脸数据库大约包括 25000 万已手工标注的人脸图片，其中 59% 为女性，41% 为男性，大部分的图片都是彩色，只有少部分是灰色图片。该数据库非常适合用于人脸识别、人脸检测、人脸对齐等方面的研究，具有很高的研究价值。图像如下图所示，需要申请帐号才可以下载，下载链接为 http://lrs.icg.tugraz.at/research/aflw/ 2.2 LFW（Labeled Faces in the Wild） ​ LFW 是一个用于研究无约束的人脸识别的数据库。该数据集包含了从网络收集的 13000 张人脸图像，每张图像都以被拍摄的人名命名。其中，有 1680 个人有两个或两个以上不同的照片。这些数据集唯一的限制就是它们可以被经典的 Viola-Jones 检测器检测到（a hummor）。图像如下图所示，下载链接为 http://vis-www.cs.umass.edu/lfw/index.html#download 2.3 AFW（Annotated Faces in the Wild） ​ AFW 数据集是使用 Flickr（雅虎旗下图片分享网站）图像建立的人脸图像库，包含 205 个图像，其中有 473 个标记的人脸。对于每一个人脸都包含一个长方形边界框，6 个地标和相关的姿势角度。数据库虽然不大，额外的好处是作者给出了其 2012 CVPR 的论文和程序以及训练好的模型。图像如下图所示，下载链接为 http://www.ics.uci.edu/~xzhu/face/ 2.4 FDDB（Face Detection Data Set and Benchmark） ​ FDDB 数据集主要用于约束人脸检测研究，该数据集选取野外环境中拍摄的 2845 个图像，从中选择 5171 个人脸图像。是一个被广泛使用的权威的人脸检测平台。图像如下图所示，下载链接为 http://vis-www.cs.umass.edu/fddb/ 2.5 WIDER FACE ​ WIDER FACE 是香港中文大学的一个提供更广泛人脸数据的人脸检测基准数据集，由 YangShuo， Luo Ping ，Loy ，Chen Change ，Tang Xiaoou 收集。它包含 32203 个图像和 393703 个人脸图像，在尺度，姿势，闭塞，表达，装扮，关照等方面表现出了大的变化。WIDER FACE 是基于 61 个事件类别组织的，对于每一个事件类别，选取其中的 40% 作为训练集，10% 用于交叉验证（cross validation），50% 作为测试集。和 PASCAL VOC 数据集一样，该数据集也采用相同的指标。和 MALF 和 Caltech 数据集一样，对于测试图像并没有提供相应的背景边界框。图像如下图所示，下载链接为 http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/ 2.6 CMU-MIT ​ CMU-MIT 是由卡内基梅隆大学和麻省理工学院一起收集的数据集，所有图片都是黑白的 gif 格式。里面包含 511 个闭合的人脸图像，其中 130 个是正面的人脸图像。图像如下图所示，没有找到官方链接，Github 下载链接为 https://github.com/watersink/CMU-MIT 2.7 GENKI ​ GENKI 数据集是由加利福尼亚大学的机器概念实验室收集。该数据集包含 GENKI-R2009a,GENKI-4K,GENKI-SZSL 三个部分。GENKI-R2009a 包含 11159 个图像，GENKI-4K 包含 4000 个图像，分为 “笑” 和 “不笑” 两种，每个图片的人脸的尺度大小，姿势，光照变化，头的转动等都不一样，专门用于做笑脸识别。GENKI-SZSL 包含 3500 个图像，这些图像包括广泛的背景，光照条件，地理位置，个人身份和种族等。图像如下图所示，下载链接为 http://mplab.ucsd.edu/，如果进不去可以，同样可以去下面的 github 下载，链接 https://github.com/watersink/GENKI 2.8 IJB-A (IARPA JanusBenchmark A) ​ IJB-A 是一个用于人脸检测和识别的数据库，包含 24327 个图像和 49759 个人脸。图像如下图所示，需要邮箱申请相应帐号才可以下载，下载链接为 http://www.nist.gov/itl/iad/ig/ijba_request.cfm 2.9 MALF (Multi-Attribute Labelled Faces) ​ MALF 是为了细粒度的评估野外环境中人脸检测模型而设计的数据库。数据主要来源于 Internet，包含 5250 个图像，11931 个人脸。每一幅图像包含正方形边界框，俯仰、蜷缩等姿势等。该数据集忽略了小于 20*20 的人脸，大约 838 个人脸，占该数据集的 7%。同时，该数据集还提供了性别，是否带眼镜，是否遮挡，是否是夸张的表情等信息。图像如下图所示，需要申请才可以得到官方的下载链接，链接为 http://www.cbsr.ia.ac.cn/faceevaluation/ 2.10 MegaFace ​ MegaFace 资料集包含一百万张图片，代表 690000 个独特的人。所有数据都是华盛顿大学从 Flickr（雅虎旗下图片分享网站）组织收集的。这是第一个在一百万规模级别的面部识别算法测试基准。 现有脸部识别系统仍难以准确识别超过百万的数据量。为了比较现有公开脸部识别算法的准确度，华盛顿大学在去年年底开展了一个名为 “MegaFace Challenge” 的公开竞赛。这个项目旨在研究当数据库规模提升数个量级时，现有的脸部识别系统能否维持可靠的准确率。图像如下图所示，需要邮箱申请才可以下载，下载链接为 http://megaface.cs.washington.edu/dataset/download.html 2.11 300W ​ 300W 数据集是由 AFLW，AFW，Helen，IBUG，LFPW，LFW 等数据集组成的数据库。图像如下图所示，需要邮箱申请才可以下载，下载链接为 http://ibug.doc.ic.ac.uk/resources/300-W/ 2.12 IMM Data Sets ​ IMM 人脸数据库包括了 240 张人脸图片和 240 个 asf 格式文件（可以用 UltraEdit 打开，记录了 58 个点的地标），共 40 个人（7 女 33 男），每人 6 张人脸图片，每张人脸图片被标记了 58 个特征点。所有人都未戴眼镜，图像如下图所示，下载链接为 http://www2.imm.dtu.dk/~aam/datasets/datasets.html 2.13 MUCT Data Sets ​ MUCT 人脸数据库由 3755 个人脸图像组成，每个人脸图像有 76 个点的地标（landmark），图片为 jpg 格式，地标文件包含 csv,rda,shape 三种格式。该图像库在种族、关照、年龄等方面表现出更大的多样性。具体图像如下图所示，下载链接为 http://www.milbo.org/muct/ 2.14 ORL (AT&amp;T Dataset) ​ ORL 数据集是剑桥大学 AT&amp;T 实验室收集的一个人脸数据集。包含了从 1992.4 到 1994.4 该实验室的成员。该数据集中图像分为 40 个不同的主题，每个主题包含 10 幅图像。对于其中的某些主题，图像是在不同的时间拍摄的。在关照，面部表情（张开眼睛，闭合眼睛，笑，非笑），面部细节（眼镜）等方面都变现出了差异性。所有图像都是以黑色均匀背景，并且从正面向上方向拍摄。 其中图片都是 PGM 格式，图像大小为 92*102，包含 256 个灰色通道。具体图像如下图所示，下载链接为 http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html 3 行人检测数据库 3.1 INRIA Person Dataset ​ Inria 数据集是最常使用的行人检测数据集。其中正样本（行人）为 png 格式，负样本为 jpg 格式。里面的图片分为只有车，只有人，有车有人，无车无人四个类别。图片像素为 70134，96160，64*128 等。具体图像如下图所示，下载链接为 http://pascal.inrialpes.fr/data/human/ 3.2 CaltechPedestrian Detection Benchmark ​ 加州理工学院的步行数据集包含大约包含 10 个小时 640x480 30Hz 的视频。其主要是在一个在行驶在乡村街道的小车上拍摄。视频大约 250000 帧（在 137 个约分钟的长段），共有 350000 个边界框和 2300 个独特的行人进行了注释。注释包括包围盒和详细的闭塞标签之间的时间对应关系。更多信息可在其 PAMI 2012 CVPR 2009 标杆的论文获得。具体图像如下图所示，下载链接为 http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/ 3.3 MIT cbcl (center for biological and computational learning)Pedestrian Data ​ 该数据集主要包含 2 个部分，一部分为 128*64 的包含 924 个图片的 ppm 格式的图片，另一部分为从打图中分别切割而出的小图，主要包含胳膊，脑袋，脚，腿，头肩，身体等。具体图像如下图所示，下载链接为 http://cbcl.mit.edu/software-datasets/PedestrianData.html，需要翻墙才可以。 4 年龄，性别数据库 4.1 Adience ​ 该数据集来源为 Flickr 相册，由用户使用 iPhone5 或者其它智能手机设备拍摄，同时具有相应的公众许可。该数据集主要用于进行年龄和性别的未经过滤的面孔估计。同时，里面还进行了相应的 landmark 的标注。是做性别年龄估计和人脸对齐的一个数据集。图片包含 2284 个类别和 26580 张图片。具体图像如下图所示，下载链接为 http://www.openu.ac.il/home/hassner/Adience/data.html#agegender 5 车辆数据库 5.1 KITTI（Karlsruhe Institute ofTechnology and Toyota Technological Institute） ​ KITTI 包含 7481 个训练图片和 7518 个测试图片。所有图片都是真彩色 png 格式。该数据集中标注了车辆的类型，是否截断，遮挡情况，角度值，2 维和 3 维 box 框，位置，旋转角度，分数等重要的信息，绝对是做车载导航的不可多得的数据集。具体图像如下图所示，下载链接为 http://www.cvlibs.net/datasets/kitti/ 6 字符数据库 6.1 MNIST（Mixed National Instituteof Standards and Technology） ​ MNIST 是一个大型的手写数字数据库，广泛用于机器学习领域的训练和测试，由纽约大学的 Yann LeCun 整理。MNIST 包含 60000 个训练集，10000 个测试集，每张图都进行了尺度归一化和数字居中处理，固定尺寸大小为 28*28。具体图像如下图所示，下载链接为 http://yann.lecun.com/exdb/mnist/","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"OpenCV之计算机视觉(导论)","slug":"OpenCV之计算机视觉-导论","date":"2021-06-07T13:06:27.000Z","updated":"2021-07-14T13:21:14.541Z","comments":true,"path":"2021/06/07/OpenCV之计算机视觉-导论/","link":"","permalink":"https://leezhao415.github.io/2021/06/07/OpenCV%E4%B9%8B%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-%E5%AF%BC%E8%AE%BA/","excerpt":"","text":"其中 core、highgui、imgproc 是最基础的模块，该课程主要是围绕这几个模块展开的，分别介绍如下： core 模块实现了最核心的数据结构及其基本运算，如绘图函数、数组操作相关函数等。 highgui 模块实现了视频与图像的读取、显示、存储等接口。 imgproc 模块实现了图像处理的基础方法，包括图像滤波、图像的几何变换、平滑、阈值分割、形态学处理、边缘检测、目标检测、运动分析和对象跟踪等。 对于图像处理其他更高层次的方向及应用，OpenCV 也有相关的模块实现 features2d 模块用于提取图像特征以及特征匹配，nonfree 模块实现了一些专利算法，如 sift 特征。 objdetect 模块实现了一些目标检测的功能，经典的基于 Haar、LBP 特征的人脸检测，基于 HOG 的行人、汽车等目标检测，分类器使用 Cascade Classification（级联分类）和 Latent SVM 等。 stitching 模块实现了图像拼接功能。 FLANN 模块（Fast Library for Approximate Nearest Neighbors），包含快速近似最近邻搜索 FLANN 和聚类 Clustering 算法。 ml 模块机器学习模块（SVM，决策树，Boosting 等等）。 photo 模块包含图像修复和图像去噪两部分。 video 模块针对视频处理，如背景分离，前景检测、对象跟踪等。 calib3d 模块即 Calibration（校准）3D，这个模块主要是相机校准和三维重建相关的内容。包含了基本的多视角几何算法，单个立体摄像头标定，物体姿态估计，立体相似性算法，3D 信息的重建等等。 G-API 模块包含超高效的图像处理 pipeline 引擎 12345678910111213141516171819# Windows下安装OpenCV，指定版本号pip install -i http://pypi.douban.com/simple --trusted-host pypi.douban.com opencv-python==3.4.2.17 # OpenCV贡献库安装（社区维护库）pip install opencv-contrib-python==3.4.2.17 # Pycharm安装opencv失败的解决方案：在依赖包仓库中添加镜像：https://pypi.tuna.tsinghua.edu.cn/simple/ 清华大学镜像库http://mirrors.aliyun.com/pypi/simple/ 阿里云镜像库http://pypi.douban.com/simple/ 豆瓣镜像库 # 验证opencvimport cv2cv2.__version__# 能正常显示版本号即认为安装成功# 安装多个第三方库pip install numpy pandas wheel jupyter notebook img[0,0] ：访问图像 img 第0行第0列 像素点的 BGR 值。图像是 BGR 格式的，得到的数值为 [0,0,255]。 img[0,0,0] ：访问图像 img 第0行第0列第0个通道 的像素值。图像是 BGR 格式的，所以第 0 个通道是 B通道 ，会得到 B 通道内第 0 行第 0 列的位置所对应的值 0。 img[0,0,1] ：访问图像 img 第0行第0列第1个通道 的像素值。图像是 BGR 格式的，所以第 1 个通道是 G通道 ，会得到 G 通道内第 0 行第 0 列的位置所对应的值 0。 img[0,0,2] ：访问图像 img 第0行第0列第2个通道 的像素值。图像是 BGR 格式的，所以第 2 个通道是 R通道 ，会得到 R 通道内第 0 行第 0 列的位置所对应的值 255。 目标分类：整个图像，输入图像，输出类别。 目标定位：单框框出来，不做分类。 目标检测：画框分类 目标分割 语义分割：对每个像素进行分类，同类别会被分到一起。 实例分割：不同的物体分别分类 深度学习三驾马车 算力：CPU、GPU、FPGA 海量数据 算法 目标检测 两阶段方法 ：提取候选框 ——&gt; 筛选目标，例：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN 单阶段方法 ：直接检测目标，例：YOLO、SSD、R-FCN CS231n：李飞飞人工智能入门课 http://cs231n.stanford.edu/ CS229：机器学习公开课 CS224：自然语言处理公开课 人体姿态检测经典模型 OpenPose 卡内基梅隆大学 / 卷积神经网络 DeepCut RMPE（AlphaPose） Mask RCNN 边缘计算（Edge Computing） 与云计算相比，实时，不需要先上传云再进行计算，本地计算。 Cut RMPE（AlphaPose） Mask RCNN 边缘计算（Edge Computing） 与云计算相比，实时，不需要先上传云再进行计算，本地计算。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"机器学习算法详解","slug":"机器学习算法详解","date":"2021-05-19T08:44:27.000Z","updated":"2021-07-14T12:59:06.201Z","comments":true,"path":"2021/05/19/机器学习算法详解/","link":"","permalink":"https://leezhao415.github.io/2021/05/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"文章目录 1 机器学习算法原理介绍 1.1 K - 近邻算法 1 定义 2 算法流程 3 K - 近邻实现 距离计算 4 拓展：fit ()、tansform ()、fit_transform () 区别 5 K 近邻算法优缺点 1.2 线性回归 1 定义 2 API 案例 3 线性回归的损失和优化 4 概念解释 5 梯度下降算法 6 欠拟合和过拟合原因及解决办法 7 正则化 (解决过拟合问题) 8 sklearn 模型的保存和加载 API 1.3 逻辑回归 1 总损失函数（对数似然损失） 2 概念解释 3 ROC 曲线 4 样本不均衡问题 1.4 决策树算法 1 信息增益、信息增益率和基尼系数 2 信息熵计算案例 1.5 集成算法 1 定义 2 集成学习中 boosting 和 Bagging 3 Bagging 及随机森林 4 boosting 1.6 聚类算法 1 定义 2 算法学习 3 API 4 案例分析 5 模型评估 6 K-Means 1.7 朴素贝叶斯 1 定义 2 算法原理 3 拉普拉斯平滑 4 案例实现 1.8 支持向量机 1 基本元素 2 基本思想 3 用途 4 硬间隔和软间隔 5 支持向量机推导 6 损失函数 7 SVM 回归 8 SVM 优缺点 1.9 EM 算法 1 基本思想 2 算法流程 1.10 HMM 模型 1 定义 2 常见术语 3 HMM 两个重要假设 4 HMM 模型算法原理 5 HMM 模型三个基本问题 6 案例实现 1.11 xgboost 算法 1 定义 2 最优模型构建方法 3 目标函数 4 案例分析 1.12 lightGBM 算法 1 定义 2 特点 3 优化特点详解 4 API 相关参数介绍 5 案例分析 2 机器学习算法实现 1. 获取数据集 2. 数据基本处理 3. 特征工程：标准化 4. 机器学习 (模型训练) 4.1 模型估计 4.2 模型调优 4.3 模型训练 5. 模型评估 1 机器学习算法原理介绍 判别模型 (discriminative model) 已知输入变量 x，通过求解条件概率分布 P (y|x) 或者直接计算 y 的值来预测 y。 例如： 线性回归（Linear Regression） 逻辑回归（Logistic Regression） 支持向量机（SVM） 传统神经网络（Traditional Neural Networks） 线性判别分析（Linear Discriminative Analysis） 条件随机场（Conditional Random Field） 生成模型（generative model） 已知输入变量 x，通过对观测值和标注数据计算联合概率分布 P (x,y) 来达到判定估算 y 的目的。 例如： 朴素贝叶斯（Naive Bayes） 隐马尔科夫模型（HMM） 贝叶斯网络（Bayesian Networks） 隐含狄利克雷分布（Latent Dirichlet Allocation） 1.1 K - 近邻算法 1 定义 K Nearest Neighbor 算法又叫 KNN 算法，如果一个样本在特征空间中的 k 个最相似 (即特征空间中最邻近) 的样本中的大多数属于某一个类别，则该样本也属于这个类别。 2 算法流程 1）计算已知类别数据集中的点与当前点之间的距离 2）按距离递增次序排序 3）选取与当前点距离最小的 k 个点 4）统计前 k 个点所在的类别出现的频率（ 分类 ：样本出现最多个数 回归 ：K 个杨样本的平均值） 5）返回前 k 个点出现频率最高的类别作为当前点的预测分类 1 K 值选择 K 值的减小就意味着整体模型变得复杂，容易发生过拟合； K 值的增大就意味着整体模型变得简单，容易发生欠拟合； 注：实际应用中，K 值一般取一个比较小的数值，例如采用交叉验证来选择最优的 K 值。 2 误差估计 近似误差：对训练集的训练误差，关注训练集，近似误差小可能出现过拟合。 估计误差：对测试集的测试误差，关注测试集，估计误差小说明对未知数据的预测能力好。 3 K - 近邻实现 线性扫描（穷举搜索） 计算输入实例与每一个训练实例的距离。计算后再查找 K 近邻。当训练集很大时，计算非常耗时。 KD 树 一种对 k 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构 kd 树是一种二叉树，表示对 k 维空间的一个划分，构造 kd 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K 维超矩形区域。kd 树的每个结点对应于一个 k 维超矩形区域。 利用 kd 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。 距离计算 欧式距离 (Euclidean Distance) 曼哈顿距离 (Manhattan Distance) 切比雪夫距离 (Chebyshev Distance) 闵可夫斯基距离 (Minkowski Distance) 其中 p 是一个变参数： 当 p=1 时，就是曼哈顿距离； 当 p=2 时，就是欧氏距离； 当 p→∞时，就是切比雪夫距离。 根据 p 的不同，闵氏距离可以表示某一类 / 种的距离。 其它距离：标准化欧氏距离 (Standardized EuclideanDistance)、余弦距离 (Cosine Distance)、汉明距离 (Hamming Distance)、杰卡德距离 (Jaccard Distance)、马氏距离 (Mahalanobis Distance)。 总结 闵氏距离的缺点： 将各个分量的量纲 (scale)，也就是 “单位” 相同的看待了； 未考虑各个分量的分布（期望，方差等）可能是不同的。 4 拓展：fit ()、tansform ()、fit_transform () 区别 fit() : Method calculates the parameters μ and σ and saves them as internal objects. 解释：简单来说，就是求得训练集 X 的均值，方差，最大值，最小值，这些训练集 X 固有的属性。 transform() : Method using these calculated parameters apply the transformation to a particular dataset. 解释：在 fit 的基础上，进行标准化，降维，归一化等操作（看具体用的是哪个工具，如 PCA，StandardScaler 等）。 fit_transform() : joins the fit() and transform() method for transformation of dataset. 解释：fit_transform 是 fit 和 transform 的组合，既包括了训练又包含了转换。 transform() 和 fit_transform() 二者的功能都是对数据进行某种统一处理（比如标准化～N (0,1)，将数据缩放 (映射) 到某个固定区间，归一化，正则化等） fit_transform(trainData) 对部分数据先拟合 fit，找到该 part 的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该 trainData 进行转换 transform，从而实现数据的标准化、归一化等等。 5 K 近邻算法优缺点 优点： 简单有效 重新训练的代价低 适合类域交叉样本 KNN 方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN 方法较其他方法更为适合。 适合大样本自动分类 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。 缺点： 惰性学习 KNN 算法是懒散学习方法（lazy learning, 基本上不学习），一些积极学习的算法要快很多 类别评分不是规格化 不像一些通过概率评分的分类 输出可解释性不强 例如决策树的输出可解释性就较强 对不均衡的样本不擅长 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的 K 个邻居中大容量类的样本占多数。该算法只计算 “最近的” 邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。 计算量较大 目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。 1.2 线性回归 1 定义 利用回归方程 (函数) 对一个或多个自变量 (特征值) 和因变量 (目标值) 之间关系进行建模的一种分析方式。 2 API 案例 1234567891011121314151617from sklearn.linear_model import LinearRegressionx = [[80, 86],[82, 80],[85, 78],[90, 90],[86, 82],[82, 90],[78, 80],[92, 94]]y = [84.2, 80.6, 80.1, 90, 83.2, 87.6, 79.4, 93.4]# 实例化APIestimator = LinearRegression()# 使用fit方法进行训练estimator.fit(x,y)print(&#x27;回归系数：\\n&#x27;, estimator.coef_)print(&#x27;预测结果：\\n&#x27;, estimator.predict([[100, 80]])) 3 线性回归的损失和优化 1 总损失函数：（ 最小二乘法 ） yi 为第 i 个训练样本的真实值 h (xi) 为第 i 个训练样本特征值组合预测函数 又称最小二乘法 2 优化算法： 2.1 正规方程 X 为特征值矩阵 y 为目标值矩阵 缺点：当特征过多过复杂时，求解速度太慢并且得不到结果 12345678910111213141516171819202122232425262728293031323334353637from sklearn.linear_model import LinearRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import mean_squared_errordef linear_model1(): &quot;&quot;&quot; 线性回归:正规方程 :return:None &quot;&quot;&quot; # 1.获取数据 data = load_boston() # 2.数据集划分 x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, random_state=22) # 3.特征工程-标准化 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) # 4.机器学习-线性回归(正规方程) # fit_intercept：是否计算偏置; estimator = LinearRegression(fit_intercept=True) estimator.fit(x_train, y_train) # 5.模型评估 # 5.1 获取系数等值 y_predict = estimator.predict(x_test) print(&quot;预测值为:\\n&quot;, y_predict) print(&quot;模型中的回归系数为:\\n&quot;, estimator.coef_) print(&quot;模型中的偏置为:\\n&quot;, estimator.intercept_) # 5.2 评价 # 均方误差 error = mean_squared_error(y_test, y_predict) print(&quot;误差为:\\n&quot;, error) return None 均方误差 (Mean Squared Error) MSE) 评价机制： 2.2 梯度下降法 (Gradient Descent) α 为学习率或者步长 梯度 在 单变量函数 中，梯度其实就是函数的 微分 ，代表着函数在某个给定点的 切线的斜率 ； 在 多变量函数 中，梯度是一个 向量 ，向量有方向，梯度的方向就指出了函数在给定点的 上升最快的方向 ； 算法选择依据： 小规模数据： 正规方程：LinearRegression (不能解决拟合问题) 岭回归 大规模数据： 梯度下降法：SGDRegressor 12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.linear_model import SGDRegressorfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import mean_squared_errordef linear_model2(): &quot;&quot;&quot; 线性回归:梯度下降法 :return:None &quot;&quot;&quot; # 1.获取数据 data = load_boston() # 2.数据集划分 x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, random_state=22) # 3.特征工程-标准化 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) # 4.机器学习-线性回归(特征方程) # loss:损失类型 loss=”squared_loss”: 普通最小二乘法 # fit_intercept：是否计算偏置 # learning_rate: 学习率 # &#x27;constant&#x27;: eta = eta0 # &#x27;optimal&#x27;: eta = 1.0 / (alpha * (t + t0)) [default] # &#x27;invscaling&#x27;: eta = eta0 / pow(t, power_t) # power_t=0.25:存在父类当中 # 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。 estimator = SGDRegressor(loss=&quot;squared_loss&quot;,fit_intercept=True,max_iter=1000,learning_rate=&quot;constant&quot;,eta0=0.1) estimator.fit(x_train, y_train) # 5.模型评估 # 5.1 获取系数等值 y_predict = estimator.predict(x_test) print(&quot;预测值为:\\n&quot;, y_predict) print(&quot;模型中的系数为:\\n&quot;, estimator.coef_) print(&quot;模型中的偏置为:\\n&quot;, estimator.intercept_) # 5.2 评价 # 均方误差 error = mean_squared_error(y_test, y_predict) print(&quot;误差为:\\n&quot;, error) return None 4 概念解释 5 梯度下降算法 全梯度下降算法(Full gradient descent) 在更新参数时使用所有的样本来进行更新 计算训练集所有样本误差，对其求和再取平均值作为目标函数。 缺点： 因为在执行每次更新时，我们需要在 整个数据集 上计算 所有的梯度 ，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。 批梯度下降法同样也不能在线更新模型，即在 运行的过程中，不能增加新的样本 。 随机梯度下降算法(Stochastic gradient descent) 每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。 优点：此过程简单，高效，通常可以较好地避免更新迭代收敛到局部最优解。 缺点：由于 SG 每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。 小批量梯度下降算法(Mini-batch gradient descent) 每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用 FG 迭代更新权重。 小批量梯度下降算法是 FG 和 SG 的折中方案，在一定程度上兼顾了以上两种方法的优点。 batch_size：被抽出的小样本集所含样本点的个数，通常设置为 2 的幂次方，利于 GPU 加速处理。 特别的，若 batch_size=1，则变成了 SG；若 batch_size=n，则变成了 FG. 随机平均梯度下降算法(Stochastic average gradient descent) 在内存中为每一个样本都维护一个旧的梯度，随机选择第 i 个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。 在 SG 方法中，虽然避开了运算成本大的问题，但对于大数据训练而言，SG 效果常不尽如人意，因为每一轮梯度更新都完全与上一轮的数据和梯度无关。SAG 能克服该问题。 它们都是为了正确地调节权重向量，通过为每个权重计算一个梯度，从而更新权值，使目标函数尽可能最小化。其差别在于样本的使用方式不同。 6 欠拟合和过拟合原因及解决办法 1 欠拟合原因以及解决办法 原因：学习到数据的特征过少 解决办法： 1） 添加其他特征项 ，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性” 三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征” 等等，都可以作为特征添加的首选项。 2） 添加多项式特征 ，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。 2 过拟合原因以及解决办法 原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点 解决办法： 1）重新 清洗数据 ，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。 2） 增大数据的训练量 ，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。 3） 正则化 4）减少特征维度，防止 维灾难 7 正则化 (解决过拟合问题) 在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响） 正则化类别 1from sklearn.linear_model import Ridge, ElasticNet, Lasso L1正则化 作用：可以使得其中一些权重直接为 0，删除这个特征的影响 LASSO回归 (正则项为权值向量的 ℓ1范数 ) 代价函数如下： Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。 L2正则化 作用：可以使得其中一些权重都很小，都接近于 0，削弱某个特征的影响 优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象 Ridge回归 （实现了 SAG ） 代价函数如下： 12345678# alpha:正则化力度，也叫 λ λ取值：0~1 1~10；# solver:会根据数据自动选择优化方法 # sag:如果数据集、特征都比较大，选择该随机梯度下降优化# normalize:数据是否进行标准化# normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据# Ridge.coef_:回归权重# Ridge.intercept_:回归偏置sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver=&quot;auto&quot;, normalize=False) Elastic Net (弹性网络) 弹性网络在岭回归和 Lasso 回归中进行了折中，通过混合比 (mix ratio) r 进行控制： r=0：弹性网络变为岭回归 r=1：弹性网络便为 Lasso 回归 弹性网络的代价函数 ： 回归模型选择： 常用：岭回归 假设只有少部分特征是有用的： 弹性网络 Lasso 一般来说，弹性网络的使用更为广泛。因为在特征维度高于训练样本数，或者特征是强相关的情况下，Lasso 回归的表现不太稳定。 在高维空间中，大多数训练数据驻留在定义特征空间的超立方体的角落中。 所需的训练实例数量随着使用的维度数量呈指数增长。 8 sklearn 模型的保存和加载 API 123456789101112131415161718192021222324252627282930313233343536373839from sklearn.externals import joblibdef load_dump_demo(): &quot;&quot;&quot; 模型保存和加载 :return: &quot;&quot;&quot; # 1.获取数据 data = load_boston() # 2.数据集划分 x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, random_state=22) # 3.特征工程-标准化 transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) # 4.机器学习-线性回归(岭回归) # # 4.1 模型训练 # estimator = Ridge(alpha=1) # estimator.fit(x_train, y_train) # # # 4.2 模型保存 # joblib.dump(estimator, &quot;./data/test.pkl&quot;) # 4.3 模型加载 estimator = joblib.load(&quot;./data/test.pkl&quot;) # 5.模型评估 # 5.1 获取系数等值 y_predict = estimator.predict(x_test) print(&quot;预测值为:\\n&quot;, y_predict) print(&quot;模型中的系数为:\\n&quot;, estimator.coef_) print(&quot;模型中的偏置为:\\n&quot;, estimator.intercept_) # 5.2 评价 # 均方误差 error = mean_squared_error(y_test, y_predict) print(&quot;误差为:\\n&quot;, error) 1.3 逻辑回归 逻辑回归（Logistic Regression）是机器学习中的一种分类模型，逻辑回归是一种分类算法，逻辑回归就是解决二分类问题的利器。 算法原理：将线性回归的输出作为逻辑回归的输入，然后经过 sigmoid 函数变换将整体的值映射到 [0，1]，再设定阈值进行分类。 1234567from klearn.linear_models import LogisticRegression# solver可选参数:&#123;&#x27;liblinear&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;,&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;&#125;， # 对于小数据集来说，“liblinear”是个不错的选择，而“sag”和&#x27;saga&#x27;对于大型数据集会更快。 # 对于多类问题，只有&#x27;newton-cg&#x27;， &#x27;sag&#x27;， &#x27;saga&#x27;和&#x27;lbfgs&#x27;可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。# penalty：正则化的种类# C：正则化力度estimator = LogisticRegression(solver=&#x27;liblinear&#x27;, penalty=‘l2’, C = 1.0) 1 总损失函数（对数似然损失） 2 概念解释 准确率 ：所有样本中预测对的比例 精确率 ：预测结果为正例样本中真实为正例的比例（预测为正的样本中的正样本） 召回率 ：真实为正例的样本中预测结果为正例的比例（正样本中预测为正的样本） F1-score ：反映模型的稳健性 123from sklearn.metrics import classification_report# labels:指定类别对应的数字 target_names：目标类别名称 return：每个类别精确率与召回率ret = classification_report(y_test, y_predict, labels=(2,4), target_names=(&quot;良性&quot;, &quot;恶性&quot;)) 3 ROC 曲线 TPR = TP / (TP + FN) &lt; 击中率 &gt; 所有真实类别为 1 的样本中，预测类别为 1 的比例 FPR = FP / (FP + TN) &lt; 虚惊率 &gt; 所有真实类别为 0 的样本中，预测类别为 1 的比例 AUC 指标 AUC 的概率意义是随机取一对正负样本，正样本得分大于负样本得分的概率 AUC 的范围在 [0, 1] 之间，并且越接近 1 越好，越接近 0.5 属于乱猜 AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。 0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 API 123from sklearn.metrics import roc_auc_score# 计算所得为ROC曲线面积roc_auc_score(y_test, y_predict) 4 样本不均衡问题 增加一些少数类样本使得正、反例数目接近，然后再进行学习。 关于类别不平衡的问题，主要有两种处理方式： 1 过采样方法 增加数量较少那一类样本的数量，使得正负样本比例均衡。 123# 使用imblearn进行随机过采样from imblearn.over_sampling import RandomOverSamplerros = RandomOverSampler(random_state=0) 1.1 过采样经典方法 1 随机过采样法 通过复制所选择的样本生成样本集 缺点：易产生模型过拟合问题 2 SMOTE 算法 (Synthetic Minority Oversampling，合成少数类过采样技术) 对每个少数类样本，从它的最近邻中随机选择一个样本，然后在两个样本之间的连线上随机选择一点作为新合成的少数类样本。 SMOTE 算法摒弃了随机过采样复制样本的做法，可以防止随机过采样中容易过拟合的问题，实践证明此方法可以提高分类器的性能。 123# SMOTE过采样from imblearn.over_sampling import SMOTEX_resampled, y_resampled = SMOTE().fit_resample(X, y) 2 欠采样方法 减少数量较多那一类样本的数量，使得正负样本比例均衡。 123# 随机欠采样from imblearn.under_sampling import RandomUnderSamplerrus = RandomUnderSampler(random_state=0) 缺点： 随机欠采样方法通过改变多数类样本比例以达到修改样本分布的目的，从而使样本分布较为均衡，但由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能会导致分类器丢失有关多数类的重要信息。 1.4 决策树算法 决策树思想的来源非常朴素，程序设计中的条件分支结构就是 if-else 结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法 是一种树形结构，本质是一颗由多个判断节点组成的树 其中每个内部节点表示一个属性上的判断， 每个分支代表一个判断结果的输出， 最后每个叶节点代表一种分类结果。 1 信息增益、信息增益率和基尼系数 信息熵 (information entropy)：度量样本集合纯度最常用的一种指标。 篮球比赛里，有 4 个球队 {A,B,C,D} ，获胜概率分别为 {1/2, 1/4, 1/8, 1/8}，求 Ent (D) 信息增益 （information gain）：以某特征划分数据集前后的熵的差值，用来衡量使用当前特征对于样本集合 D 划分效果的好坏。 2 信息熵计算案例 1.5 集成算法 1 定义 通过建立几个模型来解决单一预测问题。它的工作原理是生成多个分类器 / 模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。 2 集成学习中 boosting 和 Bagging 3 Bagging 及随机森林 采样不同数据集 训练分类器 平权投票，获取最终结果 Bagging + 决策树 / 线性回归 / 逻辑回归 / 深度学习… = bagging 集成学习方法 随机森林 随机森林 = Bagging + 决策树 包外数据：没有选择到的数据，称之为 Out-of-bag (OOB) 数据，当数据足够多，对于任意一组数据是包外数据的概率为 1/e。 经验证，包外估计是对集成分类器泛化误差的无偏估计. API 实现 1234567891011121314# n_estimators：(default = 10)森林里的树木数量# Criterion：(default =“gini”) 分割特征的测量方法# max_features=&quot;auto”,每个决策树的最大特征数量 &quot;&quot;&quot; If &quot;auto&quot;, then max_features=sqrt(n_features). If &quot;sqrt&quot;, then max_features=sqrt(n_features)(same as &quot;auto&quot;). If &quot;log2&quot;, then max_features=log2(n_features). If None, then max_features=n_features. &quot;&quot;&quot;# bootstrap：(default = True) 是否在构建树时使用放回抽样# min_samples_split 内部节点再划分所需最小样本数# min_samples_leaf 叶子节点的最小样本数# min_impurity_split: 节点划分最小不纯度sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2) 4 boosting 定义 每新加入一个弱学习器，整体能力就会得到提升，代表算法有：Adaboost，GBDT，XGBoost，LightGBM 算法步骤 1）训练第一个学习器 2）调整数据分布 3）训练第二个学习器 4）再次调整数据分布 5）依次训练学习器，调整数据分布 Bagging 集成与 Boosting 集成的区别 区别一：数据方面 Bagging：对数据进行采样训练； Boosting：根据前一轮学习结果调整数据的重要性。 区别二：投票方面 Bagging：所有学习器平权投票； Boosting：对学习器进行加权投票。 区别三：学习顺序 Bagging 的学习是并行的，每个学习器没有依赖关系； Boosting 学习是串行，学习有先后顺序。 区别四：主要作用 Bagging 主要用于提高泛化性能（解决过拟合，也可以说降低方差） Boosting 主要用于提高训练精度 （解决欠拟合，也可以说降低偏差） AdaBoost 原理图 1from sklearn.ensemble import AdaBoostClassifier GBDT GBDT 的全称是 Gradient Boosting Decision Tree，梯度提升树，在传统机器学习算法中，GBDT 算的上 TOP3 的算法。 GBDT 使用的决策树是 CART 回归树 无论是处理回归问题还是二分类以及多分类，GBDT 使用的决策树通通都是都是 CART 回归树。 在这里插入图片描述 1.6 聚类算法 1 定义 一种典型的无监督学习算法，主要用于将相似的样本自动归到一个类别中。 在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。 2 算法学习 1）随机设置 K 个特征空间内的点作为初始的聚类中心 2）对于其他每个点计算到 K 个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别 3）接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值） 4）如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程 3 API 123456# n_clusters:开始的聚类中心数量# 参数 # estimator.fit(x) # estimator.predict(x) # estimator.fit_predict(x)sklearn.cluster.KMeans(n_clusters=8) 4 案例分析 1234567891011121314151617181920212223import matplotlib.pyplot as pltfrom sklearn.datasets.samples_generator import make_blobsfrom sklearn.cluster import KMeansfrom sklearn.metrics import calinski_harabaz_score# 创建数据集# X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共4个簇，# 簇中心在[-1,-1], [0,0],[1,1], [2,2]， 簇方差分别为[0.4, 0.2, 0.2, 0.2]X, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1, -1], [0, 0], [1, 1], [2, 2]], cluster_std=[0.4, 0.2, 0.2, 0.2], random_state=9)# 数据集可视化plt.scatter(X[:, 0], X[:, 1], marker=&#x27;o&#x27;)plt.show()y_pred = KMeans(n_clusters=2, random_state=9).fit_predict(X)# 分别尝试n_cluses=2\\3\\4,然后查看聚类效果plt.scatter(X[:, 0], X[:, 1], c=y_pred)plt.show()# 用Calinski-Harabasz Index评估的聚类分数print(calinski_harabaz_score(X, y_pred)) 5 模型评估 SSE 误差平方和的值越小越好 肘部法 下降率突然变缓时即认为是最佳的 k 值 SC 系数 取值为 [-1, 1]，其值越大越好 CH 系数 分数 s 高则聚类效果越好 CH 需要达到的目的：用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。 6 K-Means K-Means 算法优缺点总结 优点： 1. 原理简单（靠近中心点），实现容易 2. 聚类效果中上（依赖 K 的选择） 3. 空间复杂度 o (N)，时间复杂度 o (IKN) 缺点： 1. 对离群点，噪声敏感 （中心点易偏移） 2. 很难发现大小差别很大的簇及进行增量计算 3. 结果不一定是全局最优，只能保证局部最优（与 K 的个数及初值选取有关） 优化方法 优化方法 思路 Canopy+kmeans Canopy 粗聚类配合 kmeans kmeans++ 距离越远越容易成为新的质心 二分 k-means 拆除 SSE 最大的簇 k-medoids 和 kmeans 选取中心点的方式不同 kernel kmeans 映射到高维空间 ISODATA 动态聚类，可以更改 K 值大小 Mini-batch K-Means 大数据集分批聚类 1.7 朴素贝叶斯 1 定义 朴素贝叶斯：假定了特征与特征之间相互独立的贝叶斯公式 朴素：假定了特征与特征相互独立 如果一个事物在一些属性条件发生的情况下，事物属于 A 的概率 &gt; 属于 B 的概率，则判定事物属于 A。 2 算法原理 分解各类先验样本数据中的特征； 计算各类数据中，各特征的条件概率；(比如：特征 1 出现的情况下，属于 A 类的概率 p (A | 特征 1)，属于 B 类的概率 p (B | 特征 1)，属于 C 类的概率 p (C | 特征 1)…) 分解待分类数据中的特征 (特征 1、特征 2、特征 3、特征 4…) 计算各特征的各条件概率的乘积，如下所示： 判断为 A 类的概率：p (A | 特征 1) * p (A | 特征 2) * p (A | 特征 3) * p (A | 特征 4)… 判断为 B 类的概率：p (B | 特征 1) * p (B | 特征 2) * p (B | 特征 3) * p (B | 特征 4)… 判断为 C 类的概率：p (C | 特征 1) * p (C | 特征 2) * p (C | 特征 3) * p (C | 特征 4)… … 结果中的最大值就是该样本所属的类别 3 拉普拉斯平滑 问题：从下面的例子的到娱乐概率为 0，这是不合理的，如果词频列表里面有很多出现次数为 0，很可能计算结果都为 0。 解决方法：拉普拉斯平滑 a 为指定的系数一般为 1，m 为训练文档中统计出的特征词个数 4 案例实现 商品评论情感分析 1）获取数据 2）数据基本处理 2.1） 取出内容列，对数据进行分析 2.2） 判定评判标准 2.3） 选择停用词 2.4） 把内容处理，转化成标准格式 2.5） 统计词的个数 2.6）准备训练集和测试集 3）模型训练 4）模型评估 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import pandas as pdimport numpy as npimport jiebaimport matplotlib.pyplot as pltfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.naive_bayes import MultinomialNB# 加载数据data = pd.read_csv(&quot;./data/书籍评价.csv&quot;, encoding=&quot;gbk&quot;)# 数据基本处理# 2.1） 取出内容列，对数据进行分析content = data[&quot;内容&quot;]# 2.2） 判定评判标准 -- 1好评;0差评data.loc[data.loc[:, &#x27;评价&#x27;] == &quot;好评&quot;, &quot;评论标号&quot;] = 1 # 把好评修改为1data.loc[data.loc[:, &#x27;评价&#x27;] == &#x27;差评&#x27;, &#x27;评论标号&#x27;] = 0# data.head()good_or_bad = data[&#x27;评价&#x27;].values # 获取数据print(good_or_bad)# [&#x27;好评&#x27; &#x27;好评&#x27; &#x27;好评&#x27; &#x27;好评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;好评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;差评&#x27;]# 2.3） 选择停用词# 加载停用词stopwords=[]with open(&#x27;./data/stopwords.txt&#x27;,&#x27;r&#x27;,encoding=&#x27;utf-8&#x27;) as f: lines=f.readlines() print(lines) for tmp in lines: line=tmp.strip() print(line) stopwords.append(line)# stopwords # 查看新产生列表#对停用词表进行去重stopwords=list(set(stopwords))#去重 列表形式print(stopwords)# 2.4） 把“内容”处理，转化成标准格式comment_list = []for tmp in content: print(tmp) # 对文本数据进行切割 # cut_all 参数默认为 False,所有使用 cut 方法时默认为精确模式 seg_list = jieba.cut(tmp, cut_all=False) print(seg_list) # &lt;generator object Tokenizer.cut at 0x0000000007CF7DB0&gt; seg_str = &#x27;,&#x27;.join(seg_list) # 拼接字符串 print(seg_str) comment_list.append(seg_str) # 目的是转化成列表形式# print(comment_list) # 查看comment_list列表。# 2.5） 统计词的个数# 进行统计词个数# 实例化对象# CountVectorizer 类会将文本中的词语转换为词频矩阵con = CountVectorizer(stop_words=stopwords)# 进行词数统计X = con.fit_transform(comment_list) # 它通过 fit_transform 函数计算各个词语出现的次数name = con.get_feature_names() # 通过 get_feature_names()可获取词袋中所有文本的关键字print(X.toarray()) # 通过 toarray()可看到词频矩阵的结果print(name)# 2.6）准备训练集和测试集# 准备训练集 这里将文本前10行当做训练集 后3行当做测试集x_train = X.toarray()[:10, :]y_train = good_or_bad[:10]# 准备测试集x_text = X.toarray()[10:, :]y_text = good_or_bad[10:]# 模型训练# 构建贝叶斯算法分类器mb = MultinomialNB(alpha=1) # alpha 为可选项，默认 1.0，添加拉普拉修/Lidstone 平滑参数# 训练数据mb.fit(x_train, y_train)# 预测数据y_predict = mb.predict(x_text)#预测值与真实值展示print(&#x27;预测值：&#x27;,y_predict)print(&#x27;真实值：&#x27;,y_text)# 模型评估print(mb.score(x_text, y_text)) 1.8 支持向量机 1 基本元素 【data】数据 【classifier】分类 【optimization】最优化 【kernelling】核方法 【hyperplane】超平面 2 基本思想 SVM (supported vector machine，支持向量机)，即寻找到一个超平面使样本分成两类，并且间隔最大。 3 用途 线性或非线性分类、回归，甚至是异常值检测 特别适用于中小型复杂数据集的分类 4 硬间隔和软间隔 硬间隔：严格地让所有实例都不在最大间隔之间，并且位于正确的一边。它只在数据是线性可分离的时候才有效；其次，它对异常值非常敏感。 软间隔：尽可能在保持最大间隔宽阔和限制间隔违例（即位于最大间隔之上，甚至在错误的一边的实例）之间找到良好的平衡。 5 支持向量机推导 6 损失函数 SVM Hinge 损失（折页损失函数、铰链损失函数） 7 SVM 回归 让尽可能多的实例位于预测线上，同时限制间隔违例（也就是不在预测线距上的实例）。 1234567891011121314151617181920212223sklearn.svm.SVC(C=1.0, kernel=&#x27;rbf&#x27;, degree=3,coef0=0.0,random_state=None)&quot;&quot;&quot;C: 惩罚系数，用来控制损失函数的惩罚系数，类似于线性回归中的正则化系数。 - C越大，相当于惩罚松弛变量，希望松弛变量接近0，即**对误分类的惩罚增大**，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱，容易导致过拟合。 - C值小，对误分类的惩罚减小，容错能力增强，泛化能力较强，但也可能欠拟合。- kernel: 算法中采用的核函数类型，核函数是用来将非线性问题转化为线性问题的一种方法。 - 参数选择有RBF, Linear, Poly, Sigmoid或者自定义一个核函数。 - 默认的是&quot;RBF&quot;，即径向基核，也就是高斯核函数； - 而Linear指的是线性核函数， - Poly指的是多项式核， - Sigmoid指的是双曲正切函数tanh核；。- degree: - 当指定kernel为&#x27;poly&#x27;时，表示选择的多项式的最高次数，默认为三次多项式； - 若指定kernel不是&#x27;poly&#x27;，则忽略，即该参数只对&#x27;poly&#x27;有用。 - 多项式核函数是将低维的输入空间映射到高维的特征空间。- coef0:核函数常数值(y=kx+b中的b值)， - 只有‘poly’和‘sigmoid’核函数有，默认值是0。 &quot;&quot;&quot; 12345678sklearn.svm.LinearSVC(penalty=&#x27;l2&#x27;, loss=&#x27;squared_hinge&#x27;, dual=True, C=1.0)&quot;&quot;&quot;penalty:正则化参数，L1和L2两种参数可选，仅LinearSVC有。loss:损失函数，有hinge和squared_hinge两种可选，前者又称L1损失，后者称为L2损失，默认是squared_hinge，其中hinge是SVM的标准损失， squared_hinge是hinge的平方dual:是否转化为对偶问题求解，默认是True。C:惩罚系数，用来控制损失函数的惩罚系数，类似于线性回归中的正则化系数。&quot;&quot;&quot; 8 SVM 优缺点 SVM 的优点： 在高维空间中非常高效； 即使在数据维度比样本数量大的情况下仍然有效； 在决策函数（称为支持向量）中使用训练集的子集，因此它也是高效利用内存的； 通用性：不同的核函数与特定的决策函数一一对应； SVM 的缺点： 如果特征数量比样本数量大得多，在选择核函数时要避免过拟合； 对缺失数据敏感； 对于核函数的高维映射解释力不强 1.9 EM 算法 1 基本思想 首先根据己经给出的观测数据，估计出模型参数的值； 然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计； 然后反复迭代，直至最后收敛，迭代结束。 2 算法流程 1.10 HMM 模型 1 定义 隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。 马尔科夫链即为状态空间中从一个状态到另一个状态转换的随机过程。 马尔科夫链的无记忆性：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。 2 常见术语 可见状态链 隐含状态链 转换概率 输出概率 3 HMM 两个重要假设 1） 齐次马尔科夫链假设 即任意时刻的隐藏状态只依赖于它前一个隐藏状态。 2） 观测独立性假设 即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。 4 HMM 模型算法原理 一个 HMM 模型，可以由隐藏状态初始概率分布 Π , 状态转移概率矩阵 A 和观测状态概率矩阵 B 决定。Π，A 决定状态序列，B 决定观测序列。 因此，HMM 模型可以由一个三元组 λ 表示如下： λ=(A,B,Π)= (状态序列，观测序列，初始状态概率分布) 5 HMM 模型三个基本问题 1）评估观察序列概率 —— 前向后向的概率计算 即给定 模型λ =(A,B,Π) 和 观测序列 O={o_1,o_2,…o_T}，计算在模型 λ 下某一个 观测序列O出现的概率 P (O|λ)。 这个问题的求解需要用到前向后向算法，是 HMM 模型三个问题中最简单的。 2）预测问题，也称为解码问题 —— 维特比（Viterbi）算法 即给定 模型λ =(A,B,Π) 和 观测序列 O={o_1,o_2,…o_T}，求给定观测序列条件下，最可能出现的对应的 状态序列 。 这个问题的求解需要用到基于动态规划的维特比算法，是 HMM 模型三个问题中复杂度居中的算法。 3）模型参数学习问题 —— 鲍姆 - 韦尔奇（Baum-Welch）算法 (状态未知) ，这是一个学习问题 即给定 观测序列 O={o_1,o_2,…o_T}，估计 模型λ =(A,B,Π) 的参数，使该模型下观测序列的条件概率 P (O∣λ) 最大。 这个问题的求解需要用到基于 EM 算法的鲍姆 - 韦尔奇算法，是 HMM 模型三个问题中最复杂的。 6 案例实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npfrom hmmlearn import hmm# 设定隐藏状态的集合states = [&quot;box 1&quot;, &quot;box 2&quot;, &quot;box3&quot;]n_states = len(states)# 设定观察状态的集合observations = [&quot;red&quot;, &quot;white&quot;]n_observations = len(observations)# 设定初始状态分布start_probability = np.array([0.2, 0.4, 0.4])# 设定状态转移概率分布矩阵transition_probability = np.array([ [0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]])# 设定观测状态概率矩阵emission_probability = np.array([ [0.5, 0.5], [0.4, 0.6], [0.7, 0.3]])# 设定模型参数model = hmm.MultinomialHMM(n_components=n_states)model.startprob_=start_probability # 初始状态分布model.transmat_=transition_probability # 状态转移概率分布矩阵model.emissionprob_=emission_probability # 观测状态概率矩阵seen = np.array([[0,1,0]]).T # 设定观测序列box = model.predict(seen)print(&quot;球的观测顺序为：\\n&quot;, &quot;, &quot;.join(map(lambda x: observations[x], seen.flatten())))# 注意：需要使用flatten方法，把seen从二维变成一维print(&quot;最可能的隐藏状态序列为:\\n&quot;， &quot;, &quot;.join(map(lambda x: states[x], box)))print(model.score(seen))# 输出结果是：-2.03854530992# 对数处理结果，概率值import mathmath.exp(model.score(seen))# ln0.13022≈−2.0385# 输出结果是：0.13021800000000003 1.11 xgboost 算法 1 定义 XGBoost（Extreme Gradient Boosting）全名叫极端梯度提升树，XGBoost 是集成学习方法的王牌，在 Kaggle 数据挖掘比赛中，大部分获胜者用了 XGBoost。 2 最优模型构建方法 构建最优模型的一般方法是最小化训练数据的损失函数 经验风险最小化 训练得到的模型复杂度较高。当训练数据较小时，模型很容易出现过拟合问题。 结构风险最小化 结构风险最小化的模型往往对训练数据以及未知的测试数据都有较好的预测 3 目标函数 目标函数，即损失函数，通过最小化损失函数来构建最优模型。 其中 yi 是模型的实际输出结果，yi 是模型的输出结果； 等式右边第一部分是模型的训练误差，第二部分是正则化项，这里的正则化项是 K 棵树的正则化项相加而来的。 XGBoost 使用 CART 树，则树的复杂度为 其中 T 为叶子节点的个数，||w|| 为叶子节点向量的模 。γ 表示节点切分的难度，λ 表示 L2 正则化系数。 XGBoost 的回归树构建方法 XGBoost 与 GDBT 的区别 区别一： XGBoost 生成 CART 树考虑了树的复杂度， GDBT 未考虑，GDBT 在树的剪枝步骤中考虑了树的复杂度。 区别二： XGBoost 是拟合上一轮损失函数的二阶导展开，GDBT 是拟合上一轮损失函数的一阶导展开，因此，XGBoost 的准确性更高，且满足相同的训练效果，需要的迭代次数更少。 区别三： XGBoost 与 GDBT 都是逐次迭代来提高模型性能，但是 XGBoost 在选取最佳切分点时可以开启多线程进行，大大提高了运行速度。 XGBoost 中封装的参数 主要由三种类型构成： 1 通用参数（general parameters）：主要是宏观函数控制； booster [缺省值 = gbtree] 决定使用哪个 booster，可以是 gbtree，gblinear 或者 dart。 gbtree 和 dart 使用基于树的模型 (dart 主要多了 Dropout)，而 gblinear 使用线性函数. silent [缺省值 = 0] 设置为 0 打印运行信息；设置为 1 静默模式，不打印 nthread [缺省值 = 设置为最大可能的线程数] 并行运行 xgboost 的线程数，输入的参数应该 &lt;= 系统的 CPU 核心数，若是没有设置算法会检测将其设置为 CPU 的全部核心数 2 Booster 参数（booster parameters）：取决于选择的 Booster 类型，用于控制每一步的 booster（tree, regressiong）； eta [缺省值 = 0.3，别名：learning_rate] 更新中减少的步长来防止过拟合。 在每次 boosting 之后，可以直接获得新的特征权值，这样可以使得 boosting 更加鲁棒。 范围： [0,1] gamma [缺省值 = 0，别名: min_split_loss]（分裂最小 loss） 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。 Gamma 指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。 范围: [0,∞] max_depth [缺省值 = 6] 这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth 越大，模型会学到更具体更局部的样本。设置为 0 代表没有限制 范围: [0,∞] min_child_weight [缺省值 = 1] 决定最小叶子节点样本权重和。XGBoost 的这个参数是最小样本权重的和. 当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用 CV 来调整。. 范围: [0,∞] subsample [缺省值 = 1] 这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1，0.5 代表平均采样，防止过拟合. 范围: (0,1] colsample_bytree [缺省值 = 1] 用来控制每棵随机采样的列数的占比 (每一列是一个特征)。 典型值：0.5-1 范围: (0,1] colsample_bylevel [缺省值 = 1] 用来控制树的每一级的每一次分裂，对列数的采样的占比。 我个人一般不太用这个参数，因为 subsample 参数和 colsample_bytree 参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。 范围: (0,1] lambda [缺省值 = 1，别名: reg_lambda] 权重的 L2 正则化项 (和 Ridge regression 类似)。 这个参数是用来控制 XGBoost 的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数 在减少过拟合上还是可以挖掘出更多用处的。. alpha [缺省值 = 0，别名: reg_alpha] 权重的 L1 正则化项。(和 Lasso regression 类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 scale_pos_weight [缺省值 = 1] 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。通常可以将其设置为负 样本的数目与正样本数目的比值。 3 学习目标参数（task parameters）：控制训练目标的表现。 objective [缺省值 = reg:linear] “reg:linear” – 线性回归 “reg:logistic” – 逻辑回归 “binary:logistic” – 二分类逻辑回归，输出为概率 “multi:softmax” – 使用 softmax 的多分类器，返回预测的类别 (不是概率)。在这种情况下，你还需要多设一个参数：num_class (类别数目) “multi:softprob” – 和 multi:softmax 参数一样，但是返回的是每个数据属于各个类别的概率。 eval_metric [缺省值 = 通过目标函数选择] 可供选择的如下所示： “rmse”: 均方根误差 “mae”: 平均绝对值误差 “logloss”: 负对数似然函数值 “ error” 二分类错误率。 其值通过错误分类数目与全部分类数目比值得到。对于预测，预测值大于 0.5 被认为是正类，其它归为负类。 “error@t”: 不同的划分阈值可以通过 ‘t’进行设置 “merror”: 多分类错误率，计算公式为 (wrong cases)/(all cases) “mlogloss”: 多分类 log 损失 “auc”: 曲线下的面积 seed [缺省值 = 0] 随机数的种子 4 案例分析 泰坦尼克号乘客存活分析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import pandas as pdimport numpy as npfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.model_selection import train_test_split# 1、获取数据titan = pd.read_csv(&quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt&quot;)# 2.数据基本处理# 2.1 确定特征值,目标值x = titan[[&quot;pclass&quot;, &quot;age&quot;, &quot;sex&quot;]]y = titan[&quot;survived&quot;]# 缺失值需要处理，将特征当中有类别的这些特征进行字典特征抽取x[&#x27;age&#x27;].fillna(x[&#x27;age&#x27;].mean(), inplace=True)x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=22)# 对于x转换成字典数据x.to_dict(orient=&quot;records&quot;)# [&#123;&quot;pclass&quot;: &quot;1st&quot;, &quot;age&quot;: 29.00, &quot;sex&quot;: &quot;female&quot;&#125;, &#123;&#125;]# 3.特征工程(字典特征抽取)transfer = DictVectorizer(sparse=False)x_train = transfer.fit_transform(x_train.to_dict(orient=&quot;records&quot;))x_test = transfer.fit_transform(x_test.to_dict(orient=&quot;records&quot;))# 4.xgboost模型训练和模型评估# 模型初步训练from xgboost import XGBClassifierxg = XGBClassifier()xg.fit(x_train, y_train)print(xg.score(x_test, y_test))# 针对max_depth进行模型调优depth_range = range(10)score = []for i in depth_range: xg = XGBClassifier(eta=1, gamma=0, max_depth=i) xg.fit(x_train, y_train) s = xg.score(x_test, y_test) print(s) score.append(s) # 结果可视化import matplotlib.pyplot as pltplt.plot(depth_range, score)plt.show() 1.12 lightGBM 算法 1 定义 LightGBM 提出的主要原因就是为了解决 GBDT 在海量数据遇到的问题，让 GBDT 可以更好更快地用于工业实践。 GBDT 在每一次迭代的时候，都需要遍历整个训练数据多次。 如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。 尤其面对工业级海量的数据，普通的 GBDT 算法是不能满足其需求的。 2 特点 在开源之后，就被别人冠以 “速度惊人”、“支持分布式”、“代码清晰易懂”、“占用内存小” 等属性。 LightGBM 主打的高效并行训练让其性能超越现有其他 boosting 工具。在 Higgs 数据集上的试验表明，LightGBM 比 XGBoost 快将近 10倍 ，内存占用率大约为 XGBoost 的 1/6 。 LightGBM 主要基于以下方面优化，提升整体特特性： 基于 Histogram（直方图）的决策树算法 Lightgbm 的 Histogram（直方图）做差加速 带深度限制的 Leaf-wise 的叶子生长策略 直接支持类别特征 直接支持高效并行 3 优化特点详解 基于 Histogram（直方图）的决策树算法 直方图算法的基本思想是 先把连续的浮点特征值离散化成 k 个整数，同时构造一个宽度为 k 的直方图。 在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。 内存消耗的降低，计算上的代价也大幅降低 Lightgbm 的 Histogram（直方图）做差加速 一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。 通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的 k 个桶。 利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。 带深度限制的 Leaf-wise 的叶子生长策略 Level-wise 便利一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。 但实际上 Level-wise 是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。 Leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。 因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。 Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。 直接支持类别特征 实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的 0/1 特征，降低了空间和时间的效率。 而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM 优化了对类别特征的支持，可以直接输入类别特征，不需要额外的 0/1 展开。并在决策树算法上增加了类别特征的决策规则。 在 Expo 数据集上的实验，相比 0/1 展开的方法，训练速度可以加速 8 倍，并且精度一致。目前来看，LightGBM 是第一个直接支持类别特征的 GBDT 工具。 直接支持高效并行 LightGBM 还具有支持高效并行的优点。LightGBM 原生支持并行学习，目前支持特征并行和数据并行的两种。 特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。 数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。 LightGBM 针对这两种并行方法都做了优化: 在特征并行算法中，通过在本地保存全部数据避免对数据切分结果的通信； 在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。 ** 基于投票的数据并行 (Voting Parallelization)** 则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。 4 API 相关参数介绍 4.1 Control Parameters Control Parameters 含义 用法 max_depth 树的最大深度 当模型过拟合时，可以考虑首先降低 max_depth min_data_in_leaf 叶子可能具有的最小记录数 默认 20，过拟合时用 feature_fraction 例如 为 0.8 时，意味着在每次迭代中随机选择 80％的参数来建树 boosting 为 random forest 时用 bagging_fraction 每次迭代时用的数据比例 用于加快训练速度和减小过拟合 early_stopping_round 如果一次验证数据的一个度量在最近的 early_stopping_round 回合中没有提高，模型将停止训练 加速分析，减少过多迭代 lambda 指定正则化 0～1 min_gain_to_split 描述分裂的最小 gain 控制树的有用的分裂 max_cat_group 在 group 边界上找到分割点 当类别数量很多时，找分割点很容易过拟合时 n_estimators 最大迭代次数 最大迭代数不必设置过大，可以在进行一次迭代后，根据最佳迭代数设置 4.2 Core Parameters Core Parameters 含义 用法 Task 数据的用途 选择 train 或者 predict application 模型的用途 选择 regression: 回归时， binary: 二分类时， multiclass: 多分类时 boosting 要用的算法 gbdt， rf: random forest， dart: Dropouts meet Multiple Additive Regression Trees， goss: Gradient-based One-Side Sampling num_boost_round 迭代次数 通常 100+ learning_rate 学习率 常用 0.1, 0.001, 0.003… num_leaves 叶子数量 默认 31 device cpu 或者 gpu metric mae: mean absolute error ， mse: mean squared error ， binary_logloss: loss for binary classification ， multi_logloss: loss for multi classification 4.3 IO parameter IO parameter 含义 max_bin 表示 feature 将存入的 bin 的最大数量 categorical_feature 如果 categorical_features = 0,1,2， 则列 0，1，2 是 categorical 变量 ignore_column 与 categorical_features 类似，只不过不是将特定的列视为 categorical，而是完全忽略 save_binary 这个参数为 true 时，则数据集被保存为二进制文件，下次读数据时速度会变快 调参建议： IO parameter 含义 num_leaves 取值应 &lt;= 2^{(max_depth)} 2 (max_depth)， 超过此值会导致过拟合 min_data_in_leaf 将它设置为较大的值可以避免生长太深的树，但可能会导致 underfitting，在大型数据集时就设置为数百或数千 max_depth 这个也是可以限制树的深度 下表对应了 Faster Speed ，better accuracy ，over-fitting 三种目的时，可以调的参数 Faster Speed better accuracy over-fitting 将 max_bin 设置小一些 用较大的 max_bin max_bin 小一些 num_leaves 大一些 num_leaves 小一些 用 feature_fraction 来做 sub-sampling 用 feature_fraction 用 bagging_fraction 和 bagging_freq 设定 bagging_fraction 和 bagging_freq training data 多一些 training data 多一些 用 save_binary 来加速数据加载 直接用 categorical feature 用 gmin_data_in_leaf 和 min_sum_hessian_in_leaf 用 parallel learning 用 dart 用 lambda_l1, lambda_l2 ，min_gain_to_split 做正则化 num_iterations 大一些， learning_rate 小一些 用 max_depth 控制树的深度 5 案例分析 鸢尾花数据集处理 1234567891011121314151617181920212223242526272829303132333435363738from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import mean_squared_errorimport lightgbm as lgb# 加载数据iris = load_iris()data = iris.datatarget = iris.targetX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)# 模型训练gbm = lgb.LGBMRegressor(objective=&#x27;regression&#x27;, learning_rate=0.05, n_estimators=20)gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric=&#x27;l1&#x27;, early_stopping_rounds=5)print(gbm.score(X_test, y_test))# 0.810605595102488# 网格搜索，参数优化estimator = lgb.LGBMRegressor(num_leaves=31)param_grid = &#123; &#x27;learning_rate&#x27;: [0.01, 0.1, 1], &#x27;n_estimators&#x27;: [20, 40]&#125;gbm = GridSearchCV(estimator, param_grid, cv=4)gbm.fit(X_train, y_train)print(&#x27;Best parameters found by grid search are:&#x27;, gbm.best_params_)# Best parameters found by grid search are: &#123;&#x27;learning_rate&#x27;: 0.1, &#x27;n_estimators&#x27;: 40&#125;# 模型调优gbm = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.1, n_estimators=40)gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric=&#x27;l1&#x27;, early_stopping_rounds=5)print(gbm.score(X_test, y_test))# 0.9536626296481988 2 机器学习算法实现 1. 获取数据集 12345671.1 方法一：from sklearn.datasets import load_irisiris = load_iris()1.2 方法二：import pandas as pddata = pd.read_csv(&quot;C:\\\\data\\\\FBlocaltion\\\\train.csv&quot;) 2. 数据基本处理 123456789# （备选）：缺失值处理data = data.replace(to_replace=&quot;?&quot;, value=np.NaN)data = data.dropna()# x_train,x_test,y_train,y_test为训练集特征值、测试集特征值、训练集目标值、测试集目标值# x：数据集的特征值 y：数据集的标签值 test_size：测试集的大小，一般为float random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。from sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22) 3. 特征工程：标准化 1234567891011121314151617181920# 通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程# 方法一：（ 标准化） &lt;在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。&gt;from sklearn.preprocessing import StandardScalertransfer = StandardScaler()# 方法二：（归一化） &lt;鲁棒性较差，只适合传统精确小数据场景。&gt;from sklearn.preprocessing import StandardScalertransfer = MinMaxScaler()# 方法三：（特征工程（字典特征抽取））from sklearn.feature_extraction import DictVectorizertransfer = DictVectorizer(sparse=False)# 特征中出现类别符号，需要进行one-hot编码处理(DictVectorizer)，x.to_dict(orient=&quot;records&quot;) 需要将数组特征转换成字典数据x_train = transfer.fit_transform(x_train.to_dict(orient=&quot;records&quot;))x_test = transfer.fit_transform(x_test.to_dict(orient=&quot;records&quot;))# 对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化。x_train = transfer.fit_transform(x_train)# 对剩余的数据（testData）使用同样的均值、方差、最大最小值等指标进行转换transform(testData)，从而保证train、test处理方式相同。x_test = transfer.transform(x_test) 4. 机器学习 (模型训练) 4.1 模型估计 123456789101112131415161718192021# 方法一：（K-近邻算法）from sklearn.neighbors import KNeighborsClassifier# n_neighbors：查询默认使用的邻居数（默认= 5）# algorithm：&#123;‘auto’，‘ball_tree’，‘kd_tree’，‘brute’&#125;estimator = KNeighborsClassifier(n_neighbors=7,algorithm=&#x27;auto&#x27;)# 方法二：（线性回归）from sklearn.linear_model import LinearRegressionestimator = LinearRegression()# 方法三：（逻辑回归）from sklearn.linear_model import LogisticRegressionestimator = LogisticRegression()# 方法四：（决策树）from sklearn.tree import DecisionTreeClassifier, export_graphviz# criterion：特征选择标准（&quot;gini&quot;或者&quot;entropy&quot;），前者代表基尼系数，后者代表信息增益。默认&quot;gini&quot;，即CART算法。# min_samples_split：内部节点再划分所需最小样本数（默认：2） # min_samples_leaf：叶子节点最少样本数（默认：1） # max_depth：决策树最大深度（10-100）# random_state：随机数种子estimator = DecisionTreeClassifier(criterion=&quot;entropy&quot;, max_depth=5) 4.2 模型调优 12345678910111213141516171819202122232425# 方法一：（网格搜索）# estimator：估计器对象 param_grid：估计器参数(dict)&#123;“n_neighbors”:[1,3,5]&#125; cv：指定几折交叉验证from sklearn.model_selection import GridSearchCVestimator = GridSearchCV(estimator, param_grid=&#123;&quot;n_neighbors&quot;: [1, 3, 5]&#125;, cv=3)# 方法二：（留出法）from sklearn.model_selection import train_test_splittrain_X , test_X, train_Y ,test_Y = train_test_split(X, Y, test_size=0.2,random_state=0)# 方法三：（留一法）from sklearn.model_selection import LeaveOneOutdata = [1, 2, 3, 4]loo = LeaveOneOut()for train, test in loo.split(data): print(&quot;%s %s&quot; % (train, test)) # 方法四：（K折交叉验证）from sklearn.model_selection import KFoldfolder = KFold(n_splits = 4, random_state=0, shuffle = False)# 方法五：（分层K折交叉验证）from sklearn.model_selection import StratifiedKFoldsfolder = StratifiedKFold(n_splits = 4, random_state = 0, shuffle = False)# 方法六：（自助法） 4.3 模型训练 1estimator.fit(x_train, y_train) 5. 模型评估 1234567891011121314151617181920212223242526# 方法一：比对真实值和预测值y_predict = estimator.predict(x_test)print(&quot;&gt;&gt;&gt;预测结果为:\\n&quot;, y_predict)print(&quot;&gt;&gt;&gt;比对真实值和预测值：\\n&quot;, y_predict == y_test)# 方法二：回归系数（线性回归）# 回归系数（regression coefficient）：在回归方程中表示自变量x 对因变量y 影响大小的参数。回归系数越大表示x 对y 影响越大，正回归系数表示y 随x 增大而增大，负回归系数表示y 随x增大而减小。print(&#x27;回归系数：\\n&#x27;, estimator.coef_)estimator.predict([[100, 80]]) # 平时成绩100， 期末成绩80的概率# 方法二：直接计算准确率score = estimator.score(x_test, y_test)print(&quot;&gt;&gt;&gt;准确率为：\\n&quot;, score)print(&quot;&gt;&gt;&gt;预测最优得分：\\n&quot;, estimator.best_score_)print(&quot;&gt;&gt;&gt;最优估计器:\\n&quot;, estimator.best_estimator_)print(&quot;&gt;&gt;&gt;最优结果：\\n&quot;, pd.DataFrame(estimator.cv_results_))# 方法三：分类评估报告生成from sklearn.metrics import classification_reportret = classification_report(y_test, y_predict, labels=(2, 4), target_names=(&quot;良性&quot;, &quot;恶性&quot;) # 方法四：AUC指标评测from sklearn.metrics import roc_auc_score# 0.5~1之间，越接近于1约好y_test = np.where(y_test &gt; 2.5, 1, 0)roc_auc_score(y_test, y_predict)","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"算法","slug":"算法","permalink":"https://leezhao415.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"机器学习算法导论","slug":"机器学习算法导论","date":"2021-05-19T08:35:27.000Z","updated":"2021-07-14T13:00:44.391Z","comments":true,"path":"2021/05/19/机器学习算法导论/","link":"","permalink":"https://leezhao415.github.io/2021/05/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/","excerpt":"","text":"文章目录 机器学习算法 1 机器学习分类依据 1.1 按照任务是否需要和环境进行交互获取经验 1.1.1 监督学习 1.1.2 强化学习 1.2 按照算法功能和形式的类似性 1.2.1 回归算法 1.2.2 基于实例的算法 1.2.3 正则化方法 1.2.4 决策树学习 1.2.5 贝叶斯方法 1.2.6 基于核的算法 1.2.7 聚类算法 1.2.8 关联规则学习 1.2.9 人工神经网络 1.2.10 深度学习 1.2.11 降低维度算法 1.2.12 集成算法 2 监督学习 2.1 有监督学习 2.1.1 K - 近邻算法 2.1.2 决策树和随机森林算法 2.1.3 朴素贝叶斯 2.1.4 线性回归 2.1.5 逻辑回归 2.1.6 支持向量机 2.1.7 集成学习 2.1.8 神经网络 2.2 无监督学习 2.2.1 聚类算法 2.2.2 主成分分析（PCA） 2.2.3 SVD 矩阵分解 2.2.4 独立成分分析 (ICA) 2.2.5 EM 算法 2.3 半监督学习 3 强化学习 3.1 概念解释 3.2 常见算法 3.3 应用场景 机器学习算法 1 机器学习分类依据 1.1 按照任务是否需要和环境进行交互获取经验 1.1.1 监督学习 1 按照训练数据是否存在标签 监督学习 监督学习是从 标记的训练数据 来推断一个功能的机器学习任务。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例。一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签。 无监督学习 所有数据 只有特征向量没有标签 ，但是可以发现这些数据呈现出聚群的结构，本质是一个相似的类型的会聚集在一起。把这些没有标签的数据分成一个一个组合，就是聚类（Clustering） 半监督学习 半监督学习在训练阶段结合了大量 未标记的数据 和 少量标签数据 。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确，而且训练成本更低。 2 按照标签是连续还是离散的 分类问题 回归问题 探索自变量与因变量之间的关系的问题，回归算法试图采用对误差的衡量来探索变量之间的关系。 1.1.2 强化学习 智能系统 从环境到行为映射 的学习，以使 奖励信号(强化信号)函数值最大 。如果 Agent 的某个行为策略导致环境正的奖赏 (强化信号)，那么 Agent 以后产生这个行为策略的趋势便会加强 常见算法 Q-Learning 时间差学习（Temporal difference learning） 应用场景 动态系统 机器人控制 1.2 按照算法功能和形式的类似性 1.2.1 回归算法 回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。 常见的算法： 最小二乘法（Ordinary Least Square） 逻辑回归（Logistic Regression） 逐步式回归（Stepwise Regression） 多元自适应回归样条（Multivariate Adaptive Regression Splines） 本地散点平滑估计（Locally Estimated Scatterplot Smoothing） 1.2.2 基于实例的算法 基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为 “赢家通吃” 学习或者 “基于记忆的学习”。 常见的算法: k-Nearest Neighbor(KNN) 学习矢量量化（Learning Vector Quantization， LVQ） 以及自组织映射算法（Self-Organizing Map ， SOM） 1.2.3 正则化方法 正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。 常见的算法： 岭回归（Ridge Regression） LASSO 回归（Least Absolute Shrinkage and Selection Operator） 弹性网络（Elastic Net） 1.2.4 决策树学习 决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。 常见的算法： CART 树（Classification And Regression Tree） ID3 (Iterative Dichotomiser 3) C4.5 Chi-squared Automatic Interaction Detection(CHAID) Decision Stump 随机森林（Random Forest） 多元自适应回归样条（MARS） 梯度推进机（Gradient Boosting Machine， GBM） 1.2.5 贝叶斯方法 贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。 常见的算法： 朴素贝叶斯算法 平均单依赖估计（Averaged One-Dependence Estimators， AODE） Bayesian Belief Network（BBN） 1.2.6 基于核的算法 基于核的算法中最著名的莫过于支持向量机（SVM）了。 基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。 常见的算法： 支持向量机（Support Vector Machine， SVM） 径向基函数（Radial Basis Function ，RBF) 线性判别分析（Linear Discriminate Analysis ，LDA) 1.2.7 聚类算法 聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。 常见的算法： k-Means 算法 期望最大化算法（Expectation Maximization， EM） 1.2.8 关联规则学习 关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。 常见的算法： Apriori 算法 Eclat 算法 1.2.9 人工神经网络 人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。 常见的算法： 感知器神经网络（Perceptron Neural Network） 反向传递（Back Propagation） Hopfield 网络 自组织映射（Self-Organizing Map, SOM） 学习矢量量化（Learning Vector Quantization， LVQ） 1.2.10 深度学习 深度学习算法是对人工神经网络的发展。 在近期赢得了很多关注， 特别是百度也开始发力深度学习后， 更是在国内引起了很多关注。 在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。 常见的算法： 受限波尔兹曼机（Restricted Boltzmann Machine， RBN） Deep Belief Networks（DBN） 卷积网络（Convolutional Network） 堆栈式自动编码器（Stacked Auto-encoders） 1.2.11 降低维度算法 像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。 常见的算法： 主成份分析（Principle Component Analysis， PCA） 偏最小二乘回归（Partial Least Square Regression，PLS） Sammon 映射 多维尺度（Multi-Dimensional Scaling, MDS） 投影追踪（Projection Pursuit） 1.2.12 集成算法 集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。 常见的算法： Boosting Bootstrapped Aggregation（Bagging） AdaBoost，堆叠泛化（Stacked Generalization， Blending） 梯度推进机（Gradient Boosting Machine, GBM） 随机森林（Random Forest） 2 监督学习 2.1 有监督学习 2.1.1 K - 近邻算法 最近邻 (k-Nearest Neighbors， KNN) 算法是一种分类算法， 1968 年由 Cover 和 Hart 提出， 应用场景有字符识别、 文本分类、 图像识别等领域。 该算法的思想是： 一个样本与数据集中的 k 个样本最相似， 如果这 k 个样本中的大多数属于某一个类别， 则该样本也属于这个类别。 KNN 算法流程： 1）计算已知类别数据集中的点与当前点之间的距离 2）按距离递增次序排序 3）选取与当前点距离最小的 k 个点 4）统计前 k 个点所在的类别出现的频率 5）返回前 k 个点出现频率最高的类别作为当前点的预测分类 2.1.2 决策树和随机森林算法 决策树(Decision Trees) 是一种树形结构，为人们提供决策依据，决策树可以用来回答 yes 和 no 问题，它通过树形结构将各种情况组合都表示出来，每个分支表示一次选择（选择 yes 还是 no），直到所有选择都进行完毕，最终给出正确答案。 随机森林(Random Forests) 专为决策树分类器设计的集成算法，是装袋法（Bagging）的一种拓展。 GBDT (梯度提升决策树) 在不改变原来模型结构的基础上提升模型的拟合能力 利用梯度下降，同损失函数的 负梯度值 作为 残差值 来拟合回归决策树。 较为出色的是 XGBoost 树提升系统 2.1.3 朴素贝叶斯 P (A|B) 是后验概率， P (B|A) 是似然，P (A) 为先验概率，P (B) 为我们要预测的值。 具体应用有：垃圾邮件检测、文章分类、情感分类、人脸识别等。 2.1.4 线性回归 线性回归 (Linear Regression) 的基本思想：找到一条线使得平面内的所有点到这条线的欧式距离和最小。这条线就是我们要求取得线。线性指的是用一条线对数据进行拟合，距离代表的是数据误差，最小二乘法可以看做是误差最小化。 2.1.5 逻辑回归 逻辑回归 (Logistic Regression) 模型是一个二分类模型，它选取不同的特征与权重来对样本进行概率分类，用一个 log 函数计算样本属于某一类的概率。即一个样本会有一定的概率属于一个类，会有一定的概率属于另一类，概率大的类即为样本所属类。 具体应用有：信用评级、营销活动成功概率、产品销售预测、某天是否将会地震发生。 2.1.6 支持向量机 支持向量机 (Support Vector Machines) 是一个二分类算法，它可以在 N 维空间找到一个 (N-1) 维的超平面，这个超平面可以将这些点分为两类。也就是说，平面内如果存在线性可分的两类点，SVM 可以找到一条最优的直线将这些点分开。 具体应用有：广告展示、性别检测、大规模图像识别等。 2.1.7 集成学习 集成学习就是将很多分类器集成在一起，每个分类器有不同的权重，将这些分类器的分类结果合并在一起，作为最终的分类结果。最初集成方法为贝叶斯决策，现在多采用 error-correcting output coding, bagging, and boosting 等方法进行集成。 那么为什集成分类器要比单个分类器效果好呢？ 1. 偏差均匀化 ：如果你将民主党与共和党的投票数算一下均值，肯定会得到你原先没有发现的结果，集成学习与这个也类似，它可以学到其它任何一种方式都学不到的东西。 2. 减少方差 ：总体的结果要比单一模型的结果好，因为其从多个角度考虑问题。类似于股票市场，综合考虑多只股票肯定要比只考虑一只股票好，这就是为什么多数据比少数据效果好原因，因为其考虑的因素更多。 3. 不容易过拟合 。如果的一个模型不过拟合，那么综合考虑多种因素的多模型就更不容易过拟合了。 2.1.8 神经网络 神经网络 (也称之为人工神经网络，ANN)(Neural networks) 算法是 80 年代机器学习界非常流行的算法，不过在 90 年代中途衰落。现在，携着 “深度学习” 之势，神经网络重装归来，重新成为最强大的机器学习算法之一。 一个简单的神经网络的逻辑架构分成输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是” 神经网络”。 2.2 无监督学习 常见的无监督学习算法分为三类： 聚类（Clustering） k-Means Hierarchical Cluster Analysis (HCA) Expectation Maximization 可视化与降维（Visualization and dimensionality reduction） Principal Component Analysis (PCA) Kernel PCA Locally-Linear Embedding (LLE) t-distributed Stochastic Neighbor Embedding (t-SNE) 关联规则学习（Association rule learning） Apriori Eclat 2.2.1 聚类算法 聚类算法就是将一堆数据进行处理，根据它们的相似性对数据进行聚类。 聚类算法有很多种，具体如下：中心聚类、关联聚类、密度聚类、概率聚类、降维、神经网络 / 深度学习。 2.2.2 主成分分析（PCA） 主成分分析是利用正交变换将一些列可能相关数据转换为线性无关数据，从而找到主成分。 PCA 主要用于简单学习与可视化中数据压缩、简化。但是 PCA 有一定的局限性，它需要你拥有特定领域的相关知识。对噪音比较多的数据并不适用。 2.2.3 SVD 矩阵分解 SVD 矩阵是一个复杂的实复负数矩阵，给定一个 m 行、n 列的矩阵 M, 那么 M 矩阵可以分解为 M = UΣV。U 和 V 是酉矩阵，Σ 为对角阵。 PCA 实际上就是一个简化版本的 SVD 分解。在计算机视觉领域，第一个脸部识别算法就是基于 PCA 与 SVD 的，用特征对脸部进行特征表示，然后降维、最后进行面部匹配。尽管现在面部识别方法复杂，但是基本原理还是类似的。 2.2.4 独立成分分析 (ICA) ICA 是一门统计技术，用于发现存在于随机变量下的隐性因素。ICA 为给观测数据定义了一个生成模型。在这个模型中，其认为数据变量是由隐性变量，经一个混合系统线性混合而成，这个混合系统未知。并且假设潜在因素属于非高斯分布、并且相互独立，称之为可观测数据的独立成分。 2.2.5 EM 算法 EM 算法也称期望最大化（Expectation-Maximum, 简称 EM）算法。 它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）等等。 EM 算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步， 一个为期望步（E 步） 一个为极大步（M 步） 所以算法被称为 EM 算法（Expectation-Maximization Algorithm）。 EM 算法受到缺失思想影响，最初是为了 解决数据缺失情况下的参数估计问题 ，其算法基础和收敛有效性等问题在 Dempster、Laird 和 Rubin 三人于 1977 年所做的文章《Maximum likelihood from incomplete data via the EM algorithm》中给出了详细的阐述。其基本思想是： 首先根据己经给出的观测数据，估计出模型参数的值； 然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计； 然后反复迭代，直至最后收敛，迭代结束。 2.3 半监督学习 3 强化学习 强化学习又称再励学习、评价学习或增强学习。智能系统 从环境到行为映射 的学习，以使 奖励信号(强化信号)函数值最大 。如果 Agent 的某个行为策略导致环境正的奖赏 (强化信号)，那么 Agent 以后产生这个行为策略的趋势便会加强。强化学习四要素： 状态(state) 、 动作(action) 、 策略（policy） 、 奖励(reward) 。 3.1 概念解释 名词 解释 智能体 学习器与决策者的角色 环境 智能体之外一切组成的、与之交互的事物 动作 智能体的行为表征 状态 智能体从环境中获取的信息 奖励 环境对于动作的反馈 策略 智能体根据状态进行下一步动作的函数 状态转移概率 智能体做出动作后进入下一状态的概率 3.2 常见算法 Q-Learning 时间差学习（Temporal difference learning） 3.3 应用场景 动态系统 机器人控制","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"算法","slug":"算法","permalink":"https://leezhao415.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"搭建个人Web服务器_LAMP","slug":"搭建个人Web服务器-LAMP","date":"2021-04-26T11:33:24.000Z","updated":"2021-07-14T13:18:02.330Z","comments":true,"path":"2021/04/26/搭建个人Web服务器-LAMP/","link":"","permalink":"https://leezhao415.github.io/2021/04/26/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BAWeb%E6%9C%8D%E5%8A%A1%E5%99%A8-LAMP/","excerpt":"","text":"项目名称：搭建个人 Web 服务器_LAMP 文章目录 1 搭建模式： 2 搭建步骤 3 常见问题解决方案 1 搭建模式： LAMP，即 Linux 系统 +Apache 服务器 +MySQL+PHP 2 搭建步骤 【第一步】安装 Linux 系统 安装 Linux 系统的方法有两种： 【1】更换电脑操作系统或者安装双系统 【2】安装 VMware 虚拟机，并在虚拟机中安装 Ubuntu 系统 【第二步】安装 Apache 服务器 123456789# ① 更新资源包sudo apt update# ② 更新软件sudo apt upgrade# ③ 更新系统软件sudo apt dist-upgrade# ④ 安装Apache服务器sudo apt install apache2#碰到继续执行吗?[Y/n] 输入y,然后按回车即可。 操作 Apache 服务器相关指令 12345678910111213141516# 安装 systemctlsudo apt-get install --reinstall systemd# 检查服务器状态：systemctl status apache2# 开启服务器：systemctl start apache2 或者 sudo /etc/init.d/apache2 start# 关闭服务器：systemctl stop apache2 或者 sudo /etc/init.d/apache2 stop# 重启服务器：systemctl restart apache2 或者sudo /etc/init.d/apache2 测试 打开浏览器，在你的主机上输入你的 IP 地址，或者输入 127.0.0.1 或者输入 localhost。 安装成功后将会出现 Apache2 Ubuntu Default Page，即 Apache2 Ubuntu 默认页面。 在终端中寻找这个文件可输入指令: 1sudo find / -name index.html 【第三步】安装 MySQL 数据库 12345678# ① 安装之前先查看你的ubuntu系统有没有安装mysql数据库dpkg -l | grep mysql# ② 安装数据库apt install mysql-server# 同样碰到继续执行吗?[Y/n] 输入y,然后按回车即可。# ③ 检查数据库是否安装成功netstat -tap | grep mysql# 如果出现以下提示信息，则表示数据库安装成功 【第四步】安装 PHP PHP 添加了支持动态网页的服务器网页处理功能，是一种常见的后端的语言。 123456789101112# ① 安装PHPsudo apt install php# 同样碰到继续执行吗?[Y/n] 输入y,然后按回车即可。# ② 查看PHP版本指令php -v# ③ 在/var/www/html文件夹下新建一个文件，命名为index.phpsudo nano /var/www/html/index.php# ④ 在里面写入如下代码&lt;?php phpinfo();?&gt;# ⑤ 按Ctrl+o -&gt; 回车保存 -&gt; Ctrl+x 退出 打开浏览器，在地址栏输入 IP 地址 /index.php 或者输入 127.0.0.1/index.php 或者输入 localhost/index.php，安装正常会弹出 PHP 的默认信息页面。 【第五步】选择安装模组和软件 12345# phpMyAdmin的安装和使用sudo apt install phpmyadmin# 同样碰到继续执行吗?[Y/n] 输入y,然后按回车即可。# 后面会出现一些选择框，依次选择Apache2 -&gt; 是 -&gt; 设定密码 完成后在浏览器中输入：IP 地址 /phpmyadmin，正常情况下将出现下面的界面。 3 常见问题解决方案 Apache 服务器启动失败，80 端口被占用问题 12345678# 查看80端口情况sudo netstat -lnp|grep 80# 杀掉进程sudo kill -9 1123# 启动Apache服务器systemctl start apache2# 查看Apache服务器状态systemctl status apache2.service","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"机器学习库-Matplotlib+Numpy+Pandas","slug":"机器学习库-Matplotlib-Numpy-Pandas","date":"2021-04-23T09:35:35.000Z","updated":"2021-07-14T13:02:23.126Z","comments":true,"path":"2021/04/23/机器学习库-Matplotlib-Numpy-Pandas/","link":"","permalink":"https://leezhao415.github.io/2021/04/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%93-Matplotlib-Numpy-Pandas/","excerpt":"","text":"文章目录 机器学习库 - Matplotlib+Numpy+Pandas 1 Matplotlib 基本使用 1.2 用途：能将数据进行可视化，更直观的呈现；使数据更加客观、更具说服力； 1.3 操作指南 1.4 常见图形绘制 1.5 代码实现 2 Numpy 基本使用 2.1 定义 2.2 特点 2.3 代码实现 2.3.1 生成 0 和 1 的数组 2.3.2 拷贝 2.3.3 生成固定范围的数组 2.3.4 生成随机数 2.3.5 数组的索引、切片 2.3.6 形状修改 2.3.7 类型修改 2.3.8 数组去重 2.3.9 逻辑运算 2.3.10 通用判断函数 3 Pandas 基本使用 3.1 定义 3.2 特点 3.3 操作 3.4 Numpy 和 Pandas 区别及联系 3.5 Series 和 Dataframe 区别 3.6 基本数据操作 3.7 DataFram 运算 3.8 Pandas 画图 3.10 缺失值处理 3.11 数据离散化 3.13 高级处理 - 交叉表与透视表 3.14 高级处理 - 分组与聚合 机器学习库 - Matplotlib+Numpy+Pandas 1 Matplotlib 基本使用 官方文档：https://matplotlib.org/stable/api/index.html#modules 1.1 特性 强大的数据可视化工具和作图库，是主要用于绘制数据图表的 Python 库，提供了绘制各类可视化图形的命令字库、简单的接口，可以方便用户轻松掌握图形的格式，绘制各类可视化图形。 基于 Numpy 的一套 Python 包，这个包提供了吩咐的数据绘图工具，主要用于绘制一些统计图形。 有一套允许定制各种属性的默认设置，可以控制 Matplotlib 中的每一个默认属性：图像大小、每英寸点数、线宽、色彩和样式、子图、坐标轴、网格属性、文字和文字属性。 1.2 用途：能将数据进行可视化，更直观的呈现；使数据更加客观、更具说服力； 1.3 操作指南 1.3.1 图像结构 12345678910import matplotlib.pyplot as plt# 1.创建画布plt.figure(figsize=(10, 10), dpi=100)# 2.绘制折线图plt.plot([1, 2, 3, 4, 5, 6 ,7], [17,17,18,15,11,11,13])# 3.显示图像plt.show() 1.3.2 中文设置问题 解决方案一： 下载中文字体（黑体，看准系统版本） 步骤一：下载 simhei 字体（或者其他的支持中文显示的字体也行） 步骤二：安装字体 linux 下：拷贝字体到 usr/share/fonts 下： 1sudo cp ~&#x2F;SimHei.ttf &#x2F;usr&#x2F;share&#x2F;fonts&#x2F;SimHei.ttf windows 和 mac 下：双击安装 步骤三：删除～/.matplotlib 中的缓存文件 12cd ~/.matplotlibrm -r * 步骤四：修改配置文件 matplotlibrc 1vi ~/.matplotlib/matplotlibrc 将文件内容修改为： 123font.family : sans-seriffont.sans-serif : SimHeiaxes.unicode_minus : False 解决方案二： 在Python脚本中动态设置matplotlibrc,这样也可以避免由于更改配置文件而造成的麻烦，具体代码如下： 123from pylab import mpl# 设置显示中文字体mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;] 有时候，字体更改后，会导致坐标轴中的部分字符无法正常显示，此时需要更改 axes.unicode_minus 参数： 12# 设置正常显示符号mpl.rcParams[&quot;axes.unicode_minus&quot;] = False 1.3.3 图像绘制 plt.plot(x, y, color='r', linestyle='--') 刻度显示 plt.xticks(刻度值，标签值) plt.yticks(刻度值，标签值) 注意：在传递进去的第一个参数必须是数字，不能是字符串，如果是字符串，需要进行替换操作 添加网格显示 plt.grid(True, linestyle=&quot;--&quot;, alpha=0.5) 设置标题 plt.title('标题'，fontsize=20, pad=20) 设置轴标签 plt.xlabel() plt.ylabel() 设置轴取值范围 plt.xlim(a, b) plt.ylim(a, b) 图像保存 plt.savefig(&quot;路径&quot;) 注意：plt.show () 会释放 figure 资源，如果在显示图像之后保存图片将只能保存空图片。 显示图例 plt.legend(loc=&quot;best&quot;) 多个坐标系显示 plt.subplots(nrows=, ncols=, figsize=(20, 8), dpi=100) 12345678910111213141516171819202122232425262728293031# 1.创建画布fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), dpi=100)# 2.绘制图像axes[1].plot(x, y_beijing, color=&quot;r&quot;, linestyle=&quot;--&quot;, label=&quot;北京&quot;)# 2.1 添加x,y轴刻度x_ticks_label = [&quot;11点&#123;&#125;分&quot;.format(i) for i in x]y_ticks = range(40)# 刻度显示axes[1].set_xticks(x[::5])axes[1].set_yticks(y_ticks[::5])axes[1].set_xticklabels(x_ticks_label[::5])# 2.2 添加网格显示axes[1].grid(True, linestyle=&quot;--&quot;, alpha=0.5)# 2.3 添加描述信息axes[1].set_xlabel(&quot;时间&quot;)axes[1].set_ylabel(&quot;温度&quot;)axes[1].set_title(&quot;中午11点--12点某城市温度变化图&quot;, fontsize=20)# 2.4 图像保存plt.savefig(&quot;./test.png&quot;)# 2.5 添加图例axes[1].legend(loc=0)# 3.图像显示plt.show() 1.4 常见图形绘制 1 折线图 定义：以折线的上升或下降来表示统计数量的增减变化的统计图 特点：能够显示数据的变化趋势，反映事物的变化情况。(变化) API： plt.plot(x, y) 2 散点图 定义：用两组数据构成多个坐标点，考察坐标点的分布，判断两变量之间是否存在某种关联或总结坐标点的分布模式。 特点：判断变量之间是否存在数量关联趋势，展示离群点 (分布规律) API： plt.scatter(x, y) 代码实现 12345678910111213141516# 0.准备数据x = [225.98, 247.07, 253.14, 457.85, 241.58, 301.01, 20.67, 288.64, 163.56, 120.06, 207.83, 342.75, 147.9 , 53.06, 224.72, 29.51, 21.61, 483.21, 245.25, 399.25, 343.35]y = [196.63, 203.88, 210.75, 372.74, 202.41, 247.61, 24.9 , 239.34, 140.32, 104.15, 176.84, 288.23, 128.79, 49.64, 191.74, 33.1 , 30.74, 400.02, 205.35, 330.64, 283.45]# 1.创建画布plt.figure(figsize=(20, 8), dpi=100)# 2.绘制散点图plt.scatter(x, y)# 3.显示图像plt.show() 3 柱状图 定义：排列在工作表的列或行中的数据可以绘制到柱状图中。 特点：绘制连离散的数据，能够一眼看出各个数据的大小，比较数据之间的差别。(统计 / 对比) API： plt.bar(x, width, align='center', color= 'r') 12align : 每个柱状图的位置对齐方式 &#123;‘center’, ‘edge’&#125; 默认： ‘center’ 代码实现 12345678910111213141516171819202122232425# 0.准备数据# 电影名字movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;东方快车谋杀案&#x27;,&#x27;寻梦环游记&#x27;,&#x27;全球风暴&#x27;,&#x27;降魔传&#x27;,&#x27;追捕&#x27;,&#x27;七十七天&#x27;,&#x27;密战&#x27;,&#x27;狂兽&#x27;,&#x27;其它&#x27;]# 横坐标x = range(len(movie_name))# 票房数据y = [73853,57767,22354,15969,14839,8725,8716,8318,7916,6764,52222]# 1.创建画布plt.figure(figsize=(20, 8), dpi=100)# 2.绘制柱状图plt.bar(x, y, width=0.5, color=[&#x27;b&#x27;,&#x27;r&#x27;,&#x27;g&#x27;,&#x27;y&#x27;,&#x27;c&#x27;,&#x27;m&#x27;,&#x27;y&#x27;,&#x27;k&#x27;,&#x27;c&#x27;,&#x27;g&#x27;,&#x27;b&#x27;])# 2.1b修改x轴的刻度显示plt.xticks(x, movie_name)# 2.2 添加网格显示plt.grid(linestyle=&quot;--&quot;, alpha=0.5)# 2.3 添加标题plt.title(&quot;电影票房收入对比&quot;)# 3.显示图像plt.show() 4 直方图 定义：由一系列高度不等的纵向条纹或线段表示数据分布的情况。 一般用横轴表示数据范围，纵轴表示分布情况。 特点：绘制连续性的数据展示一组或者多组数据的分布状况 (统计) API： plt.hist(x, bins=None) 1bins : 组距 5 饼图 定义：用于表示不同分类的占比情况，通过弧度大小来对比各种分类。 特点：分类数据的占比情况 (占比) API： plt.pie(x, labels=,autopct=,colors) 1234x:数量，自动算百分比labels:每部分名称autopct:占比显示指定%1.2f%%colors:每部分颜色 1.5 代码实现 一个坐标系中绘制多个图像 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import matplotlib.pyplot as pltimport random# from pylab import mpl## 设置显示中文字体# mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]## 设置正常显示符号# mpl.rcParams[&quot;axes.unicode_minus&quot;] = False# 0.准备数据x = range(60)y_shanghai = [random.uniform(15, 18) for i in x]y_beijing = [random.uniform(1,3) for i in x]# 1.创建画布plt.figure(figsize=(20, 8), dpi=100)# 2.绘制图像plt.plot(x, y_shanghai, label=&quot;上海&quot;)plt.plot(x, y_beijing, color=&quot;r&quot;, linestyle=&quot;--&quot;, label=&quot;北京&quot;)# 2.1 添加x,y轴刻度# 构造x,y轴刻度标签x_ticks_label = [&quot;11点&#123;&#125;分&quot;.format(i) for i in x]y_ticks = range(40)# 刻度显示plt.xticks(x[::5], x_ticks_label[::5])plt.yticks(y_ticks[::5])# 2.2 添加网格显示plt.grid(True, linestyle=&quot;--&quot;, alpha=0.5)# 2.3 添加描述信息plt.xlabel(&quot;时间&quot;)plt.ylabel(&quot;温度&quot;)plt.title(&quot;中午11点--12点某城市温度变化图&quot;, fontsize=20)# 2.4 图像保存plt.savefig(&quot;./test.png&quot;)# 2.5 添加图例plt.legend(loc=0)# 3.图像显示plt.show() 多个坐标系显示 — plt.subplots (面向对象的画图方法) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import matplotlib.pyplot as pltimport random# 0.准备数据x = range(60)y_shanghai = [random.uniform(15, 18) for i in x]y_beijing = [random.uniform(1, 5) for i in x]# 1.创建画布# plt.figure(figsize=(20, 8), dpi=100)fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), dpi=100)# 2.绘制图像# plt.plot(x, y_shanghai, label=&quot;上海&quot;)# plt.plot(x, y_beijing, color=&quot;r&quot;, linestyle=&quot;--&quot;, label=&quot;北京&quot;)axes[0].plot(x, y_shanghai, label=&quot;上海&quot;)axes[1].plot(x, y_beijing, color=&quot;r&quot;, linestyle=&quot;--&quot;, label=&quot;北京&quot;)# 2.1 添加x,y轴刻度# 构造x,y轴刻度标签x_ticks_label = [&quot;11点&#123;&#125;分&quot;.format(i) for i in x]y_ticks = range(40)# 刻度显示# plt.xticks(x[::5], x_ticks_label[::5])# plt.yticks(y_ticks[::5])axes[0].set_xticks(x[::5])axes[0].set_yticks(y_ticks[::5])axes[0].set_xticklabels(x_ticks_label[::5])axes[1].set_xticks(x[::5])axes[1].set_yticks(y_ticks[::5])axes[1].set_xticklabels(x_ticks_label[::5])# 2.2 添加网格显示# plt.grid(True, linestyle=&quot;--&quot;, alpha=0.5)axes[0].grid(True, linestyle=&quot;--&quot;, alpha=0.5)axes[1].grid(True, linestyle=&quot;--&quot;, alpha=0.5)# 2.3 添加描述信息# plt.xlabel(&quot;时间&quot;)# plt.ylabel(&quot;温度&quot;)# plt.title(&quot;中午11点--12点某城市温度变化图&quot;, fontsize=20)axes[0].set_xlabel(&quot;时间&quot;)axes[0].set_ylabel(&quot;温度&quot;)axes[0].set_title(&quot;中午11点--12点某城市温度变化图&quot;, fontsize=20)axes[1].set_xlabel(&quot;时间&quot;)axes[1].set_ylabel(&quot;温度&quot;)axes[1].set_title(&quot;中午11点--12点某城市温度变化图&quot;, fontsize=20)# # 2.4 图像保存plt.savefig(&quot;./test.png&quot;)# # 2.5 添加图例# plt.legend(loc=0)axes[0].legend(loc=0)axes[1].legend(loc=0)# 3.图像显示plt.show() 2 Numpy 基本使用 官方文档：https://numpy.org/doc/stable/reference/index.html 2.1 定义 源的 Python 科学计算库，用于快速处理任意维度的数组。 2.2 特点 支持常见的数组和矩阵操作 使用 ndarray 对象来处理多维数组 Numpy 比 Python List 区别： Python List 是一个异构（不同类型的数据）的容器 Numpy 存储的数据是同质的，类型需相同。 Numpy 比 Python List 效率高： 释放GIL ：GIL 限制了 Python 线程的并行能力，但 Numpy 不存在此问题。 C语言实现 并行计算 ：numpy 内置了并行运算功能，当系统有多个核心时，做某种计算时，numpy 会自动做并行计算 内存结构 ： ndarray 属性：反映了数组本身固有的信息 属性名字 属性解释 ndarray.shape 数组维度的元组 (行，列，维度) ndarray.ndim 数组维数 ndarray.size 数组中的元素数量 ndarray.itemsize 一个数组元素的长度（字节） ndarray.dtype 数组元素的类型 1234# Numpy的常量np.Inf 表示无穷大np.nan 表示非数字np.pi 形状 12345678# 创建不同形状的数组&gt;&gt;&gt; a &#x3D; np.array([[1,2,3],[4,5,6]])&gt;&gt;&gt; b &#x3D; np.array([1,2,3,4])&gt;&gt;&gt; c &#x3D; np.array([[[1,2,3],[4,5,6]],[[1,2,3],[4,5,6]]])&gt;&gt;&gt; a.shape&gt;&gt;&gt; b.shape&gt;&gt;&gt; c.shape 类型 123# dtype是numpy.dtype类型&gt;&gt;&gt; type(score.dtype)&lt;type &#x27;numpy.dtype&#x27;&gt; 名称 描述 简写 np.bool 用一个字节存储的布尔类型（True 或 False） ‘b’ np.int8 一个字节大小，-128 至 127 ‘i’ np.int16 整数，-32768 至 32767 ‘i2’ np.int32 整数，-2^31 至 2^32 -1 ‘i4’ np.int64 整数，-2^63 至 2^63 - 1 ‘i8’ np.uint8 无符号整数，0 至 255 ‘u’ np.uint16 无符号整数，0 至 65535 ‘u2’ np.uint32 无符号整数，0 至 2^32 - 1 ‘u4’ np.uint64 无符号整数，0 至 2^64 - 1 ‘u8’ np.float16 半精度浮点数：16 位，正负号 1 位，指数 5 位，精度 10 位 ‘f2’ np.float32 单精度浮点数：32 位，正负号 1 位，指数 8 位，精度 23 位 ‘f4’ np.float64 双精度浮点数：64 位，正负号 1 位，指数 11 位，精度 52 位 ‘f8’ np.complex64 复数，分别用两个 32 位浮点数表示实部和虚部 ‘c8’ np.complex128 复数，分别用两个 64 位浮点数表示实部和虚部 ‘c16’ np.object_ python 对象 ‘O’ np.string_ 字符串 ‘S’ np.unicode_ unicode 类型 ‘U’ 注意：若不指定，整数默认 int64，小数默认 float64 2.3 代码实现 1234567&gt;&gt;&gt; a = np.array([[1, 2, 3],[4, 5, 6]], dtype=np.float32)&gt;&gt;&gt; a.dtypedtype(&#x27;float32&#x27;)&gt;&gt;&gt; arr = np.array([&#x27;python&#x27;, &#x27;tensorflow&#x27;, &#x27;scikit-learn&#x27;, &#x27;numpy&#x27;], dtype = np.string_)&gt;&gt;&gt; arrarray([b&#x27;python&#x27;, b&#x27;tensorflow&#x27;, b&#x27;scikit-learn&#x27;, b&#x27;numpy&#x27;], dtype=&#x27;|S12&#x27;) ’|S12’ 介绍 ‘|’ 表示字节顺序（不考虑顺序，&lt; （小端）） ‘S’ 表示字符串类型 ‘12’ 表示数组中每个值最多包含 12 个字节或字符 2.3.1 生成 0 和 1 的数组 np.ones(shape, dtype) 创建全是 1 的数组 np.ones_like(a, dtype) 根据其它数列形状创建全是 1 的数组 np.zeros(shape, dtype) 创建全是 0 的数组 np.zeros_like(a, dtype) 根据其它数列形状创建全是 0 的数组 12345678910111213# np.ones()&gt;&gt;&gt;ones = np.ones([4,8])ones# np.ones_like()&gt;&gt;&gt; x = np.arange(6)&gt;&gt;&gt; x = x.reshape((2, 3))&gt;&gt;&gt; xarray([[0, 1, 2], [3, 4, 5]])&gt;&gt;&gt; np.ones_like(x)array([[1, 1, 1], [1, 1, 1]]) 2.3.2 拷贝 np.array(object, dtype) 深拷贝 np.asarray(a, dtype) 浅拷贝 12345a = np.array([[1,2,3],[4,5,6]])# 从现有的数组当中创建a1 = np.array(a)# 相当于索引的形式，并没有真正的创建一个新的a2 = np.asarray(a) 2.3.3 生成固定范围的数组 np.linspace (start, stop, num, endpoint) 等差数列 num : 要生成的等间隔样例数量，默认为 50 endpoint : 序列中是否包含 stop 值，默认为 ture np.arange(start,stop, step, dtype) 等差数列 step : 步长，默认值为 1 np.logspace(start,stop, num, [base]) 等比数列 start ：float 类型，基底 base 的 start 次幂作为左边界 stop ：float 类型，基底 base 的 stop 次幂作为右边界 num : 要生成的等比数列数量，默认为 50 base ：float 类型，选填，基底，默认 10 2.3.4 生成随机数 正态分布是一种概率分布。正态分布是具有两个参数 μ 和 σ 的连续型随机变量的分布，第一参数 μ 是服从正态分布的随机变量的均值，第二个参数 σ 是此随机变量的标准差，所以正态分布记作 N(μ，σ ) 。 np.random 模块 正态分布 np.random.randn(d0, d1, …, dn) 功能：从标准正态分布中返回一个或多个样本值 np.random.normal(loc=0.0, scale=1.0, size=None) np.random.standard_normal(size=None) 功能：返回指定形状的标准正态分布的数组。 1234567891011121314import numpy as np#randn(d0, d1, ..., dn)#standard_normal(size=None)#normal(loc=0.0, scale=1.0, size=None)a=np.random.randn(3,2) #三行两列的二维数组b=np.random.standard_normal(size=[3,2]) #三行两列的二维数组c=np.random.normal(2,3,size=[3,2]) #均值为2，标准差为3的三行两列的二维数组print(a)print(&#x27;------&#x27;)print(b)print(&#x27;------&#x27;)print(c) 均匀分布 np.random.rand(d0, d1, *...*, dn) 功能：返回 **[0.0，1.0)** 内的一组均匀分布的数。 123&gt;&gt;&gt;np.random.rand(2,4)array([[0.45203892, 0.52319708, 0.50009652, 0.34814468], [0.49214062, 0.77874009, 0.96667426, 0.94950402]]) np.random.uniform(low=0.0, high=1.0, size=None) 功能：从一个均匀分布 [low,high) 中随机采样，注意定义域是左闭右开，即包含 low，不包含 high. low: 采样下界，float 类型，默认值为 0； high: 采样上界，float 类型，默认值为 1； size: 输出样本数目，为 int 或元组 (tuple) 类型，例如，size=(m,n,k), 则输出 mnk 个样本，缺省时输出 1 个值。 12345678&gt;&gt;&gt;np.random.uniform(1, 4, size = (2,3,4)) array([[[3.55647219, 1.6756231 , 3.60677788, 2.3745676 ], [3.20914052, 1.63670762, 3.68718528, 1.25277517], [3.43555004, 3.76016213, 1.87874796, 2.41035426]], [[3.10438403, 3.35162098, 3.42743771, 3.25322821], [3.48925817, 2.5992673 , 2.89616745, 3.72254566], [3.00571278, 3.66503032, 3.9064071 , 2.84347013]]]) np.random.randint(low, high=None, size=None, dtype='l') 功能：从一个均匀分布中随机采样，生成一个整数或 N 维整数数组。 取数范围：若 high 不为 None 时，取 [low,high) 之间随机整数，否则取值 [0,low) 之间随机整数。 12345678&gt;&gt;&gt;np.random.randint(1, 4, size = (2,3,4))array([[[1, 1, 1, 3], [1, 1, 2, 3], [1, 2, 1, 2]], [[2, 2, 3, 3], [2, 1, 2, 2], [2, 2, 3, 2]]]) 随机整数数组 np.random.randint(low, high=None, size=None, dtype=int) 123456a = np.random.randint(3)b = np.random.randint(4,size=6)c = np.random.randint(2,size=(2,3))print(a)print(b)print(c) 2.3.5 数组的索引、切片 切片赋值 12345678hexo n &quot;博客的文件名&quot;&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; arr=np.arange(10)&gt;&gt;&gt; arr_slice=arr[5:8]&gt;&gt;&gt; arr_slice[0]=-1&gt;&gt;&gt; arr_slicearray([-1, 6, 7])&gt;&gt;&gt; arrarray([ 0, 1, 2, 3, 4, -1, 6, 7, 8, 9]) 给数组所有元素赋值 123&gt;&gt;&gt; arr[:]=-1&gt;&gt;&gt; arrarray([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]) 使用 copy 方法复制 12345678&gt;&gt;&gt; arr_copy=arr[:].copy()&gt;&gt;&gt; arr_copyarray([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1])&gt;&gt;&gt; arr_copy[:]=0&gt;&gt;&gt; arr_copyarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])&gt;&gt;&gt; arrarray([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]) 高阶数组索引 12345678&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; arr2d=np.array([[1,2,3],[4,5,6],[7,8,9]])&gt;&gt;&gt; arr2d[2]array([7, 8, 9])&gt;&gt;&gt; arr2d[0][2]3&gt;&gt;&gt; arr2d[0,2]3 123456789101112131415161718192021222324252627&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; arr3d=np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])&gt;&gt;&gt; arr3darray([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; arr3d[0]array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; old_values=arr3d[0].copy()&gt;&gt;&gt; arr3d[0]=42&gt;&gt;&gt; arr3darray([[[42, 42, 42], [42, 42, 42]], [[ 7, 8, 9], [10, 11, 12]]])&gt;&gt;&gt; arr3d[1,0]array([7, 8, 9])&gt;&gt;&gt; x=arr3d[1]&gt;&gt;&gt; xarray([[ 7, 8, 9], [10, 11, 12]])&gt;&gt;&gt; x[0]array([7, 8, 9]) 高维数组切片 123456789101112131415&gt;&gt;&gt; arr2d=np.array([[1,2,3],[4,5,6],[7,8,9]])&gt;&gt;&gt; arr2d[:2]array([[1, 2, 3], [4, 5, 6]])&gt;&gt;&gt; arr2d[:2,1:]array([[2, 3], [5, 6]])&gt;&gt;&gt; arr2d[1,:2]array([4, 5])&gt;&gt;&gt; arr2d[:2,2]array([3, 6])&gt;&gt;&gt; arr2d[:,:1]array([[1], [4], [7]]) 布尔型索引 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; names = np.array([&#x27;Bob&#x27;, &#x27;Joe&#x27;, &#x27;Will&#x27;, &#x27;Bob&#x27;, &#x27;Will&#x27;, &#x27;Joe&#x27;, &#x27;Joe&#x27;])&gt;&gt;&gt; data=np.random.randn(7,4)#7行4列正太分布随机数组&gt;&gt;&gt; namesarray([&#x27;Bob&#x27;, &#x27;Joe&#x27;, &#x27;Will&#x27;, &#x27;Bob&#x27;, &#x27;Will&#x27;, &#x27;Joe&#x27;, &#x27;Joe&#x27;], dtype=&#x27;&lt;U4&#x27;)&gt;&gt;&gt; dataarray([[ 0.24724057, 2.86939948, -0.82061782, -0.65745818], [-0.98602372, -0.69305692, -1.44431904, -0.85490816], [-0.73613349, 0.12700976, -1.00588979, 1.10646269], [ 1.59110894, 1.68597758, 0.39414277, 2.02308399], [-1.05607115, -0.50354292, -0.65820553, -0.77610316], [ 1.72237936, -0.07726577, 1.63462647, -0.41943148], [ 0.66744687, -1.01756773, -0.59254343, 0.19080575]])# 选出对应于名字&quot;Bob&quot;的所有行,对names和字符串&quot;Bob&quot;的比较运算将会产生一个布尔型数组。&gt;&gt;&gt; names==&#x27;Bob&#x27;array([ True, False, False, True, False, False, False])# 获取等于&#x27;Bob&#x27;的行&gt;&gt;&gt; data[names==&#x27;Bob&#x27;]array([[ 0.24724057, 2.86939948, -0.82061782, -0.65745818], [ 1.59110894, 1.68597758, 0.39414277, 2.02308399]])# 获取不同于&#x27;Bob&#x27;的行&gt;&gt;&gt; data[names!=&#x27;Bob&#x27;]array([[-0.98602372, -0.69305692, -1.44431904, -0.85490816], [-0.73613349, 0.12700976, -1.00588979, 1.10646269], [-1.05607115, -0.50354292, -0.65820553, -0.77610316], [ 1.72237936, -0.07726577, 1.63462647, -0.41943148], [ 0.66744687, -1.01756773, -0.59254343, 0.19080575]])# 对布尔索引进行列索引&gt;&gt;&gt; data[names==&#x27;Bob&#x27;,2:]array([[-0.82061782, -0.65745818], [ 0.39414277, 2.02308399]])&gt;&gt;&gt; data[names==&#x27;Bob&#x27;,3]array([-0.65745818, 2.02308399])# 反转条件符&gt;&gt;&gt; cond=names==&#x27;Will&#x27;&gt;&gt;&gt; condarray([False, False, True, False, True, False, False])&gt;&gt;&gt; data[~cond]array([[ 0.24724057, 2.86939948, -0.82061782, -0.65745818], [-0.98602372, -0.69305692, -1.44431904, -0.85490816], [ 1.59110894, 1.68597758, 0.39414277, 2.02308399], [ 1.72237936, -0.07726577, 1.63462647, -0.41943148], [ 0.66744687, -1.01756773, -0.59254343, 0.19080575]])# 布尔条件的运算，连接符还有|、&amp;之类&gt;&gt;&gt; mask=(names==&#x27;Bob&#x27;)|(names==&#x27;Will&#x27;)&gt;&gt;&gt; maskarray([ True, False, True, True, True, False, False])&gt;&gt;&gt; data[mask]array([[ 0.24724057, 2.86939948, -0.82061782, -0.65745818], [-0.73613349, 0.12700976, -1.00588979, 1.10646269], [ 1.59110894, 1.68597758, 0.39414277, 2.02308399], [-1.05607115, -0.50354292, -0.65820553, -0.77610316]])# 普通条件选取&gt;&gt;&gt; data[data&lt;0]=0&gt;&gt;&gt; dataarray([[0.24724057, 2.86939948, 0. , 0. ], [0. , 0. , 0. , 0. ], [0. , 0.12700976, 0. , 1.10646269], [1.59110894, 1.68597758, 0.39414277, 2.02308399], [0. , 0. , 0. , 0. ], [1.72237936, 0. , 1.63462647, 0. ], [0.66744687, 0. , 0. , 0.19080575]])# 布尔条件选取&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; names = np.array([&#x27;Bob&#x27;, &#x27;Joe&#x27;, &#x27;Will&#x27;, &#x27;Bob&#x27;, &#x27;Will&#x27;, &#x27;Joe&#x27;, &#x27;Joe&#x27;])&gt;&gt;&gt; data=np.random.randn(7,4)#7行4列正太分布随机数组&gt;&gt;&gt; dataarray([[-1.24077681, -0.48320904, 1.22145611, 0.00666619], [-0.65078721, -0.03482355, 1.74232625, 0.2979584 ], [-1.51669752, 2.04245014, 0.09453898, -0.85531867], [-1.51334497, 0.36947066, -0.87016919, 1.35107873], [-1.11285867, -2.20906849, 0.38269412, 1.85375798], [ 0.95132554, -1.54193589, 1.98741745, -0.60608077], [ 0.78902133, 1.41593836, 0.09430052, -0.25057659]])&gt;&gt;&gt; data[names!=&#x27;Joe&#x27;]=7&gt;&gt;&gt; dataarray([[ 7. , 7. , 7. , 7. ], [-0.65078721, -0.03482355, 1.74232625, 0.2979584 ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 0.95132554, -1.54193589, 1.98741745, -0.60608077], [ 0.78902133, 1.41593836, 0.09430052, -0.25057659]]) 花式索引 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 传入单个索引数组&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; arr=np.empty((8,4))#创建8行4列内容为随机值的数组&gt;&gt;&gt; arrarray([[2.65577744e-260, 7.70858946e+218, 6.01334668e-154, 4.47593816e-091], [7.01413727e-009, 2.96905203e+222, 2.11672643e+214, 4.56532297e-085], [4.78409596e+180, 2.44001263e-152, 2.45981714e-154, 6.83528875e+212], [6.14829725e-071, 1.05161522e-153, 1.05135742e-153, 2.43902457e-154], [4.83245960e+276, 6.03103052e-154, 7.06652000e-096, 2.65862875e-260], [1.76380220e+241, 2.30576063e-310, 9.80013217e+040, 1.55850644e-312], [1.33360318e+241, 4.09842267e-310, 2.48721655e-075, 1.04922745e-312], [1.91217285e-309, 1.18182126e-125, 6.57144273e-299, 5.54240979e-302]])&gt;&gt;&gt; for i in range(8): arr[i]=i&gt;&gt;&gt; arrarray([[0., 0., 0., 0.], [1., 1., 1., 1.], [2., 2., 2., 2.], [3., 3., 3., 3.], [4., 4., 4., 4.], [5., 5., 5., 5.], [6., 6., 6., 6.], [7., 7., 7., 7.]])&gt;&gt;&gt; arr[[4,3,0,6]]#选特定的索引下标,选取第4，3，0，6行array([[4., 4., 4., 4.], [3., 3., 3., 3.], [0., 0., 0., 0.], [6., 6., 6., 6.]])&gt;&gt;&gt; arr[[-3,-5,-7]]#选择特定的索引下标，选取第-3,-5,-7行array([[5., 5., 5., 5.], [3., 3., 3., 3.], [1., 1., 1., 1.]])# 传入多个索引数组&gt;&gt;&gt; arr=np.arange(32).reshape((8,4))&gt;&gt;&gt; arrarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]])&gt;&gt;&gt; arr[[1,5,7,2],[0,3,1,2]]#选取(1,0),(5,3),(7,1),(2,2)对应元素array([ 4, 23, 29, 10])&gt;&gt;&gt; arr[[1,5,7,2]][:,[0,3,1,2]]#先选取第1,5,7,2行，再将每行按照0,3,1,2这个顺序交换array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) 2.3.6 形状修改 ndarray.reshape(shape, order) 返回一个具有相同数据域，但 shape 不一样的视图 行、列不进行互换 123# 在转换形状的时候，一定要注意数组的元素匹配stock_change.reshape([5, 4])stock_change.reshape([-1,10]) # 数组的形状被修改为: (2, 10), -1: 表示通过待计算 ndarray.resize(new_shape) 修改数组本身的形状（需要保持元素个数前后相同） 行、列不进行互换 1234stock_change.resize([5, 4])# 查看修改后结果stock_change.shape(5, 4) ndarray.T 数组的转置 将数组的行、列进行互换 12stock_change.T.shape(4, 5) 2.3.7 类型修改 ndarray.astype(type) 返回修改了类型之后的数组 1stock_change.astype(np.int32) ndarray.tobytes([order]) 构造包含数组中原始数据字节的 Python 字节 12arr = np.array([[[1, 2, 3], [4, 5, 6]], [[12, 3, 34], [5, 6, 7]]])arr.tobytes() 常见问题：jupyter 输出太大可能导致崩溃问题 12345IOPub data rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_data_rate_limit`. 解决方案 （1）创建配置文件 12jupyter notebook --generate-configvi ~/.jupyter/jupyter_notebook_config.py （2）取消注释，修改限制 123## (bytes/sec) Maximum rate at which messages can be sent on iopub before they# are limited.c.NotebookApp.iopub_data_rate_limit = 10000000 2.3.8 数组去重 np.unique() 123temp = np.array([[1, 2, 3, 4],[3, 4, 5, 6]])&gt;&gt;&gt; np.unique(temp)array([1, 2, 3, 4, 5, 6]) 2.3.9 逻辑运算 利用布尔索引进行运算，提取满足布尔条件的值或者给满足条件的值进行赋值等。 2.3.10 通用判断函数 np.all() 123# 判断前两名同学的成绩[0:2, :]是否全及格&gt;&gt;&gt; np.all(score[0:2, :] &gt; 60)False np.any() 123# 判断前两名同学的成绩[0:2, :]是否有大于90分的&gt;&gt;&gt; np.any(score[0:2, :] &gt; 80)True 2.3.11 三元运算符 np.where() 123456789# 判断前四名学生,前四门课程中，成绩中大于60的置为1，否则为0temp = score[:4, :4]np.where(temp &gt; 60, 1, 0)# 判断前四名学生,前四门课程中，成绩中大于60且小于90的换为1，否则为0np.where(np.logical_and(temp &gt; 60, temp &lt; 90), 1, 0)# 判断前四名学生,前四门课程中，成绩中大于90或小于60的换为1，否则为0np.where(np.logical_or(temp &gt; 90, temp &lt; 60), 1, 0) 2.3.12 统计运算 min (a, axis) # 最小值 max (a, axis]) # 最大值 median (a, axis) # 中位数 mean (a, axis, dtype) # 均值 std (a, axis, dtype) # 标准差 var (a, axis, dtype) # 方差 argmin (a, axis) # 对应最小值数据的下标 argmin (a, axis) # 对应最大值数据的下标 2.3.13 数组间的运算 广播机制 ：数组在进行矢量化运算时，要求数组的形状是相等的。当形状不相等的数组执行算术运算的时候，就会出现广播机制，该机制会对数组进行扩展，使数组的 shape 属性值一样，以进行矢量化运算。 2.3.14 矩阵运算 matrix 和 array 区别 矩阵必须是 2维 的 array 可以是 多维 的 矩阵运算规则 创建矩阵 np.matrix(np.random.randint(0, 10, (2, 5))) np.identity(n) # 创建 n 维单位矩阵 矩阵操作 array1 * array2 # 矩阵乘法 array ** -1 # 矩阵转置 矩阵 API np.matmul (a,b) #矩阵乘法，禁止矩阵与标量进行计算 np.dot (a,b) # 矩阵乘法，可以进行矩阵与标量的计算 3 Pandas 基本使用 官方文档：https://pandas.pydata.org/docs/reference/index.html 3.1 定义 python+data+analysis 的组合缩写 (另一种解释为： pandas = panel + dataframe + series )，是 python 中基于 numpy 和 matplotlib 的第三方数据分析库，与后两者共同构成了 python 数据分析的基础工具包，享有数分三剑客之名。有数据分析界的瑞士军刀之称。 3.2 特点 增强图表可读性 常用的数据分析与统计功能，包括基本统计量、分组统计分析等 便捷的数据处理能力 类比 SQL 的 join 和 groupby 功能，pandas 可以很容易实现 SQL 这两个核心功能，实际上，SQL 的绝大部分 DQL 和 DML 操作在 pandas 中都可以实现 类比 Excel 的数据透视表功能，Excel 中最为强大的数据分析工具之一是数据透视表，这在 pandas 中也可轻松实现 自带正则表达式的字符串向量化操作，对 pandas 中的一列字符串进行通函数操作，而且自带正则表达式的大部分接口 丰富的时间序列向量化处理接口 读取文件方便 按索引匹配的广播机制，这里的广播机制与 numpy 广播机制还有很大不同 便捷的数据读写操作，相比于 numpy 仅支持数字索引，pandas 的两种数据结构均支持标签索引，包括 bool 索引也是支持的 封装了 Matplotlib、Numpy 的画图和计算 集成 matplotlib 的常用可视化接口，无论是 series 还是 dataframe，均支持面向对象的绘图接口 3.3 操作 Series ：一维数组结构，主要由一组数据和与之相关的索引两部分构成。 1234567891011# 导入pandasimport pandas as pd# API结构pd.Series(data=None, index=None, dtype=None)pd.Series(np.arange(10))pd.Series([6.7,5.6,3,10,2], index=[1,2,3,4,5])color_count = pd.Series(&#123;&#x27;red&#x27;:100, &#x27;blue&#x27;:200, &#x27;green&#x27;: 500, &#x27;yellow&#x27;:1000&#125;) pd.Series.data # 获得数据 pd.Series.index # 获得索引 pd.Series.value # 获得值 pd.Series.dtype # 获得类型 DataFrame ：二维数组，既有行索引，又有列索引。 12345678910111213141516171819# 导入pandasimport pandas as pd# 行索引，表明不同行，横向索引，叫index，0轴，axis=0# 列索引，表名不同列，纵向索引，叫columns，1轴，axis=1pd.DataFrame(data=None, index=None, columns=None)pd.DataFrame(np.random.randn(2,3))pd.DataFrame(np.random.randint(0, 10, (10, 10)))# 构造行索引序列subjects = [&quot;语文&quot;, &quot;数学&quot;, &quot;英语&quot;, &quot;政治&quot;, &quot;体育&quot;]# 构造列索引序列stu = [&#x27;同学&#x27; + str(i) for i in range(score_df.shape[0])]# 添加行列索引data = pd.DataFrame(score, columns=subjects, index=stu) pd.DataFrame.shape # 数组结构 pd.DataFrame.index # 数组行索引列表 pd.DataFrame.colums # 数组列索引列表 pd.DataFrame.values # 数组的值 pd.DataFrame.T # 数组转置 pd.DataFrame.head (n) # 数组前 n 行，默认 5 行 pd.DataFrame.tail (n) # 数组后 n 行，默认 5 行 pd.reset_index (drop=False) # 设置新的下标索引，drop 为是否删除原来索引，默认 False。 set_index (keys, drop=True) # 以名为 keys 的列表值设置为新索引，drop 默认值为 True。 MultiIndex：三维的数据结构 多级索引（层次化索引），可以在 Series、DataFrame 对象上拥有 2 个以及 2 个以上的索引 12345678910111213141516171819202122df = pd.DataFrame(&#123;&#x27;month&#x27;: [1, 4, 7, 10], &#x27;year&#x27;: [2012, 2014, 2013, 2014], &#x27;sale&#x27;:[55, 40, 84, 31]&#125;)# 使用DataFrame构造多索引的MultiIndex结构df = df.set_index([&#x27;year&#x27;, &#x27;month&#x27;])df&gt;&gt;&gt; saleyear month2012 1 552014 4 402013 7 842014 10 31# 打印行索引结果df.index&gt;&gt;&gt;MultiIndex(levels=[[2012, 2013, 2014], [1, 4, 7, 10]], labels=[[0, 2, 1, 2], [0, 1, 2, 3]], names=[&#x27;year&#x27;, &#x27;month&#x27;]) 代码实现 1234567arrays = [[1, 1, 2, 2], [&#x27;red&#x27;, &#x27;blue&#x27;, &#x27;red&#x27;, &#x27;blue&#x27;]]pd.MultiIndex.from_arrays(arrays, names=(&#x27;number&#x27;, &#x27;color&#x27;))# 结果MultiIndex(levels=[[1, 2], [&#x27;blue&#x27;, &#x27;red&#x27;]], codes=[[0, 0, 1, 1], [1, 0, 1, 0]], names=[&#x27;number&#x27;, &#x27;color&#x27;]) 3.4 Numpy 和 Pandas 区别及联系 Pandas 是在 Numpy 基础上实现，其核心数据结构与 Numpy 的 ndarray 十分相似，但 Pandas 与 Numpy 的关系不是替代，而是互为补充。二者之间主要区别是： 从数据结构上看： numpy 的核心数据结构是 ndarray ，支持任意维数的数组，但要求单个数组内所有数据是 同质的 ，即类型必须相同； pandas 的核心数据结构是 series 和 dataframe ，仅支持一维和二维数据，但数据内部可以是 异构 数据，仅要求同列数据类型一致即可 numpy 的数据结构仅支持 数字索引 pandas 数据结构则同时支持 数字索引 和 标签索引 从功能定位上看： numpy 虽然也支持字符串等其他数据类型，但仍然主要是用于 数值计算 ，尤其是内部集成了大量矩阵计算模块，例如基本的矩阵运算、线性代数、fft、生成随机数等，支持灵活的广播机制 pandas 主要用于 数据处理与分析 ，支持包括数据读写、数值计算、数据处理、数据分析和数据可视化全套流程操作 Excel 透视表：数据透视表（Pivot Table）是一种交互式的表，可以进行某些计算，如求和与计数等。所进行的计算与数据跟数据透视表中的排列有关。 之所以称为数据透视表，是因为可以动态地改变它们的版面布置，以便按照不同方式分析数据，也可以重新安排行号、列标和页字段。每一次改变版面布置时，数据透视表会立即按照新的布置重新计算数据。另外，如果原始数据发生更改，则可以更新数据透视表。 3.5 Series 和 Dataframe 区别 Pandas 核心数据结构有两种，即一维的 series 和二维的 dataframe，二者可以分别看做是在 numpy 一维数组和二维数组的基础上增加了相应的标签信息。正因如此，可以从两个角度理解 series 和 dataframe： series 和 dataframe 分别是一维和二维数组，因为是数组，所以 numpy 中关于数组的用法基本可以直接应用到这两个数据结构，包括数据创建、切片访问、通函数、广播机制等 series 是带标签的一维数组，所以还可以看做是 类字典 结构：标签是 key，取值是 value；而 dataframe 则可以看做是 嵌套字典 结构，其中列名是 key，每一列的 series 是 value。所以从这个角度讲，pandas 数据创建的一种灵活方式就是通过字典或者嵌套字典，同时也自然衍生出了适用于 series 和 dataframe 的类似字典访问的接口，即通过 loc 索引访问。 3.6 基本数据操作 1 索引 直接索引 – 先列后行，是需要通过索引的字符串进行获取 loc – 先行后列，是需要通过索引的字符串进行获取 iloc – 先行后列，是通过下标进行索引 ix – 先行后列，可以用上面两种方法混合进行索引 1234567891011121314151617181920212223242526272829303132# 使用loc:只能指定行列索引的名字data.loc[&#x27;2018-02-27&#x27;:&#x27;2018-02-22&#x27;, &#x27;open&#x27;]&gt;&gt;&gt;2018-02-27 23.532018-02-26 22.802018-02-23 22.88Name: open, dtype: float64# 使用iloc可以通过索引的下标去获取# 获取前3天数据,前5列的结果data.iloc[:3, :5]&gt;&gt;&gt; open high close low2018-02-27 23.53 25.88 24.16 23.532018-02-26 22.80 23.78 23.53 22.802018-02-23 22.88 23.37 22.82 22.71# 使用ix进行下表和名称组合做引data.ix[0:4, [&#x27;open&#x27;, &#x27;close&#x27;, &#x27;high&#x27;, &#x27;low&#x27;]]# 推荐使用loc和iloc来获取的方式data.loc[data.index[0:4], [&#x27;open&#x27;, &#x27;close&#x27;, &#x27;high&#x27;, &#x27;low&#x27;]]data.iloc[0:4, data.columns.get_indexer([&#x27;open&#x27;, &#x27;close&#x27;, &#x27;high&#x27;, &#x27;low&#x27;])]&gt;&gt;&gt; open close high low2018-02-27 23.53 24.16 25.88 23.532018-02-26 22.80 23.53 23.78 22.802018-02-23 22.88 22.82 23.37 22.712018-02-22 22.25 22.28 22.76 22.02 2 赋值 data[&quot;***&quot;] = ** data.*** = 3 排序 Dataframe 对象.sort_values(by=, ascending=) # ascending 默认为 True，表示升序排列 对象.sort_index() # 对索引进行排序 12345678# 按照开盘价大小进行排序 , 使用ascending指定按照大小排序data.sort_values(by=&quot;open&quot;, ascending=True).head()# 按照多个键进行排序data.sort_values(by=[&#x27;open&#x27;, &#x27;high&#x27;])# 对索引进行排序data.sort_index() series 对象.sort_values(ascending=True) 对象.sort_index() 1234567891011121314151617181920data[&#x27;p_change&#x27;].sort_values(ascending=True).head()&gt;&gt;&gt;2015-09-01 -10.032015-09-14 -10.022016-01-11 -10.022015-07-15 -10.022015-08-26 -10.01Name: p_change, dtype: float64 # 对索引进行排序data[&#x27;p_change&#x27;].sort_index().head()&gt;&gt;&gt;2015-03-02 2.622015-03-03 1.442015-03-04 1.572015-03-05 2.022015-03-06 8.51Name: p_change, dtype: float64 3.7 DataFram 运算 1 算术运算 对象.add() # 求和 对象.sub() # 求差 2 逻辑运算 1. 逻辑运算符号 12# 多个逻辑判断data[(data[&quot;open&quot;] &gt; 23) &amp; (data[&quot;open&quot;] &lt; 24)].head() 2. 逻辑运算函数 对象.query(expr) # 查询字符串 expr 1data.query(&quot;open&lt;24 &amp; open&gt;23&quot;).head() 对象.isin(values) # 判断对象是否在 values 中 12# 可以指定值进行一个判断，从而进行筛选操作data[data[&quot;open&quot;].isin([23.53, 23.85])] 3 统计运算 1. 对象.describe () # 综合分析，得出多种统计结果， count , mean , std , min , max 等 12# 计算平均值、标准差、最大值、最小值data.describe() 2. 统计函数 函数 作用 count 计数 sum 求和 mean 均值 median 中位数 min 最小值 max 最大值 mode 众数 abs 绝对值 prod 所有项乘积 std 标准差 var 方差 idxmax 最大值位置 idxmin 最小值位置 1234567891011121314# 求出最小值的位置data.idxmin(axis=0)&gt;&gt;&gt;open 2015-03-02high 2015-03-02close 2015-09-02low 2015-03-02volume 2016-07-06price_change 2015-06-15p_change 2015-09-01turnover 2016-07-06my_price_change 2015-06-15dtype: object 3. 累积统计函数 函数 作用 cumsum 计算前 1/2/3/…/n 个数的和 cummax 计算前 1/2/3/…/n 个数的最大值 cummin 计算前 1/2/3/…/n 个数的最小值 cumprod 计算前 1/2/3/…/n 个数的积 12345678910111213141516171819# 排序之后，进行累计求和data = data.sort_index()stock_rise = data[&#x27;p_change&#x27;]# plot方法集成了前面直方图、条形图、饼图、折线图stock_rise.cumsum()2015-03-02 2.622015-03-03 4.062015-03-04 5.632015-03-05 7.652015-03-06 16.162015-03-09 16.372015-03-10 18.752015-03-11 16.362015-03-12 15.032015-03-13 17.582015-03-16 20.342015-03-17 22.42 4 自定义运算 apply(func, axis=0) 123456data[[&#x27;open&#x27;, &#x27;close&#x27;]].apply(lambda x: x.max() - x.min(), axis=0)&gt;&gt;&gt;open 22.74close 22.85dtype: float64 3.8 Pandas 画图 DataFrame.plot(kind='line') kind : str，需要绘制图形的种类 line : 折线图 bar : 柱状图 barh : 水平直方图 hist : 直方图 pie : 饼状图 scatter : 散点图 详见官方文档：https://pandas.pydata.org/pandas-docs/stable/reference/frame.html pandas.Series.plot() 详见官方文档：https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html?highlight=plot#pandas.DataFrame.plot 3.9 文件读取与存储 CSV pandas.read_csv(filepath_or_buffer, sep =',', usecols ) 1234567891011# 读取文件,并且指定只获取&#x27;open&#x27;, &#x27;close&#x27;指标# sep :分隔符，默认用&quot;,&quot;隔开# usecols:指定读取的列名，列表形式data = pd.read_csv(&quot;./data/stock_day.csv&quot;, usecols=[&#x27;open&#x27;, &#x27;close&#x27;]) open close2018-02-27 23.53 24.162018-02-26 22.80 23.532018-02-23 22.88 22.822018-02-22 22.25 22.282018-02-14 21.49 21.92 DataFrame.to_csv(path_or_buf=None, sep=', ’, columns=None, header=True, index=True, mode='w', encoding=None) 12# index:存储不会讲索引值变成一列数据data[:10].to_csv(&quot;./data/test.csv&quot;, columns=[&#x27;open&#x27;], index=False) HDF5 pandas.read_hdf(path_or_buf，key =None，** kwargs) 12# 读取day_close的数据new_close = pd.read_hdf(&quot;./data/test.h5&quot;, key=&quot;day_close&quot;) DataFrame.to_hdf(path_or_buf, *key*, **\\*kwargs*) 12# 将day_close数据项存储到一个文件day_close.to_hdf(&quot;./data/test.h5&quot;, key=&quot;day_close&quot;) JSON pandas.read_json(path_or_buf=None, orient=None, typ='frame', lines=False) 12# orient指定存储的json格式，lines指定按照行去变成一个样本json_read = pd.read_json(&quot;./data/Sarcasm_Headlines_Dataset.json&quot;, orient=&quot;records&quot;, lines=True) DataFrame.to_json(*path_or_buf=None*, *orient=None*, *lines=False*) 1234# 将Pandas 对象存储为json格式# path_or_buf=None：文件地址# orient:存储的json形式，&#123;‘split’,’records’,’index’,’columns’,’values’&#125;# lines:一个对象存储为一行 3.10 缺失值处理 isnull、notnull 判断是否存在缺失值 np.any(pd.isnull(movie)) # 里面如果有一个缺失值，就返回 True np.all(pd.notnull(movie)) # 里面如果有一个缺失值，就返回 False dropna 删除 np.nan 标记的缺失值 movie.dropna() fillna 填充缺失值 movie[i].fillna(value=movie[i].mean(), inplace=True) replace 替换具体某些值 wis.replace(to_replace=&quot;?&quot;, value=np.NaN) 3.11 数据离散化 数据离散化目的： 可以用来减少给定连续属性值的个数 在连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数值代表落在每个子区间中的属性值。 实现 API pd.qcut(data, q) 对数据进行分组将数据分组，一般会与 value_counts 搭配使用，统计每组的个数 series.value_counts() # 统计分组次数 pd.cut(data, bins) # 自定义区间分组 独热编码（one-hot 编码） 把每个类别生成一个布尔列，这些列中只有一列可以为这个样本取值为 pandas.get_dummies(*data*, *prefix=None*) 12# 得出one-hot编码矩阵dummies = pd.get_dummies(p_counts, prefix=&quot;rise&quot;) 3.12 高级处理 - 合并 pd.concat ([数据 1, 数据 2], axis=1) # 数据合并 12# 按照行索引进行pd.concat([data, dummies], axis=1) pd.merge(left, right, how=‘inner’, on=None) left: DataFrame right: 另一个 DataFrame how – 以何种方式连接 on – 连接的键的依据是哪几个 123456789101112131415161718192021left = pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;], &#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;], &#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;, &#x27;A3&#x27;], &#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;, &#x27;B3&#x27;]&#125;)right = pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;], &#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;], &#x27;C&#x27;: [&#x27;C0&#x27;, &#x27;C1&#x27;, &#x27;C2&#x27;, &#x27;C3&#x27;], &#x27;D&#x27;: [&#x27;D0&#x27;, &#x27;D1&#x27;, &#x27;D2&#x27;, &#x27;D3&#x27;]&#125;)# 默认内连接result = pd.merge(left, right, on=[&#x27;key1&#x27;, &#x27;key2&#x27;])# 左连接result = pd.merge(left, right, how=&#x27;left&#x27;, on=[&#x27;key1&#x27;, &#x27;key2&#x27;])# 右连接result = pd.merge(left, right, how=&#x27;right&#x27;, on=[&#x27;key1&#x27;, &#x27;key2&#x27;])# 外链接result = pd.merge(left, right, how=&#x27;outer&#x27;, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) 3.13 高级处理 - 交叉表与透视表 交叉表： 交叉表用于计算一列数据对于另外一列数据的分组个数 (用于统计分组频率的特殊透视表) pd.crosstab(value1, value2) 透视表： 透视表是将原有的 DataFrame 的列分别作为行索引和列索引，然后对指定的列应用聚集函数 data.pivot_table(） DataFrame.pivot_table([], index=[]) 12345678910111213141516171819202122# 寻找星期几跟股票张得的关系# 1、先把对应的日期找到星期几date = pd.to_datetime(data.index).weekdaydata[&#x27;week&#x27;] = date# 2、假如把p_change按照大小去分个类0为界限data[&#x27;posi_neg&#x27;] = np.where(data[&#x27;p_change&#x27;] &gt; 0, 1, 0)# 通过交叉表找寻两列数据的关系count = pd.crosstab(data[&#x27;week&#x27;], data[&#x27;posi_neg&#x27;])# 算数运算，先求和sum = count.sum(axis=1).astype(np.float32)# 进行相除操作，得出比例pro = count.div(sum, axis=0)pro.plot(kind=&#x27;bar&#x27;, stacked=True)plt.show()# 通过透视表，将整个过程变成更简单一些data.pivot_table([&#x27;posi_neg&#x27;], index=&#x27;week&#x27;) 3.14 高级处理 - 分组与聚合 DataFrame.groupby(key, as_index=False) 1234567891011121314151617181920212223242526272829col =pd.DataFrame(&#123;&#x27;color&#x27;: [&#x27;white&#x27;,&#x27;red&#x27;,&#x27;green&#x27;,&#x27;red&#x27;,&#x27;green&#x27;], &#x27;object&#x27;: [&#x27;pen&#x27;,&#x27;pencil&#x27;,&#x27;pencil&#x27;,&#x27;ashtray&#x27;,&#x27;pen&#x27;],&#x27;price1&#x27;:[5.56,4.20,1.30,0.56,2.75],&#x27;price2&#x27;:[4.75,4.12,1.60,0.75,3.15]&#125;)&gt;&gt;&gt;color object price1 price20 white pen 5.56 4.751 red pencil 4.20 4.122 green pencil 1.30 1.603 red ashtray 0.56 0.754 green pen 2.75 3.15# 分组，求平均值col.groupby([&#x27;color&#x27;])[&#x27;price1&#x27;].mean()col[&#x27;price1&#x27;].groupby(col[&#x27;color&#x27;]).mean()&gt;&gt;&gt;colorgreen 2.025red 2.380white 5.560Name: price1, dtype: float64# 分组，数据的结构不变col.groupby([&#x27;color&#x27;], as_index=False)[&#x27;price1&#x27;].mean()&gt;&gt;&gt;color price10 green 2.0251 red 2.3802 white 5.560","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"Linux面试详解[最新版]","slug":"Linux面试详解-最新版","date":"2021-04-23T08:59:35.000Z","updated":"2021-06-26T02:41:01.176Z","comments":true,"path":"2021/04/23/Linux面试详解-最新版/","link":"","permalink":"https://leezhao415.github.io/2021/04/23/Linux%E9%9D%A2%E8%AF%95%E8%AF%A6%E8%A7%A3-%E6%9C%80%E6%96%B0%E7%89%88/","excerpt":"","text":"文章目录 1 Linux 概述 1.1 什么是 Linux 1.2 Unix 和 Linux 有什么区别？ 1.3 什么是 Linux 内核？ 1.4 Linux 的基本组件是什么？ 1.5 Linux 的体系结构 1.6 为什么 Linux 体系结构要分为用户空间和内核空间？ 1.7 BASH 和 DOS 的基本区别？ 1.8 Linux 开机启动过程？ 1.9 Linux 系统缺省的运行级别？ 1.10 Linux 使用的进程间通信方式？ 1.11 Linux 有哪些系统日志文件？ 1.12 Linux 系统安装多个桌面环境有帮助吗？ 1.13 什么是交换空间？ 1.14 什么是 root 帐户 1.15 什么是 LILO？ 1.16 什么是 BASH？ 1.17 什么是 CLI？ 1.18 什么是 GUI？ 1.19 开源的优势是什么？ 1.20 GNU 项目的重要性是什么？ 2 磁盘、目录、文件 2.1 简单 Linux 文件系统？ 2.2 Linux 的目录结构是怎样的？ 2.3 什么是 inode ？ 2.4 简述 Linux 文件系统通过 i 节点把文件的逻辑结构和物理结构转换的工作过程？ 2.5 什么是硬链接和软链接？ 2.6 RAID 是什么？ 3 网络安全 3.1 一台 Linux 系统初始化环境后需要做一些什么安全工作？ 3.2 什么叫 CC 攻击？什么叫 DDOS 攻击？ 3.3 怎么预防 CC 攻击和 DDOS 攻击？ 3.4 什么是网站数据库注入？ 4 Shell 之 sed 5 Linux 相关经典问题 5.1 如何选择 Linux 操作系统版本？ 5.2 如何规划一台 Linux 主机，步骤是怎样？ 5.3 请问当用户反馈网站访问慢，你会如何处理？ 5.3.1 有哪些方面的因素会导致网站网站访问慢？ 5.3.2 针对网站访问慢，怎么去排查？ 5.3.3 针对网站访问慢，怎么去解决？ 5.3.4 Linux 性能调优都有哪几种方法？ 6 网络通讯命令 6.1 ifconfig 命令 6.2 iptables 命令 6.3 netstat 命令 6.4 ping 命令 6.5 telnet 命令 7 系统管理命令 7.1 date 命令 7.2 free 命令 7.3 kill 命令 7.4 ps 命令 7.5 rpm 命令 7.6 top 命令 7.7 yum 命令 资源链接 1 Redis 面试题（2020 最新版） https://thinkwon.blog.csdn.net/article/details/103522351 2 MySQL 数据库面试题（2020 最新版）https://thinkwon.blog.csdn.net/article/details/10477862 3 Linux 面试题（2020 最新版） https://thinkwon.blog.csdn.net/article/details/104588679 4 架构设计 &amp; 分布式 &amp; 数据结构与算法面试题（2020 最新版）https://thinkwon.blog.csdn.net/article/details/105870730 1 Linux 概述 1.1 什么是 Linux Linux 是一套免费使用和自由传播的类 Unix 操作系统，是一个基于 POSIX 和 Unix 的多用户、多任务、支持多线程和多 CPU 的操作系统。它能运行主要的 Unix 工具软件、应用程序和网络协议。它支持 32 位和 64 位硬件。Linux 继承了 Unix 以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 1.2 Unix 和 Linux 有什么区别？ Linux 和 Unix 都是功能强大的操作系统，都是应用广泛的服务器操作系统，有很多相似之处，甚至有一部分人错误地认为 Unix 和 Linux 操作系统是一样的，然而，事实并非如此，以下是两者的区别。 开源性 Linux 是一款开源操作系统，不需要付费，即可使用；Unix 是一款对源码实行知识产权保护的传统商业软件，使用需要付费授权使用。 跨平台性 Linux 操作系统具有良好的跨平台性能，可运行在多种硬件平台上；Unix 操作系统跨平台性能较弱，大多需与硬件配套使用。 可视化界面 Linux 除了进行命令行操作，还有窗体管理系统；Unix 只是命令行下的系统。 硬件环境 Linux 操作系统对硬件的要求较低，安装方法更易掌握；Unix 对硬件要求比较苛刻，按照难度较大。 用户群体 Linux 的用户群体很广泛，个人和企业均可使用；Unix 的用户群体比较窄，多是安全性要求高的大型企业使用，如银行、电信部门等，或者 Unix 硬件厂商使用，如 Sun 等。 相比于 Unix 操作系统，Linux 操作系统更受广大计算机爱好者的喜爱，主要原因是 Linux 操作系统具有 Unix 操作系统的全部功能，并且能够在普通 PC 计算机上实现全部的 Unix 特性，开源免费的特性，更容易普及使用！ 1.3 什么是 Linux 内核？ Linux 系统的核心是内核。内核控制着计算机系统上的所有硬件和软件，在必要时分配硬件，并根据需要执行软件。 系统内存管理 应用程序管理 硬件设备管理 文件系统管理 1.4 Linux 的基本组件是什么？ 就像任何其他典型的操作系统一样，Linux 拥有所有这些组件：内核，shell 和 GUI，系统实用程序和应用程序。Linux 比其他操作系统更具优势的是每个方面都附带其他功能，所有代码都可以免费下载。 1.5 Linux 的体系结构 从大的方面讲，Linux 体系结构可以分为两块： 用户空间 (User Space) ：用户空间又包括用户的应用程序 (User Applications)、C 库 (C Library) 。 内核空间 (Kernel Space) ：内核空间又包括系统调用接口 (System Call Interface)、内核 (Kernel)、平台架构相关的代码 (Architecture-Dependent Kernel Code) 。 1.6 为什么 Linux 体系结构要分为用户空间和内核空间？ 1、现代 CPU 实现了不同的工作模式，不同模式下 CPU 可以执行的指令和访问的寄存器不同。 2、Linux 从 CPU 的角度出发，为了保护内核的安全，把系统分成了两部分。 用户空间和内核空间是程序执行的两种不同的状态，我们可以通过两种方式完成用户空间到内核空间的转移：1）系统调用；2）硬件中断。 1.7 BASH 和 DOS 的基本区别？ BASH 和 DOS 控制台之间的主要区别在于 3 个方面： BASH 命令区分大小写，而 DOS 命令则不区分； 在 BASH 下，/character 是目录分隔符，\\ 作为转义字符。在 DOS 下，/ 用作命令参数分隔符，\\ 是目录分隔符 DOS 遵循命名文件中的约定，即 8 个字符的文件名后跟一个点，扩展名为 3 个字符。BASH 没有遵循这样的惯例。 1.8 Linux 开机启动过程？ 1、主机加电自检，加载 BIOS 硬件信息。 2、读取 MBR 的引导文件 (GRUB、LILO)。 3、引导 Linux 内核。 4、运行第一个进程 init (进程号永远为 1)。 5、进入相应的运行级别。 6、运行终端，输入用户名和密码。 1.9 Linux 系统缺省的运行级别？ 关机。 单机用户模式。 字符界面的多用户模式 (不支持网络)。 字符界面的多用户模式。 未分配使用。 图形界面的多用户模式。 重启。 1.10 Linux 使用的进程间通信方式？ 1、管道 (pipe)、流管道 (s_pipe)、有名管道 (FIFO)。 2、信号 (signal) 。 3、消息队列。 4、共享内存。 5、信号量。 6、套接字 (socket) 。 1.11 Linux 有哪些系统日志文件？ 比较重要的是 /var/log/messages 日志文件。 该日志文件是许多进程日志文件的汇总，从该文件可以看出任何入侵企图或成功的入侵。 另外，如果胖友的系统里有 ELK 日志集中收集，它也会被收集进去。 1.12 Linux 系统安装多个桌面环境有帮助吗？ 通常，一个桌面环境，如 KDE 或 Gnome，足以在没有问题的情况下运行。尽管系统允许从一个环境切换到另一个环境，但这对用户来说都是优先考虑的问题。有些程序在一个环境中工作而在另一个环境中无法工作，因此它也可以被视为选择使用哪个环境的一个因素。 1.13 什么是交换空间？ 交换空间是 Linux 使用的一定空间，用于临时保存一些并发运行的程序。当 RAM 没有足够的内存来容纳正在执行的所有程序时，就会发生这种情况。 1.14 什么是 root 帐户 root 帐户就像一个系统管理员帐户，允许你完全控制系统。你可以在此处创建和维护用户帐户，为每个帐户分配不同的权限。每次安装 Linux 时都是默认帐户。 1.15 什么是 LILO？ LILO 是 Linux 的引导加载程序。它主要用于将 Linux 操作系统加载到主内存中，以便它可以开始运行。 1.16 什么是 BASH？ BASH 是 Bourne Again SHell 的缩写。它由 Steve Bourne 编写，作为原始 Bourne Shell（由 /bin/sh 表示）的替代品。它结合了原始版本的 Bourne Shell 的所有功能，以及其他功能，使其更容易使用。从那以后，它已被改编为运行 Linux 的大多数系统的默认 shell。 1.17 什么是 CLI？ 命令行界面（英语：command-line interface，缩写]：CLI）是在图形用户界面得到普及之前使用最为广泛的用户界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后，予以执行。也有人称之为字符用户界面（CUI）。 通常认为，命令行界面（CLI）没有图形用户界面（GUI）那么方便用户操作。因为，命令行界面的软件通常需要用户记忆操作的命令，但是，由于其本身的特点，命令行界面要较图形用户界面节约计算机系统的资源。在熟记命令的前提下，使用命令行界面往往要较使用图形用户界面的操作速度要快。所以，图形用户界面的操作系统中，都保留着可选的命令行界面。 1.18 什么是 GUI？ 图形用户界面（Graphical User Interface，简称 GUI，又称图形用户接口）是指采用图形方式显示的计算机操作用户界面。 图形用户界面是一种人与计算机通信的界面显示格式，允许用户使用鼠标等输入设备操纵屏幕上的图标或菜单选项，以选择命令、调用文件、启动程序或执行其它一些日常任务。与通过键盘输入文本或字符命令来完成例行任务的字符界面相比，图形用户界面有许多优点。 1.19 开源的优势是什么？ 开源允许你将软件（包括源代码）免费分发给任何感兴趣的人。然后，人们可以添加功能，甚至可以调试和更正源代码中的错误。它们甚至可以让它运行得更好，然后再次自由地重新分配这些增强的源代码。这最终使社区中的每个人受益。 1.20 GNU 项目的重要性是什么？ 这种所谓的自由软件运动具有多种优势，例如可以自由地运行程序以及根据你的需要自由学习和修改程序。它还允许你将软件副本重新分发给其他人，以及自由改进软件并将其发布给公众。 2 磁盘、目录、文件 2.1 简单 Linux 文件系统？ 在 Linux 操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。 也就是说在 Linux 系统中有一个重要的概念 **：一切都是文件 **。其实这是 Unix 哲学的一个体现，而 Linux 是重写 Unix 而来，所以这个概念也就传承了下来。在 Unix 系统中，把一切资源都看作是文件，包括硬件设备。UNIX 系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。 Linux 支持 5 种文件类型，如下图所示： 2.2 Linux 的目录结构是怎样的？ 这个问题，一般不会问。更多是实际使用时，需要知道。 Linux 文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： 1234567891011121314151617常见目录说明：/bin： 存放二进制可执行文件(ls,cat,mkdir等)，常用命令一般都在这里；/etc： 存放系统管理和配置文件；/home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示；**/usr **： 用于存放系统应用程序；/opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里；/proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息；/root： 超级用户（系统管理员）的主目录（特权阶级o）；/sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等；/dev： 用于存放设备文件；/mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统；/boot： 存放用于系统引导时使用的各种文件；**/lib **： 存放着和系统运行相关的库文件 ；/tmp： 用于存放各种临时文件，是公用的临时文件存储点；/var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等；/lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。 2.3 什么是 inode ？ 一般来说，面试不会问 inode 。但是 inode 是一个重要概念，是理解 Unix/Linux 文件系统和硬盘储存的基础。 理解 inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做 &quot;扇区&quot;（Sector）。每个扇区储存 512 字节（相当于 0.5KB）。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个 &quot;块&quot;（block）。这种由多个扇区组成的 &quot;块&quot;，是文件存取的最小单位。&quot;块&quot; 的大小，最常见的是 4KB，即连续八个 sector 组成一个 block。 文件数据都储存在 &quot;块&quot; 中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做 inode，中文译名为 &quot;索引节点&quot;。 每一个文件都有对应的 inode，里面包含了与该文件有关的一些信息。 2.4 简述 Linux 文件系统通过 i 节点把文件的逻辑结构和物理结构转换的工作过程？ Linux 通过 inode 节点表将文件的逻辑结构和物理结构进行转换。 inode 节点是一个 64 字节长的表，表中包含了文件的相关信息，其中有文件的大小、文件所有者、文件的存取许可方式以及文件的类型等重要信息。在 inode 节点表中最重要的内容是磁盘地址表。在磁盘地址表中有 13 个块号，文件将以块号在磁盘地址表中出现的顺序依次读取相应的块。 Linux 文件系统通过把 inode 节点和文件名进行连接，当需要读取该文件时，文件系统在当前目录表中查找该文件名对应的项，由此得到该文件相对应的 inode 节点号，通过该 inode 节点的磁盘地址表把分散存放的文件物理块连接成文件的逻辑结构。 2.5 什么是硬链接和软链接？ ​ 1）硬链接 由于 Linux 下的文件是通过索引节点 (inode) 来识别文件，硬链接可以认为是一个指针，指向文件索引节点的指针，系统并不为它重新分配 inode 。每添加一个一个硬链接，文件的链接数就加 1 。 不足：1）不可以在不同文件系统的文件间建立链接；2）只有超级用户才可以为目录创建硬链接。 2）软链接 软链接克服了硬链接的不足，没有任何文件系统的限制，任何用户可以创建指向目录的符号链接。因而现在更为广泛使用，它具有更大的灵活性，甚至可以跨越不同机器、不同网络对文件进行链接。 不足：因为链接文件包含有原文件的路径信息，所以当原文件从一个目录下移到其他目录中，再访问链接文件，系统就找不到了，而硬链接就没有这个缺陷，你想怎么移就怎么移；还有它要系统分配额外的空间用于建立新的索引节点和保存原文件的路径。 实际场景下，基本是使用软链接。总结区别如下： 硬链接不可以跨分区，软件链可以跨分区。 硬链接指向一个 inode 节点，而软链接则是创建一个新的 inode 节点。 删除硬链接文件，不会删除原文件，删除软链接文件，会把原文件删除。 2.6 RAID 是什么？ RAID 全称为独立磁盘冗余阵列 (Redundant Array of Independent Disks)，基本思想就是把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、 容量巨大的硬盘。RAID 通常被用在服务器电脑上，使用完全相同的硬盘组成一个逻辑扇区，因此操作系统只会把它当做一个硬盘。 RAID 分为不同的等级，各个不同的等级均在数据可靠性及读写性能上做了不同的权衡。在实际应用中，可以依据自己的实际需求选择不同的 RAID 方案。 当然，因为很多公司都使用云服务，大家很难接触到 RAID 这个概念，更多的可能是普通云盘、SSD 云盘的概念。 3 网络安全 3.1 一台 Linux 系统初始化环境后需要做一些什么安全工作？ 1、添加普通用户登陆，禁止 root 用户登陆，更改 SSH 端口号。 2、服务器使用密钥登陆，禁止密码登陆。 3、开启防火墙，关闭 SElinux ，根据业务需求设置相应的防火墙规则。 4、装 fail2ban 这种防止 SSH 暴力破击的软件。 5、设置只允许公司办公网出口 IP 能登陆服务器 (看公司实际需要) ​ 也可以安装 VPN 等软件，只允许连接 VPN 到服务器上。 6、修改历史命令记录的条数为 10 条。 7、只允许有需要的服务器可以访问外网，其它全部禁止。 8、做好软件层面的防护。 8.1 设置 nginx_waf 模块防止 SQL 注入。 8.2 把 Web 服务使用 www 用户启动，更改网站目录的所有者和所属组为 www 。 3.2 什么叫 CC 攻击？什么叫 DDOS 攻击？ CC 攻击，主要是用来攻击页面的，模拟多个用户不停的对你的页面进行访问，从而使你的系统资源消耗殆尽。 DDOS 攻击，中文名叫分布式拒绝服务攻击，指借助服务器技术将多个计算机联合起来作为攻击平台，来对一个或多个目标发动 DDOS 攻击。 攻击，即是通过大量合法的请求占用大量网络资源，以达到瘫痪网络的目的。 3.3 怎么预防 CC 攻击和 DDOS 攻击？ 防 CC、DDOS 攻击，这些只能是用硬件防火墙做流量清洗，将攻击流量引入黑洞。 流量清洗这一块，主要是买 ISP 服务商的防攻击的服务就可以，机房一般有空余流量，我们一般是买服务，毕竟攻击不会是持续长时间。 3.4 什么是网站数据库注入？ 由于程序员的水平及经验参差不齐，大部分程序员在编写代码的时候，没有对用户输入数据的合法性进行判断。 应用程序存在安全隐患。用户可以提交一段数据库查询代码，根据程序返回的结果，获得某些他想得知的数据，这就是所谓的 SQL 注入。 SQL 注入，是从正常的 WWW 端口访问，而且表面看起来跟一般的 Web 页面访问没什么区别，如果管理员没查看日志的习惯，可能被入侵很长时间都不会发觉。 如何过滤与预防？ 数据库网页端注入这种，可以考虑使用 nginx_waf 做过滤与预防。 4 Shell 之 sed 写一个 sed 命令，修改 /tmp/input.txt 文件的内容？ 要求： 删除所有空行。 一行中，如果包含 “11111”，则在 “11111” 前面插入 “AAA”，在 “11111” 后面插入 “BBB” 。比如：将内容为 0000111112222 的一行改为 0000AAA11111BBB2222 。 1234567891011121314151617181920212223242526272829303132[root@~]## cat -n /tmp/input.txt 1 000011111222 2 3 000011111222222 4 11111000000222 5 6 7 111111111111122222222222 8 2211111111 9 112222222 10 1122 11## 删除所有空行命令[root@~]## sed &#x27;/^$/d&#x27; /tmp/input.txt0000111112220000111112222221111100000022211111111111112222222222222111111111122222221122## 插入指定的字符[root@~]## sed &#x27;s#\\(11111\\)#AAA\\1BBB#g&#x27; /tmp/input.txt0000AAA11111BBB2220000AAA11111BBB222222AAA11111BBB000000222AAA11111BBBAAA11111BBB1112222222222222AAA11111BBB1111122222221122 sed 命令用法简析： # echo -e “booktestbook\\nbook\\n\\ntest”&gt;abc 实验文件创建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361.替换命令s \\# sed -n ‘s/book/BOOK/p‘ abc BOOKtestbook BOOK2.显示行内容命令-n p（p ：打印 print） 显示12-19行的内容：# sed -n ‘12,19p‘ file 如果想修改 需要加入 -i 参数 \\# sed -i ‘s/book/BOOK/p‘ abc 这里就是把源文件中的所有book 都替换成了BOOK3.上个命令的 g 参数是作用是 全部替换 比较一下加不加 g 的区别 \\# sed -n ‘s/book/BOOK/p‘ abc BOOKtestbook BOOK 没有替换所有的book字符 \\# sed -n ‘s/book/BOOK/gp‘ abc BOOKtestBOOK BOOK 替换了所有的book字符如果需要从N处开始替换，可以使用Ng \\# echo abababab | sed ‘s/ab/AB/2g‘ abABABAB \\# echo abababab | sed ‘s/ab/AB/3g‘ ababABAB4.删除命令 d \\# sed ‘/^$/d‘ abc (删除空行) booktestbook book test （类似命令 grep -v \\# grep -v &quot;^$&quot; abc booktestbook book test)sed ‘1d‘ abc （删除第一行） book testsed ‘1,$d‘ abc （$代表最后一行，删除所有行）sed ‘1,2d‘ abc （删除第一和第二行） testsed ‘$d‘ abc （删除最后一行） booktestbook booksed ‘/^b/d‘ abc （删除以b 开头的行） test #6.已匹配字符串标记 &amp; （\\w\\+ 匹配每个单词） \\# echo hello world | sed ‘s/\\w\\+/[&amp;]/‘ [hello] world \\# echo hello world | sed ‘s/\\w\\+/[&amp;]/g‘ （更加加深了对g 使用的了解） [hello] [world] 所有以192.168.1.120开头的都替换成自己加：localhost 这里以nginx的访问日中为例： \\# sed ‘s/^192.168.1.120/&amp;:localhost/g‘ /application/nginx/logs/20170202_access_www.log | awk ‘&#123;print $1&#125;‘|grep 120 192.168.1.120:localhost awk ‘&#123;print $1&#125;‘ 是提取出第一列 grep 120 找到 192.168.1.1207.子串匹配标记 \\1 \\# echo BBB bbb | sed ‘s/\\([A-Z]\\+\\) \\([a-z]\\+\\)/\\2 \\1/‘ bbb BBB \\# echo loveable | sed ‘s/\\(love\\)able/\\1evol/‘ loveevol8.选定行的范围&quot;,&quot; 在行上插入 i\\ 在行下追加 a\\ 替换本行 c\\ \\# sed ‘/^book/,/^test/i\\abc‘ abc (在以book和test开头的行上面写入abc) abc booktestbook abc book abc abc test \\# sed ‘/^book/,/^test/a\\abc‘ abc （在以book和test开头的行下面写入abc） booktestbook abc book abc abc test abc \\# sed -e ‘/^$/d‘ abc -e ‘/test/ c \\give‘ give book give9.多点编辑 命令 e \\# sed -e ‘/^$/d‘ -e ‘s/book/BOOK/g‘ abc BOOKtestBOOK BOOK test10.下一个 n 命令： （如果book被匹配，则移动到匹配行的下一个将book替换成bookabc \\# sed ‘/book/&#123;n; s/\\([a-z]\\+\\)/\\1abc/;&#125;‘ abc booktestbook bookabc test11.写入文件 w 命令 和 读取文件 r 命令 将abc中的包含book的行写入到 book \\# sed ‘/book/w book‘ abc booktestbook book test 查看文件book 和abc \\# cat book booktestbook book \\# cat abc booktestbook book test 逐行读取文件book中含有book的字符的行。如果abc中也含有book并且行号一样，则显示在后面 \\# sed ‘/book/r book‘ abc booktestbook booktestbook book book booktestbook book test12.退出 q \\# sed ‘2q‘ abc 打印出第二行后退出。 5 Linux 相关经典问题 5.1 如何选择 Linux 操作系统版本？ 一般来讲，桌面用户首选 Ubuntu ；服务器首选 RHEL 或 CentOS ，两者中首选 CentOS 。 根据具体要求： 安全性要求较高，则选择 Debian 或者 FreeBSD 。、 需要使用数据库高级服务和电子邮件网络应用的用户可以选择 SUSE 。 想要新技术新功能可以选择 Feddora ，Feddora 是 RHEL 和 CentOS 的一个测试版和预发布版本。 【重点】根据现有状况，绝大多数互联网公司选择 CentOS 。现在比较常用的是 6 系列，现在市场占有大概一半左右。另外的原因是 CentOS 更侧重服务器领域，并且无版权约束。 5.2 如何规划一台 Linux 主机，步骤是怎样？ 1、确定机器是做什么用的，比如是做 WEB 、DB、还是游戏服务器。 2、确定好之后，就要定系统需要怎么安装，默认安装哪些系统、分区怎么做。 3、需要优化系统的哪些参数，需要创建哪些用户等等的。 5.3 请问当用户反馈网站访问慢，你会如何处理？ 5.3.1 有哪些方面的因素会导致网站网站访问慢？ 1、服务器出口带宽不够用 本身服务器购买的出口带宽比较小。一旦并发量大的话，就会造成分给每个用户的出口带宽就小，访问速度自然就会慢。 跨运营商网络导致带宽缩减。例如，公司网站放在电信的网络上，那么客户这边对接是长城宽带或联通，这也可能导致带宽的缩减。 2、服务器负载过大，导致响应不过来 可以从两个方面入手分析： 分析系统负载，使用 w 命令或者 uptime 命令查看系统负载。如果负载很高，则使用 top 命令查看 CPU ，MEM 等占用情况，要么是 CPU 繁忙，要么是内存不够。 如果这二者都正常，再去使用 sar 命令分析网卡流量，分析是不是遭到了攻击。一旦分析出问题的原因，采取对应的措施解决，如决定要不要杀死一些进程，或者禁止一些访问等。 3、数据库瓶颈 如果慢查询比较多。那么就要开发人员或 DBA 协助进行 SQL 语句的优化。 如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等。然后，也可以搭建 MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。 4、网站开发代码没有优化好 例如 SQL 语句没有优化，导致数据库读写相当耗时。 5.3.2 针对网站访问慢，怎么去排查？ 1、确定问题 首先要确定是用户端还是服务端的问题。当接到用户反馈访问慢，那边自己立即访问网站看看，如果自己这边访问快，基本断定是用户端问题，就需要耐心跟客户解释，协助客户解决问题。 不要上来就看服务端的问题。一定要从源头开始，逐步逐步往下。 2、浏览器调试工具 如果访问也慢，那么可以利用浏览器的调试功能，看看加载那一项数据消耗时间过多，是图片加载慢，还是某些数据加载慢。 3、查服务器负载情况 查看服务器硬件 (网络、CPU、内存) 的消耗情况。如果是购买的云主机，比如阿里云，可以登录阿里云平台提供各方面的监控，比如 CPU、内存、带宽的使用情况。 4、查日志 如果发现硬件资源消耗都不高，那么就需要通过查日志，比如看看 MySQL 慢查询的日志，看看是不是某条 SQL 语句查询慢，导致网站访问慢。 5.3.3 针对网站访问慢，怎么去解决？ 1、如果是出口带宽问题，那么久申请加大出口带宽。 2、如果慢查询比较多，那么就要开发人员或 DBA 协助进行 SQL 语句的优化。 3、如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等等。然后也可以搭建 MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。 4、申请购买 CDN 服务，加载用户的访问。 5、如果访问还比较慢，那就需要从整体架构上进行优化咯。做到专角色专用，多台服务器提供同一个服务。 5.3.4 Linux 性能调优都有哪几种方法？ 1、Disabling daemons (关闭 daemons)。 2、Shutting down the GUI (关闭 GUI)。 3、Changing kernel parameters (改变内核参数)。 4、Kernel parameters (内核参数)。 5、Tuning the processor subsystem (处理器子系统调优)。 6、Tuning the memory subsystem (内存子系统调优)。 7、Tuning the file system (文件系统子系统调优)。 8、Tuning the network subsystem（网络子系统调优)。 6 网络通讯命令 6.1 ifconfig 命令 ifconfig 用于查看和配置 Linux 系统的网络接口。 查看所有网络接口及其状态：ifconfig -a 。 使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。 6.2 iptables 命令 iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如： 把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝 1iptables -I INPUT -s 192.168.1.101 -p tcp --dport 80 -j REJECT 开启 80 端口，因为 web 对外都是这个端口 1iptables -A INPUT -p tcp --dport 80 -j ACCEP 另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。 6.3 netstat 命令 Linux netstat 命令用于显示网络状态。 利用 netstat 指令可让你得知整个 Linux 系统的网络情况。 语法 123456789101112131415161718192021222324252627netstat [-acCeFghilMnNoprstuvVwx][-A&lt;网络类型&gt;][--ip]参数说明：-a或–all 显示所有连线中的Socket。-A&lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址。-c或–continuous 持续列出网络状态。-C或–cache 显示路由器配置的快取信息。-e或–extend 显示网络其他相关信息。-F或–fib 显示FIB。-g或–groups 显示多重广播功能群组组员名单。-h或–help 在线帮助。-i或–interfaces 显示网络界面信息表单。-l或–listening 显示监控中的服务器的Socket。-M或–masquerade 显示伪装的网络连线。-n或–numeric 直接使用IP地址，而不通过域名服务器。-N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。-o或–timers 显示计时器。-p或–programs 显示正在使用Socket的程序识别码和程序名称。-r或–route 显示Routing Table。-s或–statistice 显示网络工作信息统计表。-t或–tcp 显示TCP传输协议的连线状况。-u或–udp 显示UDP传输协议的连线状况。-v或–verbose 显示指令执行过程。-V或–version 显示版本信息。-w或–raw 显示RAW传输协议的连线状况。-x或–unix 此参数的效果和指定&quot;-A unix&quot;参数相同。–ip或–inet 此参数的效果和指定&quot;-A inet&quot;参数相同。 实例 如何查看系统都开启了哪些端口？ 12345678910[root@centos6 ~ 13:20 #55]# netstat -lnpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1035/sshdtcp 0 0 :::22 :::* LISTEN 1035/sshdudp 0 0 0.0.0.0:68 0.0.0.0:* 931/dhclientActive UNIX domain sockets (only servers)Proto RefCnt Flags Type State I-Node PID/Program name Pathunix 2 [ ACC ] STREAM LISTENING 6825 1/init @/com/ubuntu/upstartunix 2 [ ACC ] STREAM LISTENING 8429 1003/dbus-daemon /var/run/dbus/system_bus_socket 如何查看网络连接状况？ 1234567[root@centos6 ~ 13:22 #58]# netstat -anActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 0.0.0.0:22 0.0.0.0:* LISTENtcp 0 0 192.168.147.130:22 192.168.147.1:23893 ESTABLISHEDtcp 0 0 :::22 :::* LISTENudp 0 0 0.0.0.0:68 0.0.0.0:* 如何统计系统当前进程连接数？ 输入命令 netstat -an | grep ESTABLISHED | wc -l 。 输出结果 177 。一共有 177 连接数。 用 netstat 命令配合其他命令，按照源 IP 统计所有到 80 端口的 ESTABLISHED 状态链接的个数？&lt;此题目考验的是对 awk 的使用&gt; 首先，使用 netstat -an|grep ESTABLISHED 命令。结果如下： 1234567tcp 0 0 120.27.146.122:80 113.65.18.33:62721 ESTABLISHEDtcp 0 0 120.27.146.122:80 27.43.83.115:47148 ESTABLISHEDtcp 0 0 120.27.146.122:58838 106.39.162.96:443 ESTABLISHEDtcp 0 0 120.27.146.122:52304 203.208.40.121:443 ESTABLISHEDtcp 0 0 120.27.146.122:33194 203.208.40.122:443 ESTABLISHEDtcp 0 0 120.27.146.122:53758 101.37.183.144:443 ESTABLISHEDtcp 0 0 120.27.146.122:27017 23.105.193.30:50556 ESTABLISHE 6.4 ping 命令 Linux ping 命令用于检测主机。 执行 ping 指令会使用 ICMP 传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 指定接收包的次数 1ping -c 2 www.baidu.com 6.5 telnet 命令 Linux telnet 命令用于远端登入。 执行 telnet 指令开启终端机阶段作业，并登入远端主机。 语法 12345678910111213141516171819202122telnet [-8acdEfFKLrx][-b&lt;主机别名&gt;][-e&lt;脱离字符&gt;][-k&lt;域名&gt;][-l&lt;用户名称&gt;][-n&lt;记录文件&gt;][-S&lt;服务类型&gt;][-X&lt;认证形态&gt;][主机名称或IP地址&lt;通信端口&gt;]参数说明：-8 允许使用8位字符资料，包括输入与输出。-a 尝试自动登入远端系统。-b&lt;主机别名&gt; 使用别名指定远端主机名称。-c 不读取用户专属目录里的.telnetrc文件。-d 启动排错模式。-e&lt;脱离字符&gt; 设置脱离字符。-E 滤除脱离字符。-f 此参数的效果和指定&quot;-F&quot;参数相同。-F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。-k&lt;域名&gt; 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。-K 不自动登入远端主机。-l&lt;用户名称&gt; 指定要登入远端主机的用户名称。-L 允许输出8位字符资料。-n&lt;记录文件&gt; 指定文件记录相关信息。-r 使用类似rlogin指令的用户界面。-S&lt;服务类型&gt; 设置telnet连线所需的IP TOS信息。-x 假设主机有支持数据加密的功能，就使用它。-X&lt;认证形态&gt; 关闭指定的认证形态。 实例 登录远程主机 登录 IP 为 192.168.0.5 的远程主机 1telnet 192.168.0.5 7 系统管理命令 7.1 date 命令 显示或设定系统的日期与时间。 命令参数： 123456789101112131415-d&lt;字符串&gt; 显示字符串所指的日期与时间。字符串前后必须加上双引号。-s&lt;字符串&gt; 根据字符串来设置日期与时间。字符串前后必须加上双引号。-u 显示GMT。%H 小时(00-23)%I 小时(00-12)%M 分钟(以00-59来表示)%s 总秒数。起算时间为1970-01-01 00:00:00 UTC。%S 秒(以本地的惯用法来表示)%a 星期的缩写。%A 星期的完整名称。%d 日期(以01-31来表示)。%D 日期(含年月日)。%m 月份(以01-12来表示)。%y 年份(以00-99来表示)。%Y 年份(以四位数来表示)。 实例： （1）显示下一天 1date +%Y%m%d --date=&quot;+1 day&quot; //显示下一天的日期 （2）-d 参数使用 1234567date -d &quot;nov 22&quot; 今年的 11 月 22 日是星期三date -d &#x27;2 weeks&#x27; 2周后的日期date -d &#x27;next monday&#x27; (下周一的日期)date -d next-day +%Y%m%d（明天的日期）或者：date -d tomorrow +%Y%m%ddate -d last-day +%Y%m%d(昨天的日期) 或者：date -d yesterday +%Y%m%ddate -d last-month +%Y%m(上个月是几月)date -d next-month +%Y%m(下个月是几月) 7.2 free 命令 显示系统内存使用情况，包括物理内存、交互区内存 (swap) 和内核缓冲区内存。 12345678命令参数：-b 以Byte显示内存使用情况-k 以kb为单位显示内存使用情况-m 以mb为单位显示内存使用情况-g 以gb为单位显示内存使用情况-s&lt;间隔秒数&gt; 持续显示内存-t 显示内存使用总合 实例： （1）显示内存使用情况 123freefree -kfree -m （2）以总和的形式显示内存的使用信息 1free -t （3）周期性查询内存使用情况 1free -s 10 7.3 kill 命令 发送指定的信号到相应进程。不指定型号将发送 SIGTERM（15）终止指定进程。如果任无法终止该程序可用 &quot;-KILL&quot; 参数，其发送的信号为 SIGKILL (9) ，将强制结束进程，使用 ps 命令或者 jobs 命令可以查看进程号。root 用户将影响用户的进程，非 root 用户只能影响自己的进程。 常用参数： 12345-l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称-a 当处理当前进程时，不限制命令名和进程号的对应关系-p 指定kill 命令只打印相关进程的进程号，而不发送任何信号-s 指定发送信号-u 指定用户 实例： （1）先使用 ps 查找进程 pro1，然后用 kill 杀掉 1kill -9 $(ps -ef | grep pro1) 7.4 ps 命令 ps (process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 top linux 上进程有 5 种状态: 运行 (正在运行或在运行队列中等待) 中断 (休眠中，受阻，在等待某个条件的形成或接受到信号) 不可中断 (收到信号不唤醒和不可运行，进程必须等待直到有中断发生) 僵死 (进程已终止，但进程描述符存在，直到父进程调用 wait4 () 系统调用后释放) 停止 (进程收到 SIGSTOP, SIGSTP, SIGTIN, SIGTOU 信号后停止运行运行) ps 工具标识进程的 5 种状态码: D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 12345678910命令参数：-A 显示所有进程a 显示所有进程-a 显示同一终端下所有进程c 显示进程真实名称e 显示环境变量f 显示进程间的关系r 显示当前终端运行的进程-aux 显示所有包含其它使用的进程 实例： （1）显示当前所有进程环境变量及进程间关系 1ps -ef （2）显示当前所有进程 1ps -A （3）与 grep 联用查找某进程 1ps -aux | grep apache （4）找出与 cron 与 syslog 这两个服务有关的 PID 号码 1ps aux | grep &#x27;(cron|syslog)&#x27; 7.5 rpm 命令 Linux rpm 命令用于管理套件。 rpm (redhat package manager) 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 易于安装，升级，间接提升了 Linux 的适用度。 查看系统自带 jdk rpm -qa | grep jdk 删除系统自带 jdk rpm -e --nodeps 查看 jdk 显示的数据 安装 jdk rpm -ivh jdk-7u80-linux-x64.rpm 7.6 top 命令 显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等 123456常用参数：-c 显示完整的进程命令-s 保密模式-p &lt;进程号&gt; 指定进程显示-n &lt;次数&gt;循环显示次数 实例： 1234567top - 14:06:23 up 70 days, 16:44, 2 users, load average: 1.25, 1.32, 1.35Tasks: 206 total, 1 running, 205 sleeping, 0 stopped, 0 zombieCpu(s): 5.9%us, 3.4%sy, 0.0%ni, 90.4%id, 0.0%wa, 0.0%hi, 0.2%si, 0.0%stMem: 32949016k total, 14411180k used, 18537836k free, 169884k buffersSwap: 32764556k total, 0k used, 32764556k free, 3612636k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 28894 root 22 0 1501m 405m 10m S 52.2 1.3 2534:16 java 前五行是当前系统情况整体的统计信息区。 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下： 14:06:23 — 当前系统时间 up 70 days, 16:44 — 系统已经运行了 70 天 16 小时 44 分钟（在这期间系统没有重启过的吆！） 2 users — 当前有 2 个用户登录系统 load average: 1.15, 1.42, 1.44 — load average 后面的三个数分别是 1 分钟、5 分钟、15 分钟的负载情况。 load average 数据是每隔 5 秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑 CPU 的数量，结果高于 5 的时候就表明系统在超负荷运转了。 第二行，Tasks — 任务（进程），具体信息说明如下： 系统现在共有 206 个进程，其中处于运行中的有 1 个，205 个在休眠（sleep），stoped 状态的有 0 个，zombie 状态（僵尸）的有 0 个。 第三行，cpu 状态信息，具体属性说明如下： 5.9% us — 用户空间占用 CPU 的百分比。 3.4% sy — 内核空间占用 CPU 的百分比。 0.0% ni — 改变过优先级的进程占用 CPU 的百分比 90.4% id — 空闲 CPU 百分比 0.0% wa — IO 等待占用 CPU 的百分比 0.0% hi — 硬中断（Hardware IRQ）占用 CPU 的百分比 0.2% si — 软中断（Software Interrupts）占用 CPU 的百分比 备注：在这里 CPU 的使用比率和 windows 概念不同，需要理解 linux 系统用户空间和内核空间的相关知识！ 第四行，内存状态，具体信息如下： 32949016k total — 物理内存总量（32GB） 14411180k used — 使用中的内存总量（14GB） 18537836k free — 空闲内存总量（18GB） 169884k buffers — 缓存的内存量 （169M） 第五行，swap 交换分区信息，具体信息说明如下： 32764556k total — 交换区总量（32GB） 0k used — 使用的交换区总量（0K） 32764556k free — 空闲交换区总量（32GB） 3612636k cached — 缓冲的交换区总量（3.6GB） 第六行，空行。 第七行以下：各进程（任务）的状态监控，项目列信息说明如下： PID — 进程 id USER — 进程所有者 PR — 进程优先级 NI — nice 值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位 kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位 kb。RES=CODE+DATA SHR — 共享内存大小，单位 kb S — 进程状态。D = 不可中断的睡眠状态 R = 运行 S = 睡眠 T = 跟踪 / 停止 Z = 僵尸进程 % CPU — 上次更新到现在的 CPU 时间占用百分比 % MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的 CPU 时间总计，单位 1/100 秒 COMMAND — 进程名称（命令名 / 命令行） 123456789top 交互命令h 显示top交互命令帮助信息c 切换显示命令名称和完整命令行m 以内存使用率排序P 根据CPU使用百分比大小进行排序T 根据时间/累计时间进行排序W 将当前设置写入~/.toprc文件中o或者O 改变显示项目的顺序 7.7 yum 命令 yum（ Yellow dog Updater, Modified）是一个在 Fedora 和 RedHat 以及 SUSE 中的 Shell 前端软件包管理器。 基於 RPM 包管理，能够从指定的服务器自动下载 RPM 包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。 yum 提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 1. 列出所有可更新的软件清单命令：yum check-update 2. 更新所有软件命令：yum update 3. 仅安装指定的软件命令：yum install &lt;package_name&gt; 4. 仅更新指定的软件命令：yum update &lt;package_name&gt; 5. 列出所有可安裝的软件清单命令：yum list 6. 删除软件包命令：yum remove &lt;package_name&gt; 7. 查找软件包 命令：yum search 8. 清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (= yum clean packages; yum clean oldheaders) : 清除缓存目录下的软件包及旧的 headers 实例 安装 pam-devel 1yum install pam-devel","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"操作系统","slug":"操作系统","permalink":"https://leezhao415.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"搭建个人Web服务器_NGHG","slug":"搭建个人Web服务器-NGHG","date":"2021-04-21T10:44:05.000Z","updated":"2021-07-14T13:06:48.192Z","comments":true,"path":"2021/04/21/搭建个人Web服务器-NGHG/","link":"","permalink":"https://leezhao415.github.io/2021/04/21/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BAWeb%E6%9C%8D%E5%8A%A1%E5%99%A8-NGHG/","excerpt":"","text":"项目名称：搭建个人 Web 服务器_NGHG 文章目录 1 搭建模式： 2 搭建步骤 3 创建博客 4 常见问题 1 搭建模式： NGHG，即 Node.js + Git + Hexo + Github 2 搭建步骤 【第一步】安装 Node.js 下载网址：https://nodejs.org/en/ 结果验证： 1234# 查看node版本node -v# 查看npm版本npm -v 【第二步】安装 Git 下载网址：https://git-scm.com/downloads 结果验证： 【第三步】安装 Hexo 新建文件夹 MyBlog （名字可自行设置，本文件夹主要存放系统文件及后续要上传的资源） 打开文件夹，右键 Git Bash Here 依次执行下面指令 123456789101112131415npm install -g hexo# 2.安装所需附加组件npm install# 3.初始化hexo框架hexo init# 4.编译生成静态页面hexo g# 5.启动本地服务hexo s此时就可以通过http://localhost:4000 访问web页面了 界面 【第四步】关联 GitHub 1.1 GitHub 创建仓库 打开 Git bash，输入如下指令，连接 GitHub。 12345# 用户名连接 修改为你自己的GitHub用户名git config --global user.name &quot;yourname&quot;# 邮箱连接 修改为你自己的GitHub绑定邮箱git config --global user.email &quot;youremail&quot; 1git config --list # 查看绑定详情 配置 SSHkey 添加到 GitHub 12ssh-keygen -t rsa -C &quot;你的邮箱&quot;# 回车即可 回到 GitHub 中，点击 settings—&gt;SSH and GPG keys—&gt;New SSH key。 回到 git bash 输入如下命令，得到我们的 key。 1cat ~/.ssh/id_rsa.pub 将我们复制的 key 放入中间部分，标题随意，完成后点击 Add SSH key。 结果验证 1ssh -T git@github.com 上传到 Github 所建仓库，修改文件属性。 123type: gitrepository: git@github.com:LeeZhao415/LeeZhao415.github.io.gitbranch: master 安装 hexo-deployer-git，便于展示内容。 1npm install hexo-deployer-git --save 执行下列指令 1234567hexo clean # 清理数据库hexo g # 生成内容hexo s # 打开服务器hexo d # 展示内容 结果验证 测试访问 在浏览器中输入 http://leezhao415.github.io 访问 3 创建博客 1hexo n &quot;博客的文件名&quot; 执行下列代码后，打开 http://leezhao415.github.io 即可 1234567hexo clean # 清理数据库hexo g # 生成内容hexo s # 打开服务器hexo d # 展示内容 4 常见问题 打开链接是提示如下： 原因： Hexo 无法解析模板文件 解决方案： 使用以下的命令 123npm install hexo-renderer-ejs --savenpm install hexo-renderer-stylus --savenpm install hexo-renderer-marked --save","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"Jupyter notebook操作说明","slug":"Jupyter-notebook操作说明","date":"2021-04-20T12:07:12.000Z","updated":"2021-07-14T13:26:20.290Z","comments":true,"path":"2021/04/20/Jupyter-notebook操作说明/","link":"","permalink":"https://leezhao415.github.io/2021/04/20/Jupyter-notebook%E6%93%8D%E4%BD%9C%E8%AF%B4%E6%98%8E/","excerpt":"","text":"项目名称：Jupyter notebook 操作说明详解 文章目录 1 启动 2 界面介绍 3 常用菜单操作： 4 命令模式和编辑模式 5 快捷操作 6 魔法函数 7 Jupyter 扩展 8 修改主题、字体等 9 附加说明 1 启动 1234# 开启jupyter notebookjupyter notebook# 开启jupyterlabjupyter lab 关闭 jupyter：在终端中按下键盘上的 Ctrl+C 2 界面介绍 快捷命令栏功能：保存文件，添加新 cell，剪切选中的 cell，复制选中的 cell，粘贴选中的 cell，将选中 cell 上移，将选中 cell 下移，执行选中的 cell，终止 kernel，重启 kernel，重启 kernel 并重新运行所有 cell。 Code 点击的四个选项： Code：写 python 代码 MarkDown：写 MarkDown 代码，通常用于注释 Raw NBConvert：一个转换工具 Heading：快捷添加 MarkDown 标题 3 常用菜单操作： 修改文件名：可以点击文件名称，在弹出的框中直接修改或者 File—&gt;Rename 新建 Notebook 文件：File—&gt;New Notebook—&gt;python3 下载文件：File—&gt;Download as —&gt; 选择目标格式即可下载 合并 cell：Edit—&gt;Merge Cell Above 或 Merge Cell Below 重启 Kernel 并清除所有输出：Kernel—&gt;Restart &amp; Clear Output 重启 Kernel 并运行所有 cell：Kernel—&gt;Restart &amp; Run All 停止当前 Notebook 运行：Kernel—&gt;ShutDown 4 命令模式和编辑模式 每一个 cell 有两种模式：命令模式和编辑模式。 最左侧是蓝色的条是命令模式 最左侧是绿色的条表示编辑模式 (此时 cell 中有光标，可以进行代码编写)。 在命令模式下，按下 enter 或者鼠标单击代码框可以进入编辑模式。 在编辑模式下，按下 esc 或者 鼠标单击代码框左侧区域 即可进入命令模式。 5 快捷操作 5.1 代码运行 ctrl + enter ：运行当前 cell 的代码，运行完后依然保持在当前 cell shift + enter ：运行当前 cell 的代码，运行完后跳转到下一个 cell，如果是最后一个 cell 则会新增一个 工具条上的 Run 5.2 创建新 cell 命令模式下，按下字母 a (above)，会在当前 cell 上方增加一个 cell 命令模式下，按下字母 b (blove)，会在当前 cell 下方增加一个 cell 工具条上的 ➕ 按钮 5.3 删除当前 cell 命令模式下，按下字母 x ，即可删除当前 cell 命令模式下，连续两次按下字母 d ，即可删除当前 cell 工具条上的 剪刀 按钮 5.4 合并 cell 命令模式下，选中多个 cell， shift + M 可以进行合并 Edit—&gt;Merge Cell Above/Merge Cell Down 5.5 拆分 cell 编辑模式下，以光标所在之处为分界点， ctrl+shift+'-' ，可以进 cell 的拆分 Edit—&gt;Split Cell 5.6 保存代码 命令模式下，按下字母 s ，就能创建一个 checkpoint 工具条上的 保存按钮 File—&gt;Save and Checkpoint 5.7 查找替换 编辑模式下， ESC + F Edit—&gt;Find and Replace 5.8 折叠输出 编辑模式下， ESC+O 在 cell 左侧位置上 双击 即可折叠 Cell—&gt;Current Output—&gt;Toggle 5.9 显示代码行号 命令模式下，按下字母 l (L 的小写，键盘 k 右侧的 l) 键，就能显示当前 cell 的行号 shift + l ，显示所有 cell 的行号 View—&gt;Toggle Line Numbers 5.10 切换代码类型 命令模式下，按下 m 切换到 MarkDown 模式，再按下 y 切换到 code 模式 工具条上 手动切换 Cell—&gt;Cell Type—&gt;选择相应的代码类型 5.11 注释代码 选中代码，command+/ (windows 下是 Ctrl+/) 5.12 Tab 键的使用 只输入变量或者函数的前几个字母，按下 tab 可以自动补全 已经输入函数，连续按下 shift+tab 可以查询函数的具体用法，(一直可以按 4 次) 5.13 执行 shell 命令 英文状态下的感叹号 + 普通的 shell 命令，例如： !pwd ， !ls 等 6 魔法函数 7 Jupyter 扩展 1pip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib nbextension install 安装完之后，重新启动一下 jupyter 服务，就可以看到 Nbextensions 选项卡。我们只需要勾选相应的插件，在每一个 notebook 的工具条中就会出现相应的扩展。 8 修改主题、字体等 12345安装此工具：pip install --upgrade jupyterthemes查看可用主题：jt -l 8.1 Table of Contents 这个扩展一般用于整个文件的目录很多的时候。首先在 Nbextensions 选项卡中勾选该插件，然后在工具条中就可以看到该扩展按钮。如果我们在 notebook 中使用了 MarkDown 设置了我们的标题，点击该扩展，就会在左侧生成目录，点击左侧的齿轮，可以在最顶部添加一个 cell 专门用来显示目录。点击左侧和顶部的链接都可以快速跳转到相应的位置。还可以进行目录的折叠。注意到此时菜单栏上也多了一个 “Navigate” 标签，同样显示了目录的情况。 8.2 Autopep8 这是一个将代码按照 PEP8 进行格式化的插件，前提是需要通过 pip install autopep8 安装 autopep8，安装完之后需要重启 jupyter notebook 服务才能生效。同样在 Nbextention 选项卡中勾选 Autopep8，在工具栏中会多一个 “锤子” 一样的按钮，可以帮助我们排版代码，使其符合 pep8 标准。 8.3 Variable inspector 该插件可以帮助我们查看当前 notebook 中所有的变量的名称，类型，大小和值。省去了 df.shape，type () 等语句的执行，也代替了前文提到的魔法函数 “% whos” 的执行，读者可以自行尝试一下。 8.4 Code folding 顾名思义，该插件可以对代码进行一定的折叠，例如遇到 class，def 等关键字，而且主体代码又很长时，折叠代码会方便阅读，这一点也让 jupyter notebook 更像一个 IDE。 8.5 Execute time 该插件可以显示每一个 cell 中代码的执行时间。 除此之外还有一些其他常见的插件扩展，例如 Notify，Collapsible headings 等，读者可以自行探索查看，并配置使用。 9 附加说明 **9.1 数学公式编辑：** 这个其实是 MarkDown 功能的延伸，需要将 cell 的代码类型改为 MarkDown，然后在一对美元符号之间写入 LaTex 公式，(例如 $E=mc^2$ )，运行 cell 之后就可以正常显示公式了。 9.2 关于 jupyter lab 和 jupyter hub jupyter lab 是基于 jupyter notebook 的新版本，是包括了 Notebook 的下一代的有模块化的界面，可以在同一个窗口同时打开好几个 notebook 或文件（HTML, TXT, Markdown 等等），都以标签的形式展示，于是就更像是一个 IDE。除了界面上存在差异，使用方式上和 notebook 没有大的差别。 9.3 关于 jupyter notebook 与 pycharm 的差别 notebook 是更 “轻” 量级的，适合小白使用。各个 cell 之间具有相对独立，且变量共享的特点。另外，notebook 能够保存中间结果，方便演示，适合数据分析人员或者数据科学家使用。 JupyterLab——Jupyter Notebooks 的进化 JupyterLab 是 Jupyter Notebooks 的进化版。其支持更加灵活和更加强大的项目操作方式，但具有和 Jupyter Notebooks 一样的组件。JupyterLab 环境与 Jupyter Notebooks 环境完全一样，但具有生产力更高的体验。 JupyterLab 让你能在一个窗口中排布你的笔记本、终端、文本文件和输出结果工作区！你只需拖放你需要的单元即可。你也可以编辑 Markdown、CSV 和 JSON 等常用文件格式并实时预览修改所造成的影响。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"机器学习导读","slug":"机器学习导读","date":"2021-04-20T12:05:55.000Z","updated":"2021-07-14T13:02:50.408Z","comments":true,"path":"2021/04/20/机器学习导读/","link":"","permalink":"https://leezhao415.github.io/2021/04/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AF%BB/","excerpt":"","text":"文章目录 机器学习导读 1 虚拟环境的作用 2 人工智能必备三要素 3 AI、ML、DL 4 IT 行业需要机器学习？ 5 人工智能分支 6 进程、线程、协程 7 机器学习工作流程 8 机器学习算法分类 9 深度学习 (Deep Learning) 机器学习导读 1 虚拟环境的作用 1234567创建新的环境：mkvirtualenv [env-name] 查看环境列表：lsvirtualenv复制环境：cpvirtualenv [env-name] 删除环境：rmvirtualenv [env-name] 切换虚拟环境：workon [env-name] 退出虚拟环境：deactivate 2 人工智能必备三要素 数据： 学习资料 算法： 学习方法 计算力：智能体的计算能力 CPU ：核心处理器 职责：管理资源、任务调度、图形计算 处理 IO 密集型任务， 应用：文件 IO、网络 IO、标准 IO IO 操作：CPU 需要等待 GPU ：协处理球 职责：分担 CPU 工作 应用：适合计算密集任务 TPU ：Tensor 张量处理器 职责：专门机器学习的处理器 3 AI、ML、DL 机器学习是人工智能的一个实现途径 深度学习是机器学习的一个方法发展来的 4 IT 行业需要机器学习？ 解决问题靠的是 编码 ，传统行业靠的是硬编码、固定编码。需要掌握更多的专业知识，编码的代码量也非常复杂。 机器学习可以 动态解决问题 ，具有自我生命力、自我成长。 5 人工智能分支 计算机视觉（CV） 指机器感知环境的能力。这一技术类别中的经典任务有图像形成、图像处理、图像提取和图像的三维推理。物体检测和人脸识别是其比较成功的研究领域。 自然语言处理（NLP） 语音识别 识别语音 (说出的语言) 并将其转换成对应文本的技术。相反的任务 (文本转语音 / TTS) 也是这一领域内一个类似的研究主题。 文本挖掘/分类 主要是指文本分类，该技术可用于理解、组织和分类结构化或非结构化文本文档。其涵盖的主要任务有句法分析、情绪分析和垃圾信息检测。 机器翻译 利用机器的力量自动将一种自然语言 (源语言) 的文本翻译成另一种语言 (目标语言)。 机器人 机器人学 (Robotics) 研究的是机器人的设计、制造、运作和应用，以及控制它们的计算机系统、传感反馈和信息处理。 鸡尾酒会效应（cocktail party effect）是指人的一种听力选择能力，在这种情况下，注意力集中在某一个人的谈话之中而忽略背景中其他的对话或噪音。该效应揭示了人类听觉系统中令人惊奇的能力，即我们可以在噪声中谈话。 6 进程、线程、协程 进程： 资源分配的最小单位 ：保存在硬盘上的程序运行以后，会在内存空间里形成一个独立的内存体，这个内存体有自己独立的地址空间，有自己的堆，上级挂靠单位是操作系统。操作系统会以进程为单位，分配系统资源（CPU 时间片、内存等资源）。 线程：又称为轻量级进程 (Lightweight Process，LWP），是 操作系统调度（CPU调度）执行的最小单位 。 协程：一种比线程更加轻量级的存在，协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行）。这样带来的好处就是性能得到了很大的提升，不会像线程切换那样消耗资源。 7 机器学习工作流程 从原始的数据空间，找到一个 f (x)，将其映射到高层语义空间。 1 获取数据 一行数据称为一个样本，多行数据称为 样本集 （数据集） 一列数据称为一个特征，也称为 属性 数据集分为训练数据和测试数据 2 数据预处理 对数据进行缺失值、去除异常值等处理 3 特征工程 定义： 使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好作用的过程 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。 研究内容： 特征提取 （feature extraction） 将任意数据（如文本或者图像）转换为可用于机器学习的数字特征 在进行模型训练的时候，训练数据是行列形式的 对于文本、图片这种非行列形式的数据，要进行转换 特点：从无到有 特征预处理 （feature preprocessing） 通过一些转换函数将特征数据转换成更加适合算法模型的特征的过程 特征降维 (feature decomposition) 在某些限定条件下，降低随机变量个数，得到一组 “不相关” 主变量的过程 特征选择 （feature selection） 从特征中选择出一些重要特征，但不会改变原来的数据。 特征组合 （feature crosses） 把具有相关性的特征通过加法、乘法等合并成一个特征 4 机器学习（模型训练） 选择合适的算法对模型进行训练 5 模型评估 定义：对训练好的模型进行评估 分类： 分类模型评估 评价指标：精确率、召回率、F1-score、AUC 指标等 回归模型评估 评价指标：相对平方误差（Relative Squared Error，RSE）、平均绝对误差（Mean Absolute Error，MAE)、相对绝对误差（Relative Absolute Error，RAE) 聚类模型评估 拟合 ：模型评估用于评价训练好的的模型的表现效果 欠拟合 定义：模型学习的太过粗糙，连训练集中的样本数据特征关系都没有学出来。 解决方案：对数据集加特征 过拟合 定义：所建的机器学习模型或者是深度学习模型在训练样本中表现得过于优越，导致在测试数据集中表现不佳。（泛化能力差或普适性差） 解决方案： （1）在数据基本处理阶段进行数据清洗 （2）增加训练样本 （3）通过惩罚机制限制学习力度，正则化。 8 机器学习算法分类 监督学习 解决问题：分类问题、回归问题。 分类问题预测出是一个离散值。 回归问题预测出的是连续值。 输入样本：都是有目标值（标签）的数据。 非监督学习 解决问题：聚类问题。 输入样本：没有目标值（标签）的数据。 半监督学习 一部分数据有标签、一部分数据没有标签。 强化学习 强化学习是一种特殊的机器学习方法。 传统的机器学习方法输入样本，输出的是：动作。 强化学习输入的也是样本（环境信息） 强化学习是一种与环境交互式的学习方法。 智能体、环境、动作、奖励。 9 深度学习 (Deep Learning) 也称为深度结构学习 (Deep Structured Learning)、层次学习 (Hierarchical Learning) 或者是深度机器学习 (Deep Machine Learning）是一类算法集合，是机器学习的一个分支。 “深度学习” 这个词语很古老，它在 1986 年由 Dechter 在机器学习领域提出，然后在 2000 年有 Aizenberg 等人引入到人工神经网络中。而现在，由于 Alex Krizhevsky 在 2012 年使用卷积网络结构赢得了 ImageNet 比赛之后受到大家的瞩目。 卷积网络之父： Yann LeCun 神经网络各层负责内容： 1 层：负责识别颜色及简单纹理 2 层：一些神经元可以识别更加细化的纹理，布纹，刻纹，叶纹等 3 层：一些神经元负责感受黑夜里的黄色烛光，高光，萤火，鸡蛋黄色等。 4 层：一些神经元识别萌狗的脸，宠物形貌，圆柱体事物，七星瓢虫等的存在。 5 层：一些神经元负责识别花，黑眼圈动物，鸟，键盘，原型屋顶等。","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"机器学习","slug":"机器学习","permalink":"https://leezhao415.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"数据结构及算法详解","slug":"数据结构及算法详解","date":"2021-04-20T12:05:37.000Z","updated":"2021-07-14T12:39:17.556Z","comments":true,"path":"2021/04/20/数据结构及算法详解/","link":"","permalink":"https://leezhao415.github.io/2021/04/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8F%8A%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"文章目录 1 算法的衡量标准 2 数据结构 3 排序算法 3.1 排序 3.2 算法稳定性 3.3 排序算法 4 二分查找 4.1 二分查找 4.2 代码实现 4.2.1 递归版本 4.2.2 递归优化版本 4.2.3 非递归版本 4.2.4 二分查找 - 位置 4.2.5 第一个位置 4.2.6 最后一个位置 5 非线性数据结构 - 树 1 算法的衡量标准 1.1 算法 解决问题的办法，是一种独立的存在的解决问题的方法和思想，它不依赖于代码。代码只不过是对算法的一种表达和实现。 1.2 数据结构 存储和组织数据的方式 1.3 时间复杂度 在规模量级上对算法的衡量，表示一个算法随着问题规模不断变化的最主要趋势 计算规则（6 条） 基本操作 ：只有常数项，时间复杂度为 O (1） 顺序结构 ：时间复杂度按 加法 进行计算 循环结构、递归结构 ：时间复杂度按 乘法 进行计算 分支结构 ：时间复杂度取 最大值 判断一个算法的效率时，往往只需要关注操作数量的 最高次项 ，忽略系数、低阶及常数项 在没有特殊说明的情况下，我们所分析的算法的时间复杂度都是指 最坏时间复杂度 （提供了一种保证，表明算法在此种程度的基本操作中一定能完成工作。） 函数嵌套 ，时间复杂度按 乘法 计算 O (1） 执行次数恒定的计算 1.4 空间复杂度 一个算法在运行过程中临时消耗内存的大小的度量 1.5 算法的复杂度 算法的时间复杂度和空间复杂度的合称 1.6 算法五大特性 ①输入：算法具有 0 个或多个输入 ②输出：算法至少有 1 个或多个输出 ③有穷性：算法在有限的步骤之后会自动结束而不会无限循环，并且每一个步骤可以在可接受的时间内完成 ④确定性：算法中的每一步都有确定的含义，不会出现二义性 ⑤可行性：算法的每一步都是可行的，也就是说每一步都能够执行有限的次数完成 1.7 常见时间复杂度 执行次数函数举例 阶 非正式术语 12 O(1) 常数阶 2n+3 O(n) 线性阶 3n2+2n+1 O(n2) 平方阶 5log2n+20 O(logn) 对数阶 6n3+2n2+3n+4 O(n3) 立方阶 时间复杂度曲线 所消耗的时间从小到大: O(1) &lt; O(logn) &lt; O(n) &lt; O(nlogn) &lt; O(n2) &lt; O(n3) 时间复杂度越低，效率越高 2 数据结构 2.1 数据结构 数据对象中数据元素之间的关系 - 内置数据结构：list、元组、字典等 扩展数据结构：栈、队列等 2.1.1 程序 = 数据结构（问题载体） + 算法 高效的程序需要在数据结构基础上设计和选择算法 2.1.2 元素外置：地址和数据分开存储 地址存储占用的内存空间 32 位操作系统： 4位 64 位操作系统： 8位 2.1.3 数据结构 1 线性结构 1.1 顺序表 一体式顺序表 分离式顺序表 1.2 链表 单链表 元素域： 存放具体数据 链接域： 存放下一个节点的位置 1.3 栈 1.3.1 特点： ① 一种运算受限的 线性表 ，仅允许在表的一端进行插入和删除运算，这一端被称为 栈顶 ，相对地，把另一端称为 栈底 . ② 处理数据的时候符合 先进后出（FILO） 特点 1.3.2 作用：函数里面有可能要使用到局部变量，不能总是用全局变量。而局部变量在函数使用完毕之后就销毁了，那么将局部变量存储在栈中既能不浪费空间又能及时销毁. 1.3.3 代码实现 1 链表实现栈 123456789101112131415161718192021222324252627282930class Node: def __init__(self, data, next=None): self.data = data self.next = nextclass LinkedStack: def __init__(self): # 最上面的元素 self.top = None def push(self, value: int): newtop = Node(value) newtop.next = self.top self.top = newtop def pop(self): if self.top: value = self.top.data self.top = self.top.next return valueif __name__ == &quot;__main__&quot;: stack = LinkedStack() for i in range(9): stack.push(i) for i in range(9): ele = stack.pop() print(ele) 1.4 队列 特点： ① 一种操作受限的 线性表 ，只允许在表的头部（front）(队头) 进行删除操作，而在表的尾部（rear）（队尾）进行插入操作。 ② 处理数据的时候符合 先进先出（FIFO） 特点 作用： 在任务处理类的系统中，先把用户发起的任务请求接收过来存到队列中，然后后端开启多个应用程序从队列中取任务进行处理，队列可以起到了 缓冲压力 的作用 代码实现 列表队列优化 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 尾部入 头部出class ArrayQueue: def __init__(self, capacity):# capacity 容量 # 申请固定长度空间 self.__items = [0]*capacity self.__capacity = capacity # 头部指针 self.__head = 0 # 尾部指针(下一个元素添加到这个位置) self.__tail = 0 def getItmes(self): return self.__items def enqueue(self, item): # 如果存满了,返回False if self.__head==0 and self.__tail==self.__capacity: return False # 没有存满,指针到最后了 需要先把元素往前挪 if self.__tail==self.__capacity: # 指针到最后了,容量没有满 for i in range(0, self.__tail - self.__head): self.__items[i] = self.__items[i + self.__head] # 修改尾部指针 self.__tail = self.__tail - self.__head # 修改头部指针 self.__head = 0 # 元素添加 self.__items[self.__tail] = item self.__tail += 1 return True def dequeue(self): # 非空 if self.__head != self.__tail: item = self.__items[self.__head] # 把头部往后移动1位 self.__head += 1 return item else: return NonearrayQ = ArrayQueue(6)arrayQ.enqueue(10)arrayQ.enqueue(20)print(arrayQ.getItmes())print(arrayQ.dequeue())print(arrayQ.getItmes())arrayQ.enqueue(30)arrayQ.enqueue(40)arrayQ.enqueue(50)arrayQ.enqueue(60)arrayQ.enqueue(70)print(arrayQ.getItmes()) 链表实现队列 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 节点class Node: def __init__(self, data, next=None): # 数据 self.data = data # 链接域 self.next = nextclass LinkedQueue: def __init__(self): self.__head = None self.__tail = None def enqueue(self, value): &#x27;&#x27;&#x27;入队列&#x27;&#x27;&#x27; new_node = Node(value) if self.__tail: # 非空 self.__tail.next = new_node else: # 空链表 self.__head = new_node # 尾部指向新的节点 self.__tail = new_node def dequeue(self): if self.__head: # 头部不为空 # 获取头部元素 value = self.__head.data # 头部指向原来的下一个节点 self.__head = self.__head.next if not self.__head: # 如果头部指向None 链表为空 self.__tail = None return valueif __name__ == &quot;__main__&quot;: linkQ = LinkedQueue() linkQ.enqueue(10) linkQ.enqueue(20) linkQ.enqueue(30) print(linkQ.dequeue()) print(linkQ.dequeue()) print(linkQ.dequeue()) 双端队列 ​ 特点： 一种具有队列和栈的性质的数据结构 双端队列中的元素可以从两端弹出，其限定插入和删除操作在表的两端进行 双端队列可以在队列任意一端入队和出队 2 非线性结构 树 3 排序算法 3.1 排序 使一串记录，按照其中的某个或某些关键字的大小，递增或递减的排列起来的操作。 3.2 算法稳定性 假定在待排序的记录序列中，存在多个具有相同的关键字的记录，若经过排序，这些记录的相对次序保持不变，则称这种排序算法是稳定的，否则称为不稳定的。 3.3 排序算法 不稳定算法 : 选择排序、快速排序、希尔排序、堆排序 稳定算法 : 冒泡排序、插入排序、归并排序和基数排序 3.3.1 代码实现 1 冒泡排序 特点： 重复地走访过要排序的元素列，依次比较两个相邻的元素，如果顺序（如从大到小、首字母从从 Z 到 A）错误就把他们交换过来。走访元素的工作是重复地进行直到没有相邻元素需要交换，也就是说该元素列已经排序完成 名字由来：因为越小的元素会经由交换慢慢 “浮” 到数列的顶端（升序或降序排列），就如同碳酸饮料中二氧化碳的气泡最终会上浮到顶端一样，故名 “冒泡排序” 复杂度分析： 最差时间复杂度 : O(n^2) 最优时间复杂度 : O(n) # 遍历一遍发现没有任何元素发生了位置交换，终止排序 算法稳定性：稳定算法 原地排序：是 1234567891011121314151617181920212223# n-1+n-2+...+1def bubble_sort(alist): &#x27;&#x27;&#x27;冒泡排序&#x27;&#x27;&#x27; length = len(alist) count= 0 for i in range(length-1): flag = False for j in range(length-1-i): count+=1 if alist[j] &gt; alist[j+1]: alist[j], alist[j+1] = alist[j+1], alist[j] flag = True if not flag: break print(count)# 4+3+2+1if __name__ == &#x27;__main__&#x27;: # l = [5,3,4,7,2] l = [2,3,4,5,7] bubble_sort(l) print(l) 2 选择排序 特点： 第一次从待排序的数据元素中选出最小（或最大）的一个元素，存放在序列的起始位置，然后再从剩余的未排序元素中寻找到最小（大）元素，然后放到已排序的序列的末尾。以此类推，直到全部待排序的数据元素的个数为零. 复杂度分析： 最差时间复杂度 : O(n^2) 最优时间复杂度 : O(n^2) 算法稳定性：不稳定算法 原地排序：是 12345678910111213141516171819def select_sort(alist): # 5 length = len(alist) for i in range(length-1): # 设置目标 # targetIndex = i # 设置最小值索引 minIndex = i for j in range(i+1,length): if alist[j] &lt; alist[minIndex]: minIndex = j # 如果目标和最小值索引不同 交换 if minIndex!=i: alist[i], alist[minIndex] = alist[minIndex], alist[i]if __name__ == &#x27;__main__&#x27;: alist = [1,3,4,10,0,1000,88] select_sort(alist) print(alist) 3 插入排序 特点： 将一个数据插入到已经排好序的有序数据中，从而得到一个新的、个数加一的有序数据，算法适用于少量数据的排序。（每步将一个待排序的记录，按排序要求插入到前面已经排序的数据中适当位置上，直到全部插入完为止） 复杂度分析： 最差时间复杂度: O(n^2) 最优时间复杂度: O(n) 算法稳定性：稳定的排序方法 原地排序：是 123456789101112131415def insert_sort(alist): length = len(alist) for i in range(1,length): # i每一次需要排序的结束索引 # 0 - 1 for j in range(i,0,-1): if alist[j] &lt; alist[j-1]: alist[j], alist[j-1] = alist[j-1], alist[j] else: breakif __name__ == &#x27;__main__&#x27;: alist = [1, 100, 99, 20, 5, 1000] insert_sort(alist) print(alist) 4 快速排序 特点： 通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 排序流程： (1) 首先设定一个分界值，通过该分界值将数组分成左右两部分 (2) 将大于或等于分界值的数据集中到数组右边，小于分界值的数据集中到数组的左边，此时，左边部分中各元素都小于或等于分界值，而右边部分中各元素都大于或等于分界值 (3) 然后，左边和右边的数据可以独立排序。对于左侧的数组数据，又可以取一个分界值，将该部分数据分成左右两部分，同样在左边放置较小值，右边放置较大值。右侧的数组数据也做类似处理 (4) 重复上述过程，可以看出，这是一个递归定义。通过递归将左侧部分排好序后，再递归排好右侧部分的顺序，当左、右两个部分各数据排序完成后，整个数组的排序也就完成了 复杂度分析： 最优时间复杂度: O(nlogn) 最差时间复杂度: O(n^2) 算法稳定性：不稳定 原地排序：是 1234567891011121314151617181920212223242526272829def quick_sort(alist,start,end): if start&gt;=end: return # 界限值 mid = alist[start] # 左右游标 left = start right = end while left &lt; right: # 从右边开始找寻小于mid的值 归类到左边 while alist[right] &gt;= mid and left &lt; right: right -= 1 alist[left] = alist[right] # 从左边开始找寻大于mid的值 归类到右边 while alist[left] &lt; mid and left &lt; right: left += 1 alist[right] = alist[left] # left=right alist[left] = mid # 排序左边 quick_sort(alist,start,left-1) # 排序右边 quick_sort(alist,left+1,end)if __name__ == &#x27;__main__&#x27;: alist = [5,9,8,7,6,4,3,2,1] quick_sort(alist,0,len(alist)-1) print(alist) 4 二分查找 4.1 二分查找 特点 二分查找又称`折半查找`，它是一种效率较高的查找方法，①必须采用顺序存储结构 ②必须按关键字大小有序排列。 基本思想 将数组分为三部分，依次是中值前，中值，中值后，将要查找的值与中值进行比较，若小于中值则在中值前面找，若大于中值则在中值后面找，等于中值时直接返回。 复杂度分析 最差时间复杂度: O(logn) 最优时间复杂度: O(1) 4.2 代码实现 4.2.1 递归版本 1234567891011121314151617181920def binary_search(alist, item): # 数列的长度 n = len(alist) if n==0: return False # 递归的结束条件 # 中间值索引 mid = n // 2 if item == alist[mid]: return True elif item &lt; alist[mid]: # 左边 return binary_search(alist[:mid],item) elif item &gt; alist[mid]: # 右边 return binary_search(alist[mid+1:],item)if __name__ == &#x27;__main__&#x27;: alist = [1,2,3,4,5,6,7,8,9] print(binary_search(alist, 5)) 4.2.2 递归优化版本 123456789101112131415161718192021222324def binary_search_achieve(alist, item, start, end): &quot;&quot;&quot;二分查找&quot;&quot;&quot; if start &gt; end: return -1 # 中间值索引 mid = (start + end) // 2 if item == alist[mid]: return mid elif item &lt; alist[mid]: return binary_search_achieve(alist, item, start, mid - 1) elif item &gt; alist[mid]: return binary_search_achieve(alist, item, mid + 1, end)def binary_search(alist, item): return binary_search_achieve(alist, item, 0, len(alist) - 1)# 40亿 32 2^32if __name__ == &#x27;__main__&#x27;: alist = [1, 2, 3, 4, 5, 6, 7, 8, 9] print(binary_search(alist, 8)) print(binary_search(alist, 100)) 4.2.3 非递归版本 1234567891011121314151617181920212223242526272829def binary_search(alist, item): &quot;&quot;&quot;二分查找&quot;&quot;&quot; # 设置起始位置 获取中间值 start = 0 end = len(alist) - 1 # 必须要= 需要比较最后的一个元素是否是需要的数据 while start &lt;= end: # 获取中间值 # mid = (start+end)//2 # end//2+start//2 mid = start+(end-start)//2 # start+(end-start)//2 if item == alist[mid]: return mid elif item &lt; alist[mid]: end = mid - 1 elif item &gt; alist[mid]: start = mid + 1 # 没有找到想要找的数字 return -1# 最好 O(1) 最坏 O(logn)if __name__ == &#x27;__main__&#x27;: alist = [1,2,3,4,5,6,7,8,9] print(binary_search(alist, 1)) print(binary_search(alist, 5)) print(binary_search(alist, 100)) 4.2.4 二分查找 - 位置 1234567891011121314151617181920212223242526272829303132def binary_search_first(alist, item): &quot;&quot;&quot;二分查找 如果找到返回索引 没有找到返回-1&quot;&quot;&quot; # 设置起始位置 获取中间值 start = 0 end = len(alist) - 1 # 必须要= 需要比较最后的一个元素是否是需要的数据 while start &lt;= end: # 获取中间值 mid = (start + end)//2 if item == alist[mid]: # 索引为0 不需要继续查找 if mid==0 or alist[mid-1]!=item: return mid else: end = mid - 1 elif item &lt; alist[mid]: end = mid - 1 elif item &gt; alist[mid]: start = mid + 1 # 没有找到想要找的数字 return -1# 最好 O(1) 最坏 O(logn)if __name__ == &#x27;__main__&#x27;: alist = [1,2,3,4,5,5,5,6,7,8,9] # print(binary_search(alist, 1)) # print(binary_search(alist, 1)) # 获取最后一个5 # print(binary_search_last(alist, 5)) print(binary_search_first(alist, 5)) 4.2.5 第一个位置 12345678910111213141516171819202122232425262728def binary_search(alist, item): &quot;&quot;&quot;二分查找 如果找到返回索引 没有找到返回-1&quot;&quot;&quot; # 设置起始位置 获取中间值 start = 0 end = len(alist) - 1 # 必须要= 需要比较最后的一个元素是否是需要的数据 while start &lt;= end: # 获取中间值 mid = (start + end)//2 if item == alist[mid]: # 判断是否是第一个3 if mid==0 or alist[mid-1]!=item: return mid else: end = mid - 1 elif item &lt; alist[mid]: end = mid - 1 elif item &gt; alist[mid]: start = mid + 1 # 没有找到想要找的数字 return -1 # 最好 O(1) 最坏 O(logn) if __name__ == &#x27;__main__&#x27;: alist = [1,2,3,3,3,4,5] print(binary_search(alist, 3)) 4.2.6 最后一个位置 12345678910111213141516171819202122232425262728def binary_search(alist, item): &quot;&quot;&quot;二分查找 如果找到返回索引 没有找到返回-1&quot;&quot;&quot; # 设置起始位置 获取中间值 start = 0 maxIndex = len(alist) - 1 end = maxIndex # 必须要= 需要比较最后的一个元素是否是需要的数据 while start &lt;= end: # 获取中间值 mid = (start + end)//2 if item == alist[mid]: if mid == maxIndex or alist[mid+1]!=item: return mid else: start = mid + 1 elif item &lt; alist[mid]: end = mid - 1 elif item &gt; alist[mid]: start = mid + 1 # 没有找到想要找的数字 return -1# 最好 O(1) 最坏 O(logn)if __name__ == &#x27;__main__&#x27;: alist = [1,2,3,3,3,4,5] print(binary_search(alist, 3)) 5 非线性数据结构 - 树 5.1 特点 一种 非线性结构 ，用来模拟具有树状结构性质的数据集合。它是由 n（n&gt;=1）个有限节点组成一个具有层次关系的集合。 ①每个节点有零个或多个子节点 ②没有父节点的节点称为根节点 ③每一个非根节点有且只有一个父节点 ④除了根节点外，每个子节点可以分为多个不相交的子树 名字由来：因为它看起来像一棵倒挂的树，也就是说它是根朝上，而叶朝下的. 拓展：线性结构和非线性结构 线性结构：数据元素之间存在着 “一对一” 的线性关系的数据结构 特点 ①集合中必存在唯一的一个 &quot; 第一个元素” ②集合中必存在唯一的一个 &quot;最后的元素&quot; ③除最后元素之外，其它数据元素均有唯一的 &quot;后继&quot; ④除第一元素之外，其它数据元素均有唯一的 &quot; 前驱” 非线性结构：一个结点元素可能对应多个直接前驱和多个后继的数据结构 5.2 树的术语 节点的度 ：一个节点含有的子树的个数称为该节点的度 树的度 ：一棵树中，最大的节点的度称为树的度 叶节点或终端节点 ：度为零的节点 父亲节点或父节点 ：若一个节点含有子节点，则这个节点称为其子节点的父节点 孩子节点或子节点 ：一个节点含有的子树的根节点称为该节点的子节点 兄弟节点 ：具有相同父节点的节点互称为兄弟节点 节点的层次 ：从根开始定义起，根为第 1 层，根的子节点为第 2 层，以此类推 树的高度或深度 ：树中节点的最大层次 堂兄弟节点 ：父节点在同一层的节点互为堂兄弟 节点的祖先 ：从根到该节点所经分支上的所有节点 子孙 ：以某节点为根的子树中任一节点都称为该节点的子孙 森林 ：由 m（m&gt;=0）棵互不相交的树的集合称为森林 5.3 树的分类 5.3.1 无序树 树中任意节点的子节点之间没有顺序关系，这种树称为无序树，也称为自由树 5.3.2 有序树 树中任意节点的子节点之间有顺序关系，这种树称为有序树 - 霍夫曼树 （用于信息编码）：带权路径最短的二叉树称为哈夫曼树或最优二叉树 - B树 ：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多于两个的子树 1 二叉树 特点：每个节点最多含有两个子树的树，通常子树被称作 “左子树”（left subtree）和 “右子树”（right subtree） 分类 完全二叉树 ：对于一颗二叉树，假设其深度为 d (d&gt;1)。除了第 d 层外，其它各层的节点数目均已达最大值，且第 d 层所有节点从左向右连续地紧密排列的二叉树。 满二叉树 ：所有叶节点都在最底层的完全二叉树 平衡二叉树 （AVL 树）：当且仅当任何节点的两棵子树的高度差不大于 1 的二叉树 排序二叉树 （二叉查找树（英语：Binary Search Tree），也称二叉搜索树、有序二叉树） 基本要求 （1）若左子树不空，则左子树上所有节点的值均小于它的根节点的值 （2）若右子树不空，则右子树上所有节点的值均大于它的根节点的值 （3）左、右子树也分别为二叉排序树 排序二叉树包含空树 存储方式 顺序存储 ：将数据结构存储在固定的数组中，虽然在遍历速度上有一定的优势，但因所占空间比较大，是非主流二叉树存储方式。 链式存储 ：由于对节点的个数无法掌握，常见树的存储表示都转换成二叉树进行处理，子节点个数最多为 2 应用场景 ①xml，html 等，那么编写这些东西的解析器的时候，不可避免用到树 ②路由协议就是使用了树的算法 ③mysql 数据库索引 ④文件系统的目录结构 ⑤所以很多经典的 AI 算法其实都是树搜索，此外机器学习中的 decision tree 也是树结构 性质： 性质 1: 在二叉树的第 i 层上至多有 2^(i-1) 个结点（i&gt;0） 性质 2: 深度为 k 的二叉树至多有 2^k-1 个结点（k&gt;0） 性质 3: 对于任意一棵二叉树，如果其叶结点数为 N0，而度数为 2 的结点总数为 N2，则 N0=N2+1 性质 4: 最多有 n 个结点的完全二叉树的深度必为 log2(n+1) 性质 5: 对完全二叉树，若从上至下、从左至右编号，则编号为 i 的结点，其左孩子编号必为 2i ，其右孩子编号必为 2i＋1 , 其父节点的编号必为 i//2 （i＝1 时为根，除外） 代码实现 完全二叉树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import queueclass Node(object): &quot;&quot;&quot;节点类&quot;&quot;&quot; def __init__(self, item): # 数据类型 10 self.item = item # Node self.lchild = None # Node self.rchild = Noneclass BinaryTree(object): &quot;&quot;&quot;二叉树&quot;&quot;&quot; def __init__(self, node=None): # Node类型 根节点 self.root = node def add(self, item): add_node = Node(item) # 空二叉树 if self.root == None: self.root = add_node return my_queue = queue.Queue() # 把root放入到队列中 my_queue.put(self.root) while True: # 获取元素 node = my_queue.get() # 左右节点是否为空 if node.lchild == None: node.lchild = add_node return if node.rchild == None: node.rchild = add_node return # 放入左右节点 my_queue.put(node.lchild) my_queue.put(node.rchild) def breadh_travel(self): &quot;&quot;&quot;广度优先遍历&quot;&quot;&quot; if not self.root: return # 定义队列 my_queue = queue.Queue() # 放入根节点 my_queue.put(self.root) while not my_queue.empty(): node = my_queue.get() # 当前节点数据打印 print(node.item,end=&#x27;&#x27;) # 左右节点添加到队列中 if node.lchild: my_queue.put(node.lchild) if node.rchild: my_queue.put(node.rchild) def preorder_travel_out(self): self.__preorder_travel(self.root) def __preorder_travel(self, root): &#x27;&#x27;&#x27;先序遍历: 给根节点 可以把这个节点按照根左右的方式遍历出来&#x27;&#x27;&#x27; if root: # 根 print(root.item,end=&#x27;&#x27;)# 0 # 左子树 self.__preorder_travel(root.lchild) # 右子树 self.__preorder_travel(root.rchild) # 给一个节点 就可以对这个节点以及这个节点一下的节点进行中序遍历 def inorder_travel(self, root): &quot;&quot;&quot;中序遍历 左 根 右&quot;&quot;&quot; if root is not None: self.inorder_travel(root.lchild) print(root.item, end=&quot;&quot;) self.inorder_travel(root.rchild) # 给一个节点 就可以对这个节点以及这个节点一下的节点进行后序遍历 def postorder_travel(self, root): &quot;&quot;&quot;后序遍历 根 左 右&quot;&quot;&quot; if root is not None: self.postorder_travel(root.lchild) self.postorder_travel(root.rchild) print(root.item, end=&quot;&quot;)if __name__ == &#x27;__main__&#x27;: tree = BinaryTree() tree.add(&quot;0&quot;) tree.add(&quot;1&quot;) tree.add(&quot;2&quot;) tree.add(&quot;3&quot;) tree.add(&quot;4&quot;) tree.add(&quot;5&quot;) tree.add(&quot;6&quot;) tree.add(&quot;7&quot;) tree.add(&quot;8&quot;) tree.add(&quot;9&quot;) tree.postorder_travel(tree.root) 完全二叉树添加节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import queueclass Node(object): &quot;&quot;&quot;节点类&quot;&quot;&quot; def __init__(self, item): self.item = item self.lchild = None self.rchild = Noneclass BinaryTree(object): &quot;&quot;&quot;完全二叉树&quot;&quot;&quot; def __init__(self, node=None): self.root = node def add(self, item): &quot;&quot;&quot;添加节点&quot;&quot;&quot; if self.root == None: self.root = Node(item) return # 队列 que = queue.Queue() # 从尾部添加数据 que.put(self.root) while True: # 从头部取出数据 node = que.get() # 判断左节点是否为空 if node.lchild == None: node.lchild = Node(item) return else: que.put(node.lchild) if node.rchild == None: node.rchild = Node(item) return else: que.put(node.rchild) def breadh_travel(self): &quot;&quot;&quot;广度优先遍历&quot;&quot;&quot; if self.root == None: return # 队列 que = queue.Queue() # 添加数据 que.put(self.root) while not que.empty(): # while queue: # 取出数据 node = que.get() # 当前节点数据 print(node.item, end=&quot;&quot;) # 判断左右子节点是否为空 if node.lchild is not None: que.put(node.lchild) if node.rchild is not None: que.put(node.rchild)if __name__ == &#x27;__main__&#x27;: tree = BinaryTree() tree.add(&quot;A&quot;) tree.add(&quot;B&quot;) tree.add(&quot;C&quot;) tree.add(&quot;D&quot;) tree.add(&quot;E&quot;) tree.add(&quot;F&quot;) tree.add(&quot;G&quot;) tree.add(&quot;H&quot;) tree.add(&quot;I&quot;) tree.breadh_travel() 二叉树深度优先遍历 广度优先可以找到最短路径 深度优先往往可以很快找到搜索路径 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104class Node(object): &quot;&quot;&quot;节点类&quot;&quot;&quot; def __init__(self, item): self.item = item self.lchild = None self.rchild = Noneclass BinaryTree(object): &quot;&quot;&quot;完全二叉树&quot;&quot;&quot; def __init__(self, node=None): self.root = node def add(self, item): &quot;&quot;&quot;添加节点&quot;&quot;&quot; if self.root == None: self.root = Node(item) return # 队列 queue = [] # 从尾部添加数据 queue.append(self.root) while True: # 从头部取出数据 node = queue.pop(0) # 判断左节点是否为空 if node.lchild == None: node.lchild = Node(item) return else: queue.append(node.lchild) if node.rchild == None: node.rchild = Node(item) return else: queue.append(node.rchild) def breadh_travel(self): &quot;&quot;&quot;广度优先遍历&quot;&quot;&quot; if self.root == None: return # 队列 queue = [] # 添加数据 queue.append(self.root) while len(queue)&gt;0: # 取出数据 node = queue.pop(0) print(node.item, end=&quot;&quot;) # 判断左右子节点是否为空 if node.lchild is not None: queue.append(node.lchild) if node.rchild is not None: queue.append(node.rchild) # 给一个节点 就可以对这个节点以及这个节点一下的节点进行先序遍历 def preorder_travel(self, root): &quot;&quot;&quot;先序遍历 根 左 右&quot;&quot;&quot; if root is not None: print(root.item, end=&quot;&quot;) # 需要对1和1以下的节点先序遍历 self.preorder_travel(root.lchild) self.preorder_travel(root.rchild) # 给一个节点 就可以对这个节点以及这个节点一下的节点进行中序遍历 def inorder_travel(self, root): &quot;&quot;&quot;中序遍历 左 根 右&quot;&quot;&quot; if root is not None: self.inorder_travel(root.lchild) print(root.item, end=&quot;&quot;) self.inorder_travel(root.rchild) # 给一个节点 就可以对这个节点以及这个节点一下的节点进行后序遍历 def postorder_travel(self, root): &quot;&quot;&quot;后序遍历 根 左 右&quot;&quot;&quot; if root is not None: self.postorder_travel(root.lchild) self.postorder_travel(root.rchild) print(root.item, end=&quot;&quot;)if __name__ == &#x27;__main__&#x27;: tree = BinaryTree() tree.add(&quot;0&quot;) tree.add(&quot;1&quot;) tree.add(&quot;2&quot;) tree.add(&quot;3&quot;) tree.add(&quot;4&quot;) tree.add(&quot;5&quot;) tree.add(&quot;6&quot;) tree.add(&quot;7&quot;) tree.add(&quot;8&quot;) tree.add(&quot;9&quot;) tree.preorder_travel(tree.root) print() tree.inorder_travel(tree.root) print() tree.postorder_travel(tree.root) 根据遍历结果反推二叉树 知道 中序遍历 和 先序遍历 或者 后序遍历 就可以推出二叉树的结构","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://leezhao415.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}]},{"title":"数据库MySQL详解_Python版","slug":"数据库MySQL详解-Python版","date":"2021-04-20T12:05:20.000Z","updated":"2021-06-26T02:45:39.349Z","comments":true,"path":"2021/04/20/数据库MySQL详解-Python版/","link":"","permalink":"https://leezhao415.github.io/2021/04/20/%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E8%AF%A6%E8%A7%A3-Python%E7%89%88/","excerpt":"","text":"文章目录 1 数据库基本知识 2 数据库管理系统 3 MySQL 环境搭建 4 客户端 Navicat 5 MySQL 数据类型 6 数据完整性和约束 7 数据库操作流程 7.1 安装数据库服务端 7.2 安装数据库客户端 7.3 开启服务端 7.4 客户端链接服务端 7.5 登录数据库 7.6 创建数据库文件 7.7 创建数据表 7.8 操作数据表 8 数据表查询 9 MySQL 高级进阶 1 数据库基本知识 1.1 概念 以一定格式进行组织的数据的集合 1.2 特点 持久性存储 读写速度极高 保证数据有效性 对程序的支持性比较好，容易拓展 1.3 作用 存储数据 1.4 分类 关系型数据库：采用关系模型来组织数据的数据库，关系模型指的就是二维数据表模型 MySQL：双授权政策，体积小，速度快 Oracle SQL Server 非关系型数据库：NoSQL（Not Only SQL），非关联型。强调Key-Value 存储 MongoDB 2 数据库管理系统 2.1 概念 DBMS （Database Management System）, 为管理数据库而设计的软件系统 2.2 组成 数据库文件集合：存储数据 数据库服务器 数据库客户端 2.3 SQL 语句 作用：实现数据库客户端和数据库服务端进行通讯 含义：结构化查询语言，用来操作 RDBMS（Relational Database Management System）的数据库语言。 2.4 RDBMS 的核心元素 字段：一列数据类型相同的数据 记录：一行记录某个事物的完整信息的数据 数据表：由若干字段和记录组成 数据库：由若干数据表组成 主键：唯一标识一行记录的字段 3 MySQL 环境搭建 3.1 服务端搭建 安装 1sudo apt-get install mysql-server 启动 123sudo service mysql startps -ajx|grep mysql # 查看进程中是否存在MySQL 停止 1sudo service mysql stop 3.2 客户端安装 12345sudo apt-get install mysql-clientmysql -uroot -pmysql #连接命令quit #退出 3.3 MySQL 配置文件 配置文件路径 1/etc/mysql/mysql.conf.d/mysqld.cnf 配置文件信息 123456789bind -address 表示服务器绑定的ip,默认为127.0.0.1port 表示端口，默认为3306datadir 表示数据库目录，默认为/var/lib/mysqlgeneral_log_file 表示普通日志，默认为/var/log/mysql/mysql.loglog_error表示错误日志，默认为/var/log/mysql/error.log 配置文件操作命令 1234# 进入目录cd /etc/mysql/mysql.conf.d# 编辑配置文件vi mysqld.cnf 4 客户端 Navicat MySQL 数据库本身自带命令行工具，但使用上功能性和易用性不是太好。 安装 1234567cd #安装文件所在目录tar -axvf #安装文件名cd #解压的安装文件./start.mysql #运行mysql 5 MySQL 数据类型 5.1 数值 整型 123456789TINYINT #1BytesSMALLINT #2BytesMEDIUMINT #3BytesINT #4Bytes #-2^31~2^31-1或0~2^32-1BIGINT #8Bytes 浮点型 12345float #保留6位小数double #保留16位小数decimal #定点数 例如：decimal(5,2) 5位数字，其中2位小数 5.2 日期 / 时间 123456789date 年-月-日datetime 年-月-日 时:分:秒timestamp 年-月-日 时:分:秒time 时:分:秒year 5.3 字符串（字符） char(n) 定长字符串 n 个字符 varchar(n) 变长字符串 n+1 个字符 text 5.4 枚举类型 enum 6 数据完整性和约束 6.1 数据完整性 用于保证数据的正确性，核实其约束条件 参照完整性 约束 作用：保证数据完整性和一致性 约束类型 123456789NOT NULL 非空约束PRIMARY KEY 主键约束UNIQUE KEY 唯一约束DEFAULT 默认约束FOREIGN KEY 外键约束 7 数据库操作流程 7.1 安装数据库服务端 1sudo apt-get install mysql-server 7.2 安装数据库客户端 1sudo apt-get install mysql-client 7.3 开启服务端 1sudo service mysql start 7.4 客户端链接服务端 7.5 登录数据库 123mysql -uroot -pmysql mysql -uroot -p --输入密码 7.5.1 退出数据库 12345quit exit 快捷操作：Ctrl+D 7.5.2 显示数据库版本 123select version(); 快捷操作：Ctrl +Shift +V 7.5.3 显示时间 1select now(); 7.5.4 快捷操作 1234567Ctrl + a #到行首 Ctrl + l #清屏 Ctrl + e #到行尾 Ctrl + C+回车 #结束 7.6 创建数据库文件 1234567891011show databases; --查看所有数据库 show create database 数据库名称; --查看创建的数据信息,包括字符集及名称等 select database(); --查看当前使用的数据库 create database 数据库名称 charset=utf8; use 数据库名称; --使用数据库 drop database 数据库名称; --删除指定数据库 7.7 创建数据表 指定字段 字段类型 int varchar 约束 7.8 操作数据表 7.8.1 查看表结构 1show tables; --查看当前所有的表 123desc 表名; --查看表结构show create table 表名; --查看创建的数据表详细信息 7.8.2 约束 1234567int unsigned --无符号整型auto_increment --自动增长not null --非空primary key --主键 7.8.3 创建数据表 12345678create table heima( id int unsigned primary key auto_increment not null, name varchar(20) not null, age int unsigned default 0, high decimal(5,2), gender enum(&quot;男&quot;,&quot;女&quot;), cls_id int unsigned ); ​ 7.8.4 数据表操作 增 1alter table 表名 add 列名 类型; --增加字段 删 123alter table drop table 表名; --删除字段drop table 表名; --删除数据表 改 12345#修改类型及约束alter table 表名 modify 列名 类型及约束;# 重命名alter table 表名 change 原名 新名 类型及约束; 查 1desc 表名; --查看表结构 7.8.5 表数据操作 增 12345insert into 表名 values(); --全列插入insert into 表名 (表头1，表头2) values(值1，值2); --部分列插入insert into 表名 values( ),( ); --多行插入 删 12#方法一delete from 表名 where 条件； 1234#方法二alter table 表名 add is_delete bit default 0; --定义is_delete字段来保留并标识删除的数据update 表名 set is_delete=1 where 条件; --将删除的数据的标志物置1select * from 表名 where is_delete=1; --查询删除的数据 改 123update 表名 set 列名=值;update 表名 set 列名1=值 where 【条件】; 查 12345select * from 表名;select 列1，列2... from 表名;select 列1 as 别名1， 列名2 as 别名2 from 表名; 文本编辑器 notepad++ sublime Text 8 数据表查询 8.1 去重查询 1select distinct 列名 from 表名; 8.2 起别名查询 字段起别名 1select 字段名 as 别名 from 表名; 数据表起别名 1select 字段名 from 表名 as 别名; 8.3 where 查询 对数据表中的数据进行筛选，结果为 True 的记录会出现在结果集中。 1select * from 表名 where 条件; 比较运算符：=、&gt;、&lt;、&gt;=、&lt;=、&lt;&gt; 或者！=、!&gt;、!&lt; 1234-- 查询年龄不为18岁的所有学生的名字select * from students where age != 18;select * from students where age &lt;&gt; 18; 逻辑运算符：and、or、not 12-- 不在 18岁以上的女性 这个范围内的信息select * from students where not (age&gt;18 and gender=&quot;女&quot;); 模糊查询：like、%(替代任意多个)、_(替代任意一个)、[ ]（指定一个字符、字符串或范围）、[^]（指定字符以外的任一个字符） 12--查询姓名中 有 &quot;小&quot; 所有的名字select * from students where name like &quot;%小%&quot;; 范围查询：in、between … and … 12--查询身高在165-180，年龄在18-34岁的女性的姓名、性别、身高select name as &#x27;姓名&#x27;,gender as &#x27;性别&#x27;,height as &#x27;身高&#x27; from students where (height between 165 and 180) and (age between 18 and 34) and (gender = &#x27;女&#x27;); 空值判断：is null、is not null 12-- 判非空is not nullselect * from students where height is not null; order 排序查询 12-- 查询年龄在18到34岁之间的女性，身高从高到矮排序, 如果身高相同的情况下按照年龄从大到小排序select * from students where (age between 18 and 34) and gender=2 order by height desc, age desc; 12--查询身高在165-180的男性或者年龄在20-40的女性，并以年龄升序排序，如果相同则以身高降序排列。select * from students where (height between 165 and 180 and gender=1) and (age between 20 and 40 and gender=2) order by age asc,height desc; 聚合函数 将当前所在表当做一个组进行统计 count ()：计算总行数 12-- 查询男性有多少人select count(*) from students where gender=1; max ()：计算最大值 12-- 查询最大的年龄select max(age) from students; min ()：计算最小值 12-- 查询最低身高select min(height) from students; sum ()：求和 12-- 计算所有人的年龄总和select sum(age) from students; avg ()：求平均值 12-- 计算平均年龄select avg(age) from students; 123--round() 四舍五入取值-- 计算所有人的平均年龄，保留2位小数select round(avg(age),2) from students; 8.4 group 分组查询 group by 将查询结果按照 1 个或者多个字段进行分组 group by 12-- 按照性别分组,查询所有的性别select gender from students group by gender; group by + group_concat() group_concat (字段名) 作用：根据分组结果，使用 group_concat () 来放置每一个分组中某字段的集合 12-- 查询同种性别中的姓名select group_concat(name),gender from students group by gender; group by + 聚合函数 作用：聚合函数在和 group by 结合使用的时候 统计的对象是每一个分组 12345-- 计算每种性别中的人数select gender,count(*) from students group by gender;-- 查询每组性别的平均年龄select avg(age),gender from students group by gender; group by + having 作用 : having 作用和 where 类似，但 having 只能用于 group by 对分组后的每组数据过滤 而 where 是用来过滤表数据 12-- 查询平均年龄超过30岁的性别，以及姓名 having avg(age) &gt; 30(重点)select group_concat(name),gender from students group by gender having avg(age) &gt; 30; group by + with rollup with rollup 的作用是：在数据表最后新增一行，来记录当前表中该字段对应的操作结果，一般是汇总结果 12-- with rollup 汇总的作用(了解)select count(*),gender from students group by gender with rollup; 8.5 limit 分页查询 Why：如果数据庞大，直接 select * 会导致数据库崩溃 含义：限制取出记录的数量，limit 要写在 SQL 语句的最后。 语法格式：limit 起始记录 记录数 12-- 每页显示2个，显示第4页的信息, 按照年龄从小到大排序select * from students order by age asc limit 6,2; 8.6 连接查询 当查询结果来源于多个表的时候，需要将多张表的内容连接成一个大的数据集进行汇总显示。 内连接：inner join 查询的结果为两个表符合条件匹配的数据集 语法格式：select 字段 from 表 A inner join 表 B on 表 A. 字段 1 = 表 B. 字段 2 1234567891011-- select ... from 表A inner join 表B;select * from students inner join classes;-- 查询 有能够对应班级的学生以及班级信息select * from students inner join classes on students.cls_id=classes.id;-- 显示姓名、班级select students.name,classes.name from students inner join classes on students.cls_id=classes.id;-- 给数据表起名字select s.name,c.name from students as s inner join classes as c on s.cls_id=c.id; on 和 where 的区别： on 连接条件 where 连接后进行筛选的条件 外连接 左连接：left join 主表在SQL语句的左边，连接后主表中未匹配的从表的数据对应字段以Null展示。 语法格式：主表 left join 从表 on 连接条件； 12-- 查询每位学生对应的班级信息select * from students left join classes on students.cls_id=classes.id; 右连接：right join 主表在SQL语句的右边，连接后主表中未匹配的从表的数据对应字段以Null展示。 语法格式：主表 right join 从表 on 连接条件； 12-- 将数据表名字互换位置，用left join完成select * from classes right join students on students.cls_id=classes.id; 自连接 Why：加快查询速度，减少数据表占用空间。 source 资源 # 快速导入资源数据库 12-- 从sql文件中导入文件source areas.sql 1234567-- 查询省的名称为“广东省”的所有城市select city.* from areas as cityinner joinareas as provinceon city.pid=province.aidwhere province.atitle=&quot;广东省&quot;; 8.7 子查询 定义：把一个查询的结果当做另一个查询的条件 分类 标量子查询：子查询返回的结果是一个数据 (一行一列) 列子查询：返回的结果是一列 (一列多行) 行子查询：返回的结果是一行 (一行多列) 语法格式： 12345--(1)查询出高于平均身高的信息(height)select * from students where height &gt; (select avg(height) from students); --(2)查询学生的班级号能够对应的 学生名字select * from students where cls_id in (select id from classes); 9 MySQL 高级进阶 9.1 实战操作 创建数据库 1create database 数据库 charset=utf8; 打开数据库 1use 数据库; show databases；查看所有数据库 show create database 数据库；查看创建的数据库信息 创建数据表 1create table 数据表 (...); desc 数据表；查看数据表结构信息 show tables; 查看所有的数据表列表 插入数据 1insert into 数据库 values(...); select * from 数据表； 常用数据表操作 - 数据查询 12345678910111213141516171819202122232425262728293031323334353637-- 查询类型 cate_name 为 &#x27;超级本&#x27; 的商品名称 name 、价格 price ( where )select name,price from goods where cate_name=&quot;超级本&quot;;-- 显示商品的种类-- 1 分组的方式( group by ) select cate_name from goods group by cate_name;-- 2 去重的方法( distinct )select distinct cate_name from goods;-- 求所有电脑产品的平均价格 avg ,并且保留两位小数( round )select round(avg(price),2) from goods;-- 显示 每种类型 cate_name (由此可知需要分组)的 平均价格select avg(price),cate_name from goods group by cate_name;-- 查询 每种类型 的商品中 最贵 max 、最便宜 min 、平均价 avg 、数量 countselect cate_name,max(price),min(price),avg(price),count(*) from goods group by cate_name;-- 查询所有价格大于 平均价格 的商品，并且按 价格降序 排序 order desc-- 1 查询平局价格(avg_price)select avg(price) as avg_price from goods;-- 2 使用子查询select * from goods where price&gt;(select avg(price) as avg_price from goods) order by price desc;-- 查询每种类型中最贵的电脑信息(难)-- 1 查找 每种类型 中 最贵的 max_price 价格select max(price) as max_price,cate_name from goods group by cate_name;-- 2 关联查询 inner join 每种类型 中最贵的物品信息select * from goodsinner join(select max(price) as max_price,cate_name from goods group by cate_name) as max_price_goodson goods.cate_name=max_price_goods.cate_name and goods.price=max_price_goods.max_price; - 数据表优化 - 优化步骤 （1）创建商品种类表 （2）同步数据到商品种类表中 （3）更新商品信息表数据 （4）修改商品信息表表结构 1234567891011121314151617181920212223242526第一步 创建表 (商品种类表 goods_cates )create table if not exists goods_cates( id int unsigned primary key auto_increment, name varchar(40) not null);第二步 同步 商品分类表 数据 将商品的所有 (种类信息) 写入到 (商品种类表) 中-- 按照 分组 的方式查询 goods 表中的所有 种类(cate_name)select cate_name from goods group by cate_name;--插入分组后的数据，注意：语句中没有values关键字insert into goods_cates(name) (select cate_name from goods group by cate_name);第三部 同步 商品表 数据 通过 goods_cates 数据表来更新 goods 表-- 因为要通过 goods_cates表 更新 goods 表 所以要把两个表连接起来select * from goods inner join goods_cates on goods.cate_name=goods_cates.name; -- 把 商品表 goods 中的 cate_name 全部替换成 商品分类表中的 商品id ( update ... set )update (goods inner join goods_cates on goods.cate_name=goods_cates.name) set goods.cate_name=goods_cates.id;第四部 修改表结构-- 查看表结构(注意 两个表中的 外键类型需要一致)desc goods;-- 修改表结构 alter table 字段名字不同 change,把 cate_name 改成 cate_id int unsigned not nullalter table goods change cate_name cate_id int unsigned not null; 9.2 外键 定义：一个数据表的主键 A 在另一个数据表 B 中出现，则称为 A 是数据表 B 的外键。 作用：限制无效信息的插入 操作说明 123456789101112131415161718192021-- 外键的使用-- 向goods表里插入任意一条数据insert into goods (name,cate_id,brand_id,price) values(&#x27;老王牌拖拉机&#x27;, 10, 10,&#x27;6666&#x27;);-- 约束 数据的插入 使用 外键 foreign key-- alter table goods add foreign key (brand_id) references goods_brands(id);alter table goods add foreign key(cate_id) references goods_cates(id); alter table goods add foreign key(brand_id) references goods_brands(id); -- 失败原因 老王牌拖拉机 delete-- delete from goods where name=&quot;老王牌拖拉机&quot;;delete from goods where name=&quot;老王牌拖拉机&quot;;-- 如何取消外键约束-- 需要先获取外键约束名称,该名称系统会自动生成,可以通过查看表创建语句来获取名称show create table goods;-- 获取名称之后就可以根据名称来删除外键约束alter table goods drop foreign key goods_ibfk_1;alter table goods drop foreign key goods_ibfk_2; 9.3 视图 定义：能够把复杂 SQL 语句的功能封装起来的一个虚表 特点 视图是对若干张基本表的引用，一张虚表， 不存储具体的数据（基本表数据发生了改变，视图也会跟着改变） 方便操作，特别是查询操作，减少复杂的 SQL 语句，增强可读性，复用性； 操作说明 1234567891011121314151617181920212223-- 视图的定义方式create view 视图名称(一般使用v开头) as select语句; -- 查出学生的id,姓名,年龄,性别 和 学生的 班级select s.id,s.name,s.age,s.gender,c.name as cls_name from students as s inner join classes as c on s.cls_id=c.id;-- 创建上述结果的视图( v_students )-- create view v_students as --注意：连接后的字段名不能相同，若相同，需用as其别名区分。create view v_students as select s.id,s.name,s.age,s.gender,c.name as cls_name from students as s inner join classes as c on s.cls_id=c.id;--查看视图show tables;-- 删除视图（drop view 视图名字）drop view v_students; 9.4 事务 定义：Transaction，是指作为一个基本工作单元执行的一系列 SQL 语句的操作，要么完全地执行，要么完全地都不执行。 应用 A 用户和 B 用户是银行的储户，现在 A 要给 B 转账 500 元，那么需要做以下几件事： 检查 A 的账户余额 &gt; 500 元； A 账户中扣除 500 元； B 账户中增加 500 元； 事务四大特性 ACID 原子性 (Atomicity)[ætəˈmɪsəti] 一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功， 要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作。 一致性 (Consistency) 数据库总是从一个一致性的状态转换到另一个一致性的状态。（在前面的例子中，一致性确保了即使在执行第三、四条语句之间时系统崩溃，支票账户中也不会损失 500 元，因为事务最终没有提交，所以事务中所做的修改也不会保存到数据库中。） 隔离性 (Isolation) 通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。（在前面的例子中，当执行完第三条语句、第四条语句还未开始时，此时有另外的一个账户汇总程序开始运行，则其看到支票帐户的余额并没有被减去 500 元。） 持久性 (Durability) 一旦事务提交，则其所做的修改会永久保存到数据库。（此时即使系统崩溃，修改的数据也不会丢失。） 事务的使用 开启事务 开启事务后执行修改命令，变更会维护到本地缓存中，而不维护到物理表中 begin; 或者 start transaction; 提交事务 将缓存中的数据变更维护到物理表中 commit; 回滚事务 放弃缓存中变更的数据 表示事务执行失败 应该回到开始事务前的状态 rollback; MySQL 默认的存储引擎：innodb 查看指令：show engines \\G; 9.5 索引 定义：一种特殊的文件 (InnoDB 数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的位置信息。 使用说明 12345678查看表中已有索引show index from 表名;创建索引alter table 表名 add index 索引名【可选】（字段名，…）;删除索引drop index 索引名称 on 表名; 优缺点 优点： 加快数据的查询速度 缺点： 创建索引会浪费时间和占用磁盘空间，并且随着数据量的增加所耗费的时间会越来越多。 使用原则 经常发生数据更新的表避免使用过多的索引 数据量小的表没有必要使用索引 数据量较大同时不会频发发生数据更改的表可以使用索引 代码实现 1234567891011121314151617181920212223-- 创建测试表create table test_index(title varchar(10));-- 向表中插入10万条数据python3 insert_data.py-- 验证索引性能-- 没有索引-- 开启时间检测：set profiling=1;-- 查找第1万条数据ha-99999select * from test_index where title=&quot;ha-99999&quot;;-- 查看执行时间show profiles;-- 有索引-- 给title字段创建索引alter table test_index add index(title);-- 查找第1万条数据ha-99999select * from test_index where title=&quot;ha-99999&quot;;-- 查看执行时间show profiles; 9.6 数据库设计三范式 定义：设计关系数据库时，遵从不同的规范要求，设计出合理的关系型数据库，这些不同的规范要求被称为不同的范式， 各种范式呈递次规范，越高的范式数据库冗余越小。 第一范式（1NF）: 强调的是字段的原子性，即一个字段不能够再分成其他几个字段。 第二范式（2NF）: 满足 1NF 的基础上，另外包含两部分内容： 一是表必须有一个主键 二是非主键字段必须完全依赖于主键，而不能只依赖于主键的一部分 主键可以由多个字段共同组成 12345create table test ( name varchar(19), id int, primary key (name,id) ) 第三范式（3NF）: 满足 2NF，另外非主键字段必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键字段 A 依赖于非主键字段 B，非主键字段 B 依赖于主键的情况。 其他范式：巴斯科德范式、第五范式等 9.7 E-R 模型及表间关系 定义：实体 - 关系模型，用来描述数据库存储数据的结构模型 表现形式； 实体：用矩形表示，并标注实体名称 属性：用椭圆表示，并标注属性名称 关系：用菱形表示，并标注关系名称 E-R 模型三种关系: 一对一 一对多 (1-n) 多对多 (n-n) 9.8 Python 连接 MySQL 数据库编程：通过使用程序代码的方式去连接 MySQL 数据库，然后对 MySQL 数据库进行增删改查的方式。 使用步骤：（调用 pymysql） ①导入 pymysql 包 import pymysql ②创建连接对象 connect() ③获取游标对象 连接对象.cursor() ④ pymysql 完成数据的查询操作 游标对象.execute() ⑤ 关闭游标和连接 游标对象.close() 连接对象.close() 代码实现 查询操作 12345678910111213141516171819202122# 导入pymysql包import pymysql# 创建连接对象conn = pymysql.connect(host=&quot;localhost&quot;, port=3306, user=&quot;root&quot;, password=&quot;mysql&quot;, database=&quot;python_test_1&quot;, charset=&quot;utf8&quot;)# 获取游标对象cur = conn.cursor()# pymysql完成数据的查询操作sql = &quot;select * from students;&quot;# 这里content获取的是sql影响的行数# content = cur.execute(sql)# print(content)cur.execute(sql)content = cur.fetchone()print(content)content = cur.fetchall()print(content)# 关闭游标和连接cur.close()conn.close() 增删改操作 123456789101112131415161718192021222324252627# 导入pymysql包import pymysql# 创建连接对象conn = pymysql.connect(host=&quot;localhost&quot;, port=3306, user=&quot;root&quot;, password=&quot;mysql&quot;, database=&quot;python_test_1&quot;, charset=&quot;utf8&quot;)# 获取游标对象cs = conn.cursor()# 增加数据# sql = &quot;insert into students(name) values(&#x27;老王&#x27;)&quot;# cs.execute(sql)# 删除数据# sql = &quot;delete from students where id=18&quot;# cs.execute(sql)# 修改数据sql = &quot;update students set name=&#x27;老王&#x27; where id=1;&quot;cs.execute(sql)for i in content: print(i)# 提交操作conn.commit()# 关闭游标和连接cs.close()conn.close() 9.9 SQL 语句参数化 目的：防止 SQL 注入 SQL 注入：用户提交带有恶意的数据与 SQL 语句进行字符串方式的拼接，从而影响了 SQL 语句的语义，最终产生数据泄露的现象。 参数化实现 12345678910111213141516171819202122232425262728293031# 导入pymysql包import pymysql# 创建连接对象conn = pymysql.connect(host=&quot;localhost&quot;, port=3306, user=&quot;root&quot;, password=&quot;mysql&quot;, database=&quot;python_test_1&quot;, charset=&quot;utf8&quot;)# 获取游标对象cs = conn.cursor()# 不安全的方式# 根据id查询学生信息# find_name = input(&quot;请输入您要查询的学生姓名:&quot;)# sql = &quot;select * from students where name=&#x27;%s&#x27;&quot; % find_name# # 显示所有的数据# cs.execute(sql)# content = cs.fetchall()# for i in content:# print(i)# 安全的方式# 根据id查询学生信息find_name = input(&quot;请输入您要查询的学生姓名:&quot;)sql = &quot;select * from students where name=%s&quot;# 显示所有的数据cs.execute(sql, [find_name])content = cs.fetchall()for i in content: print(i)# 关闭游标和连接cs.close()conn.close()","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"数据库原理","slug":"数据库原理","permalink":"https://leezhao415.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/"}]},{"title":"静态Web服务器搭建代码实现_Python","slug":"静态Web服务器搭建代码实现-Python","date":"2021-04-20T12:04:55.000Z","updated":"2021-07-14T12:40:46.929Z","comments":true,"path":"2021/04/20/静态Web服务器搭建代码实现-Python/","link":"","permalink":"https://leezhao415.github.io/2021/04/20/%E9%9D%99%E6%80%81Web%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-Python/","excerpt":"","text":"文章目录 1 浏览器网络请求流程 2 搭建静态 web 服务器 2.1 Web 服务器开发流程 2.2 返回指定页面 2.3 多任务版服务器 2.4 面向对象服务端 3 动态端口 1 浏览器网络请求流程 浏览器首先链接 DNS 服务器，获取域名对应的 ip 地址 通过 ip 和 http 默认端口 80 链接服务器 浏览器将请求数据组合成 http 协议的请求文本，发送给服务端 服务端收到请求，需要解析请求文本，根据请求资源路径找到对应的资源 把资源按照 http 响应格式组成响应文本发送给浏览器 浏览器收到响应文本之后进行解析展示 2 搭建静态 web 服务器 123python -m http.server # linuxpython3 -m http.server -m 运行模块 __name_以__main_ 2.1 Web 服务器开发流程 编写一个 TCP 服务端程序 获取浏览器发送的 HTTP 请求报文数据 读取固定页面数据，把页面数据组装成 HTTP 响应报文数据发送给浏览器。 HTTP 响应报文数据发送完成以后，关闭服务于客户端的套接字。 123456789101112131415161718192021222324252627282930313233343536373839404142import socketimport timeif __name__ == &#x27;__main__&#x27;: # 1.编写一个TCP服务端程序 # 创建socekt tcp_server_socekt = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 设置端口复用 tcp_server_socekt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) # 绑定地址 tcp_server_socekt.bind((&quot;&quot;, 8080)) # 设置监听 tcp_server_socekt.listen(128) while True: # 2.获取浏览器发送的HTTP请求报文数据 # 建立链接 浏览器充当客户端 访问页面 访问服务端 client_socekt, client_addr = tcp_server_socekt.accept() # 获取浏览器的请求信息 client_request_data = client_socekt.recv(1024).decode() print(client_request_data) # 3.读取固定页面数据，把页面数据组装成HTTP响应报文数据发送给浏览器 # with open(&quot;./static/index.html&quot;, &quot;rb&quot;) as f: # file_data = f.read() # 二进制读模式 f = open(&quot;./static/index.html&quot;, &quot;rb&quot;) # f = open(&quot;./static/favicon.ico&quot;, &quot;rb&quot;) file_data = f.read() # 应答行 response_line = &quot;HTTP/1.1 200 OK\\r\\n&quot; # 应答头 response_header = &quot;Server:haha\\r\\n&quot; # 应答体 response_body = file_data # 应答数据 response_data = (response_line + response_header + &quot;\\r\\n&quot;).encode() + response_body client_socekt.send(response_data) # 4.HTTP响应报文数据发送完成以后，关闭服务于客户端的套接字 client_socekt.close() 2.2 返回指定页面 解析请求行中的资源路径 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 获取浏览器的请求信息client_request_data = client_socekt.recv(1024).decode()if len(client_request_data) &lt;= 1: print(&quot;客户端已经关闭&quot;) print(client_request_data)else: # 获取用户请求资源的路径 request_data = client_request_data.split(&quot; &quot;) print(request_data) # 求资源的路径 request_path = request_data[1] if request_path == &quot;/&quot;: request_path = &quot;/index.html&quot; # 3.读取固定页面数据，把页面数据组装成HTTP响应报文数据发送给浏览器 # 根据请求资源的路径，读取指定文件的数据 try: with open(&quot;./static&quot; + request_path, &quot;rb&quot;) as f: file_data = f.read() except Exception as e: # 返回404错误数据 # 应答行 response_line = &quot;HTTP/1.1 404 Not Found\\r\\n&quot; # 应答头 response_header = &quot;Server:pwb\\r\\n&quot; # 应答体 response_body = &quot;404 Not Found sorry&quot; # 应答数据 # 组装指定文件数据的响应报文，发送给浏览器 response_data = (response_line + response_header + &quot;\\r\\n&quot; + response_body).encode() client_socket.send(response_data) else: # 应答行 response_line = &quot;HTTP/1.1 200 OK\\r\\n&quot; # 应答头 response_header = &quot;Server:pwb\\r\\n&quot; # 应答体 response_body = file_data # 应答数据 # 组装指定文件数据的响应报文，发送给浏览器 response_data = (response_line + response_header + &quot;\\r\\n&quot;).encode() + response_body client_socket.send(response_data) finally: # 4.HTTP响应报文数据发送完成以后，关闭服务于客户端的套接字 client_socket.close() 2.3 多任务版服务器 开启多个线程处理接收和发送操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import socketimport threading# 获取用户请求资源的路径# 根据请求资源的路径，读取指定文件的数据# 组装指定文件数据的响应报文，发送给浏览器# 判断请求的文件在服务端不存在，组装404状态的响应报文，发送给浏览器def handle_client_request(client_socekt): # 获取浏览器的请求信息 client_request_data = client_socekt.recv(1024).decode() print(client_request_data) # 获取用户请求资源的路径 requst_data = client_request_data.split(&quot; &quot;) print(requst_data) # 判断客户端是否关闭 if len(requst_data) == 1: client_socekt.close() return # 求资源的路径 request_path = requst_data[1] if request_path == &quot;/&quot;: request_path = &quot;/index.html&quot; # 3.读取固定页面数据，把页面数据组装成HTTP响应报文数据发送给浏览器 # 根据请求资源的路径，读取指定文件的数据 try: with open(&quot;./static&quot; + request_path, &quot;rb&quot;) as f: file_data = f.read() except Exception as e: # 返回404错误数据 # 应答行 response_line = &quot;HTTP/1.1 404 Not Found\\r\\n&quot; # 应答头 response_header = &quot;Server:pwb\\r\\n&quot; # 应答体 response_body = &quot;404 Not Found sorry&quot; # 应答数据 # 组装指定文件数据的响应报文，发送给浏览器 response_data = (response_line + response_header + &quot;\\r\\n&quot; + response_body).encode() client_socekt.send(response_data) else: # 应答行 response_line = &quot;HTTP/1.1 200 OK\\r\\n&quot; # 应答头 response_header = &quot;Server:pwb\\r\\n&quot; # 应答体 response_body = file_data # 应答数据 # 组装指定文件数据的响应报文，发送给浏览器 response_data = (response_line + response_header + &quot;\\r\\n&quot;).encode() + response_body client_socekt.send(response_data) finally: # 4.HTTP响应报文数据发送完成以后，关闭服务于客户端的套接字 client_socekt.close()if __name__ == &#x27;__main__&#x27;: # 1.编写一个TCP服务端程序 # 创建socekt tcp_server_socekt = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 设置端口复用 tcp_server_socekt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) # 绑定地址 tcp_server_socekt.bind((&quot;&quot;, 8080)) # 设置监听 tcp_server_socekt.listen(128) while True: # 2.获取浏览器发送的HTTP请求报文数据 # 建立链接 client_socekt, client_addr = tcp_server_socekt.accept() # 创建子线程 sub_thread = threading.Thread(target=handle_client_request, args=(client_socekt,)) sub_thread.start() 关闭浏览器，服务端报错 原因：客户端关闭，服务端 recv 方法不会阻塞，一直读取，读取空数据 12345# 获取浏览器的请求信息client_request_data = client_socekt.recv(1024).decode()# 客户端断开连接 需要停止if not client_request_data:return 2.4 面向对象服务端 定义 Server 类 在__init__中创建 socket 定义 start 开启接收客户端请求 客户端请求接收到之后开启新线程执行后续的接收和发送数据操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import socketimport threading# 获取用户请求资源的路径# 根据请求资源的路径，读取指定文件的数据# 组装指定文件数据的响应报文，发送给浏览器# 判断请求的文件在服务端不存在，组装404状态的响应报文，发送给浏览器class HttpWebServer: def __init__(self): # 1.编写一个TCP服务端程序 # 创建socekt self.tcp_server_socekt = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 设置端口复用 self.tcp_server_socekt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True) # 绑定地址 self.tcp_server_socekt.bind((&quot;&quot;, 8080)) # 设置监听 self.tcp_server_socekt.listen(128) def handle_client_request(self, client_socekt): # while True: # 获取浏览器的请求信息 client_request_data = client_socekt.recv(1024).decode() print(client_request_data) # 获取用户请求资源的路径 request_data = client_request_data.split(&quot; &quot;) print(request_data) # 判断客户端是否关闭 if len(request_data) == 1: client_socekt.close() return # 求资源的路径 request_path = request_data[1] if request_path == &quot;/&quot;: request_path = &quot;/index.html&quot; # 3.读取固定页面数据，把页面数据组装成HTTP响应报文数据发送给浏览器 # 根据请求资源的路径，读取指定文件的数据 try: with open(&quot;./static&quot; + request_path, &quot;rb&quot;) as f: file_data = f.read() except Exception as e: # 返回404错误数据 # 应答行 response_line = &quot;HTTP/1.1 404 Not Found\\r\\n&quot; # 应答头 response_header = &quot;Server:pwb\\r\\n&quot; # 应答体 response_body = &quot;404 Not Found sorry&quot; # 应答数据 # 组装指定文件数据的响应报文，发送给浏览器 response_data = (response_line + response_header + &quot;\\r\\n&quot; + response_body).encode() client_socekt.send(response_data) else: # 应答行 response_line = &quot;HTTP/1.1 200 OK\\r\\n&quot; # 应答头 response_header = &quot;Server:pwb\\r\\n&quot; # 应答体 response_body = file_data # 应答数据 # 组装指定文件数据的响应报文，发送给浏览器 response_data = (response_line + response_header + &quot;\\r\\n&quot;).encode() + response_body client_socekt.send(response_data) finally: # 4.HTTP响应报文数据发送完成以后，关闭服务于客户端的套接字 client_socekt.close() def start(self): while True: # 2.获取浏览器发送的HTTP请求报文数据 # 建立链接 client_socekt, client_addr = self.tcp_server_socekt.accept() # 创建子线程 sub_thread = threading.Thread(target=self.handle_client_request, args=(client_socekt,)) sub_thread.start()if __name__ == &#x27;__main__&#x27;: # 创建服务器对象 my_web_server = HttpWebServer() # 启动服务器 my_web_server.start() 3 动态端口 获取通过命令行执行 python 文件传递的参数 123456import sys# 没有传参数 默认就是模块名argv = sys.argvif len(argv)==2 and argv[1].isdigit(): print(argv[1]) 默认没有传递参数，只有一个元素 文件名 sub_thread.start() if name == ‘main’: # 创建服务器对象 my_web_server = HttpWebServer () # 启动服务器 my_web_server.start () 123456789101112## 动态端口获取通过命令行执行python文件传递的参数​```pythonimport sys# 没有传参数 默认就是模块名argv = sys.argvif len(argv)==2 and argv[1].isdigit(): print(argv[1]) 默认没有传递参数，只有一个元素 文件名","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"网络编程","slug":"网络编程","permalink":"https://leezhao415.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"URL及HTTP协议详解","slug":"URL及HTTP协议详解","date":"2021-04-19T12:25:01.000Z","updated":"2021-07-14T13:19:16.883Z","comments":true,"path":"2021/04/19/URL及HTTP协议详解/","link":"","permalink":"https://leezhao415.github.io/2021/04/19/URL%E5%8F%8AHTTP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"文章目录 1 URL 2 HTTP 协议 3 HTTP 报文 4 HTTP 协议通信 1 URL 统一资源定位符，(Uniform Resoure Locator) 通俗理解就是网络资源地址，通过 URL 能够找到对应的资源数据。 URL 地址：https://www.baidu.com/10/E178J2O489FH.html?page=1&amp;count=10 ​ &lt;协议部分 + 域名 + 资源路径 + 查询参数&gt; 协议部分: https://、http://、ftp:// 域名：IP 地址的别名，它是用点进行分割使用英文字母和数字组成的名字，使用域名目的就是方便的记住某台主机 IP 地址。 参数说明：? 后面的 page 表示第一个参数，后面的参数都使用 &amp; 进行连接 2 HTTP 协议 HTTP 协议的全称是 (HyperText Transfer Protocol)，超文本传输协议。 超文本是指在文本数据的基础上还包括非文本数据，非文本数据有图片、音乐、视频等，而这些非文本数据会使用链接的方式进行加载显示，通俗来说超文本就是带有链接的文本数据也就是我们常说的网页数据。 HTTP 协议的制作者是蒂姆・伯纳斯 - 李，1991 年设计出来的，HTTP 协议设计之前目的是传输网页数据的，现在允许传输任意类型的数据。 传输 HTTP 协议格式的数据是基于 TCP 传输协议的，发送数据之前需要先建立连接。 TCP 传输协议是用来保证网络中传输的数据的安全性的，HTTP 协议是用来规定这些数据的具体格式的。 HTTP 协议规定的数据格式是浏览器和 Web 服务器通信数据的格式，也就是说浏览器和 Web 服务器通信需要使用 HTTP 协议。 3 HTTP 报文 HTTP 请求报文 GET 方式的请求报文：获取 Web 服务器数据 123456789101112---- 请求行 ----GET /a/b/c HTTP/1.1 # GET请求方式 请求资源路径 HTTP协议版本---- 请求头 -----Host: www.itcast.cn # 服务器的主机地址和端口号,默认是80Connection: keep-alive # 和服务端保持长连接Upgrade-Insecure-Requests: 1 # 让浏览器升级不安全请求，使用https请求User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36 # 用户代理，也就是客户端的名称Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8 # 可接受的数据类型Accept-Encoding: gzip, deflate # 可接受的压缩格式Accept-Language: zh-CN,zh;q=0.9 #可接受的语言Cookie: pgv_pvi=1246921728; # 登录用户的身份标识---- 空行 ---- 注意：每项数据之间使用:\\r\\n POST 方式的请求报文：向 Web 服务器提交数据 12345678910---- 请求行 ----POST /xmweb?host=mail.itcast.cn&amp;_t=1542884567319 HTTP/1.1 # POST请求方式 请求资源路径 HTTP协议版本---- 请求头 ----Host: mail.itcast.cn # 服务器的主机地址和端口号,默认是80Connection: keep-alive # 和服务端保持长连接Content-Type: application/x-www-form-urlencoded # 告诉服务端请求的数据类型User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36 # 客户端的名称---- 空行 -------- 请求体 ----username=hello&amp;pass=hello # 请求参数 注意：每项数据之间使用:\\r\\n 总结： 一个 HTTP 请求报文可以由请求行、请求头、空行和请求体 4 个部分组成。 请求行是由三部分组成: 请求方式 请求资源路径 HTTP 协议版本 GET 方式的请求报文没有请求体，只有请求行、请求头、空行组成。 POST 方式的请求报文可以有请求行、请求头、空行、请求体四部分组成。 注意：POST 方式可以允许没有请求体，但是这种格式很少见。 HTTP 响应报文 12345678910--- 响应行/状态行 ---HTTP/1.1 200 OK # HTTP协议版本 状态码 状态描述--- 响应头 ---Server: Tengine # 服务器名称Content-Type: text/html; charset=UTF-8 # 内容类型Connection: keep-alive # 和客户端保持长连接Date: Fri, 23 Nov 2018 02:01:05 GMT # 服务端的响应时间--- 空行 ------ 响应体 ---&lt;!DOCTYPE html&gt;&lt;html lang=“en”&gt; …&lt;/html&gt; # 响应给客户端的数据 注意：每项数据之间使用:\\r\\n HTTP 状态码：表示 Web 服务器响应状态 状态码 说明 200 服务器已成功处理了请求 400 错误的请求，请求地址或者参数有误 404 请求资源在服务器不存在 500 服务器内部源代码出现错误 总结： 一个 HTTP 响应报文是由响应行、响应头、空行和响应体 4 个部分组成。 响应行是由三部分组成：HTTP 协议版本 状态码 状态描述，最常见的状态码是 200 4 HTTP 协议通信 通信原理 注意：每一次浏览器和服务器的数据通讯，都是成对出现的即请求和响应， 同时每一次请求和响应都必须符合 HTTP 协议的格式 谷歌浏览器开发者工具的使用 标签选项说明： 元素（Elements）：用于查看或修改 HTML 标签 控制台（Console）：执行 JS 代码 源代码（Sources）：查看静态资源文件，断点调试 JS 代码 网络（Network）：查看 http 协议的通信过程 使用说明： ①点击 Network 标签选项 ②在浏览器地址栏输入百度的网址，就能看到请求百度首页的 HTTP 的通信过程 ③这里的每项记录都是请求 + 响应的一次过程 查看 HTTP 协议的通信过程 总结： 谷歌浏览器的开发者工具是查看 http 协议的通信过程利器，通过 Network 标签选项可以查看每一次的请求和响应的通信过程，调出开发者工具的通用方法是在网页右击选择检查。 Headers 选项总共有三部分组成: ①General: 主要信息 ②Response Headers: 响应头 ③Request Headers: 请求头 Response 选项是查看响应体信息的","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"网络通信","slug":"网络通信","permalink":"https://leezhao415.github.io/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/"}]},{"title":"Python基础及进阶","slug":"Python基础及进阶","date":"2021-04-19T12:20:07.000Z","updated":"2021-06-26T02:42:06.896Z","comments":true,"path":"2021/04/19/Python基础及进阶/","link":"","permalink":"https://leezhao415.github.io/2021/04/19/Python%E5%9F%BA%E7%A1%80%E5%8F%8A%E8%BF%9B%E9%98%B6/","excerpt":"","text":"@Pycharm 快捷方式 文章目录 2021.03.04 2021.03.05 2021.03.06 2021.03.08 2021.03.09 2021.03.10 2021.03.11 2021.03.12 2021.03.14 2021.03.15 2021.03.19 2021.03.20 2021.03.21 2021.03.22 2021.03.23 2021.03.25 2021.03.28 2021.04.03 Ctrl+Alt+L （代码格式化） Ctrl+Alt+M （提取方法） shift +enter 跳转到下一行 Ctrl + Shift + 方向键 移动代码 shift + tab 退一个 Tab 键 Ctrl + Q 查看函数的说明文档 Ctrl + F12 查看当前项目定义的函数的列表 Alt + Enter 提示错误信息 shift +F6 Pycharm 中替换功能 2021.03.04 （1）内存（RAM）：存储计算机运行的数据 （2）计算机：高速计算的电子计算器，数值计算、逻辑计算、记忆存储。 （3）冯诺依曼体系：输入设备（键鼠扫）、输出设备（显打）、运算器、控制器、存储器。 （4）系统软件（建立计算机和人之间的联系）、应用软件（实现特定的功能）。 （5）编程语言：Java、Python、C、C#、Cpp （6）应用领域：网络爬虫、人工智能、机器学习、数据分析、Web 开发、自动化运维、自动化测试等。 （7）Google 开源机器学习框架：TensorFlow 开源社区主推学习框架：Scikit-learn 百度开源深度学习框架：Paddle （8）Python 缺点：运行速度慢 不适用 Python：性能要求比较高的场景：游戏引擎、底层开发。 （9）爬虫：爬取关键信息，获取感兴趣内容。 （10）TIOBE 2021 02 2021.02 编程语言排行榜 （11）基础、流程控制、数据序列、函数、文件操作、面向对象、模块 / 包 / 异常。 （12）解释型：解释器读取一段，解释执行一段。 编译型：通过编译器将程序转化为二进制机器码执行 （13）Python 解释器：CPython（官方版解释器，用 C 写的）、IPython（内核 CPython，外观）、PyPy（动态编译）、Jython（把 Python 语言编写成 Java 的字节码文件）、ironPython（编译成.Net 字节码文件） （14）IDE（集成开发环境）：代码提示、命令简化、debug 调试 。例如 Pycharm （15）注释：单行注释（#(习惯：# 后加一个空格) ctrl + /）、多行注释（’’‘Python’’' 或者 &quot;&quot;“Python”&quot;&quot;） Ctrl+Alt+L （代码格式化） （16）变量：存储数据的内存，内存的别名 （17）变量命名规则：大驼峰 MyName 、小驼峰 myName、下划线 my_name 不可使用数字开头，不能使用内置关键字，严格区分大小写，以数字、字母、下划线开头。 （18）定义不变的量：字母都大写 （19）Typora 使用快捷 ​ 展示路径：Ctrl+Shift+I ​ 表格： |A 列名称 | B 列名称 | （20）debug: 程序运行模式，检查程序的执行细节 。 打断点 -&gt;debug 运行 （21）数据类型：元组 (tuple)、列表 (list)、集合 (set)、字典 (dict) （22）格式化输出：% d 、% s、% f、%04d (不足补零，超出原样显示)、%.5f ​ \\n 换行 \\t 制表符 一个 Tab (4 个空格) （23）Python 跨平台语言 ：Windows、Linux、MacOS 2021.03.05 （1）Python 解释型、动态数据类型、面向对象 （2）查看系统函数帮助，按住 Ctrl，点击对应函数 （3）shift +enter 跳转到下一行 ​ Ctrl + Shift + 方向键 移动代码 ​ shift + tab 退一个 Tab 键 （4）float-&gt;int 强转 向下取整 （5）str(10).print ==&gt; print(str(10)) （6）优先级：（）——&gt; ** ——&gt; / * // %——&gt; + - ​ 算术运算符 ——&gt; 复合赋值运算 例：a *= 1+2 ==&gt; a=a (1+2)——&gt; 赋值运算符 （7）数字的逻辑运算，and 有零为零，否则返回后面的值 or 全零为零，否则返回前面的非零值 ​ not 零为 False，非零为 True。 优先级：Not &gt; and &gt; or （8）切片是指对操作的对象截取其中一部分的操作。字符串、列表、元组都支持切片操作。 ​ 切片的语法：[起始：结束：步长] ​ 注意：选取的区间从 &quot;起始&quot; 位开始，到 &quot;结束&quot; 位的前一位结束（不包含结束位本身)，步长表示选取间隔。 2021.03.06 （1）if 条件： elif: else: （2）字符串可以表示 bool 型数据，空格表示 False，非空格表示 True。 （3）while 先判断 后执行 2021.03.08 （1）pass 语句 补充条件语句，占位。 （2）如果使用了 continue ，则一定要在 continue 前使用计数器。 ​ break 终止后退出 ​ continue 跳过继续 （3）while…else… 中 else 后为正常结束后要显示的内容，非正常情况终止不会执行 else 语句 ​ break 可以使程序异常退出。 ​ continue 程序是正常执行的，循环结束后 else 语句会执行。 （4）” “ ”…&quot;&quot;&quot; 三引号支持换行 2021.03.09 （1）切片 如果步长为负数，表示倒序选取 （2）replace 修改字符串中对应的数据，原有的字符串不变。（字符串属于不可变数据类型） ​ split 分隔成列表 ​ join 类似于 split 的反操作，将列表合并成字符串。 （3）capitalize 字符串首字母大写 ​ title 字符串每个单词的首字母大写 ​ lower 大写转小写 ​ upper 小写转大写 （4）面向百度编程：Python 需求 （5） 去空白： ​ lstrip 删除左侧空格 ​ rstrip 删除右侧空格 strip 删除两侧的空格 （6）对齐方式： ​ ljust (num,’_’) 在 num 长度内左对齐，多余位置用下划线补齐 ​ rjust (num,’_’) 在 num 长度内右对齐，多余位置用下划线补齐 ​ center (num,’_’) 在 num 长度内居中对齐，多余位置用下划线补齐 （7）字符串判断： ​ startswith () 判断字符串开头是否以某个子串开头的 ​ endswith () 判断字符串是否以某个子串结尾 ​ isalpha () 判断是否是字母 ​ isdigit () 判断是否是数字 ​ isalnum () 判断是否是数字和字母的组合 ​ isspace () 判断是否都是空格 （8）列表操作： ​ append 追加整个序列到列表的结尾，修改了原列表。 ​ extend 追加序列中的元素到列表的结尾 ​ insert 在特定下标增加一个数据，原位置的数据后移。 ​ del 删除指定数据或者列表 ​ pop 删除指定下标的数据，若不指定下标，默认删除最后一个数，pop 函数会返回删除的元素。 ​ remove 删除指定数据 ​ clear 清空列表 ​ reverse 逆置 ​ sort （key =none,reverse = false） 排序 ，默认升序 ​ 例如：sort (reverse = True) 降序排序 ​ copy 复制列表 ​ 2021.03.10 （1）if 与 While 区别： ​ if 子句结束时，程序继续执行 if 语句之后的语句。 while 子句结束时，程序执行跳回到 while 语句开始处。 （2）在用于条件时，0、0.0 和’ '（空字符串）被认为是 False，其他值被认为是 True。 (3) for 循环 1234total = 0for num in range(101): total = total + num print(total) （4）实际上，只能在 while 和 for 循环内部使用 continue 和 break 语句。如果试图在别处使用这些语句，Python 将 报错。 （5）range () 函数也可以有第三个参数。前两个参数分别是起始值和终止值（不包含），第三个 参数是 “步长”。步长是每次迭代后循环变量增加的值。（类比切片作记忆） （6）**from random import ***。 使用这种形式的 import 语句，调用 random 模块中的函数时不需要 random. 前缀。 ​ 但是，使用完整的名称会让代码更可读，所以最好是使用普通形式的 import 语句。 （7）sys 模块 12345678import syswhile True: print(&#x27;Type exit to exit:\\n&#x27;) response = input() if response == &#x27;exit&#x27;: sys.exit() print(&#x27;You typed &#x27; + response + &#x27;.&#x27;) （8）def 中的变量称为‘变元’，保存在变元中的值，在函数返回后就会被销毁。 （9）在 Python 中有一个值称为 None，它表示没有值。None 是 NoneType 数据类型的唯一值（其他编程语言可能称这个值为 null、nil 或 undefined）。就像布尔值 True 和 False 一样，None 必须大写首字母 N。 （10）print (sep = ‘分隔符’，end = ‘结束符’) 函数 1234&gt;&gt;&gt; print(&#x27;cats&#x27;, &#x27;dogs&#x27;, &#x27;mice&#x27;)cats dogs mice&gt;&gt;&gt; print(&#x27;cats&#x27;, &#x27;dogs&#x27;, &#x27;mice&#x27;, sep=&#x27;,&#x27;)cats,dogs,mice （11）可以将 “作用域” 看成是变量的容器。当作用域被销毁时，所有保存在该作用域内的变量的值就被丢弃了。 ​ 全局作用域，它是在程序开始时创建的。如果程序终止，全局作用域就被销毁，它的所有变量就被丢弃了。否则，下次你运行程序的时候，这些变量就会记住它们上次运行时的值。 ​ 一个函数被调用时，就创建了一个局部作用域。在这个函数内赋值的所有变量，存在于该局部作用域内。该函数返回时，这个局部作用域就被销毁了，这些变量就丢失了。下次调用这个函数，局部变量不会记得该函数上次被调用时它们保存的值。 作用域知识点如下： 全局作用域中的代码不能使用任何局部变量； 但是，局部作用域可以访问全局变量； 一个函数的局部作用域中的代码，不能使用其他局部作用域中的变量。 如果在不同的作用域中，你可以用相同的名字命名不同的变量。也就是说，可以有一个名为 spam 的局部变量，和一个名为 spam 的全局变量。 （12）全局变量和局部变量 局部作用域不能使用其他局部作用域内的变量 1234567891011def spam(): eggs = 99 bacon() print(eggs) def bacon(): ham = 101 eggs = 0 spam()&gt;&gt;&gt; 99 全局变量可以在局部作用域中读取 12345678def spam(): print(eggs) eggs = 42spam()# print(eggs)&gt;&gt;&gt;42 名称相同的局部变量和全局变量 123456789101112131415161718def spam(): eggs = &#x27;spam local&#x27; print(eggs) # prints &#x27;spam local&#x27;def bacon(): eggs = &#x27;bacon local&#x27; print(eggs) # prints &#x27;bacon local&#x27; spam() print(eggs) # prints &#x27;bacon local&#x27;eggs = &#x27;global&#x27;bacon()print(eggs) # prints &#x27;global&#x27;&gt;&gt;&gt; bacon localspam localbacon localglobal （13）global 语句 如果需要在一个函数内修改全局变量，就使用 global 语句。如果在函数的顶部有 global eggs 这样的代码，它就告诉 Python，“在这个函数中，eggs 指的是全局变量，所以不要用这个名字创建一个局部变量。” 123456789def spam(): global eggs eggs = &#x27;spam&#x27; eggs = &#x27;global&#x27;spam()print(eggs)&gt;&gt;&gt;spam 区分一个变量是处于局部作用域还是全局作用域： 1．如果变量在全局作用域中使用（即在所有函数之外），它就总是全局变量。 2．如果在一个函数中，有针对该变量的 global 语句，它就是全局变量。 3．否则，如果该变量用于函数中的赋值语句，它就是局部变量。 4．但是，如果该变量没有用在赋值语句中，它就是全局变量。 （14）计算一个字符串中每个字符出现的次数 1234567str = &#x27;It was a bright cold day in April, and the clocks were striking thirteen.&#x27; count = &#123;&#125; for character in str: count.setdefault(character, 0) count[character] += 1 print(count) （15）漂亮的打印 12345678import pprintstr = &#x27;It was a bright cold day in April, and the clocks were strikingthirteen.&#x27;count = &#123;&#125;for character in str: count.setdefault(character, 0) count[character] += 1pprint.pprint(count) 2021.03.11 （1）dict 大括号，键值对，逗号隔开。 （2）key : 不可变类型（int float 字符串 元组） 列表不可以 （3）del dict1 删除字典 dict1.clear 清空保留空字典 （4）在字典中 in 只能判断键是否在字典中 （5）get () 查找值 keys () 查找所有的键 values () 查找所有的值 ​ items () 查找字典中所有的键值对，返回元组，元组的元素分别对应 key 和 value () （6）set () 集合无序，可以去重，可变类型。 ​ add () 增加元素到集合中，可添加不可变的容器，例如：元组、int、float、字符串 ​ update () 增加数据序列，例如：列表、元组 ​ remove () 删除指定数据，不存在则报错。 ​ discard () 删除指定数据，不存在则不报错 ​ pop () 随机删除数据，并返回删除的数据。 ​ in 查找数据是否在集合内 ​ not in 查找数据是否不在集合内 ​ 集合可以用 for 循环遍历 （7）+ 合并 字符串、列表、元组等有序的容器都支持合并，字典不支持合并。 ​ * 复制 字符串、列表、元组都有序的容器支持合并，字典不支持合并。 ​ in 支持所有的容器 ​ not in 支持所有的容器 公共方法： len() del() min() max() range() enumerate() 返回结果是元组，元组第一个数据时对应下标，第二个元素为数据的值。 （8）列表推导式 123456789101112131415161718192021222324252627282930313233list1 = [i for i in range(10)]list2 = [str(i) for i in range(10)]# 结合if语句的列表推导式list3 = [str(i) for i in range(10) if i % 2 == 0] # 多个for循环实现循环嵌套的列表推导式list4 = [(i, j) for i in range(1, 3) for j in range(3)]# 列表推导式实现一一对应展示l1 = [&#x27;a&#x27;，&#x27;b&#x27;，&#x27;c&#x27;]l2 = [1，2，3]list5 = [(i, j) for i, j in zip(l1, l2)]# 分组一个listl1 = [i for i in range(1，101)]print(l1[::3])# 字典推导式list6 = [i:i**2 for i in range(1，5)]# 两个列表组合的字典list7 = [&#x27;name&#x27;, &#x27;age&#x27;, &#x27;gender&#x27;]list8 = [&#x27;Tom&#x27;, 20, &#x27;man&#x27;]dict1 = &#123;list7[i]: list8[i] for i in range(len(list7))&#125;print(dict1)# 集合推导式list8 = [1, 1, 2]set1 = &#123;i ** 2 for i in list8&#125;print(set1) # &#123;1, 4&#125; 2021.03.12 （1）物品清单罗列 12345678910111213141516171819# inventory.pystuff = &#123;&#x27;rope&#x27;: 1, &#x27;torch&#x27;: 6, &#x27;gold coin&#x27;: 42, &#x27;dagger&#x27;: 1, &#x27;arrow&#x27;: 12&#125;def displayInventory(inventory): print(&quot;Inventory:&quot;) item_total = 0 for k, v in inventory.items(): print(str(v) + &#x27;\\t&#x27; + k) item_total += v print(&quot;Total number of items: &quot; + str(item_total))displayInventory(stuff)&gt;&gt;&gt;Inventory:1 rope6 torch42 gold coin1 dagger12 arrowTotal number of items: 62 （2）return 退出函数，返回值 （3）函数的说明文档： 12345678def maxDigit(a, b): &quot;&quot;&quot;最大值函数&quot;&quot;&quot; if a &gt;= b: return a else: return bhelp(maxDigit) # 查看函数的说明文档 （4）Ctrl + Alt +M 抽取函数 Ctrl + Q 查看函数的说明文档 （5）猜数字 123456789101112131415161718192021222324252627282930313233# This is a guess the number game. import randomsecretNumber = random.randint(1, 20)print(&#x27;I am thinking of a number between 1 and 20.&#x27;)# Ask the player to guess 6 times.for guessesTaken in range(1, 7): print(&#x27;Take a guess.&#x27;) guess = int(input()) if guess &lt; secretNumber: print(&#x27;Your guess is too low.&#x27;) elif guess &gt; secretNumber: print(&#x27;Your guess is too high.&#x27;) else: break # This condition is the correct guess! if guess == secretNumber: print(&#x27;Good job! You guessed my number in &#x27; + str(guessesTaken) + &#x27; guesses!&#x27;) else: print(&#x27;Nope. The number I was thinking of was &#x27; + str(secretNumber)) &gt;&gt;&gt;I am thinking of a number between 1 and 20. Take a guess.10Your guess is too low.Take a guess.15Your guess is too low.Take a guess.17Your guess is too high.Take a guess.16Good job! You guessed my number in 4 guesses! （6）函数：封装代码，高效的代码重用。 （7）变量的传递引用 变量包含对列表值的引用，而不是列表值本身。 在变量必须保存可变数据类型的值时，例如列表或字典，Python 就使用引用。 对于不可变的数据类型的值，例如字符串、整型或元组，Python 变量就保存值本身。 （8）函数调用时，如果有位置参数，则要保证位置参数在关键字参数前面。 （9）关键字列表查看 12import keywordprint(keyword.kwlist) 2021.03.14 （1）Ctrl + F12 查看当前项目定义的函数的列表。 Alt + Enter 提示错误信息 （2）递归：在函数内部调用函数本身，代码量少。 123456789101112# 3 + 2 + 1def sum_numbers(num): # 1.如果是1，直接返回1 -- 出⼝ if num == 1: return 1 # 2.如果不是1，重复执⾏累加并返回结果 return num + sum_numbers(num - 1)sum_result = sum_numbers(3)# 输出结果为6print(sum_result) （3） lambda 表达式（匿名函数） 只有一行实现 可以有参数 函数名代表地址，可以通过地址加括号来调用函数。 1234567891011lambda 参数：表达式# lambda lambda a，b: a+b # 输入a和b，计算a+b的值。# 带判断的lambda表达式lambda a,b:a if a &gt; b else b# lambda 带key的表达式 （4）高阶函数 函数有函数参数，或者函数有函数返回值。 sort () 123l = [&#x27;abcdef&#x27;,&#x27;ghf&#x27;,&#x27;treh&#x27;]l.sort(key=lambda ele:len(ele))print(l) # [&#x27;ghf&#x27;, &#x27;treh&#x27;, &#x27;abcdef&#x27;] map(func, lst) # 将传入的函数变量 func 作用到 lst 变量的每个元素。 12345678my_list = [1, 2, 3, 4, 5]def f(x): return x ** 2result = map(f, my_list)# result返回内存地址，list(result)返回操作后的结果。print(type(result), result, list(result)) reduce(function, list) # 函数会对参数序列中元素进行累计 1234567891011import functoolsmy_list = [1, 2, 3, 4, 5]def f(x1, x2): return x1 + x2result = functools.reduce(f, my_list)print(result) filter(func,lst) # 过滤序列，过滤掉不符合条件的元素，返回一个 filter 对象，如果要转换为列表，可以使用 list () 来转换. 123456789my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]def f(x): return x % 2 == 0result = filter(f, my_list)print(list(result)) 2021.03.15 （1）文件操作： read() 不加参数时默认读取所有内容，加数字表示读取的字符长度。（换行符会有字节占位） readlines() 将文件的内容以列表的形式展示成一行内容。参数为数字时，表示将数字以内的字符串所在行都打印出来。 readline() 一次读取一行内容，参数为数字时，表示读取当前行数字个字符内容，超出则读取当前行所有内容。 （2）文件指针操作 ​ seek (偏移量，起始位置) 用于移动文件指针。 起始位置： 0： 文件开头 1： 当前位置 2：文件结尾 例如： seek (0,2) 指针在文件的结尾 seek (n,0) 指针在文件的开头，从 n+1 个字符读文件。 seek (0,0) 等价于 seek (0)，指针放在文件开头。 seek (-2,2) 以结尾为基准，偏移两个字节，即读取后两个字符。 （3）相对路径 ./ 根目录 …/ 上一级目录 …/…/ 上上一级目录 （4）文件的相关操作： 文件重命名 os 模块中的 rename () 可以完成对文件的重命名操作 rename (需要修改的文件名，新的文件名) 12import osos.rename(&quot;毕业论文.txt&quot;, &quot;毕业论文-最终版.txt&quot;) 删除文件 os 模块中的 remove () 可以完成对文件的删除操作 remove (待删除的文件名) 12import osos.remove(&quot;毕业论文.txt&quot;) 创建文件夹 12import osos.mkdir(&quot;张三&quot;) 获取当前目录 12import osos.getcwd() 改变默认目录 12import osos.chdir(&quot;../&quot;) 获取目录列表 12import osos.listdir(&quot;./&quot;) 删除文件夹 12import osos.rmdir(&quot;张三&quot;) 2021.03.19 （1）面向对象：强调对象（实体），我们充当指挥者的角色 面向过程：强调过程（动作），我们充当**执行者**的角色 （2）面向对象的特征：继承、封装、多态。 面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过继承机制。 通过继承创建的新类称为子类或派生类，被继承的类称为基类、父类或超类。 （3）用类去创建对象 ==&gt; 用类去实例化对象 （4）类是对一系列具有相同特征（属性）和行为（方法）的事物的统称，遵循大驼峰命名规则。 ​ 对象是具体的类 （5）self : 调用该函数的对象，不需要传递。也可以用其它字符代替。 ​ 类里面获取属性：self. 属性名 （6）魔法方法：类里面都具备的方法，以‘–’开头结尾的都是魔法方法，创建对象的时候自动执行。 成员属性：作用域：通过对象访问 局部变量：作用域：在函数内部使用 （7) 魔法方法 123456789# __init__ 初始化对象，可以传递参数。# __str__ 代替对象的内存地址为reture的返回值,当使用print输出对象的时候，直接打印这个返回值，作为对这个对象的描写。# __del__ 删除对象时，会调用该方法 （析构函数）# __repr__ 转化为供解释器读取的形式# __cmp__ ( self, x ) 对象比较 运算符重载 Python 同样支持运算符重载，实例如下： 12345678910111213141516class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return &#x27;Vector (%d, %d)&#x27; % (self.a, self.b) def __add__(self,other): return Vector(self.a + other.a, self.b + other.b) v1 = Vector(2,10)v2 = Vector(5,-2)print v1 + v2&gt;&gt;&gt;Vector(7,8) Python 内置类属性 dict : 类的属性（包含一个字典，由类的数据属性组成） doc : 类的文档字符串 name: 类名 module: 类定义所在的模块（类的全名是’main.className’，如果类位于一个导入模块 mymod 中，那么 className.module 等于 mymod） bases : 类的所有父类构成元素（包含了一个由所有父类组成的元组） 12345678910111213141516171819202122232425262728class Employee: &#x27;所有员工的基类&#x27; empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print &quot;Total Employee %d&quot; % Employee.empCount def displayEmployee(self): print &quot;Name : &quot;, self.name, &quot;, Salary: &quot;, self.salary print &quot;Employee.__doc__:&quot;, Employee.__doc__print &quot;Employee.__name__:&quot;, Employee.__name__print &quot;Employee.__module__:&quot;, Employee.__module__print &quot;Employee.__bases__:&quot;, Employee.__bases__print &quot;Employee.__dict__:&quot;, Employee.__dict__&gt;&gt;&gt;Employee.__doc__: 所有员工的基类Employee.__name__: EmployeeEmployee.__module__: __main__Employee.__bases__: ()Employee.__dict__: &#123;&#x27;__module__&#x27;: &#x27;__main__&#x27;, &#x27;displayCount&#x27;: &lt;function displayCount at 0x10a939c80&gt;, &#x27;empCount&#x27;: 0, &#x27;displayEmployee&#x27;: &lt;function displayEmployee at 0x10a93caa0&gt;, &#x27;__doc__&#x27;: &#x27;\\xe6\\x89\\x80\\xe6\\x9c\\x89\\xe5\\x91\\x98\\xe5\\xb7\\xa5\\xe7\\x9a\\x84\\xe5\\x9f\\xba\\xe7\\xb1\\xbb&#x27;, &#x27;__init__&#x27;: &lt;function __init__ at 0x10a939578&gt;&#125; Python 对象销毁 (垃圾回收) Python 使用了引用计数这一简单技术来跟踪和回收垃圾。 在 Python 内部记录着所有使用中的对象各有多少引用。 一个内部跟踪变量，称为一个引用计数器。 当对象被创建时， 就创建了一个引用计数， 当这个对象不再需要时， 也就是说， 这个对象的引用计数变为 0 时， 它被垃圾回收。但是回收不是 &quot;立即&quot; 的， 由解释器在适当的时机，将垃圾对象占用的内存空间回收。 垃圾回收机制不仅针对引用计数为 0 的对象，同样也可以处理循环引用的情况。循环引用指的是，两个对象相互引用，但是没有其他变量引用他们。这种情况下，仅使用引用计数是不够的。 Python 的垃圾收集器实际上是一个引用计数器和一个循环垃圾收集器。作为引用计数的补充，垃圾收集器也会留心被分配的总量很大（即未通过引用计数销毁的那些）的对象。 在这种情况下， 解释器会暂停下来， 试图清理所有未引用的循环。 2021.03.20 （1）多继承 继承多个父类，优先继承第一个父类 子类可以调用所有父类中不同名的方法 如果子类和父类拥有同名属性和方法，则子类创建出来的对象会直接调用子类自己的属性和方法，即子类重写父类方法。 子类调用父类的同名方法和属性： 先调用自己子类方法的初始化 1234def make_cake（self）: # 添加自己的初始化 self.__init__() # 不需要加self print(&#x27;子类打印内容&#x27;) 将父类同名属性和方法再次封装 12345def make_master_cake(self，name): # 首先调用init初始化属性 Master.__init__(self,name) # 调用父类同名的方法和属性 Master.make_cake(self) （2）print (类名.__mro__) # 显示该类的继承关系 （3）多层继承： super () 调用父类，适用单继承 如果重写了__init__ 时，要继承父类的构造方法，可以使用 super 关键字： 1super(子类，self).__init__(参数1，参数2，....) 还有一种经典写法： 1父类名称.__init__(self,参数1，参数2，...) （4）定义私有属性和方法（私有属性和方法不能被子类继承） 类的私有属性 __private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。 类的方法 在类的内部，使用 def 关键字可以为类定义一个方法，与一般函数定义不同，类方法必须包含参数 self, 且为第一个参数 类的私有方法 __private_method：两个下划线开头，声明该方法为私有方法，不能在类的外部调用。在类的内部调用 self.__private_methods 123456789101112131415161718192021222324class JustCounter: __secretCount = 0 # 私有变量 publicCount = 0 # 公开变量 def count(self): self.__secretCount += 1 self.publicCount += 1 print self.__secretCount counter = JustCounter()counter.count()counter.count()print(counter.publicCount)print(counter.__secretCount) # 报错，实例不能访问私有变量&gt;&gt;&gt;122Traceback (most recent call last): File &quot;test.py&quot;, line 17, in &lt;module&gt; print counter.__secretCount # 报错，实例不能访问私有变量AttributeError: JustCounter instance has no attribute &#x27;__secretCount&#x27; 只能在类内使用，外部打印是不能的。 Python 不允许实例化的类访问私有数据，但你可以使用 object._className__attrName（ 对象名._类名__私有属性名 ）访问属性 12345678class Runoob: __site = &quot;www.runoob.com&quot;runoob = Runoob()print(runoob._Runoob__site)&gt;&gt;&gt;www.runoob.com （5） 获取和修改私有属性值 get_XX 在类里面定义函数调用私有属性，外部调用该方法显示私有属性值。 set_XX 在类里面定义函数调用私有属性，外部调用该方法设置私有属性值。 （6）多态（同一功能，不同表现形式 —— 静态类型语言） （7）封装 隐藏内部实现的细节，只保留功能接口。 前提是私有化 函数、类、模块、包等都是封装 ​ shift +F6 Pycharm 中替换功能 （8）单下划线、双下划线、头尾双下划线说明： __foo__: 定义的是特殊方法，一般是系统定义名字 ，类似 init () 之类的。 _foo: 以单下划线开头的表示的是 protected 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 from module import * __foo: 双下划线的表示的是私有类型 (private) 的变量，只能是允许这个类本身进行访问了。 （9）继承 object 类的是新式类，不继承 object 类的是经典类 1234567891011121314151617class A: def foo(self): print(&#x27;called A.foo()&#x27;)class B(A): passclass C(A): def foo(self): print(&#x27;called C.foo()&#x27;)class D(B, C): passif __name__ == &#x27;__main__&#x27;: d = D() d.foo() B、C 是 A 的子类，D 多继承了 B、C 两个类，其中 C 重写了 A 中的 foo () 方法。 如果 A 是经典类（如上代码），当调用 D 的实例的 foo () 方法时，Python 会按照深度优先的方法去搜索 foo () ，路径是 B-A-C ，执行的是 A 中的 foo () ； 如果 A 是新式类，当调用 D 的实例的 foo () 方法时，Python 会按照广度优先的方法去搜索 foo () ，路径是 B-C-A ，执行的是 C 中的 foo () 。 因为 D 是直接继承 C 的，从逻辑上说，执行 C 中的 foo () 更加合理，因此新式类对多继承的处理更为合乎逻辑。 在 Python 3.x 中的新式类貌似已经兼容了经典类，无论 A 是否继承 object 类， D 实例中的 foo () 都会执行 C 中的 foo () 。 但是在 Python 2.7 中这种差异仍然存在，因此还是推荐使用新式类，要继承 object 类。 （10）所有的实例都共享类属性，因此，当在 class 语句外修改类属性时，会导致所有由这个类创建的实例的类属性都随之变化，除非实例本身有同名的实例属性对类属性进行了覆盖，比如代码中的 d.cls_pre = ‘ddddd’。 1234567891011121314151617181920212223242526272829303132class CA(object): cls_pre = &#x27;aaaaa&#x27; def __init__(self): self.obj_pre = &#x27;bbbbb&#x27;a = CA()b = CA()print(a.cls_pre, a.obj_pre)print(b.cls_pre, b.obj_pre)CA.cls_pre = &#x27;ccccc&#x27;c = CA()d = CA()d.cls_pre = &#x27;ddddd&#x27;print(a.cls_pre, a.obj_pre)print(b.cls_pre, b.obj_pre)print(c.cls_pre, c.obj_pre)print(d.cls_pre, d.obj_pre)&gt;&gt;&gt;aaaaa bbbbbaaaaa bbbbbccccc bbbbbccccc bbbbbccccc bbbbbddddd bbbbb 代码中，将类属性 CA.cls_pre 重新赋值为 ‘ccccc’。在修改类属性之后，不仅是后续创建的类实例 c 的 cls_pre 发生变化，在修改类属性之前的创建的类实例 a、b 的类属性 cls_pre 都发生了变化。 2021.03.21 Python 内置函数： abs() 1234567891011print(abs(-45))print(abs(100.12))print(abs(119L))print(abs(1+2j)) # 如果为复数时，则返回复数的绝对值，即a^2 + b^2 开根。&gt;&gt;&gt;45100.121192.23606797749979 divmod () 返回一个包含商和余数的元组 (a //b, a % b) 123456&gt;&gt;&gt;divmod(7, 2)(3, 1)&gt;&gt;&gt; divmod(8, 2)(4, 0)&gt;&gt;&gt; divmod(1+2j,1+0.5j)((1+0j), 1.5j) staticmethod 返回函数的静态方法，该方法不强制要求传递参数，即静态方法无需实例化，减少不必要的内存占用和性能消耗 12345678910111213class C(object): @staticmethod def f(): print(&#x27;runoob&#x27;); C.f(); # 静态方法无需实例化cobj = C()cobj.f() # 也可以实例化后调用&gt;&gt;&gt;runoobrunoob staticmethod 参数要求是 Callable, 也就是说 Class 也是可以的： 12345678910class C1(object): @staticmethod class C2(object): def __init__(self, val = 1): self.val = val def shout(self): print(&quot;Python世界第%d!&quot;%self.val)tmp = C1.C2(0)print(type(tmp)) # 输出 : &lt;class &#x27;__main__.C1.C2&#x27;&gt;tmp.shout() # 输出 : Python世界第0! all () 函数用于判断给定的可迭代参数 iterable 中的所有元素是否都为 TRUE，如果是返回 True，否则返回 False。元素除了是 0、空、None、False 外都算 True。 注意：空元组、空列表返回值为 True，这里要特别注意。 所以的为真则为 Ture 123456789101112131415161718&gt;&gt;&gt; all([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;]) # 列表list，元素都不为空或0True&gt;&gt;&gt; all([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;&#x27;, &#x27;d&#x27;]) # 列表list，存在一个为空的元素False&gt;&gt;&gt; all([0, 1，2, 3]) # 列表list，存在一个为0的元素False &gt;&gt;&gt; all((&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;)) # 元组tuple，元素都不为空或0True&gt;&gt;&gt; all((&#x27;a&#x27;, &#x27;b&#x27;, &#x27;&#x27;, &#x27;d&#x27;)) # 元组tuple，存在一个为空的元素False&gt;&gt;&gt; all((0, 1, 2, 3)) # 元组tuple，存在一个为0的元素False &gt;&gt;&gt; all([]) # 空列表True&gt;&gt;&gt; all(()) # 空元组True any () 函数用于判断给定的可迭代参数 iterable 是否全部为 False，则返回 False，如果有一个为 True，则返回 True。 如果都为空、0、false，则返回 false，如果不都为空、0、false，则返回 true。 注意：空元组、空列表返回值为 False，这里要特别注意。 任何一个有效则为 True 123456789101112131415161718192021222324&gt;&gt;&gt;any([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;]) # 列表list，元素都不为空或0True &gt;&gt;&gt; any([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;&#x27;, &#x27;d&#x27;]) # 列表list，存在一个为空的元素True &gt;&gt;&gt; any([0, &#x27;&#x27;, False]) # 列表list,元素全为0,&#x27;&#x27;,falseFalse &gt;&gt;&gt; any((&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;)) # 元组tuple，元素都不为空或0True &gt;&gt;&gt; any((&#x27;a&#x27;, &#x27;b&#x27;, &#x27;&#x27;, &#x27;d&#x27;)) # 元组tuple，存在一个为空的元素True &gt;&gt;&gt; any((0, &#x27;&#x27;, False)) # 元组tuple，元素全为0,&#x27;&#x27;,falseFalse &gt;&gt;&gt; any([]) # 空列表False &gt;&gt;&gt; any(()) # 空元组False enumerate () 函数用于将一个可遍历的数据对象 (如列表、元组或字符串) 组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。 12345678910111213141516171819202122232425262728&gt;&gt;&gt;seasons = [&#x27;Spring&#x27;, &#x27;Summer&#x27;, &#x27;Fall&#x27;, &#x27;Winter&#x27;]&gt;&gt;&gt; list(enumerate(seasons))[(0, &#x27;Spring&#x27;), (1, &#x27;Summer&#x27;), (2, &#x27;Fall&#x27;), (3, &#x27;Winter&#x27;)]&gt;&gt;&gt; list(enumerate(seasons, start=1)) # 下标从 1 开始[(1, &#x27;Spring&#x27;), (2, &#x27;Summer&#x27;), (3, &#x27;Fall&#x27;), (4, &#x27;Winter&#x27;)]# 普通for循环&gt;&gt;&gt;i = 0&gt;&gt;&gt; seq = [&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;]&gt;&gt;&gt; for element in seq:... print i, seq[i]... i +=1... 0 one1 two2 three# For循环使用enumerate&gt;&gt;&gt;seq = [&#x27;one&#x27;, &#x27;two&#x27;, &#x27;three&#x27;]&gt;&gt;&gt; for i, element in enumerate(seq):... print i, element... 0 one1 two2 three 123456789101112# 巧妙的利用enumerate（）批量修改列表中的元素list1 = [&#x27;01&#x27;,&#x27;02&#x27;,&#x27;03&#x27;]unit_element = &#x27;1&#x27;# list1=[&quot;1&quot;+str for str in list1] #推导式实现拼接for i,element in enumerate(list1): list1[i] = unit_element + element # 字符串拼接print(list1)&gt;&gt;&gt;[&#x27;101&#x27;, &#x27;102&#x27;, &#x27;103&#x27;] ord© 返回值是对应的 ASCII 码（十进制整数）。 123456&gt;&gt;&gt;ord(&#x27;a&#x27;)97&gt;&gt;&gt; ord(&#x27;b&#x27;)98&gt;&gt;&gt; ord(&#x27;c&#x27;)99 isinstance () 函数来判断一个对象是否是一个已知的类型，类似 type ()。 isinstance () 与 type () 区别： type () 不会认为子类是一种父类类型，不考虑继承关系。 isinstance () 会认为子类是一种父类类型，考虑继承关系。 如果要判断两个类型是否相同推荐使用 isinstance ()。 12345678910111213141516171819202122&gt;&gt;&gt;a = 2&gt;&gt;&gt; isinstance (a,int)True&gt;&gt;&gt; isinstance (a,str)False&gt;&gt;&gt; isinstance (a,(str,int,list)) # 是元组中的一个返回 TrueTrue# type()和isinstance()的区别class A: pass class B(A): pass isinstance(A(), A) # returns Truetype(A()) == A # returns Trueisinstance(B(), A) # returns Truetype(B()) == A # returns False pow() 方法返回 xy（x 的 y 次方） 的值。 pow (x, y [, z]) 等效于 pow (x,y) % z，当 z 这个参数不存在时 x,y 不限制是否为 float 类型，而当使用第三个参数的时候要保证前两个参数只能为整数。 pow (x, y) 并不等价与 x**y，因为 pow 函数会把整数转换为浮点数，会出现误差。 1234567891011121314151617181920import math # 导入 math 模块 print f&#x27;math.pow(100, 2):&#123;math.pow(100, 2)&#125;&#x27;# 使用内置，查看输出结果区别print f&#x27;pow(100, 2):&#123;pow(100, 2)&#125;&#x27; print f&#x27;math.pow(100, -2) : &#123;math.pow(100, -2)&#125;&#x27;print f&#x27;math.pow(2, 4) :&#123;math.pow(2, 4)&#125;&#x27;print f&#x27;math.pow(3, 0) : &#123;math.pow(3, 0)&#125;&#x27;#pow(x, y[, z]) 等效于 pow(x,y) %zprint(4**2.5%3) # 结果为2.0&gt;&gt;&gt;math.pow(100, 2) : 10000.0pow(100, 2) : 10000math.pow(100, -2) : 0.0001math.pow(2, 4) : 16.0math.pow(3, 0) : 1.02.0 sum() 方法对序列进行求和计算。 iterable – 可迭代对象，如：列表、元组、集合。 start – 指定相加的参数，如果没有设置这个值，默认为 0。 1234567&gt;&gt;&gt;sum([0,1,2]) 3 &gt;&gt;&gt; sum((2, 3, 4), 1) # 元组计算总和后再加 110&gt;&gt;&gt; sum([0,1,2,3,4], 2) # 列表计算总和后再加 212 12345678910111213import numpy as npa = np.array([[1,2],[3,4]])# 按行相加，并且保持其二维特性print(np.sum(a, axis=1, keepdims=True))# 按行相加，不保持其二维特性print(np.sum(a, axis=1))&gt;&gt;&gt;array([[3], [7]])array([3, 7]) issubclass (class, classinfo) 用于判断参数 class 是否是类型参数 classinfo 的子类。 123456class A: passclass B(A): pass print(issubclass(B,A)) # 返回 True bin() 返回一个整数 int 或者长整数 long int 的二进制表示。 12345&gt;&gt;&gt;bin(10)&#x27;0b1010&#x27;&gt;&gt;&gt; bin(20)&#x27;0b10100&#x27; range (start, stop [, step]) 函数可创建一个整数列表，一般用在 for 循环中。 参数说明： start: 计数从 start 开始。默认是从 0 开始。例如 range（5）等价于 range（0， 5）; stop: 计数到 stop 结束，但不包括 stop。例如：range（0， 5） 是 [0, 1, 2, 3, 4] 没有 5 step：步长，默认为 1。例如：range（0， 5） 等价于 range (0, 5, 1) 1234567891011121314&gt;&gt;&gt;range(10) # 从 0 开始到 10[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; range(1, 11) # 从 1 开始到 11[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; range(0, 30, 5) # 步长为 5[0, 5, 10, 15, 20, 25]&gt;&gt;&gt; range(0, 10, 3) # 步长为 3[0, 3, 6, 9]&gt;&gt;&gt; range(0, -10, -1) # 负数[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]&gt;&gt;&gt; range(0)[]&gt;&gt;&gt; range(1, 0)[] range () 函数返回的结果是一个整数序列的对象，而不是列表。 123456789&gt;&gt;&gt; type(range(10))&lt;class &#x27;range&#x27;&gt;&gt;&gt;&gt;help(range)Return an object...# 将range（）函数返回一个列表&gt;&gt;&gt; list(range(10))[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 12345678910111213print([i for i in range(10)])# 用来做计算print([x*x for x in range(2, 8)])# 两层循环,生成全排列的listprint(type([m + n for m in &#x27;ABC&#x27; for n in &#x27;XYZ&#x27;]))print([m + n for m in &#x27;ABC&#x27; for n in &#x27;XYZ&#x27;])# 这种方式生成单个strfor m in &#x27;ABC&#x27;: for n in &#x27;XYZ&#x27;: print(type(m + n)) print(m + n) range 生成的列表，列表中的元素相互独立，即地址各不相同，这与 * 号重复列表生成的列表大相径庭，前者类似深拷贝，后者则纯粹是浅拷贝，如下所示： 12345678910111213141516171819a = [0] * 3print u&#x27;这是浅拷贝&#x27;for num in a: print id(num)b = range(3)print u&#x27;这是深拷贝&#x27;for num in b: print id(num)&gt;&gt;&gt;这是浅拷贝548135685481356854813568这是深拷贝548135685481354454813520 callable() 函数用于检查一个对象是否是可调用的。 如果返回 True，object 仍然可能调用失败；但如果返回 False，调用对象 object 绝对不会成功。 对于函数、方法、lambda 函式、 类以及实现了 __call__ 方法的类实例，它都返回 True。 12345678910111213141516171819202122232425262728&gt;&gt;&gt;callable(0)False&gt;&gt;&gt; callable(&quot;runoob&quot;)False &gt;&gt;&gt; def add(a, b):... return a + b... &gt;&gt;&gt; callable(add) # 函数返回 TrueTrue&gt;&gt;&gt; class A: # 类... def method(self):... return 0... &gt;&gt;&gt; callable(A) # 类返回 TrueTrue&gt;&gt;&gt; a = A()&gt;&gt;&gt; callable(a) # 没有实现 __call__, 返回 FalseFalse&gt;&gt;&gt; class B:... def __call__(self):... return 0... &gt;&gt;&gt; callable(B)True&gt;&gt;&gt; b = B()&gt;&gt;&gt; callable(b) # 实现 __call__, 返回 TrueTrue locals() 函数会以字典类型返回当前位置的全部局部变量。 对于函数，方法，lambda 函式，类，以及实现了 __call__ 方法的类实例，它都返回 True。 123456&gt;&gt;&gt;def runoob(arg): # 两个局部变量：arg、z... z = 1... print (locals())... &gt;&gt;&gt; runoob(4)&#123;&#x27;z&#x27;: 1, &#x27;arg&#x27;: 4&#125; # 返回一个名字/值对的字典 reduce() 函数会对参数序列中元素进行累积。 函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。 1234567891011121314from functools import reducedef add(x, y) : # 两数相加 return x + ysum1 = reduce(add, [1,2,3,4,5]) # 调用add函数，计算列表和：1+2+3+4+5sum2 = reduce(lambda x, y: x+y, [1,2,3,4,5]) # 使用 lambda 匿名函数print(sum1)print(sum2)&gt;&gt;&gt;1515 123456789from functools import reducedef add(x,y): return x + yprint (reduce(add, range(1, 101)))&gt;&gt;&gt;5050 12345678# 统计某字符串重复次数:from functools import reducesentences = [&#x27;The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. &#x27;] word_count =reduce(lambda a,x:a+x.count(&quot;learning&quot;),sentences,0)print(word_count)&gt;&gt;&gt;2 1234567# 实现字符串反转from functools import reduce str1=&quot;hello&quot;print(reduce(lambda x,y:y+x,str1))# 输出 olleh chr () 用一个范围在 range（256）内的（就是 0～255）整数作参数，返回一个对应的字符。 1234&gt;&gt;&gt;print chr(0x30), chr(0x31), chr(0x61) # 十六进制0 1 a&gt;&gt;&gt; print chr(48), chr(49), chr(97) # 十进制0 1 a classmethod（）装饰器 访问私有类或者类属性使用，cls 指代当前类 12345678910111213class A(object): bar = 1 def func1(self): # self 指代对象 print (&#x27;foo&#x27;) @classmethod def func2(cls): print (&#x27;func2&#x27;) print (cls.bar) # 调用类属性 cls().func1() # 调用类中方法 A.func2() # 不需要实例化 staticmethod() 。+ 1 2021.03.22 （1）异常 检查时异常 运行时异常 解决异常的方法 捕捉异常可以使用 try/except 语句。 try/except 语句用来检测 try 语句块中的错误，从而让 except 语句捕获异常信息并处理。 如果你不想在异常发生时结束你的程序，只需在 try 里捕获它。 12345678try:&lt;语句&gt; #运行别的代码except &lt;名字&gt;：&lt;语句&gt; #如果在try部份引发了&#x27;name&#x27;异常except &lt;名字&gt;，&lt;数据&gt;:&lt;语句&gt; #如果引发了&#x27;name&#x27;异常，获得附加的数据else:&lt;语句&gt; #如果没有异常发生 try 的工作原理：当开始一个 try 语句后，python 就在当前程序的上下文中作标记，这样当异常出现时就可以回到这里，try 子句先执行，接下来会发生什么依赖于执行时是否出现异常。 如果当 try 后的语句执行时发生异常，python 就跳回到 try 并执行第一个匹配该异常的 except 子句，异常处理完毕，控制流就通过整个 try 语句（除非在处理异常时又引发新的异常）。 如果在 try 后的语句里发生了异常，却没有匹配的 except 子句，异常将被递交到上层的 try，或者到程序的最上层（这样将结束程序，并打印默认的出错信息）。 如果在 try 子句执行时没有发生异常，python 将执行 else 语句后的语句（如果有 else 的话），然后控制流通过整个 try 语句 异常名称 描述 BaseException 所有异常的基类 SystemExit 解释器请求退出 KeyboardInterrupt 用户中断执行 (通常是输入 ^C) Exception 常规错误的基类 StopIteration 迭代器没有更多的值 GeneratorExit 生成器 (generator) 发生异常来通知退出 StandardError 所有的内建标准异常的基类 ArithmeticError 所有数值计算错误的基类 FloatingPointError 浮点计算错误 OverflowError 数值运算超出最大限制 ZeroDivisionError 除 (或取模) 零 (所有数据类型) AssertionError 断言语句失败 AttributeError 对象没有这个属性 EOFError 没有内建输入，到达 EOF 标记 EnvironmentError 操作系统错误的基类 IOError 输入 / 输出操作失败 OSError 操作系统错误 WindowsError 系统调用失败 ImportError 导入模块 / 对象失败 LookupError 无效数据查询的基类 IndexError 序列中没有此索引 (index) KeyError 映射中没有这个键 MemoryError 内存溢出错误 (对于 Python 解释器不是致命的) NameError 未声明 / 初始化对象 (没有属性) UnboundLocalError 访问未初始化的本地变量 ReferenceError 弱引用 (Weak reference) 试图访问已经垃圾回收了的对象 RuntimeError 一般的运行时错误 NotImplementedError 尚未实现的方法 SyntaxError Python 语法错误 IndentationError 缩进错误 TabError Tab 和空格混用 SystemError 一般的解释器系统错误 TypeError 对类型无效的操作 ValueError 传入无效的参数 UnicodeError Unicode 相关的错误 UnicodeDecodeError Unicode 解码时的错误 UnicodeEncodeError Unicode 编码时错误 UnicodeTranslateError Unicode 转换时错误 Warning 警告的基类 DeprecationWarning 关于被弃用的特征的警告 FutureWarning 关于构造将来语义会有改变的警告 OverflowWarning 旧的关于自动提升为长整型 (long) 的警告 PendingDeprecationWarning 关于特性将会被废弃的警告 RuntimeWarning 可疑的运行时行为 (runtime behavior) 的警告 SyntaxWarning 可疑的语法的警告 UserWarning 用户代码生成的警告 使用 except 而带多种异常类型 12345678try: 正常的操作 ......................except(Exception1[, Exception2[,...ExceptionN]]]): 发生以上多个异常中的一个，执行这块代码 ......................else: 如果没有异常执行这块代码 1234567891011121314# finally 触发异常&quot;&quot;&quot;当在try块中抛出一个异常，立即执行finally块代码。finally块中的所有语句执行后，异常被再次触发，并执行except块代码。&quot;&quot;&quot;try: fh = open(&quot;testfile&quot;, &quot;w&quot;) try: fh.write(&quot;这是一个测试文件，用于测试异常!!&quot;) finally: print &quot;关闭文件&quot; fh.close()except IOError: print &quot;Error: 没有找到文件或读取文件失败&quot; 1234567891011121314151617 # 触发异常后，后面的代码就不会再执行 # 定义函数def mye( level ): if level &lt; 1: raise Exception,&quot;Invalid level!&quot; # 触发异常后，后面的代码就不会再执行try: mye(0) # 触发异常except Exception,err: print 1,errelse: print 2 &gt;&gt;&gt;$ python test.py 1 Invalid level! （2）Python 模块 (Module)，是一个 Python 文件，以 .py 结尾，包含了 Python 对象定义和 Python 语句。 模块让你能够有逻辑地组织你的 Python 代码段。 把相关的代码分配到一个模块里能让你的代码更好用，更易懂。 模块能定义函数，类和变量，模块里也能包含可执行的代码。 dir () 函数 返回一个排好序的字符串列表，内容是一个模块里定义过的名字。 返回的列表容纳了在一个模块里定义的所有模块，变量和函数。 1234567891011121314# 导入内置math模块import math content = dir(math) print content;&gt;&gt;&gt;[&#x27;__doc__&#x27;, &#x27;__file__&#x27;, &#x27;__name__&#x27;, &#x27;acos&#x27;, &#x27;asin&#x27;, &#x27;atan&#x27;, &#x27;atan2&#x27;, &#x27;ceil&#x27;, &#x27;cos&#x27;, &#x27;cosh&#x27;, &#x27;degrees&#x27;, &#x27;e&#x27;, &#x27;exp&#x27;, &#x27;fabs&#x27;, &#x27;floor&#x27;, &#x27;fmod&#x27;, &#x27;frexp&#x27;, &#x27;hypot&#x27;, &#x27;ldexp&#x27;, &#x27;log&#x27;,&#x27;log10&#x27;, &#x27;modf&#x27;, &#x27;pi&#x27;, &#x27;pow&#x27;, &#x27;radians&#x27;, &#x27;sin&#x27;, &#x27;sinh&#x27;, &#x27;sqrt&#x27;, &#x27;tan&#x27;, &#x27;tanh&#x27;] 在这里，特殊字符串变量__name__指向模块的名字，__file__指向该模块的导入文件名。 globals () 和 locals () 函数 根据调用地方的不同，globals () 和 locals () 函数可被用来返回全局和局部命名空间里的名字。 如果在函数内部调用 locals()，返回的是所有能在该函数里访问的局部命名。 如果在函数内部调用 globals()，返回的是所有在该函数里能访问的全局名字。 两个函数的返回类型都是字典。所以名字们能用 keys() 函数摘取。 2021.03.23 （1）模块导入 **__name__** 在模块中运行：__main__ 在模块外运行：模块名称 2021.03.25 （1）操作系统：向下控制硬件，向上控制软件。 （2）操作命令快捷指令 ctrl + L 显示文件的绝对路径 Ctrl+Alt+T 打开终端 Ctrl+Shift+C 命令行复制 Ctrl+Shift+V 命令行粘贴 Ctrl +Shift +’+’ 放大终端的字体 Ctrl+’-’ 缩小终端的字体 arch 机器的处理器架构 uname -r 内核版本、 cat /proc/cpuinfo 处理器信息 cat /proc/version 发行版本信息 ls 查看当前路径下的目录信息 ls python* 查找所有以 python 开头的文件 cd 进入路径目录 pwd 显示当前路径 tree 按树结构显示路径下的目录信息 clear =&gt; ctrl + L sudo apt install terminator 第三方终端 cd - 回到上一次目录 cd … 切换到上层目录 cd . 切换 到当前目录 cd ~ 切换到当前用户的主目录 cd …/… 切换到上两层的目录 touch 创建文件 (夹) mkdir 创建文件夹 mkdir -p aa/bb/cc 创建多级目录 rm 删除文件 rmdir 文件夹名 删除空文件夹 rm -r 文件夹名 递归删除文件夹 rm -f 强制删除 rm -i 交互式提示 rm -rf 递归强制删除文件或文件夹 cp cp -r 递归拷贝文件 (recursive) mv 剪切文件 / 文件夹。重命名 ls -lh 智能显示文件列表 ls -al =&gt; ll mv -v 显示文件的操作信息 &gt; 重定向到其它文件 &gt;&gt; 重定向追加到文件 cat a.txt b.txt &gt; c.txt more 命令 f 前 b 后 q 退出 | 管道 ln -s 软链接，创建快捷方式 ln 硬链接，备份数据 2021.03.28 (1) 多任务: 并发：任务数 &gt; CPU 核心数，CPU 交替执行 并行：任务数 &lt;=CPU 核心数，每个 CPU 都可以执行一个任务 (2) 进程是资源分配的最小单位，一个运行程序就是用一个进程. 默认启动的是主进程，后续启动的为子进程 进程创建的步骤: import multiprocessing 进程对象 = multiprocessing.Process (target= 函数名) 进程对象.start () 获取进程的方法 os.getpid () 获取当前进程 ID os.getppid () 获取父进程 ID 函数本身无进程之分，被调用时会区分 如果直接调用，则为主进程。 若被进程调用，则为子进程。 IPC inter process communication 进程间共享全局变量使用 进程守护 进程对象 = multiprocessing.Process (target= 函数名) # 进程对象.daemon = True # （被动）守护主进程方法一 进程对象.start () # 进程对象.terminate () # （主动）守护主进程方法二 进程对象.join () 把并行执行 变成 串行执行，用于在一个进程执行后去执行另一个进程的场景 ps -a 所有进程 -x 无控制台终端 -u ps -aux|grep pycharm ps -aux --sort -pcpu | less 根据 CPU 使用来升序排序 ps -aux --sort -pmem | less 根据 内存使用 来升序排序 ps -aux --sort -pcpu,+pmem | head -n 10 将 CPU 使用和内存使用合并到一个命令，并通过管道显示前 10 个结果。 ps -axjf 树形显示进程 ps -f -C getty 使用 - f 参数来查看格式化的一个名为 getty 的进程的信息 kill -9 进程编号 线程 程序执行的最小单位 实现多任务的一种方式 进程默认会有一个线程 进程是分配资源，线程是执行单元 同一个进程中，多个线程共享资源 import threading 线程对象 = threading.Thread (target = 函数名， daemon = True) # 方法一 ： 守护主进程 # 线程对象.setDaemon（True） 方法二： 守护主线程 线程对象.start () 线程执行是无序的 获取线程名称： threading.current_thread ().name 线程同步：为解决多线程之间共享全局变量数据出现资源竞争，导致出现错误的情况（修改全局变量的操作是非原子性的） 互斥锁 创建互斥锁 mutex = threading.Lock () 上锁 mutex.acquire () 解锁 mutex.release () join 和互斥锁的异同： 同：都是将并发变成串行 异： join 是将一个任务整体串行 互斥锁可以将一个任务中的某一段代码串行 2021.04.03 property 属性 定义 property 属性有两种方式: 装饰器方式 类属性方式 装饰器方式: @property 修饰获取值的方法 @方法名.setter 修饰设置值的方法 类属性方式: 类属性 = property (获取值方法，设置值方法) with 语句 执行完成以后自动调用关闭文件操作，即使出现异常也会自动调用关闭文件操作。 上下文管理器 一个类只要实现了 __enter__()和__exit__() 这个两个方法，通过该类创建的对象我们就称之为上下文管理器。 with 语句之所以这么强大，背后是由上下文管理器做支撑的 使用 open 函数创建的文件对象就是一个上下文管理器对象。 __enter__ 表示上文方法，需要返回一个操作文件对象 __exit__ 表示下文方法，with 语句执行完成会自动执行，即使出现异常也会执行该方法。 Python 提供了 with 语句用于简化资源释放的操作，使用 with 语句操作建立在上下文管理器 (实现 __enter__和__exit__ ) 的基础上 生成器 根据程序员制定的规则循环生成数据，当条件不成立时则生成数据结束。数据不是一次性全部生成出来，而是使用一个，再生成一个，可以节约大量的内存。 next 函数获取生成器中的下一个值 for 循环遍历生成器中的每一个值 yield 关键字 只要在 def 函数里看到 yield 关键字就是生成器 生成器是根据算法生成数据的一种机制，每次调用生成器只生成一个值，可以节省大量内存。 生成器的创建有两种方式: 生成器推导式 yield 关键字 深拷贝和浅拷贝 浅拷贝： 不可变类型的浅拷贝说明: 通过上面的执行结果可以得知，不可变类型进行浅拷贝不会给拷贝的对象开辟新的内存空间，而只是拷贝了这个对象的引用。 可变类型的浅拷贝说明: 通过上面的执行结果可以得知，可变类型进行浅拷贝只对可变类型的第一层对象进行拷贝，对拷贝的对象会开辟新的内存空间进行存储，子对象不进行拷贝。 浅拷贝存在的问题: 虽然浅拷贝可以解决最开始直接赋值存在的问题，但如果数据内部有子元素为可变类型还会有问题 深拷贝： 不可变类型的深拷贝说明: 不可变类型进行深拷贝如果子对象没有可变类型则不会进行拷贝，而只是拷贝了这个对象的引用，否则会对该对象到最后一个可变类型的每一层对象就行拷贝，对每一层拷贝的对象都会开辟新的内存空间进行存储 可变类型的深拷贝说明: 可变类型进行深拷贝会对该对象到最后一个可变类型的每一层对象就行拷贝，对每一层拷贝的对象都会开辟新的内存空间进行存储。 浅拷贝和深拷贝的区别 浅拷贝最多拷贝对象的一层 深拷贝可能拷贝对象的多层 正则表达式 在实际开发过程中经常会有查找符合某些复杂规则的字符串的需要，比如：邮箱、图片地址、手机号码等，这时候想匹配或者查找符合某些规则的字符串就可以使用正则表达式了。 正则表达式就是记录文本规则的代码 # 导入 re 模块import re# 使用 match 方法进行匹配操作result = re.match(正则表达式,要匹配的字符串)# 如果上一步匹配到数据的话，可以使用 group 方法来提取数据result.group()&lt;!--code￼71--> 12345678910111213141516171819202122232425262728293031323334353637383940414243import re# 如果hello的首字符小写，那么正则表达式需要小写的hret = re.match(&quot;h&quot;,&quot;hello Python&quot;) print(ret.group())# 如果hello的首字符大写，那么正则表达式需要大写的Hret = re.match(&quot;H&quot;,&quot;Hello Python&quot;) print(ret.group())# 大小写h都可以的情况ret = re.match(&quot;[hH]&quot;,&quot;hello Python&quot;)print(ret.group())ret = re.match(&quot;[hH]&quot;,&quot;Hello Python&quot;)print(ret.group())ret = re.match(&quot;[hH]ello Python&quot;,&quot;Hello Python&quot;)print(ret.group())# 匹配0到9第一种写法ret = re.match(&quot;[0123456789]Hello Python&quot;,&quot;7Hello Python&quot;)print(ret.group())# 匹配0到9第二种写法ret = re.match(&quot;[0-9]Hello Python&quot;,&quot;7Hello Python&quot;)print(ret.group())ret = re.match(&quot;[0-35-9]Hello Python&quot;,&quot;7Hello Python&quot;)print(ret.group())# 下面这个正则不能够匹配到数字4，因此ret为Noneret = re.match(&quot;[0-35-9]Hello Python&quot;,&quot;4Hello Python&quot;)# print(ret.group())&gt;&gt;&gt;hHhHHello Python7Hello Python7Hello Python7Hello Python 123456789101112131415161718192021222324252627282930import re# 普通的匹配方式ret = re.match(&quot;嫦娥1号&quot;,&quot;嫦娥1号发射成功&quot;) print(ret.group())ret = re.match(&quot;嫦娥2号&quot;,&quot;嫦娥2号发射成功&quot;) print(ret.group())ret = re.match(&quot;嫦娥3号&quot;,&quot;嫦娥3号发射成功&quot;) print(ret.group())# 使用\\d进行匹配ret = re.match(&quot;嫦娥\\d号&quot;,&quot;嫦娥1号发射成功&quot;) print(ret.group())ret = re.match(&quot;嫦娥\\d号&quot;,&quot;嫦娥2号发射成功&quot;) print(ret.group())ret = re.match(&quot;嫦娥\\d号&quot;,&quot;嫦娥3号发射成功&quot;) print(ret.group())&gt;&gt;&gt;嫦娥1号嫦娥2号嫦娥3号嫦娥1号嫦娥2号嫦娥3号 123456789101112import rematch_obj = re.match(&quot;\\D&quot;, &quot;f&quot;)if match_obj: # 获取匹配结果 print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;f 123456789101112131415161718192021222324import re# 空格属于空白字符match_obj = re.match(&quot;hello\\sworld&quot;, &quot;hello world&quot;)if match_obj: result = match_obj.group() print(result)else: print(&quot;匹配失败&quot;)# \\t 属于空白字符match_obj = re.match(&quot;hello\\sworld&quot;, &quot;hello\\tworld&quot;)if match_obj: result = match_obj.group() print(result)else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;hello worldhello world 123456789101112131415161718192021import rematch_obj = re.match(&quot;hello\\Sworld&quot;, &quot;hello&amp;world&quot;)if match_obj:result = match_obj.group()print(result)else:print(&quot;匹配失败&quot;)match_obj = re.match(&quot;hello\\Sworld&quot;, &quot;hello$world&quot;)if match_obj:result = match_obj.group()print(result)else:print(&quot;匹配失败&quot;)&gt;&gt;&gt;hello&amp;world hello$world 123456789101112import re# 匹配非特殊字符中的一位match_obj = re.match(&quot;\\w&quot;, &quot;A&quot;)if match_obj: # 获取匹配结果 print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;A 12345678910# 匹配特殊字符中的一位match_obj = re.match(&quot;\\W&quot;, &quot;&amp;&quot;)if match_obj: # 获取匹配结果 print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;&amp; 代码 功能 * 匹配前一个字符出现 0 次或者无限次，即可有可无 + 匹配前一个字符出现 1 次或者无限次，即至少有 1 次 ? 匹配前一个字符出现 1 次或者 0 次，即要么有 1 次，要么没有 匹配前一个字符出现 m 次 匹配前一个字符出现从 m 到 n 次 123456789101112131415import reret = re.match(&quot;[A-Z][a-z]*&quot;,&quot;M&quot;)print(ret.group())ret = re.match(&quot;[A-Z][a-z]*&quot;,&quot;MnnM&quot;)print(ret.group())ret = re.match(&quot;[A-Z][a-z]*&quot;,&quot;Aabcdef&quot;)print(ret.group())&gt;&gt;&gt;MMnnAabcdef 123456789101112import rematch_obj = re.match(&quot;t.+o&quot;, &quot;two&quot;)if match_obj: print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;two 1234567891011import rematch_obj = re.match(&quot;https?&quot;, &quot;http&quot;)if match_obj: print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;https 12345678910111213import re# 匹配出，8到20位的密码，可以是大小写英文字母、数字、下划线ret = re.match(&quot;[a-zA-Z0-9_]&#123;6&#125;&quot;,&quot;12a3g45678&quot;)print(ret.group())ret = re.match(&quot;[a-zA-Z0-9_]&#123;8,20&#125;&quot;,&quot;1ad12f23s34455ff66&quot;)# &#123;2,&#125; 表示2-&gt;无穷print(ret.group())&gt;&gt;&gt;12a3g41ad12f23s34455ff66 匹配开头和结尾 代码 功能 ^ 匹配字符串开头 $ 匹配字符串结尾 123456789101112import re# 匹配以数字开头的数据match_obj = re.match(&quot;^\\d.*&quot;, &quot;3hello&quot;)if match_obj: # 获取匹配结果 print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;3hello 123456789101112import re# 匹配以数字结尾的数据match_obj = re.match(&quot;.*\\d$&quot;, &quot;hello5&quot;)if match_obj: # 获取匹配结果 print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;hello5 1234567891011# 匹配以数字开头中间内容不管以数字结尾match_obj = re.match(&quot;^\\d.*\\d$&quot;, &quot;4hello4&quot;)if match_obj: # 获取匹配结果 print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;4hello4 1234567891011121314# [^指定字符]: 表示除了指定字符都匹配import rematch_obj &#x3D; re.match(&quot;[^aeiou]&quot;, &quot;h&quot;)if match_obj: # 获取匹配结果 print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;h 代码 功能 | 匹配左右任意一个表达式 (ab) 将括号中字符作为一个分组 \\num 引用分组 num 匹配到的字符串 (?P&lt;name&gt;) 分组起别名 (?P=name) 引用别名为 name 分组匹配到的字符串 1234567891011121314151617181920import re# 水果列表fruit_list = [&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;, &quot;pear&quot;]# 遍历数据for value in fruit_list: # | 匹配左右任意一个表达式 match_obj = re.match(&quot;apple|pear&quot;, value) if match_obj: print(&quot;%s是我想要的&quot; % match_obj.group()) else: print(&quot;%s不是我要的&quot; % value) &gt;&gt;&gt;apple是我想要的banana不是我要的orange不是我要的pear是我想要的 123456789101112131415import re# 匹配出163、126、qq等邮箱match_obj = re.match(&quot;[a-zA-Z0-9_]&#123;4,20&#125;@(163|126|qq|sina|yahoo)\\.com&quot;, &quot;hello@163.com&quot;)if match_obj: print(match_obj.group()) # 获取分组数据 print(match_obj.group(1))else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;hello@163.com163 1234567891011121314151617import re# 匹配qq:10567这样的数据，提取出来qq文字和qq号码match_obj = re.match(&quot;(qq):([1-9]\\d&#123;4,10&#125;)&quot;, &quot;qq:10567&quot;)if match_obj: print(match_obj.group()) # 分组:默认是1一个分组，多个分组从左到右依次加1 print(match_obj.group(1)) # 提取第二个分组数据 print(match_obj.group(2))else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;qq10567 123456789101112131415161718match_obj = re.match(&quot;&lt;[a-zA-Z1-6]+&gt;.*&lt;/[a-zA-Z1-6]+&gt;&quot;, &quot;&lt;html&gt;hh&lt;/div&gt;&quot;)if match_obj: print(match_obj.group())else: print(&quot;匹配失败&quot;)match_obj = re.match(&quot;&lt;([a-zA-Z1-6]+)&gt;.*&lt;/\\\\1&gt;&quot;, &quot;&lt;html&gt;hh&lt;/html&gt;&quot;)if match_obj: print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;&lt;html&gt;hh&lt;/div&gt;&lt;html&gt;hh&lt;/html&gt; 12345678910# 匹配出&lt;html&gt;&lt;h1&gt;www.itcast.cn&lt;/h1&gt;&lt;/html&gt;match_obj = re.match(&quot;&lt;([a-zA-Z1-6]+)&gt;&lt;([a-zA-Z1-6]+)&gt;.*&lt;/\\\\2&gt;&lt;/\\\\1&gt;&quot;, &quot;&lt;html&gt;&lt;h1&gt;www.itcast.cn&lt;/h1&gt;&lt;/html&gt;&quot;)if match_obj: print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;&lt;html&gt;&lt;h1&gt;www.itcast.cn&lt;/h1&gt;&lt;/html&gt; 12345678910# 匹配出&lt;html&gt;&lt;h1&gt;www.itcast.cn&lt;/h1&gt;&lt;/html&gt;match_obj = re.match(&quot;&lt;(?P&lt;name1&gt;[a-zA-Z1-6]+)&gt;&lt;(?P&lt;name2&gt;[a-zA-Z1-6]+)&gt;.*&lt;/(?P=name2)&gt;&lt;/(?P=name1)&gt;&quot;, &quot;&lt;html&gt;&lt;h1&gt;www.itcast.cn&lt;/h1&gt;&lt;/html&gt;&quot;)if match_obj: print(match_obj.group())else: print(&quot;匹配失败&quot;) &gt;&gt;&gt;&lt;html&gt;&lt;h1&gt;www.itcast.cn&lt;/h1&gt;&lt;/html&gt;","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程语言","slug":"编程语言","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}]},{"title":"Pycharm快捷操作","slug":"Pycharm快捷操作","date":"2021-04-19T02:56:02.000Z","updated":"2021-06-26T02:41:53.530Z","comments":true,"path":"2021/04/19/Pycharm快捷操作/","link":"","permalink":"https://leezhao415.github.io/2021/04/19/Pycharm%E5%BF%AB%E6%8D%B7%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Pycharm 快捷操作 文章目录 1、编辑（Editing） 2、查找 / 替换 (Replace) 3、运行 (Running) 4、调试 (Debugging) 5、导航 (Navigation) 6、搜索相关 (Search) 7、重构 (Refactoring) 8、控制 VCS 9、模版 (Live Temp) 10、基本 (General) 1、编辑（Editing） Ctrl + Space 基本的代码完成（类、方法、属性） Ctrl + Alt + Space 快速导入任意类 Ctrl + Shift + Enter 语句完成 Ctrl + P 参数信息（在方法中调用参数） Ctrl + Q 快速查看文档 F1 外部文档 Shift + F1 外部文档，进入 web 文档主页 Ctrl + Shift + Z 重做 Ctrl + 悬浮 / 单击鼠标左键 简介 / 进入代码定义 Ctrl + F1 显示错误描述或警告信息 Alt + Insert 自动生成代码 Ctrl + O 重新方法 Ctrl + Alt + T 选中 Ctrl + / 行注释 / 取消行注释 Ctrl + Shift + / 块注释 Ctrl + W 选中增加的代码块 Ctrl + Shift + W 回到之前状态 Ctrl + Shift + ]/[ 选定代码块结束 / 开始 Alt + Enter 快速修正 Ctrl + Alt + L 代码格式化 Ctrl + Alt + O 优化导入 Ctrl + Alt + I 自动缩进 Tab Shift + Tab 缩进 / 不缩进当前行 Ctrl+X/Shift+Delete 剪切当前行 / 选定的代码块到剪贴板 Ctrl+C/Ctrl+Insert 复制当前行 / 选定的代码块到剪贴板 Ctrl+V/Shift+Insert 从剪贴板粘贴 Ctrl + Shift + V 从最近的缓冲区粘贴 Ctrl + D 复制选定的区域或行 Ctrl + Y 删除选定的行 Ctrl + Shift + J 添加智能线 Ctrl + Enter 智能线切割 Shift + Enter 另起一行 Ctrl + Shift + U 在选定的区域或代码块间切换 Ctrl + Delete 删除到字符结束 Ctrl + Backspace 删除到字符开始 Ctrl + Numpad+/- 展开 / 折叠代码块（当前位置的：函数，注释等） Ctrl + shift + Numpad+/- 展开 / 折叠所有代码块 Ctrl + F4 关闭运行的选项卡 2、查找 / 替换 (Replace) F3 下一个 Shift + F3 前一个 Ctrl + R 替换 Ctrl + Shift + F 或者连续 2 次敲击 shift 全局查找 Ctrl + Shift + R 全局替换 3、运行 (Running) Alt + Shift + F10 运行模式配置 Alt + Shift + F9 调试模式配置 Shift + F10 运行 Shift + F9 调试 Ctrl + Shift + F10 运行编辑器配置 Ctrl + Alt + R 运行 manage.py 任务 4、调试 (Debugging) F8 跳过 F7 进入 Shift + F8 退出 Alt + F9 运行游标 Alt + F8 验证表达式 Ctrl + Alt + F8 快速验证表达式 F9 恢复程序 Ctrl + F8 断点开关 Ctrl + Shift + F8 查看断点 5、导航 (Navigation) Ctrl + N 跳转到类 Ctrl + Shift + N 跳转到符号 Alt + Right/Left 跳转到下一个 / 前一个编辑的选项卡（代码文件） Alt + Up/Down 跳转到上一个 / 下一个方法 F12 回到先前的工具窗口 Esc 从工具窗口回到编辑窗口 Shift + Esc 隐藏运行的、最近运行的窗口 Ctrl + Shift + F4 关闭主动运行的选项卡 Ctrl + G 查看当前行号、字符号 Ctrl + E 当前文件弹出，打开最近使用的文件列表 Ctrl+Alt+Left/Right 后退 / 前进 Ctrl+Shift+Backspace 导航到最近编辑区域 Alt + F1 查找当前文件或标识 Ctrl+B / Ctrl+Click 跳转到声明 Ctrl + Alt + B 跳转到实现 Ctrl + Shift + I 查看快速定义 Ctrl + Shift + B 跳转到类型声明 Ctrl + U 跳转到父方法、父类 Ctrl + ]/[ 跳转到代码块结束、开始 Ctrl + F12 弹出文件结构 Ctrl + H 类型层次结构 Ctrl + Shift + H 方法层次结构 Ctrl + Alt + H 调用层次结构 F2 / Shift + F2 下一条、前一条高亮的错误 F4 / Ctrl + Enter 编辑资源、查看资源 Alt + Home 显示导航条 F11 书签开关 Ctrl + Shift + F11 书签助记开关 Ctrl + #[0-9] 跳转到标识的书签 Shift + F11 显示书签 6、搜索相关 (Search) Alt + F7/Ctrl + F7 文件中查询用法 Ctrl + Shift + F7 文件中用法高亮显示 Ctrl + Alt + F7 显示用法 7、重构 (Refactoring) F5 复制 F6 剪切 Alt + Delete 安全删除 Shift + F6 重命名 Ctrl + F6 更改签名 Ctrl + Alt + N 内联 Ctrl + Alt + M 提取方法 Ctrl + Alt + V 提取属性 Ctrl + Alt + F 提取字段 Ctrl + Alt + C 提取常量 Ctrl + Alt + P 提取参数 8、控制 VCS Ctrl + K 提交项目 Ctrl + T 更新项目 Alt + Shift + C 查看最近的变化 Alt + ’ VCS 快速弹出 9、模版 (Live Temp) Ctrl + Alt + J 当前行使用模版 Ctrl +Ｊ 插入模版 10、基本 (General) Alt + #[0-9] 打开相应的工具窗口 Ctrl + Alt + Y 同步 Ctrl + Shift + F12 最大化编辑开关 Alt + Shift + F 添加到最喜欢 Alt + Shift + I 根据配置检查当前文件 Ctrl + ’快速切换当前计划 Ctrl + Alt + S 打开设置页 Ctrl + Shift + A 查找编辑器里所有的动作 Ctrl + Tab 在窗口间进行切换","categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]}],"categories":[{"name":"Hot","slug":"Hot","permalink":"https://leezhao415.github.io/categories/Hot/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"AIGC前沿","slug":"AIGC前沿","permalink":"https://leezhao415.github.io/tags/AIGC%E5%89%8D%E6%B2%BF/"},{"name":"CV未来","slug":"CV未来","permalink":"https://leezhao415.github.io/tags/CV%E6%9C%AA%E6%9D%A5/"},{"name":"目标跟踪","slug":"目标跟踪","permalink":"https://leezhao415.github.io/tags/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"},{"name":"多模态","slug":"多模态","permalink":"https://leezhao415.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"},{"name":"模型部署","slug":"模型部署","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"},{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"模型优化","slug":"模型优化","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"},{"name":"智能家居","slug":"智能家居","permalink":"https://leezhao415.github.io/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"},{"name":"深度学习","slug":"深度学习","permalink":"https://leezhao415.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"VSLAM","slug":"VSLAM","permalink":"https://leezhao415.github.io/tags/VSLAM/"},{"name":"目标检测（人脸检测）","slug":"目标检测（人脸检测）","permalink":"https://leezhao415.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%EF%BC%89/"},{"name":"且读文摘","slug":"且读文摘","permalink":"https://leezhao415.github.io/tags/%E4%B8%94%E8%AF%BB%E6%96%87%E6%91%98/"},{"name":"视频理解","slug":"视频理解","permalink":"https://leezhao415.github.io/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/"},{"name":"数据集","slug":"数据集","permalink":"https://leezhao415.github.io/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"},{"name":"深度模型（目标检测）","slug":"深度模型（目标检测）","permalink":"https://leezhao415.github.io/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%EF%BC%88%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%89/"},{"name":"NCNN部署","slug":"NCNN部署","permalink":"https://leezhao415.github.io/tags/NCNN%E9%83%A8%E7%BD%B2/"},{"name":"深度学习环境配置","slug":"深度学习环境配置","permalink":"https://leezhao415.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"Linux","slug":"Linux","permalink":"https://leezhao415.github.io/tags/Linux/"},{"name":"模型性能指标","slug":"模型性能指标","permalink":"https://leezhao415.github.io/tags/%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/"},{"name":"寒窑赋","slug":"寒窑赋","permalink":"https://leezhao415.github.io/tags/%E5%AF%92%E7%AA%91%E8%B5%8B/"},{"name":"计算机视觉库","slug":"计算机视觉库","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%BA%93/"},{"name":"深度模型","slug":"深度模型","permalink":"https://leezhao415.github.io/tags/%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B/"},{"name":"概述","slug":"概述","permalink":"https://leezhao415.github.io/tags/%E6%A6%82%E8%BF%B0/"},{"name":"YOLOX","slug":"YOLOX","permalink":"https://leezhao415.github.io/tags/YOLOX/"},{"name":"三维建模","slug":"三维建模","permalink":"https://leezhao415.github.io/tags/%E4%B8%89%E7%BB%B4%E5%BB%BA%E6%A8%A1/"},{"name":"YOLOX目标检测","slug":"YOLOX目标检测","permalink":"https://leezhao415.github.io/tags/YOLOX%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"多任务学习模型","slug":"多任务学习模型","permalink":"https://leezhao415.github.io/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"},{"name":"PaddlePaddle","slug":"PaddlePaddle","permalink":"https://leezhao415.github.io/tags/PaddlePaddle/"},{"name":"知识蒸馏","slug":"知识蒸馏","permalink":"https://leezhao415.github.io/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"},{"name":"CV数据集","slug":"CV数据集","permalink":"https://leezhao415.github.io/tags/CV%E6%95%B0%E6%8D%AE%E9%9B%86/"},{"name":"ReID","slug":"ReID","permalink":"https://leezhao415.github.io/tags/ReID/"},{"name":"度量学习","slug":"度量学习","permalink":"https://leezhao415.github.io/tags/%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/"},{"name":"人脸识别","slug":"人脸识别","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"},{"name":"表面缺陷检测","slug":"表面缺陷检测","permalink":"https://leezhao415.github.io/tags/%E8%A1%A8%E9%9D%A2%E7%BC%BA%E9%99%B7%E6%A3%80%E6%B5%8B/"},{"name":"CV算法","slug":"CV算法","permalink":"https://leezhao415.github.io/tags/CV%E7%AE%97%E6%B3%95/"},{"name":"编程工具","slug":"编程工具","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"},{"name":"MOT","slug":"MOT","permalink":"https://leezhao415.github.io/tags/MOT/"},{"name":"IOU","slug":"IOU","permalink":"https://leezhao415.github.io/tags/IOU/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://leezhao415.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"大数据框架","slug":"大数据框架","permalink":"https://leezhao415.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/"},{"name":"CV/目标检测工具箱","slug":"CV-目标检测工具箱","permalink":"https://leezhao415.github.io/tags/CV-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%B7%A5%E5%85%B7%E7%AE%B1/"},{"name":"NLP/评估指标","slug":"NLP-评估指标","permalink":"https://leezhao415.github.io/tags/NLP-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"},{"name":"NLP/数据增强工具","slug":"NLP-数据增强工具","permalink":"https://leezhao415.github.io/tags/NLP-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%B7%A5%E5%85%B7/"},{"name":"机器学习/损失函数","slug":"机器学习-损失函数","permalink":"https://leezhao415.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"name":"科研项目成果","slug":"科研项目成果","permalink":"https://leezhao415.github.io/tags/%E7%A7%91%E7%A0%94%E9%A1%B9%E7%9B%AE%E6%88%90%E6%9E%9C/"},{"name":"计算机顶会","slug":"计算机顶会","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%A1%B6%E4%BC%9A/"},{"name":"人工智能/CV","slug":"人工智能-CV","permalink":"https://leezhao415.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-CV/"},{"name":"Transformer/DETR(CV)","slug":"Transformer-DETR-CV","permalink":"https://leezhao415.github.io/tags/Transformer-DETR-CV/"},{"name":"自然语言处理NLP","slug":"自然语言处理NLP","permalink":"https://leezhao415.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP/"},{"name":"梯度更新","slug":"梯度更新","permalink":"https://leezhao415.github.io/tags/%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0/"},{"name":"激活函数","slug":"激活函数","permalink":"https://leezhao415.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},{"name":"OpenCV之DNN模块","slug":"OpenCV之DNN模块","permalink":"https://leezhao415.github.io/tags/OpenCV%E4%B9%8BDNN%E6%A8%A1%E5%9D%97/"},{"name":"名人名言","slug":"名人名言","permalink":"https://leezhao415.github.io/tags/%E5%90%8D%E4%BA%BA%E5%90%8D%E8%A8%80/"},{"name":"Python数据分析","slug":"Python数据分析","permalink":"https://leezhao415.github.io/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"NLP-BERT","slug":"NLP-BERT","permalink":"https://leezhao415.github.io/tags/NLP-BERT/"},{"name":"NLP-模型优化","slug":"NLP-模型优化","permalink":"https://leezhao415.github.io/tags/NLP-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"},{"name":"NLP-发展史","slug":"NLP-发展史","permalink":"https://leezhao415.github.io/tags/NLP-%E5%8F%91%E5%B1%95%E5%8F%B2/"},{"name":"NLP","slug":"NLP","permalink":"https://leezhao415.github.io/tags/NLP/"},{"name":"计算机视觉CV","slug":"计算机视觉CV","permalink":"https://leezhao415.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV/"},{"name":"算法","slug":"算法","permalink":"https://leezhao415.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"操作系统","slug":"操作系统","permalink":"https://leezhao415.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"机器学习","slug":"机器学习","permalink":"https://leezhao415.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数据库原理","slug":"数据库原理","permalink":"https://leezhao415.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/"},{"name":"网络编程","slug":"网络编程","permalink":"https://leezhao415.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"网络通信","slug":"网络通信","permalink":"https://leezhao415.github.io/tags/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1/"},{"name":"编程语言","slug":"编程语言","permalink":"https://leezhao415.github.io/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}]}