{
    "version": "https://jsonfeed.org/version/1",
    "title": "且听风吟，御剑于心！ • All posts by \"aigc前沿\" tag",
    "description": "",
    "home_page_url": "https://leezhao415.github.io",
    "items": [
        {
            "id": "https://leezhao415.github.io/2024/03/15/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91AIGC%E5%90%AF%E5%85%832024/",
            "url": "https://leezhao415.github.io/2024/03/15/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91AIGC%E5%90%AF%E5%85%832024/",
            "title": "【精华】AIGC启元2024",
            "date_published": "2024-03-15T08:16:00.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#aigc-%E5%89%8D%E6%B2%BF\">AIGC 前沿</a>\n<ul>\n<li><a href=\"#1-gemini-15-pro%E8%B0%B7%E6%AD%8C%E6%96%B0%E4%B8%80%E4%BB%A3%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B\">(1) Gemini 1.5 Pro（谷歌新一代多模态大模型）</a></li>\n<li><a href=\"#2-sora%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B\">(2) Sora（文本生成视频大模型）</a></li>\n<li><a href=\"#3-emo%E9%98%BF%E9%87%8C%E7%94%9F%E6%88%90%E5%BC%8Fai%E6%A8%A1%E5%9E%8B\">(3) EMO（阿里生成式 AI 模型）</a></li>\n<li><a href=\"#4-playground-v25%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B\">(4) Playground v2.5（文生图大模型）</a></li>\n<li><a href=\"#5-vsp-llm%E5%94%87%E8%AF%AD%E8%AF%86%E5%88%AB\">(5) VSP-LLM（唇语识别）</a></li>\n<li><a href=\"#6-ideogram-ai-%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B\">(6) Ideogram ai  （文生图大模型）</a></li>\n<li><a href=\"#7-ltx-studio%E7%94%9F%E6%88%90%E5%BC%8Fai%E7%94%B5%E5%BD%B1%E5%88%B6%E4%BD%9C%E5%B9%B3%E5%8F%B0\">(7) LTX studio（生成式 AI 电影制作平台）</a></li>\n<li><a href=\"#8-claude3llm\">(8) Claude3（LLM）</a></li>\n<li><a href=\"#9-open-sora%E6%96%87%E7%94%9F%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B\">(9) Open Sora（文生视频大模型）</a></li>\n<li><a href=\"#10-yi-9bllm\">(10) Yi-9B（LLM）</a></li>\n<li><a href=\"#11-cares-copilot10%E5%A4%9A%E6%A8%A1%E6%80%81%E6%89%8B%E6%9C%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B\">(11) CARES Copilot1.0（多模态手术大模型）</a></li>\n<li><a href=\"#12-figure-01%E9%80%9A%E7%94%A8%E6%9C%BA%E5%99%A8%E4%BA%BAfigure-ai-openai\">(12) Figure 01 通用机器人（Figure AI + OpenAI）</a></li>\n<li><a href=\"#13-devinai%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%8A%A9%E6%89%8B\">(13) Devin（AI 软件工程师助手）</a></li>\n<li><a href=\"#14-behavior-1k%E6%9D%8E%E9%A3%9E%E9%A3%9E%E5%9B%A2%E9%98%9F%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E5%9F%BA%E5%87%86\">(14) BEHAVIOR-1K（李飞飞团队 — 具身智能基准）</a></li>\n<li><a href=\"#15-mm1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%8B%B9%E6%9E%9C%E5%85%AC%E5%8F%B8%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B\">(15) MM1 大模型（苹果公司多模态大模型）</a></li>\n<li><a href=\"#16-aesopagent%E8%BE%BE%E6%91%A9%E9%99%A2%E6%99%BA%E8%83%BD%E4%BD%93%E9%A9%B1%E5%8A%A8%E7%9A%84%E8%BF%9B%E5%8C%96%E7%B3%BB%E7%BB%9F\">(16) AesopAgent（达摩院 — 智能体驱动的进化系统）</a></li>\n<li><a href=\"#17-cogview3%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B\">(17) CogView3（文生图大模型）</a></li>\n<li><a href=\"#18-autodev%E5%BE%AE%E8%BD%AF%E5%9B%A2%E9%98%9F%E5%85%A8%E8%87%AA%E5%8A%A8-ai-%E9%A9%B1%E5%8A%A8%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6\">(18) AutoDev（微软团队全自动 AI 驱动软件开发框架）</a></li>\n<li><a href=\"#19-vloggergoogle%E5%9B%BE%E7%94%9F%E9%9F%B3%E9%A2%91%E9%A9%B1%E5%8A%A8%E8%A7%86%E9%A2%91%E6%96%B9%E6%B3%95\">(19) VLOGGER（Google 图生音频驱动视频方法）</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h2><span id=\"aigc-前沿\"> AIGC 前沿</span></h2>\n<h3><span id=\"1-gemini-15-pro谷歌新一代多模态大模型\"> (1) Gemini 1.5 Pro（谷歌新一代多模态大模型）</span></h3>\n<p><em><strong>2024.02.16</strong></em>  谷歌新一代多模态大模型 Gemini 1.5 Pro，在性能上超越 OpenAI 的 GPT-4 Turbo，堪称业界最强大模型。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://mp.weixin.qq.com/s?__biz=MzU5OTI0NTc3Mg==&amp;mid=2247530889&amp;idx=1&amp;sn=0686f28b493fb61fdcd3e619b9a97517\">“打假” Sora，谷歌 Gemini 1.5 Pro 第一波评测出炉｜甲子光年</a><br>\n<strong>官网链接：</strong> <a href=\"https://openai.com/sora\">https://openai.com/sora</a></p>\n<h3><span id=\"2-sora文本生成视频大模型\"> (2) Sora（文本生成视频大模型）</span></h3>\n<p><em><strong>2024.02.16</strong></em>  Sora 文本生成视频的大模型。它所展现出来的能力几乎可以 “碾压” 目前全球能实现文本生成视频的大模型 包 括 Runway、Pika、Stable Video Diffusion 等 20 多个产品。<br>\n　　用户仅需输入简短一句话，Sora 就可生成一段长达 60 秒的视频，远远超过市面上同类型级别的 AI 视频生成时长。在此之前，AI 视频模型生成时长几乎在 10 秒以内，而 “明星模型” Runway 和 Pika 等也仅有 3 到 4 秒。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://www.thepaper.cn/newsDetail_forward_26415514\">Sora 到底有多强？</a> |  <a href=\"https://mp.weixin.qq.com/s/zmKeNWrU6me3leNpdNH2wQ\">微软最新 Sora 综述</a><br>\n<strong>官网链接：</strong> <a href=\"https://ai.google.dev/gemma\">Gemma Open Models</a></p>\n<h3><span id=\"3-emo阿里生成式-ai-模型\"> (3) EMO（阿里生成式 AI 模型）</span></h3>\n<p><em><strong>2024.02.28</strong></em>  生成式 AI 模型 EMO（Emote Portrait Alive）。EMO 仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容 “张嘴” 唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://baijiahao.baidu.com/s?id=1792189739510241856&amp;wfr=spider&amp;for=pc\">阿里 EMO 模型，一张照片就能造谣</a><br>\n<strong>官网链接：</strong> <a href=\"https://humanaigc.github.io/emote-portrait-alive/\">https://humanaigc.github.io/emote-portrait-alive/</a></p>\n<h3><span id=\"4-playground-v25文生图大模型\"> (4) Playground v2.5（文生图大模型）</span></h3>\n<p><em><strong>2024.02.28</strong></em>  Playground 在去年发布 Playground v2.0 之后再次开源新的文生图模型 Playground v2.5。相比上一个版本，Playground v2.5 在美学质量，颜色和对比度，多尺度生成以及以人为中心的细节处理有比较大的提升。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://zhuanlan.zhihu.com/p/684287454\">超过 Midjourney v5.2 的开源文生图大模型 Playground v2.5 来了</a><br>\n<strong>官网链接：</strong> <a href=\"https://playground.com/\">https://playground.com/</a></p>\n<h3><span id=\"5-vsp-llm唇语识别\"> (5) VSP-LLM（唇语识别）</span></h3>\n<p><em><strong>2024.02.28</strong></em>  一种通过观察视频中人的嘴型来理解和翻译说话内容的技术，也就是识别唇语。该技术能够将视频中的唇动转化为文本（视觉语音识别），并将这些唇动直接翻译成目标语言的文本 (视觉语音翻译)。不仅如此，VSP-LLM 还能智能识别和去除视频中不必要的重复信息，使处理过程更加快速和准确。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://www.chinaz.com/2024/0228/1599901.shtml\">VSP-LLM：可通过观察视频中人的嘴型来识别唇语</a><br>\n<strong>官网链接：</strong> <a href=\"https://github.com/sally-sh/vsp-llm\">https://github.com/sally-sh/vsp-llm</a></p>\n<h3><span id=\"6-ideogram-ai-文生图大模型\"> (6) Ideogram ai  （文生图大模型）</span></h3>\n<p><em><strong>2024.02.29</strong></em>  Ideogram 发布了最新的 Ideogram1.0 图像生成模型，该模型具有强大的文字生成能力和提示词理解能力。Ideogram1.0 在文本渲染准确性方面实现了飞跃。</p>\n<p><strong>推荐文章：</strong><a href=\"https://www.chinaz.com/2024/0229/1599986.shtml\">Ideogram 1.0 图像生成模型发布 文字生成能力更强大了</a><br>\n<strong>官网链接：</strong><a href=\"https://top.aibase.com/tool/ideogram-ai\">https://top.aibase.com/tool/ideogram-ai</a></p>\n<h3><span id=\"7-ltx-studio生成式-ai-电影制作平台\"> (7) LTX studio（生成式 AI 电影制作平台）</span></h3>\n<p><em><strong>2024.02.29</strong></em>  生成式 AI 电影制作平台 —LTX Studio，用户只需要输入文本就能生成超 25 秒的微电影视频，同时可对镜头切换、角色、场景一致性、摄像机、灯光等进行可视化精准控制。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://new.qq.com/rain/a/20240229A02EA500\">效果比 Sora 惊艳，著名 AI 平台大动作！文本生成超 25 秒视频，带背景音乐、转场等效果</a><br>\n<strong>官网链接：</strong> <a href=\"https://ltx.studio\">https://ltx.studio</a></p>\n<h3><span id=\"8-claude3llm\"> (8) Claude3（LLM）</span></h3>\n<p><em><strong>2024.03.04</strong></em>  Claude3 是由 Anthropic 发布的最新的 AI 大模型系列，同时，Claude3 是多模态大模型 ，具有强大的 “视觉能力”。Claude3 Opus 已经在部分行业行为准则中的表现优于 OpenAI 的 GPT-4 和谷歌的 Gemini Ultra，如本科生水平知识（MMLU）、研究生级别专家推理（GPQA）和基础数学（GSM8K）。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://new.qq.com/rain/a/20240307A09O1N00\">OpenAI 劲敌出现！Claude3 正式发布，超越 GTP-4?</a><br>\n<strong> 官网链接：</strong> <a href=\"https://www.anthropic.com/claude\">https://www.anthropic.com/claude</a></p>\n<h3><span id=\"9-open-sora文生视频大模型\"> (9) Open Sora（文生视频大模型）</span></h3>\n<p><em><strong>2024.03.01</strong></em>  北大团队联合兔展发起了一项 Sora 复现计划 ——Open Sora</p>\n<p><strong>推荐文章：</strong> <a href=\"https://baijiahao.baidu.com/s?id=1792479658318662669&amp;wfr=spider&amp;for=pc\">北大与兔展智能发起复现 Sora，框架已开源</a><br>\n<strong>官网链接：</strong><br>\n<a href=\"https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html\">https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html</a><br>\n<a href=\"https://github.com/PKU-YuanGroup/Open-Sora-Plan\">https://github.com/PKU-YuanGroup/Open-Sora-Plan</a></p>\n<h3><span id=\"10-yi-9bllm\"> (10) Yi-9B（LLM）</span></h3>\n<p><em><strong>2024.03.06</strong></em>  李开复旗下 AI 公司零一万物的最新力作 ——Yi-9B 大模型正式对外开源发布。这款具有 90 亿参数的大模型，在代码和数学能力上达到了前所未有的高度，同时保持了对消费级显卡的良好兼容性，为广大开发者和研究人员提供了前所未有的便利性和强大功能。<br>\n　　Yi-9B 作为 Yi 系列中的新成员，被誉为 “理科状元”，特别加强了在代码和数学方面的学习能力。相较于市场上其他类似规模的开源模型，如 Mistral-7B、SOLAR-10.7B、Gemma-7B 等，Yi-9B 展现出了最佳的性能表现。特别值得一提的是，Yi-9B 既提供了浮点数版本（BF 16），也提供了整数版本（Int8），使其能够轻松部署在包括 RTX 4090 和 RTX 3090 在内的消费级显卡上，大大降低了使用门槛和成本。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://baijiahao.baidu.com/s?id=1792881393208541121&amp;wfr=spider&amp;for=pc\">零一万物开源 Yi-9B 大模型，消费级显卡可用，代码数学历史最强</a><br>\n<strong>官网链接：</strong> <a href=\"https://github.com/01-ai/Yi\">https://github.com/01-ai/Yi</a></p>\n<h3><span id=\"11-cares-copilot10多模态手术大模型\"> (11) CARES Copilot1.0（多模态手术大模型）</span></h3>\n<p><em><strong>2024.03.11</strong></em>  CARES Copilot 是由中国科学院香港创新院 AI 中心研发的一个可信赖、可解释、面向医疗垂直领域并能与智能医疗设备高度集成的大模型系统。CARES Copilot 1.0 实现了图像、文本、语音、视频、MRI、CT、超声等多模态的手术数据理解。支持超过 100K 上下文的长窗口理解和高效分析，能理解超过 3000 页的复杂手术教材，对于年轻医生的培训和教学具有极高的实用价值。此外，该系统能通过深度检索功能，快速精确地提取手术教材、专家指南、医学论文等专业文档的信息，确保其提供的答案具有高度的可信度和可追溯性。经测试，系统能在一秒钟内完成百万级数据的快速检索，同时保持 95% 的准确率。该系统已在多家医院的不同科室进行了内部测试和迭代优化。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://baijiahao.baidu.com/s?id=1793375529838669070&amp;wfr=spider&amp;for=pc\">CARES Copilot 1.0 多模态手术大模型发布，可实现轻量化部署</a><br>\n<strong>官网链接：</strong> /</p>\n<h3><span id=\"12-figure-01-通用机器人figure-ai-openai\"> (12) Figure 01 通用机器人（Figure AI + OpenAI）</span></h3>\n<p><em><strong>2024.03.13</strong></em>  Figure 01 通用机器人由 Figure AI 和 OpenAI 合作完成。展示视频中，Figure AI 人形机器人具有视觉能力并能表述所见画面，它伸手拿起桌上的苹果，并解释了这么做的原因，人类的提问后，这台人形机器人 “思索” 2~3 秒后便能顺畅作答，手部动作速度则接近人类。据视频介绍，机器人采用了端到端神经网络。<br>\n　　该人形机器人由 OpenAI 提供了视觉推理和语言理解，Figure AI 的神经网络则提供快速、灵巧的机器人动作。人形机器人将摄像机的图像输入和麦克风接收的语音文字输入 OpenAI 提供的视觉语言大模型（VLM）中，该模型可以理解图像和文字。Figure 机载相机以 10hz 的频率拍摄画面，随后神经网络以 200hz 的频率输出 24 个自由度动作。画面中的人形机器人不依赖远程操作，行为都是学习而得的。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://baijiahao.baidu.com/s?id=1793472537987147749&amp;wfr=spider&amp;for=pc\">与 OpenAI 合作 13 天后，Figure 人形机器人展示与人类对话能力</a><br>\n<strong>官网链接：</strong> /</p>\n<h3><span id=\"13-devinai-软件工程师助手\"> (13) Devin（AI 软件工程师助手）</span></h3>\n<p><em><strong>2024.03.13</strong></em>  一家成立不到两个月但拥有十名天才工程师的初创公司 Cognition 推出了一款名为 Devin 的人工智能（AI）助手，可以协助人类软件工程师完成诸多开发任务。Devin 不同于现有其他 AI 编码者，它可以从零构建网站、自行部署应用、修复漏洞、学习新技术等，人类只需扮演一个下指令和监督的角色。<br>\n　　这是第一个真正意义上完全自主的 AI 软件工程师，一亮相即掀起轩然大波，因为人们担心：人类程序员是不是真要失业了？</p>\n<p><strong>推荐文章：</strong> <a href=\"https://finance.eastmoney.com/a/202403153013211460.html\">人类程序员真要失业？首位 “AI 软件工程师” 亮相引爆科技圈</a><br>\n<strong>官网链接：</strong> /</p>\n<h3><span id=\"14-behavior-1k李飞飞团队-具身智能基准\"> (14) BEHAVIOR-1K（李飞飞团队 — 具身智能基准）</span></h3>\n<p><em><strong>2024.02.27</strong></em>  来自斯坦福、得克萨斯大学奥斯汀分校等大学的研究团队推出了一项以人为本的机器人技术综合模拟基准 ——BEHAVIOR-1K。<br>\n　　BEHAVIOR-1K 包括两个部分，由 “您希望机器人为您做什么？” 这一问题的广泛调查结果指导和推动。第一部分是对 1000 种日常活动的定义，以 50 个场景（房屋、花园、餐厅、办公室等）为基础，其中有 9000 多个标注了丰富物理和语义属性的物体。其次是 OMNIGIBSON，这是一个模拟环境，通过对刚体、可变形体和液体进行逼真的物理模拟和渲染来支持这些活动。<br>\n　　实验表明，BEHAVIOR-1K 中的活动是长视距的，并且依赖于复杂的操作技能，这两点对于最先进的机器人学习解决方案来说仍然是一个挑战。为了校准 BEHAVIOR-1K 的模拟与现实之间的差距，研究团队进行了一项初步研究，将在模拟公寓中使用移动机械手学习到的解决方案转移到现实世界中。<br>\n　　研究团队希望 BEHAVIOR-1K 以人为本的特性、多样性和现实性能使其在具身智能和机器人学习研究中发挥重要作用。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://zhuanlan.zhihu.com/p/669737780\">stanford Behavior-1k—— 包含一千种日常任务的具身智能 benchmark</a><br>\n<strong> 官网链接：</strong> /</p>\n<h3><span id=\"15-mm1-大模型苹果公司多模态大模型\"> (15) MM1 大模型（苹果公司多模态大模型）</span></h3>\n<p><em><strong>2024.03.15</strong></em>  苹果公司最新发布了一款名为 MM1 的大型多模态基础模型，拥有 300 亿参数，采用了 MoE 架构，并且超过一半的作者是华人。<br>\n　　该模型采用了 MoE 变体，并且在预训练指标和多项多模态基准测试上表现出了领先水平。研究者通过多项消融试验，探讨了模型架构、预训练数据选择以及训练程序等方面的重要性。他们发现，图像分辨率、视觉编码器损失和预训练数据在建模设计中都起着关键作用。<br>\n　　MM1 的发布标志着苹果在多模态领域的重要进展，也为未来苹果可能推出的相关产品奠定了技术基础。该研究的成果对于推动生成式人工智能领域的发展具有重要意义，值得业界密切关注。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://www.chinaz.com/2024/0315/1603636.shtml\">苹果大模型 MM1 入场：参数达到 300 亿 超半数作者是华人</a><br>\n<strong>论文地址：</strong> <a href=\"https://arxiv.org/pdf/2403.09611.pdf\">https://arxiv.org/pdf/2403.09611.pdf</a></p>\n<h3><span id=\"16-aesopagent达摩院-智能体驱动的进化系统\"> (16) AesopAgent（达摩院 — 智能体驱动的进化系统）</span></h3>\n<p><em><strong>2024.03.15</strong></em>  阿里达摩院提出了一个关于故事到视频制作的智能体驱动进化系统 ——AesopAgent，它是智能体技术在多模态内容生成方面的实际应用。<br>\n　　该系统在一个统一的框架内集成了多种生成功能，因此个人用户可以轻松利用这些模块。这一创新系统可将用户故事提案转化为脚本、图像和音频，然后将这些多模态内容整合到视频中。此外，动画单元（如 Gen-2 和 Sora）可以使视频更具感染力。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://www.zhihu.com/pin/1751650851838750720\">阿里达摩院提出 AesopAgent：从故事到视频制作，智能体驱动的进化系统</a><br>\n<strong>论文地址：</strong> <a href=\"https://arxiv.org/pdf/2403.07952.pdf\">https://arxiv.org/pdf/2403.07952.pdf</a></p>\n<h3><span id=\"17-cogview3文生图大模型\"> (17) CogView3（文生图大模型）</span></h3>\n<p><em><strong>2024.03.10</strong></em>  文生图系统的最新进展主要是由扩散模型推动的。然而，单级文本到图像扩散模型在计算效率和图像细节细化方面仍面临挑战。为了解决这个问题，来自清华大学和智谱 AI 的研究团队提出了 CogView3—— 一个能提高文本到图像扩散性能的创新级联框架。<br>\n　　据介绍，CogView3 是第一个在文本到图像生成领域实现 relay diffusion 的模型，它通过首先创建低分辨率图像，然后应用基于中继（relay-based）的超分辨率来执行任务。这种方法不仅能产生有竞争力的文本到图像输出，还能大大降低训练和推理成本。<br>\n　　实验结果表明，在人类评估中，CogView3 比目前最先进的开源文本到图像扩散模型 SDXL 高出 77.0%，而所需的推理时间仅为后者的 1/2。经过提炼（distilled）的 CogView3 变体性能与 SDXL 相当，而推理时间仅为后者的 1/10。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://www.zhihu.com/pin/1750532992102223872\">CogView3：更精细、更快速的文生图</a><br>\n<strong>论文地址：</strong> <a href=\"https://arxiv.org/pdf/2403.05121.pdf\">https://arxiv.org/pdf/2403.05121.pdf</a></p>\n<h3><span id=\"18-autodev微软团队全自动-ai-驱动软件开发框架\"> (18) AutoDev（微软团队全自动 AI 驱动软件开发框架）</span></h3>\n<p><em><strong>2024.03.10</strong></em>  微软团队推出了全自动 AI 驱动软件开发框架 AutoDev，该框架专为自主规划和执行复杂的软件工程任务而设计。AutoDev 使用户能够定义复杂的软件工程目标，并将其分配给 AutoDev 的自主 AI 智能体来实现。这些 AI 智能体可以对代码库执行各种操作，包括文件编辑、检索、构建过程、执行、测试和 git 操作。它们还能访问文件、编译器输出、构建和测试日志、静态分析工具等。这使得 AI 智能体能够以完全自动化的方式执行任务并全面了解所需的上下文信息。<br>\n　　此外，AutoDev 还将所有操作限制在 Docker 容器内，建立了一个安全的开发环境。该框架结合了防护栏以确保用户隐私和文件安全，允许用户在 AutoDev 中定义特定的允许或限制命令和操作。<br>\n　　研究团队在 HumanEval 数据集上对 AutoDev 进行了测试，在代码生成和测试生成方面分别取得了 91.5% 和 87.8% 的 Pass@1 好成绩，证明了它在自动执行软件工程任务的同时维护安全和用户控制的开发环境方面的有效性。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://blog.csdn.net/m0_67695717/article/details/135610166\">AutoDev 1.5.3：精准的自动化测试生成、本地模型强化与流程自动化优化</a><br>\n<strong>论文地址：</strong> /</p>\n<h3><span id=\"19-vloggergoogle-图生音频驱动视频方法\"> (19) VLOGGER（Google 图生音频驱动视频方法）</span></h3>\n<p><em><strong>2024.03.14</strong></em>  Google Research 提出了一种从单张人物输入图像生成音频驱动人类视频的方法 ——VLOGGER，它建立在最近成功的生成扩散模型基础之上。<br>\n　　VLOGGER 由两部分组成，一是随机人体到三维运动扩散模型，二是一种基于扩散的新型架构，它通过空间和时间控制来增强文本到图像模型。这有助于生成长度可变的高质量视频，并可通过人脸和身体的高级表示轻松控制。<br>\n　　与之前的工作相比，这一方法不需要对每个人进行训练，不依赖于人脸检测和裁剪，能生成完整的图像（不仅仅是人脸或嘴唇），并能考虑广泛的情况（如可见躯干或不同的主体身份），这对于正确合成交流的人类至关重要。研究团队还提出了一个包含三维姿势和表情注释的全新多样化数据集 MENTOR，它比以前的数据集大一个数量级（800000 identities），并且包含动态手势。研究团队在其上训练并简化了他们的主要技术贡献。<br>\n　　VLOGGER 在三个公共基准测试中的表现达到了 SOTA，考虑到图像质量、身份保留和时间一致性，同时还能生成上半身手势。VLOGGER 在多个多样性指标方面的表现都表明其架构选择和 MENTOR 的使用有利于大规模训练一个公平、无偏见的模型。最后，研究团队还展示了在视频编辑和个性化方面的应用。</p>\n<p><strong>推荐文章：</strong> <a href=\"https://www.zhihu.com/pin/1751650384068771840\">VLOGGER：基于多模态扩散的具身虚拟形象合成</a><br>\n<strong>论文地址：</strong> [<a href=\"https://arxiv.org/pdf/2403.08764.pdf\">https://arxiv.org/pdf/2403.08764.pdf</a>](</p>\n",
            "tags": [
                "人工智能",
                "AIGC前沿"
            ]
        }
    ]
}