<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://leezhao415.github.io</id>
    <title>且听风吟，御剑于心！ • Posts by &#34;aigc前沿&#34; tag</title>
    <link href="https://leezhao415.github.io" />
    <updated>2024-03-15T08:16:00.000Z</updated>
    <category term="人工智能/CV" />
    <category term="Transformer/DETR(CV)" />
    <category term="人工智能" />
    <category term="数据集" />
    <category term="大数据框架" />
    <category term="编程工具" />
    <category term="NLP" />
    <category term="模型部署" />
    <category term="数据结构与算法" />
    <category term="Python数据分析" />
    <category term="网络通信" />
    <category term="YOLOX" />
    <category term="CV算法" />
    <category term="VSLAM" />
    <category term="NCNN部署" />
    <category term="YOLOX目标检测" />
    <category term="多模态" />
    <category term="目标跟踪" />
    <category term="目标检测（人脸检测）" />
    <category term="深度学习" />
    <category term="CV未来" />
    <category term="NLP-BERT" />
    <category term="且读文摘" />
    <category term="自然语言处理NLP" />
    <category term="IOU" />
    <category term="OpenCV之DNN模块" />
    <category term="深度模型" />
    <category term="NLP-模型优化" />
    <category term="激活函数" />
    <category term="梯度更新" />
    <category term="人脸识别" />
    <category term="概述" />
    <category term="名人名言" />
    <category term="寒窑赋" />
    <category term="度量学习" />
    <category term="NLP/评估指标" />
    <category term="机器学习/损失函数" />
    <category term="智能家居" />
    <category term="机器学习" />
    <category term="CV/目标检测工具箱" />
    <category term="科研项目成果" />
    <category term="模型性能指标" />
    <category term="计算机顶会" />
    <category term="表面缺陷检测" />
    <category term="计算机视觉CV" />
    <category term="网络编程" />
    <category term="NLP/数据增强工具" />
    <category term="计算机视觉" />
    <category term="模型优化" />
    <category term="三维建模" />
    <category term="计算机视觉库" />
    <category term="深度学习环境配置" />
    <category term="多任务学习模型" />
    <category term="知识蒸馏" />
    <category term="数据库原理" />
    <category term="算法" />
    <category term="操作系统" />
    <category term="深度模型（目标检测）" />
    <category term="视频理解" />
    <category term="ReID" />
    <category term="MOT" />
    <category term="AIGC前沿" />
    <category term="NLP-发展史" />
    <category term="编程语言" />
    <category term="CV数据集" />
    <category term="Linux" />
    <category term="PaddlePaddle" />
    <entry>
        <id>https://leezhao415.github.io/2024/03/15/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91AIGC%E5%90%AF%E5%85%832024/</id>
        <title>【精华】AIGC启元2024</title>
        <link rel="alternate" href="https://leezhao415.github.io/2024/03/15/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91AIGC%E5%90%AF%E5%85%832024/"/>
        <content type="html">&lt;meta name=&#34;referrer&#34; content=&#34;no-referrer&#34;&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;文章目录&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#aigc-%E5%89%8D%E6%B2%BF&#34;&gt;AIGC 前沿&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#1-gemini-15-pro%E8%B0%B7%E6%AD%8C%E6%96%B0%E4%B8%80%E4%BB%A3%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(1) Gemini 1.5 Pro（谷歌新一代多模态大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-sora%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(2) Sora（文本生成视频大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-emo%E9%98%BF%E9%87%8C%E7%94%9F%E6%88%90%E5%BC%8Fai%E6%A8%A1%E5%9E%8B&#34;&gt;(3) EMO（阿里生成式 AI 模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-playground-v25%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(4) Playground v2.5（文生图大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-vsp-llm%E5%94%87%E8%AF%AD%E8%AF%86%E5%88%AB&#34;&gt;(5) VSP-LLM（唇语识别）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#6-ideogram-ai-%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(6) Ideogram ai  （文生图大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#7-ltx-studio%E7%94%9F%E6%88%90%E5%BC%8Fai%E7%94%B5%E5%BD%B1%E5%88%B6%E4%BD%9C%E5%B9%B3%E5%8F%B0&#34;&gt;(7) LTX studio（生成式 AI 电影制作平台）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#8-claude3llm&#34;&gt;(8) Claude3（LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#9-open-sora%E6%96%87%E7%94%9F%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(9) Open Sora（文生视频大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#10-yi-9bllm&#34;&gt;(10) Yi-9B（LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#11-stable-diffusion-3lvm&#34;&gt;(11) Stable Diffusion 3（LVM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#12-cares-copilot10%E5%A4%9A%E6%A8%A1%E6%80%81%E6%89%8B%E6%9C%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(12) CARES Copilot1.0（多模态手术大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#13-figure-01%E9%80%9A%E7%94%A8%E6%9C%BA%E5%99%A8%E4%BA%BAfigure-ai-openai&#34;&gt;(13) Figure 01 通用机器人（Figure AI + OpenAI）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#14-devinai%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%8A%A9%E6%89%8B&#34;&gt;(14) Devin（AI 软件工程师助手）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#15-behavior-1k%E6%9D%8E%E9%A3%9E%E9%A3%9E%E5%9B%A2%E9%98%9F%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E5%9F%BA%E5%87%86&#34;&gt;(15) BEHAVIOR-1K（李飞飞团队 — 具身智能基准）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#16-mm1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%8B%B9%E6%9E%9C%E5%85%AC%E5%8F%B8%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(16) MM1 大模型（苹果公司多模态大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#17-aesopagent%E8%BE%BE%E6%91%A9%E9%99%A2%E6%99%BA%E8%83%BD%E4%BD%93%E9%A9%B1%E5%8A%A8%E7%9A%84%E8%BF%9B%E5%8C%96%E7%B3%BB%E7%BB%9F&#34;&gt;(17) AesopAgent（达摩院 — 智能体驱动的进化系统）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#18-cogview3%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(18) CogView3（文生图大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#19-autodev%E5%BE%AE%E8%BD%AF%E5%9B%A2%E9%98%9F%E5%85%A8%E8%87%AA%E5%8A%A8-ai-%E9%A9%B1%E5%8A%A8%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6&#34;&gt;(19) AutoDev（微软团队全自动 AI 驱动软件开发框架）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#20-vloggergoogle%E5%9B%BE%E7%94%9F%E9%9F%B3%E9%A2%91%E9%A9%B1%E5%8A%A8%E8%A7%86%E9%A2%91%E6%96%B9%E6%B3%95&#34;&gt;(20) VLOGGER（Google 图生音频驱动视频方法）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#21-textmonkeymonkey%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%96%87%E6%A1%A3%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8&#34;&gt;(21) TextMonkey（Monkey 多模态大模型在文档领域的应用）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#22-open-sora-10%E6%96%87%E7%94%9F%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(22) Open-Sora 1.0（文生视频大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#23-grok-1%E9%A9%AC%E6%96%AF%E5%85%8B%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&#34;&gt;(23) Grok-1（马斯克开源大语言模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#24-blackwell-gb200%E8%8B%B1%E4%BC%9F%E8%BE%BE%E6%96%B0%E4%B8%80%E4%BB%A3ai%E5%8A%A0%E9%80%9F%E5%8D%A1&#34;&gt;(24) Blackwell GB200（英伟达新一代 AI 加速卡）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#25-kimimoonshot-ai-%E6%99%BA%E8%83%BD%E5%8A%A9%E6%89%8B&#34;&gt;(25) Kimi（Moonshot AI 智能助手）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#26-suno-v3%E9%9F%B3%E4%B9%90chatgpt%E6%97%B6%E5%88%BB&#34;&gt;(26) Suno v3（音乐 ChatGPT 时刻）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#27-morasora%E7%9A%84%E9%80%9A%E6%89%8D%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B&#34;&gt;(27) Mora（Sora 的通才视频生成模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#28-mistral-7b-v02&#34;&gt;(28) Mistral 7B v0.2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#29-dbrxllm&#34;&gt;(29) DBRX（LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#30-grok-15llm&#34;&gt;(30) Grok-1.5（LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#31-voice-engineopenai%E9%9F%B3%E9%A2%91%E6%A8%A1%E5%9E%8B&#34;&gt;(31) Voice Engine（OpenAI 音频模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#32-jambamamba-transformer&#34;&gt;(32) Jamba（Mamba + Transformer）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#33-streamingt2v%E6%96%87%E7%94%9F%E8%A7%86%E9%A2%91&#34;&gt;(33) StreamingT2V（文生视频）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#34-safe-longfactdeepmind%E6%A0%B9%E6%B2%BB%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%BB%E8%A7%89%E9%97%AE%E9%A2%98&#34;&gt;(34) SAFE + LongFact（DeepMind 根治大模型幻觉问题）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#35-swe-agentai%E7%A8%8B%E5%BA%8F%E5%91%98&#34;&gt;(35) SWE-agent（AI 程序员）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#36-stable-audio-20%E9%9F%B3%E4%B9%90chatgpt-20&#34;&gt;(36) Stable Audio 2.0（音乐 ChatGPT 2.0）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#37-coig-cqia%E4%B8%AD%E6%96%87%E6%8C%87%E4%BB%A4%E8%B0%83%E4%BC%98%E6%95%B0%E6%8D%AE%E9%9B%86&#34;&gt;(37) COIG-CQIA（中文指令调优数据集）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#38-command-rllm&#34;&gt;(38) Command R+（LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#39-mod%E8%B0%B7%E6%AD%8C%E6%9B%B4%E6%96%B0transformer%E6%9E%B6%E6%9E%84&#34;&gt;(39) MoD（谷歌更新 Transformer 架构）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#40-open-sora-plan%E5%9B%BD%E4%BA%A7sora&#34;&gt;(40) Open-Sora-Plan（国产 Sora）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#41-grok-15v%E9%A9%AC%E6%96%AF%E5%85%8B-%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B&#34;&gt;(41) Grok-1.5V（马斯克 - 多模态模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#42-gpt-4-turbogpt-4%E5%8D%87%E7%BA%A7&#34;&gt;(42) GPT-4 Turbo（GPT-4 升级）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#43-mistral-822bmoe%E5%86%8D%E5%8D%87%E7%BA%A7&#34;&gt;(43) Mistral-8×22B（MoE 再升级）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#44-llama-3meta-llm&#34;&gt;(44) Llama-3（Meta LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#45-megalodonmeta-%E4%B8%8A%E4%B8%8B%E6%96%87%E9%95%BF%E5%BA%A6%E4%B8%8D%E5%8F%97%E9%99%90%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84&#34;&gt;(45) MEGALODON（Meta 上下文长度不受限的神经网络架构）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#46-phi-3-mini%E5%BE%AE%E8%BD%AF-%E6%9C%80%E5%BC%BA%E5%B0%8F%E5%8F%82%E6%95%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(46) Phi-3 Mini（微软 - 最强小参数大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#47-%E4%B8%AD%E6%96%87%E7%89%88llama3&#34;&gt;(47) 中文版 Llama3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#48-qwen15-110b%E5%9B%BD%E4%BA%A7llama3&#34;&gt;(48) Qwen1.5-110B（国产 Llama3）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#49-vidu%E5%9B%BD%E4%BA%A7sora&#34;&gt;(49) Vidu（国产 Sora）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#50-video-mamba-suitemamba%E8%A7%86%E9%A2%91%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8&#34;&gt;(50) Video Mamba Suite（Mamba 视频领域应用）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#51-kan%E5%85%A8%E6%96%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84&#34;&gt;(51) KAN（全新神经网络架构）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#52-meshy-3%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%903d%E6%A8%A1%E5%9E%8B&#34;&gt;(52) Meshy 3（文本生成 3D 模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#53-memgptllm%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86%E6%A1%86%E6%9E%B6&#34;&gt;(53) MemGPT（LLM 记忆管理框架）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#54-vimvision-mambamamba%E9%AB%98%E6%80%A7%E8%83%BD%E8%A7%86%E8%A7%89%E7%89%88&#34;&gt;(54) Vim（Vision Mamba (Mamba 高性能视觉版)）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;span id=&#34;aigc-前沿&#34;&gt; AIGC 前沿&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;span id=&#34;1-gemini-15-pro谷歌新一代多模态大模型&#34;&gt; (1) Gemini 1.5 Pro（谷歌新一代多模态大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.16&lt;/strong&gt;&lt;/em&gt;  谷歌新一代多模态大模型 Gemini 1.5 Pro，在性能上超越 OpenAI 的 GPT-4 Turbo，堪称业界最强大模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzU5OTI0NTc3Mg==&amp;amp;mid=2247530889&amp;amp;idx=1&amp;amp;sn=0686f28b493fb61fdcd3e619b9a97517&#34;&gt;“打假” Sora，谷歌 Gemini 1.5 Pro 第一波评测出炉｜甲子光年&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://openai.com/sora&#34;&gt;https://openai.com/sora&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;2-sora文本生成视频大模型&#34;&gt; (2) Sora（文本生成视频大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.16&lt;/strong&gt;&lt;/em&gt;  Sora 文本生成视频的大模型。它所展现出来的能力几乎可以 “碾压” 目前全球能实现文本生成视频的大模型 包 括 Runway、Pika、Stable Video Diffusion 等 20 多个产品。&lt;br&gt;
　　用户仅需输入简短一句话，Sora 就可生成一段长达 60 秒的视频，远远超过市面上同类型级别的 AI 视频生成时长。在此之前，AI 视频模型生成时长几乎在 10 秒以内，而 “明星模型” Runway 和 Pika 等也仅有 3 到 4 秒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.thepaper.cn/newsDetail_forward_26415514&#34;&gt;Sora 到底有多强？&lt;/a&gt; |  &lt;a href=&#34;https://mp.weixin.qq.com/s/zmKeNWrU6me3leNpdNH2wQ&#34;&gt;微软最新 Sora 综述&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://ai.google.dev/gemma&#34;&gt;Gemma Open Models&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;3-emo阿里生成式-ai-模型&#34;&gt; (3) EMO（阿里生成式 AI 模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.28&lt;/strong&gt;&lt;/em&gt;  生成式 AI 模型 EMO（Emote Portrait Alive）。EMO 仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容 “张嘴” 唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1792189739510241856&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;阿里 EMO 模型，一张照片就能造谣&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://humanaigc.github.io/emote-portrait-alive/&#34;&gt;https://humanaigc.github.io/emote-portrait-alive/&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;4-playground-v25文生图大模型&#34;&gt; (4) Playground v2.5（文生图大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.28&lt;/strong&gt;&lt;/em&gt;  Playground 在去年发布 Playground v2.0 之后再次开源新的文生图模型 Playground v2.5。相比上一个版本，Playground v2.5 在美学质量，颜色和对比度，多尺度生成以及以人为中心的细节处理有比较大的提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/684287454&#34;&gt;超过 Midjourney v5.2 的开源文生图大模型 Playground v2.5 来了&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://playground.com/&#34;&gt;https://playground.com/&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;5-vsp-llm唇语识别&#34;&gt; (5) VSP-LLM（唇语识别）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.28&lt;/strong&gt;&lt;/em&gt;  一种通过观察视频中人的嘴型来理解和翻译说话内容的技术，也就是识别唇语。该技术能够将视频中的唇动转化为文本（视觉语音识别），并将这些唇动直接翻译成目标语言的文本 (视觉语音翻译)。不仅如此，VSP-LLM 还能智能识别和去除视频中不必要的重复信息，使处理过程更加快速和准确。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.chinaz.com/2024/0228/1599901.shtml&#34;&gt;VSP-LLM：可通过观察视频中人的嘴型来识别唇语&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://github.com/sally-sh/vsp-llm&#34;&gt;https://github.com/sally-sh/vsp-llm&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;6-ideogram-ai-文生图大模型&#34;&gt; (6) Ideogram ai  （文生图大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.29&lt;/strong&gt;&lt;/em&gt;  Ideogram 发布了最新的 Ideogram1.0 图像生成模型，该模型具有强大的文字生成能力和提示词理解能力。Ideogram1.0 在文本渲染准确性方面实现了飞跃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt;&lt;a href=&#34;https://www.chinaz.com/2024/0229/1599986.shtml&#34;&gt;Ideogram 1.0 图像生成模型发布 文字生成能力更强大了&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt;&lt;a href=&#34;https://top.aibase.com/tool/ideogram-ai&#34;&gt;https://top.aibase.com/tool/ideogram-ai&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;7-ltx-studio生成式-ai-电影制作平台&#34;&gt; (7) LTX studio（生成式 AI 电影制作平台）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.29&lt;/strong&gt;&lt;/em&gt;  生成式 AI 电影制作平台 —LTX Studio，用户只需要输入文本就能生成超 25 秒的微电影视频，同时可对镜头切换、角色、场景一致性、摄像机、灯光等进行可视化精准控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://new.qq.com/rain/a/20240229A02EA500&#34;&gt;效果比 Sora 惊艳，著名 AI 平台大动作！文本生成超 25 秒视频，带背景音乐、转场等效果&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://ltx.studio&#34;&gt;https://ltx.studio&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;8-claude3llm&#34;&gt; (8) Claude3（LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.04&lt;/strong&gt;&lt;/em&gt;  Claude3 是由 Anthropic 发布的最新的 AI 大模型系列，同时，Claude3 是多模态大模型 ，具有强大的 “视觉能力”。Claude3 Opus 已经在部分行业行为准则中的表现优于 OpenAI 的 GPT-4 和谷歌的 Gemini Ultra，如本科生水平知识（MMLU）、研究生级别专家推理（GPQA）和基础数学（GSM8K）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://new.qq.com/rain/a/20240307A09O1N00&#34;&gt;OpenAI 劲敌出现！Claude3 正式发布，超越 GTP-4?&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 官网链接：&lt;/strong&gt; &lt;a href=&#34;https://www.anthropic.com/claude&#34;&gt;https://www.anthropic.com/claude&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;9-open-sora文生视频大模型&#34;&gt; (9) Open Sora（文生视频大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.01&lt;/strong&gt;&lt;/em&gt;  北大团队联合兔展发起了一项 Sora 复现计划 ——Open Sora&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1792479658318662669&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;北大与兔展智能发起复现 Sora，框架已开源&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt;&lt;br&gt;
&lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html&#34;&gt;https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;https://github.com/PKU-YuanGroup/Open-Sora-Plan&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;10-yi-9bllm&#34;&gt; (10) Yi-9B（LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.06&lt;/strong&gt;&lt;/em&gt;  李开复旗下 AI 公司零一万物的最新力作 ——Yi-9B 大模型正式对外开源发布。这款具有 90 亿参数的大模型，在代码和数学能力上达到了前所未有的高度，同时保持了对消费级显卡的良好兼容性，为广大开发者和研究人员提供了前所未有的便利性和强大功能。&lt;br&gt;
　　Yi-9B 作为 Yi 系列中的新成员，被誉为 “理科状元”，特别加强了在代码和数学方面的学习能力。相较于市场上其他类似规模的开源模型，如 Mistral-7B、SOLAR-10.7B、Gemma-7B 等，Yi-9B 展现出了最佳的性能表现。特别值得一提的是，Yi-9B 既提供了浮点数版本（BF 16），也提供了整数版本（Int8），使其能够轻松部署在包括 RTX 4090 和 RTX 3090 在内的消费级显卡上，大大降低了使用门槛和成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1792881393208541121&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;零一万物开源 Yi-9B 大模型，消费级显卡可用，代码数学历史最强&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://github.com/01-ai/Yi&#34;&gt;https://github.com/01-ai/Yi&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;11-stable-diffusion-3lvm&#34;&gt; (11) Stable Diffusion 3（LVM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.06&lt;/strong&gt;&lt;/em&gt;  Stable Diffusion 3 采用了与 Sora 相同的 DiT（Diffusion Transformer）架构，一经发布就引起了不小的轰动。与之前的版本相比，Stable Diffusion 3 生成的图在质量上实现了很大改进，支持多主题提示，文字书写效果也更好了（明显不再乱码）。&lt;br&gt;
　　Stability AI 表示，Stable Diffusion 3 是一个模型系列，参数量从 800M 到 8B 不等。这个参数量意味着，它可以在很多便携式设备上直接跑，大大降低了 AI 大模型的使用门槛。&lt;br&gt;
　　在最新发布的论文中，Stability AI 表示，在基于人类偏好的评估中，Stable Diffusion 3 优于当前最先进的文本到图像生成系统，如 DALL・E 3、Midjourney v6 和 Ideogram v1。不久之后，他们将公开该研究的实验数据、代码和模型权重。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/mH6IzExPPBpX8YTwxlP6dA?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;Stable Diffusion 3 论文终于发布，架构细节大揭秘，对复现 Sora 有帮助？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/TF1-xPN78sxwCufFLCT9jQ?version=4.1.22.6014&amp;amp;platform=win&#34;&gt;突发！Stable Diffusion 3，可通过 API 使用啦&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;论文链接：&lt;/strong&gt; &lt;a href=&#34;https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf&#34;&gt;https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;12-cares-copilot10多模态手术大模型&#34;&gt; (12) CARES Copilot1.0（多模态手术大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.11&lt;/strong&gt;&lt;/em&gt;  CARES Copilot 是由中国科学院香港创新院 AI 中心研发的一个可信赖、可解释、面向医疗垂直领域并能与智能医疗设备高度集成的大模型系统。CARES Copilot 1.0 实现了图像、文本、语音、视频、MRI、CT、超声等多模态的手术数据理解。支持超过 100K 上下文的长窗口理解和高效分析，能理解超过 3000 页的复杂手术教材，对于年轻医生的培训和教学具有极高的实用价值。此外，该系统能通过深度检索功能，快速精确地提取手术教材、专家指南、医学论文等专业文档的信息，确保其提供的答案具有高度的可信度和可追溯性。经测试，系统能在一秒钟内完成百万级数据的快速检索，同时保持 95% 的准确率。该系统已在多家医院的不同科室进行了内部测试和迭代优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793375529838669070&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;CARES Copilot 1.0 多模态手术大模型发布，可实现轻量化部署&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;13-figure-01-通用机器人figure-ai-openai&#34;&gt; (13) Figure 01 通用机器人（Figure AI + OpenAI）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.13&lt;/strong&gt;&lt;/em&gt;  Figure 01 通用机器人由 Figure AI 和 OpenAI 合作完成。展示视频中，Figure AI 人形机器人具有视觉能力并能表述所见画面，它伸手拿起桌上的苹果，并解释了这么做的原因，人类的提问后，这台人形机器人 “思索” 2~3 秒后便能顺畅作答，手部动作速度则接近人类。据视频介绍，机器人采用了端到端神经网络。&lt;br&gt;
　　该人形机器人由 OpenAI 提供了视觉推理和语言理解，Figure AI 的神经网络则提供快速、灵巧的机器人动作。人形机器人将摄像机的图像输入和麦克风接收的语音文字输入 OpenAI 提供的视觉语言大模型（VLM）中，该模型可以理解图像和文字。Figure 机载相机以 10hz 的频率拍摄画面，随后神经网络以 200hz 的频率输出 24 个自由度动作。画面中的人形机器人不依赖远程操作，行为都是学习而得的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793472537987147749&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;与 OpenAI 合作 13 天后，Figure 人形机器人展示与人类对话能力&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;14-devinai-软件工程师助手&#34;&gt; (14) Devin（AI 软件工程师助手）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.13&lt;/strong&gt;&lt;/em&gt;  一家成立不到两个月但拥有十名天才工程师的初创公司 Cognition 推出了一款名为 Devin 的人工智能（AI）助手，可以协助人类软件工程师完成诸多开发任务。Devin 不同于现有其他 AI 编码者，它可以从零构建网站、自行部署应用、修复漏洞、学习新技术等，人类只需扮演一个下指令和监督的角色。&lt;br&gt;
　　这是第一个真正意义上完全自主的 AI 软件工程师，一亮相即掀起轩然大波，因为人们担心：人类程序员是不是真要失业了？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://finance.eastmoney.com/a/202403153013211460.html&#34;&gt;人类程序员真要失业？首位 “AI 软件工程师” 亮相引爆科技圈&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;15-behavior-1k李飞飞团队-具身智能基准&#34;&gt; (15) BEHAVIOR-1K（李飞飞团队 — 具身智能基准）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.27&lt;/strong&gt;&lt;/em&gt;  来自斯坦福、得克萨斯大学奥斯汀分校等大学的研究团队推出了一项以人为本的机器人技术综合模拟基准 ——BEHAVIOR-1K。&lt;br&gt;
　　BEHAVIOR-1K 包括两个部分，由 “您希望机器人为您做什么？” 这一问题的广泛调查结果指导和推动。第一部分是对 1000 种日常活动的定义，以 50 个场景（房屋、花园、餐厅、办公室等）为基础，其中有 9000 多个标注了丰富物理和语义属性的物体。其次是 OMNIGIBSON，这是一个模拟环境，通过对刚体、可变形体和液体进行逼真的物理模拟和渲染来支持这些活动。&lt;br&gt;
　　实验表明，BEHAVIOR-1K 中的活动是长视距的，并且依赖于复杂的操作技能，这两点对于最先进的机器人学习解决方案来说仍然是一个挑战。为了校准 BEHAVIOR-1K 的模拟与现实之间的差距，研究团队进行了一项初步研究，将在模拟公寓中使用移动机械手学习到的解决方案转移到现实世界中。&lt;br&gt;
　　研究团队希望 BEHAVIOR-1K 以人为本的特性、多样性和现实性能使其在具身智能和机器人学习研究中发挥重要作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/669737780&#34;&gt;stanford Behavior-1k—— 包含一千种日常任务的具身智能 benchmark&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 官网链接：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;16-mm1-大模型苹果公司多模态大模型&#34;&gt; (16) MM1 大模型（苹果公司多模态大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.15&lt;/strong&gt;&lt;/em&gt;  苹果公司最新发布了一款名为 MM1 的大型多模态基础模型，拥有 300 亿参数，采用了 MoE 架构，并且超过一半的作者是华人。&lt;br&gt;
　　该模型采用了 MoE 变体，并且在预训练指标和多项多模态基准测试上表现出了领先水平。研究者通过多项消融试验，探讨了模型架构、预训练数据选择以及训练程序等方面的重要性。他们发现，图像分辨率、视觉编码器损失和预训练数据在建模设计中都起着关键作用。&lt;br&gt;
　　MM1 的发布标志着苹果在多模态领域的重要进展，也为未来苹果可能推出的相关产品奠定了技术基础。该研究的成果对于推动生成式人工智能领域的发展具有重要意义，值得业界密切关注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.chinaz.com/2024/0315/1603636.shtml&#34;&gt;苹果大模型 MM1 入场：参数达到 300 亿 超半数作者是华人&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.09611.pdf&#34;&gt;https://arxiv.org/pdf/2403.09611.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;17-aesopagent达摩院-智能体驱动的进化系统&#34;&gt; (17) AesopAgent（达摩院 — 智能体驱动的进化系统）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.15&lt;/strong&gt;&lt;/em&gt;  阿里达摩院提出了一个关于故事到视频制作的智能体驱动进化系统 ——AesopAgent，它是智能体技术在多模态内容生成方面的实际应用。&lt;br&gt;
　　该系统在一个统一的框架内集成了多种生成功能，因此个人用户可以轻松利用这些模块。这一创新系统可将用户故事提案转化为脚本、图像和音频，然后将这些多模态内容整合到视频中。此外，动画单元（如 Gen-2 和 Sora）可以使视频更具感染力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.zhihu.com/pin/1751650851838750720&#34;&gt;阿里达摩院提出 AesopAgent：从故事到视频制作，智能体驱动的进化系统&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.07952.pdf&#34;&gt;https://arxiv.org/pdf/2403.07952.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;18-cogview3文生图大模型&#34;&gt; (18) CogView3（文生图大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.10&lt;/strong&gt;&lt;/em&gt;  文生图系统的最新进展主要是由扩散模型推动的。然而，单级文本到图像扩散模型在计算效率和图像细节细化方面仍面临挑战。为了解决这个问题，来自清华大学和智谱 AI 的研究团队提出了 CogView3—— 一个能提高文本到图像扩散性能的创新级联框架。&lt;br&gt;
　　据介绍，CogView3 是第一个在文本到图像生成领域实现 relay diffusion 的模型，它通过首先创建低分辨率图像，然后应用基于中继（relay-based）的超分辨率来执行任务。这种方法不仅能产生有竞争力的文本到图像输出，还能大大降低训练和推理成本。&lt;br&gt;
　　实验结果表明，在人类评估中，CogView3 比目前最先进的开源文本到图像扩散模型 SDXL 高出 77.0%，而所需的推理时间仅为后者的 1/2。经过提炼（distilled）的 CogView3 变体性能与 SDXL 相当，而推理时间仅为后者的 1/10。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.zhihu.com/pin/1750532992102223872&#34;&gt;CogView3：更精细、更快速的文生图&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.05121.pdf&#34;&gt;https://arxiv.org/pdf/2403.05121.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;19-autodev微软团队全自动-ai-驱动软件开发框架&#34;&gt; (19) AutoDev（微软团队全自动 AI 驱动软件开发框架）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.10&lt;/strong&gt;&lt;/em&gt;  微软团队推出了全自动 AI 驱动软件开发框架 AutoDev，该框架专为自主规划和执行复杂的软件工程任务而设计。AutoDev 使用户能够定义复杂的软件工程目标，并将其分配给 AutoDev 的自主 AI 智能体来实现。这些 AI 智能体可以对代码库执行各种操作，包括文件编辑、检索、构建过程、执行、测试和 git 操作。它们还能访问文件、编译器输出、构建和测试日志、静态分析工具等。这使得 AI 智能体能够以完全自动化的方式执行任务并全面了解所需的上下文信息。&lt;br&gt;
　　此外，AutoDev 还将所有操作限制在 Docker 容器内，建立了一个安全的开发环境。该框架结合了防护栏以确保用户隐私和文件安全，允许用户在 AutoDev 中定义特定的允许或限制命令和操作。&lt;br&gt;
　　研究团队在 HumanEval 数据集上对 AutoDev 进行了测试，在代码生成和测试生成方面分别取得了 91.5% 和 87.8% 的 Pass@1 好成绩，证明了它在自动执行软件工程任务的同时维护安全和用户控制的开发环境方面的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://blog.csdn.net/m0_67695717/article/details/135610166&#34;&gt;AutoDev 1.5.3：精准的自动化测试生成、本地模型强化与流程自动化优化&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;20-vloggergoogle-图生音频驱动视频方法&#34;&gt; (20) VLOGGER（Google 图生音频驱动视频方法）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.14&lt;/strong&gt;&lt;/em&gt;  Google Research 提出了一种从单张人物输入图像生成音频驱动人类视频的方法 ——VLOGGER，它建立在最近成功的生成扩散模型基础之上。&lt;br&gt;
　　VLOGGER 由两部分组成，一是随机人体到三维运动扩散模型，二是一种基于扩散的新型架构，它通过空间和时间控制来增强文本到图像模型。这有助于生成长度可变的高质量视频，并可通过人脸和身体的高级表示轻松控制。&lt;br&gt;
　　与之前的工作相比，这一方法不需要对每个人进行训练，不依赖于人脸检测和裁剪，能生成完整的图像（不仅仅是人脸或嘴唇），并能考虑广泛的情况（如可见躯干或不同的主体身份），这对于正确合成交流的人类至关重要。研究团队还提出了一个包含三维姿势和表情注释的全新多样化数据集 MENTOR，它比以前的数据集大一个数量级（800000 identities），并且包含动态手势。研究团队在其上训练并简化了他们的主要技术贡献。&lt;br&gt;
　　VLOGGER 在三个公共基准测试中的表现达到了 SOTA，考虑到图像质量、身份保留和时间一致性，同时还能生成上半身手势。VLOGGER 在多个多样性指标方面的表现都表明其架构选择和 MENTOR 的使用有利于大规模训练一个公平、无偏见的模型。最后，研究团队还展示了在视频编辑和个性化方面的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.zhihu.com/pin/1751650384068771840&#34;&gt;VLOGGER：基于多模态扩散的具身虚拟形象合成&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.08764.pdf&#34;&gt;https://arxiv.org/pdf/2403.08764.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;21-textmonkeymonkey-多模态大模型在文档领域的应用&#34;&gt; (21) TextMonkey（Monkey 多模态大模型在文档领域的应用）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.15&lt;/strong&gt;&lt;/em&gt;  TextMonkey 是 Monkey 在文档领域的重要升级，突破了通用文档理解能力的边界，在场景文字识别、办公文档摘要生成、数学问题问答、文档版式分析，表格理解，图表问答，电子文档关键信息抽取等 12 项等文档权威数据集以及在国际上规模最全的文档图像智能数据集 OCRBench 上取得了显著突破，通用文档理解性能大幅超越现有方法。&lt;br&gt;
　　TextMonkey 能帮助我们结构化图表、表格以及文档数据，通过将图像内容转化为轻量级的数据交换格式，方便记录和提取。TextMonkey 也能作为智能手机代理，无需接触后端，仅需语音输入及屏幕截图，即能够模仿人类的点击手势，能够在手机上执行各种任务，自主操控手机应用程序。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt;&lt;br&gt;
&lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793666944109402323&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;华科大研发多模态大模型 “猴子” 升级&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://zhuanlan.zhihu.com/p/685937695&#34;&gt; [全网首发中文版] TextMonkey: An OCRFree Large Multimodal Model for Understanding Document&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;GitHub 仓库地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/Yuliang-Liu/Monkey&#34;&gt;https://github.com/Yuliang-Liu/Monkey&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2311.06607.pdf&#34;&gt;https://arxiv.org/pdf/2311.06607.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;22-open-sora-10文生视频大模型&#34;&gt; (22) Open-Sora 1.0（文生视频大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.17&lt;/strong&gt;&lt;/em&gt;  Colossal-AI 团队全面开源全球首个类 Sora 架构视频生成模型 「Open-Sora 1.0」，涵盖了整个训练流程，包括数据处理、所有训练细节和模型权重，携手全球 AI 热爱者共同推进视频创作的新纪元。&lt;br&gt;
　　Colossal-AI 团队深入解读 Sora 复现方案的多个关键维度，包括模型架构设计、训练复现方案、数据预处理、模型生成效果展示以及高效训练优化策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793833243872165307&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;没等来 OpenAI，等来了 Open-Sora 全面开源&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; GitHub 仓库地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/hpcaitech/Open-Sora&#34;&gt;https://github.com/hpcaitech/Open-Sora&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;23-grok-1马斯克开源大语言模型&#34;&gt; (23) Grok-1（马斯克开源大语言模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.17&lt;/strong&gt;&lt;/em&gt;  马斯克宣布开源 Grok-1，这使得 Grok-1 成为当前参数量最大的开源大语言模型，拥有 3140 亿参数，远超 OpenAI GPT-3.5 的 1750 亿。有意思的是，Grok-1 宣布开源的封面图为 Midjourney 生成，可谓 “AI helps AI”。&lt;br&gt;
　　Grok-1 是一个规模较大（314B 参数）的模型，需要有足够 GPU 内存的机器才能使用示例代码测试模型。网友表示这可能需要一台拥有 628 GB GPU 内存的机器。此外，该存储库中 MoE 层的实现效率并不高，之所以选择该实现是为了避免需要自定义内核来验证模型的正确性。&lt;br&gt;
　　目前已开源的热门大模型包括 Meta 的 Llama2、法国的 Mistral 等。通常来说，发布开源模型有助于社区展开大规模的测试和反馈，意味着模型本身的迭代速度也能加快。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793830961836866098&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;马斯克用行动反击 开源自家顶级大模型 压力给到 OpenAI&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;GitHub 仓库地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/xai-org/grok-1&#34;&gt;https://github.com/xai-org/grok-1&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 官方博客：&lt;/strong&gt; &lt;a href=&#34;https://x.ai/blog/grok-os&#34;&gt;https://x.ai/blog/grok-os&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 模型磁力链接：&lt;/strong&gt; &lt;a href=&#34;https://academictorrents.com/details/5f96d43576e3d386c9ba65b883210a393b68210e&#34;&gt;https://academictorrents.com/details/5f96d43576e3d386c9ba65b883210a393b68210e&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;24-blackwell-gb200英伟达新一代-ai-加速卡&#34;&gt; (24) Blackwell GB200（英伟达新一代 AI 加速卡）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.18&lt;/strong&gt;&lt;/em&gt;  英伟达公司于 2024 年的 GTC 大会上宣布了下一代人工智能超级计算机的问世，同时推出了备受业界瞩目的 AI 加速卡 ——Blackwell GB200。这款加速卡的发布，标志着人工智能领域又迈出了坚实的一步，其强大的性能、成本及能耗的突破，预计将引领 AI 技术的全新发展。&lt;br&gt;
　　Blackwell GB200 采用了英伟达新一代 AI 图形处理器架构 Blackwell，相较于前一代 Hopper 架构，其性能实现了巨大的飞跃。GB200 由两个 B200 Blackwell GPU 和一个基于 Arm 的 Grace CPU 组成，这种独特的组合使得其在处理大语言模型推理任务时，性能比 H100 提升高达 30 倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793911137057474891&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;性能飙升 30 倍，能耗骤降 25 倍！英伟达发布 Blackwell GB200！&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;25-kimimoonshot-ai-智能助手&#34;&gt; (25) Kimi（Moonshot AI 智能助手）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.18&lt;/strong&gt;&lt;/em&gt;  国内 AI 创业公司月之暗面（Moonshot AI）宣布在大模型长上下文窗口技术上取得新的突破，Kimi 智能助手已支持 200 万字超长无损上下文，短短五个月内 “长文本” 输入量提升 10 倍，并于即日起开启产品 “内测”。&lt;br&gt;
　　月之暗面创始人杨植麟博士表示，通往通用人工智能（AGI）的话，无损的长上下文将会是一个很关键的基础技术。历史上所有的模型架构演进，本质上都是在提升有效的、无损的上下文长度。上下文长度可能存在摩尔定律，但需要同时优化长度和无损压缩水平两个指标，才是有意义的规模化。&lt;br&gt;
　　月之暗面联合创始人 周昕宇则向钛媒体 App 透露，月之暗面即将在今年内推出自研的多模态大模型。同时，商业化也在快速推进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793878336231855457&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;对话月之暗面：Kimi 智能助手支持 200 万字无损输入，年内将发布多模态模型｜钛媒体 AGI&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;26-suno-v3音乐-chatgpt-时刻&#34;&gt; (26) Suno v3（音乐 ChatGPT 时刻）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.24&lt;/strong&gt;&lt;/em&gt;  AI 初创公司 Suno AI 重磅推出了第一款可制作「广播级」的音乐生成模型 ——V3，一时间在网上掀起轩然大波。仅用几秒的时间，V3 便可以创作出 2 分钟的完整歌曲。为了激发人们的创作灵感，Suno v3 还新增了更丰富的音乐风格和流派选项，比如古典音乐、爵士乐、Hiphop、电子等新潮曲风。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/BRzmlw-uE2C6ROF2O2_-nw&#34;&gt;音乐 ChatGPT 时刻来临！Suno V3 秒生爆款歌曲，12 人团队创现象级 AI&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;27-morasora-的通才视频生成模型&#34;&gt; (27) Mora（Sora 的通才视频生成模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.24&lt;/strong&gt;&lt;/em&gt;  理海大学联手微软团队一种新型的多 AI 智能体框架 ———Mora。Mora 更像是 Sora 的通才视频生成。通过整合多个 SOTA 的视觉 AI 智能体，来复现 Sora 展示的通用视频生成能力。具体来说，Mora 能够利用多个视觉智能体，在多种任务中成功模拟 Sora 的视频生成能力，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本到视频生成&lt;/li&gt;
&lt;li&gt;基于文本条件的图像到视频生成&lt;/li&gt;
&lt;li&gt;扩展已生成视频&lt;/li&gt;
&lt;li&gt;视频到视频编辑&lt;/li&gt;
&lt;li&gt;拼接视频&lt;/li&gt;
&lt;li&gt;模拟数字世界&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/GkJwyVFVwxih-ZQWWBIuNg&#34;&gt;Sora 不开源，微软给你开源！全球最接近 Sora 视频模型诞生，12 秒生成效果逼真炸裂&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.13248&#34;&gt;https://arxiv.org/abs/2403.13248&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;28-mistral-7b-v02&#34;&gt; (28) Mistral 7B v0.2&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.24&lt;/strong&gt;&lt;/em&gt;  这次开源的 Mistral 7B v0.2 Base Model ，是 Mistral-7B-Instruct-v0.2 背后的原始预训练模型，后者属于该公司的「Mistral Tiny」系列。&lt;br&gt;
此次更新主要包括三个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将 8K 上下文提到了 32K；&lt;/li&gt;
&lt;li&gt;Rope Theta = 1e6；&lt;/li&gt;
&lt;li&gt;取消滑动窗口。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/R56Ob5dZjMh1alhMin8DZw&#34;&gt;32K 上下文，Mistral 7B v0.2 基模型突然开源了&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;29-dbrxllm&#34;&gt; (29) DBRX（LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.28&lt;/strong&gt;&lt;/em&gt;  超级独角兽 Databricks 重磅推出 1320 亿参数的开源模型 ——DBRX。全球最强开源大模型王座易主，超越了 Llama 2、Mixtral 和 Grok-1。MoE 又立大功！这个过程只用了 2 个月，1000 万美元，和 3100 块 H100。采用了细粒度 MoE 架构，而且每次输入仅使用 360 亿参数，实现了更快的每秒 token 吞吐量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/88zvF3vwtTJcGl__HR6hBg?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;全球最强开源模型一夜易主，1320 亿参数推理飙升 2 倍！&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;项目地址：&lt;/strong&gt;&lt;br&gt;
&lt;a href=&#34;https://github.com/databricks/dbrx&#34;&gt;https://github.com/databricks/dbrx&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://huggingface.co/databricks&#34;&gt;https://huggingface.co/databricks&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;30-grok-15llm&#34;&gt; (30) Grok-1.5（LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.29&lt;/strong&gt;&lt;/em&gt;  马斯克发布 Grok-1.5，强化推理和上下文，HumanEval 得分超 GPT-4 继开源 Grok-1 后，xAI 刚刚官方发布了他们的最新模型 Grok-1.5。据介绍，Grok-1.5 能够进行长语境理解和高级推理，并将于近日在 xAI 平台上向早期测试者和现有 Grok 用户开放。&lt;br&gt;
Grok-1.5 最显著的改进之一是其在编码和数学相关任务中的表现。在给出的测试结果中，Grok-1.5 在 MATH 基准测试中取得了 50.6% 的得分，在 GSM8K 基准测试中取得了 90% 的得分。此外，在评估代码生成和解决问题能力的 HumanEval 基准测试中，Grok-1.5 获得了 74.1% 的高分，超过了 GPT-4。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/RJ_KBxX1eNzMOGiGQ1iPCg?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;马斯克发布 Grok-1.5，强化推理和上下文，HumanEval 得分超 GPT-4&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 项目主页：&lt;/strong&gt; &lt;a href=&#34;https://x.ai/blog/grok-1.5&#34;&gt;https://x.ai/blog/grok-1.5&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;31-voice-engineopenai-音频模型&#34;&gt; (31) Voice Engine（OpenAI 音频模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.30&lt;/strong&gt;&lt;/em&gt;  OpenAI 在官网首次展示了全新自定义音频模型 “Voice Engine”。用户只需要提供 15 秒左右的参考声音，通过 Voice Engine 就能生成几乎和原音一模一样的全新音频，在清晰度、语音连贯、音色、自然度等方面比市面上多数产品都强很多。&lt;br&gt;
除了能合成音频之外，OpenAI 还展示了 Voice Engine 很多其他际商业用途，例如，一位失去声音表达能力的女孩，在 Voice Engine 帮助下能像以前一样正常发音说话。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/ErMhYBEjjDMpJfPlj9NiIw?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;OpenAI 首次展示音频模型 Voice Engine，生成的声音太逼真了！&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;32-jambamamba-transformer&#34;&gt; (32) Jamba（Mamba + Transformer）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.29&lt;/strong&gt;&lt;/em&gt;  AI21 Labs 推出并开源了一种名为「Jamba」的新方法，在多个基准上超越了 transformer。&lt;br&gt;
　　Mamba 的 SSM 架构可以很好地解决 transformer 的内存资源和上下文问题。然而，Mamba 方法很难提供与 transformer 模型相同的输出水平。&lt;br&gt;
　　Jamba 将基于结构化状态空间模型 (SSM) 的 Mamba 模型与 transformer 架构相结合，旨在将 SSM 和 transformer 的最佳属性结合在一起。&lt;br&gt;
　　Jamba 模型具有以下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一个基于 Mamba 的生产级模型，采用新颖的 SSM-Transformer 混合架构；&lt;/li&gt;
&lt;li&gt;与 Mixtral 8x7B 相比，长上下文上的吞吐量提高了 3 倍；&lt;/li&gt;
&lt;li&gt;提供对 256K 上下文窗口的访问；&lt;/li&gt;
&lt;li&gt;公开了模型权重；&lt;/li&gt;
&lt;li&gt;同等参数规模中唯一能够在单个 GPU 上容纳高达 140K 上下文的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/DPga3zigenLkPG587QCxbQ?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;打败 Transformer，Mamba 时代来了？&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;33-streamingt2v文生视频&#34;&gt; (33) StreamingT2V（文生视频）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.01&lt;/strong&gt;&lt;/em&gt;  Picsart 人工智能研究所、德克萨斯大学和 SHI 实验室的研究人员联合推出了 StreamingT2V 视频模型。通过文本就能直接生成 2 分钟、1 分钟等不同时间，动作一致、连贯、没有卡顿的高质量视频。&lt;br&gt;
　　虽然 StreamingT2V 在视频质量、多元化等还无法与 Sora 媲美，但在高速运动方面非常优秀，这为开发长视频模型提供了技术思路。&lt;br&gt;
　　研究人员表示，理论上，StreamingT2V 可以无限扩展视频的长度，并正在准备开源该视频模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/CXPpQaPL5wiog5C0U1MXxQ?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;文本直接生成 2 分钟视频，即将开源模型 StreamingT2V&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.14773&#34;&gt;https://arxiv.org/abs/2403.14773&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Github 地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/Picsart-AI-Research/StreamingT2V&#34;&gt;https://github.com/Picsart-AI-Research/StreamingT2V&lt;/a&gt;（即将开源）&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;34-safe-longfactdeepmind-根治大模型幻觉问题&#34;&gt; (34) SAFE + LongFact（DeepMind 根治大模型幻觉问题）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.03&lt;/strong&gt;&lt;/em&gt;  Google DeepMind 的人工智能专家团队和斯坦福大学的研究者发布了一篇名为《衡量大型语言模型长篇事实性》（Long-form factuality in large language models）的研究论文，研究者们对长篇事实性问题进行了深度探究，并对语言模型在长篇事实性上的表现进行了全面评估。&lt;br&gt;
　　他们推出了一套新的数据集 ——LongFact，其中包含了 2,280 个涵盖 38 个不同话题的引导问题；同时，提出了一个新颖的评估方法 ——SAFE（Self-contained Accuracy with Google Evidence），该方法运用语言模型代理人和 Google 搜索查询技术来进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/jjzmO6vN3sDLYYLH2YMTzg?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;人类标注的时代已经结束？DeepMind 开源 SAFE 根治大模型幻觉问题&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.18802&#34;&gt;https://arxiv.org/abs/2403.18802&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Github 地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/google-deepmind/long-form-factuality&#34;&gt;https://github.com/google-deepmind/long-form-factuality&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;35-swe-agentai-程序员&#34;&gt; (35) SWE-agent（AI 程序员）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.03&lt;/strong&gt;&lt;/em&gt;  自从 “AI 程序员” Devin 问世之后，近期的一大趋势就是程序员们争先恐后地要让自己失业，试图抢先造出比自己更强大的程序员。&lt;br&gt;
　　普林斯顿大学为软件工程界迎来了一位新星 ——SWE-agent，论文将在 4 月 10 日正式发布，目前项目已在 GitHub 上开源。&lt;br&gt;
　　SWE-agent 的特点就是将 GPT-4 这样的大型语言模型（LLMs）转化为软件工程代理，使其能够修复真实 GitHub 仓库中的错误和问题。SWE-agent 在软件工程基准测试中的准确度与 Devin 相当，在解决 GitHub 仓库问题上的性能甚至超过了 Devin：SWE-agent 平均只需 93 秒就能修完 Bug。&lt;br&gt;
　　完整的 SWE-bench 基准测试结果显示，SWE-agent 修复了 12.29% 的问题，Debin 则是 13.84%—— 但 SWE-agent 有一大优势：开源。这一成绩也表明，开源模型有能力追赶甚至超越闭源模型的性能。SWE Agent 的高精度显示了其处理复杂软件工程任务的能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/R9DLQWQVLo8dR7cqKIn4ew?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;华人开源最强「AI 程序员」炸场，让 GPT-4 自己修 Bug！&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Github 地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent&#34;&gt;https://github.com/princeton-nlp/SWE-agent&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;36-stable-audio-20音乐-chatgpt-20&#34;&gt; (36) Stable Audio 2.0（音乐 ChatGPT 2.0）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.04&lt;/strong&gt;&lt;/em&gt;  Stability AI 发布了 Stable Audio 2.0。&lt;br&gt;
　　普仅仅用一条自然语言指令，它就能以 44.1 kHz 的立体声质量，创作出高质量、结构完整的音乐作品。&lt;br&gt;
而且，每首曲目最长可达 3 分钟！相比之下，Suno 最长可创作 2 分钟，这方面可是被 Stable Audio 2 完爆了。&lt;br&gt;
并且，Audo 2.0 的音频到音频功能，目前只有 Meta 的 MusicGen 可以做到，连 Suno 都做不到。&lt;br&gt;
　　模型已经在 Stable Audio 官网上免费开放使用了，并且很快就能通过 Stable Audio API 提供服务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/TxJoUe9uYWNZ5KqWy1x68g&#34;&gt;音乐 ChatGPT 2.0 来了！AI 作曲家被踢馆，亲测周杰伦爆款大翻车&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;体验地址：&lt;/strong&gt; &lt;a href=&#34;https://stability.ai/news/stable-audio-2-0&#34;&gt;https://stability.ai/news/stable-audio-2-0&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;37-coig-cqia中文指令调优数据集&#34;&gt; (37) COIG-CQIA（中文指令调优数据集）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.05&lt;/strong&gt;&lt;/em&gt;  最近，大型语言模型（LLM）取得了重大进展，特别是在英语方面。然而，LLM 在中文指令调优方面仍然存在明显差距。现有的数据集要么以英语为中心，要么不适合与现实世界的中国用户交互模式保持一致。&lt;br&gt;
　　为了弥补这一差距，一项由 10 家机构联合发布的研究提出了 COIG-CQIA（全称 Chinese Open Instruction Generalist - Quality Is All You Need），这是一个高质量的中文指令调优数据集。数据来源包括问答社区、维基百科、考试题目和现有的 NLP 数据集，并且经过严格过滤和处理。&lt;br&gt;
　　此外，该研究在 CQIA 的不同子集上训练了不同尺度的模型，并进行了深入的评估和分析。本文发现，在 CQIA 子集上训练的模型在人类评估以及知识和安全基准方面取得了具有竞争力的结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://app.myzaker.com/news/article.php?pk=660e54a18e9f091e56270448&#34;&gt;弱智吧：大模型变聪明，有我一份贡献&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.18058.pdf&#34;&gt;https://arxiv.org/pdf/2403.18058.pdf&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 数据地址：&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/datasets/m-a-p/COIG-CQIA&#34;&gt;https://huggingface.co/datasets/m-a-p/COIG-CQIA&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;38-command-rllm&#34;&gt; (38) Command R+（LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.05&lt;/strong&gt;&lt;/em&gt;  知名类 ChatGPT 平台 Cohere 在官网发布了全新模型 ——Command R+。&lt;br&gt;
　　据悉，Command R + 有 1040 亿参数，支持英语、中文、法语、德语等 10 种语言。最大特色之一是，Command R + 对内置的 RAG（检索增强生成）进行了全面强化，其性能仅次于 GPT-4 tubro，高于市面上多数开源模型。&lt;br&gt;
　　目前，Cohere 已经开源了 Command R + 的权重，但只能用于学术研究无法商业化。想商业应用，用户可以通过微软 Azure 云使用该模型或者 Cohere 提供的 API。&lt;br&gt;
　　&lt;br&gt;
&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/PBiXl1GIoElXodAOYEkTLw&#34;&gt;Cohere 发布 RAG 增强版大模型并开源权重，支持中文、1040 亿参数&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;模型地址：&lt;/strong&gt;&lt;br&gt;
官方版：&lt;a href=&#34;https://huggingface.co/CohereForAI/c4ai-command-r-plus&#34;&gt;https://huggingface.co/CohereForAI/c4ai-command-r-plus&lt;/a&gt;&lt;br&gt;
 量化版：&lt;a href=&#34;https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit&#34;&gt;https://huggingface.co/CohereForAI/c4ai-command-r-plus-4bit&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;39-mod谷歌更新-transformer-架构&#34;&gt; (39) MoD（谷歌更新 Transformer 架构）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.08&lt;/strong&gt;&lt;/em&gt;  谷歌终于更新了 Transformer 架构。最新发布的 Mixture-of-Depths（MoD），改变了以往 Transformer 计算模式。&lt;br&gt;
　　它通过动态分配大模型中的计算资源，跳过一些不必要计算，显著提高训练效率和推理速度。&lt;br&gt;
　　结果显示，在等效计算量和训练时间上，MoD 每次向前传播所需的计算量更小，而且后训练采样过程中步进速度提高 50%。&lt;br&gt;
　　&lt;br&gt;
&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/tf4vo9Znb2z3ZPoMVKWjHA&#34;&gt;谷歌更新 Transformer 架构，更节省计算资源！50% 性能提升&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.02258&#34;&gt;https://arxiv.org/abs/2404.02258&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;40-open-sora-plan国产-sora&#34;&gt; (40) Open-Sora-Plan（国产 Sora）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.10&lt;/strong&gt;&lt;/em&gt;  自从今年 2 月 OpenAI 发布了基于日语词汇 “Sora” 所开发的惊艳视频生成技术以来，该技术以其能够将简短文本描述转化为高清一分钟视频而引起了全球技术界的广泛关注。北京大学及兔展智能携手于三月启动了开源项目 Open-Sora-Plan，旨在通过开源方式再现 Sora 技术，并训练涵盖无条件视频生成、类似视频生成以及文本驱动视频生成等多个技术模块的综合模型。&lt;br&gt;
　　日前，Open-Sora-Plan 已成功发布了 v1.0.0 版本，显著提升了视频生成效果和文本引导控制功能，目前还在训练更高分辨率（超过 1024）和更长持续时间（超过 10 秒）的视频内容。项目核心技术架构包括 Video VQ-VAE、Denoising Diffusion Transformer 以及 Condition Encoder，其中 CausalVideoVAE 架构尤为关键，它结合了变分自编码器（VAE）和矢量量化（VQ）原理，有效实现视频数据的高效压缩和重建，并且特别优化了对首帧图像的处理，使其既能单独编码静态图像又能无缝应用于视频编码，进而助力扩散模型精准捕捉视频的空间细节，提升视觉品质。&lt;br&gt;
　　&lt;br&gt;
&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/srG2o9eYG38RHTfhQeTqhw&#34;&gt;国产开源 Sora：Open-Sora-Plan 支持华为昇腾芯片，生成 10 秒高清视频&lt;/a&gt;	&lt;br&gt;
&lt;strong&gt;GitHub 地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;https://github.com/PKU-YuanGroup/Open-Sora-Plan&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Hugging Face 在线演示：&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/spaces/LanguageBind/Open-Sora-Plan-v1.0.0&#34;&gt;https://huggingface.co/spaces/LanguageBind/Open-Sora-Plan-v1.0.0&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;41-grok-15v马斯克-多模态模型&#34;&gt; (41) Grok-1.5V（马斯克 - 多模态模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.15&lt;/strong&gt;&lt;/em&gt;  马斯克推出的多模态模型 Grok-1.5V 在多项基准测试中超越 GPT-4V，具有强大的文档、图标、截图和照片处理能力。通过 RealWorldQA 基准测试，Grok-1.5V 在理解物理世界方面表现出色。未来几个月，图像、音频、视频等多模态上的理解和生成能力将有望得到重大改进。&lt;br&gt;
　　&lt;br&gt;
&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1796382565750407051&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;马斯克新作！Grok-1.5V 多模态模型发布：数字与物理世界完美融合&lt;/a&gt;	&lt;br&gt;
&lt;strong&gt;官网地址：&lt;/strong&gt; &lt;a href=&#34;https://x.ai/blog/grok-1.5v&#34;&gt;https://x.ai/blog/grok-1.5v&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;42-gpt-4-turbogpt-4-升级&#34;&gt; (42) GPT-4 Turbo（GPT-4 升级）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.12&lt;/strong&gt;&lt;/em&gt;  OpenAI 官方宣布，新版 GPT-4 Turbo 今天开始向所有付费 ChatGPT 用户开放。知识库截止时间已经更新为 2024 年 4 月。&lt;br&gt;
　　据介绍，字少事大的新版本在写作、数学、逻辑推理和编码等多个方面都有了显著的提升。&lt;br&gt;
　　现在，当你使用 ChatGPT 写作时，你会发现新版本的响应速度更快，交流更加直接，而且它会更多地使用口语化的表达方式。&lt;br&gt;
　　简言之，新版本在写作上更加贴近人类的自然语言习惯，多了一些人味，少了点 AI 味。&lt;br&gt;
　　例如，当你需要发送短信提醒朋友回复生日晚宴的邀请时，以往 GPT 版本会像小莎士比亚一样提供满满的情绪价值，虽然情感丰富，但也显得絮絮叨叨，而现在的回复则更言简意赅，直接传达核心信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.huxiu.com/article/2897158.html&#34;&gt;刚刚，ChatGPT 大更新，GPT-4 又变聪明了&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;43-mistral-822bmoe-再升级&#34;&gt; (43) Mistral-8×22B（MoE 再升级）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.14&lt;/strong&gt;&lt;/em&gt;  Mistral AI 引发了 AI 领域的广泛关注，他们开源了一款拥有 1760 亿参数的巨型模型 ——Mixtral 8x22B。这款模型不仅在规模上达到了前所未有的高度，而且在多个性能基准测试中展示了卓越的能力，确立了新的行业标准。&lt;br&gt;
　　Mixtral 8x22B 继续沿用 Mistral AI 的专家混合（MoE）架构，这是一种将不同的网络专家集成到一个统一框架中的技术，允许模型根据任务需求动态调用最合适的专家处理数据。这种架构不仅提高了处理速度，还显著提升了模型在复杂任务上的表现。&lt;br&gt;
　　该模型的一大创新是其高效的专家选择机制。在每个处理步骤中，模型通过一个专门设计的路由网络决定哪些专家最适合当前的任务。这种机制使得 8x22B 能够优化其计算资源，减少不必要的计算开销。此外，Mixtral 8x22B 通过其 1760 亿参数和 64K 的上下文窗口，能够处理比以往任何模型都要长的文本输入，这对于长文本的理解和生成特别有价值。例如，在自动文档摘要或详细的故事生成任务中，该模型能够展示出更好的连贯性和文本理解能力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.zhihu.com/question/652456714/answer/3466256420&#34;&gt;如何看待 MistralAI 开源 Mistral-8×22B 模型？&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Huggingface 模型下载：&lt;/strong&gt;&lt;a href=&#34;https://huggingface.co/mistral-community&#34;&gt;https://huggingface.co/mistral-community&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;AI 快站模型免费加速下载：&lt;/strong&gt;&lt;a href=&#34;https://aifasthub.com/models/mistralai/mixtral-8x22b&#34;&gt;https://aifasthub.com/models/mistralai/mixtral-8x22b&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;44-llama-3meta-llm&#34;&gt; (44) Llama-3（Meta LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.19&lt;/strong&gt;&lt;/em&gt;  全球科技、社交巨头 Meta 在官网，正式发布了开源大模型 ——Llama-3。&lt;br&gt;
　　据悉，Llama-3 共有 80 亿、700 亿两种参数，分为基础预训练和指令微调两种模型（还有一个超 4000 亿参数正在训练中）。&lt;br&gt;
　　与 Llama-2 相比，Llama-3 使用了 15T tokens 的训练数据，在推理、数学、代码生成、指令跟踪等能力获得大幅度提升。&lt;br&gt;
　　此外，Llama-3 还使用了分组查询注意力、掩码等创新技术，帮助开发者以最低的能耗获取绝佳的性能。很快，Meta 就会发布 Llama-3 的论文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/P8nh848z8pqG3JnAjdJcHw?version=4.1.22.6014&amp;amp;platform=win&#34;&gt;重磅！Llama-3，最强开源大模型正式发布！&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;开源地址：&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6&#34;&gt;https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Github 地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/meta-llama/llama3/&#34;&gt;https://github.com/meta-llama/llama3/&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 英伟达在线体验 Llama-3：&lt;/strong&gt; &lt;a href=&#34;https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain&#34;&gt;https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;45-megalodonmeta-上下文长度不受限的神经网络架构&#34;&gt; (45) MEGALODON（Meta 上下文长度不受限的神经网络架构）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.12&lt;/strong&gt;&lt;/em&gt;  来自 Meta、南加州大学、CMU、UCSD 等公司、机构引入了 MEGALODON，一种用于高效序列建模的神经架构，上下文长度不受限制。&lt;br&gt;
　　MEGALODON 继承了 MEGA（带有门控注意力的指数移动平均）的架构，并进一步引入了多种技术组件来提高其能力和稳定性，包括复数指数移动平均（CEMA）、时间步归一化层、归一化注意力机制和具有两个特征的预归一化（pre-norm）残差配置。&lt;br&gt;
　　在与 LLAMA2 的直接比较中，MEGALODON 在 70 亿参数和 2 万亿训练 token 的规模上取得了比 Transformer 更好的效率。MEGALODON 的训练损失达到 1.70，处于 LLAMA2-7B (1.75) 和 13B (1.67) 之间。MEGALODON 相对于 Transformers 的改进在不同任务和模式的一系列基准测试中表现强劲。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/eg1RY_TVLeRdXR3Ynu5VVw?version=4.1.22.6014&amp;amp;platform=win&#34;&gt;Meta 无限长文本大模型来了：参数仅 7B，已开源&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2404.08801.pdf&#34;&gt;https://arxiv.org/pdf/2404.08801.pdf&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;GitHub 地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/XuezheMax/megalodon&#34;&gt;https://github.com/XuezheMax/megalodon&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;46-phi-3-mini微软-最强小参数大模型&#34;&gt; (46) Phi-3 Mini（微软 - 最强小参数大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.23&lt;/strong&gt;&lt;/em&gt;  Phi-3-mini 是微软 Phi 家族的第 4 代，有预训练和指令微调多种模型，参数只有 38 亿训练数据却高达 3.3T tokens，比很多数百亿参数的模型训练数据都要多，这也是其性能超强的主要原因之一。&lt;br&gt;
　　Phi-3-mini 对内存的占用极少，可以在 iPhone14 等同类手机中部署使用该模型。尽管受到移动硬件设备的限制，但每秒仍能生成 12 个 tokens 数据。&lt;br&gt;
　　值得一提的是，微软在预训练 Phi-3-mini 时使用了合成数据，能帮助大模型更好地理解语言架构、表达方式、文本语义理解、逻辑推理以及特定业务场景的专业术语等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://new.qq.com/rain/a/20240424A00Z6900&#34;&gt;微软开源最强小参数大模型 —Phi-3 Mini&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 开源地址：&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3&#34;&gt;https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Ollama 地址：&lt;/strong&gt; &lt;a href=&#34;https://ollama.com/library/phi3&#34;&gt;https://ollama.com/library/phi3&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 技术报告：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.14219&#34;&gt;https://arxiv.org/abs/2404.14219&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;47-中文版-llama3&#34;&gt; (47) 中文版 Llama3&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.25&lt;/strong&gt;&lt;/em&gt;  最近，Meta 推出了 Llama 3，为开源大模型树立了新的标杆。和以往的原始 Llama 模型一样，Llama 3 对中文的支持效果欠佳，经常会出现你用中文提问，它用英文或中文 + 英文回复的现象。因此，要想让国内用户用上该模型，开发者还需对其进行微调。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/SrWWC5f1MLkmfoY_v3XK6g?version=4.1.22.6014&amp;amp;platform=win&#34;&gt;中文版 Llama3 开源了！！&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;GitHub 地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/CrazyBoyM/llama3-Chinese-chat&#34;&gt;https://github.com/CrazyBoyM/llama3-Chinese-chat&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;HuggingFace 地址：&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat?continueFlag=5a1e5d88eed977ffb39d9b451be2a81d&#34;&gt;https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat?continueFlag=5a1e5d88eed977ffb39d9b451be2a81d&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;48-qwen15-110b国产-llama3&#34;&gt; (48) Qwen1.5-110B（国产 Llama3）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.27&lt;/strong&gt;&lt;/em&gt;  开源界最近属实是太疯狂了，前有 Llama3-70B 模型开源，后有 Qwen1.5 开源千亿（110B）级别参数模型。&lt;br&gt;
　　Qwen 你真的让我开始捉摸不透了，1.5 系列已经从 0.5B、1.8B、7B、14B、32B、72B 到现在的 110B、还有 Code 系列模型、MOE 系列模型，太全了，感觉已经快把中文开源模型市场给垄断了。&lt;br&gt;
　　模型结构与之前模型相似，采用 Transformer-Decoder 架构，并使用分组查询注意力（Grouped Query Attention，GQA），加速模型推理计算。模型的最大长度为 32K，支持英、中、法、西、德、俄、日、韩、越等多种语言。&lt;br&gt;
　　在基础能力上的效果全面领先 72B 模型，与 Llama3-70B 模型也是平分秋色。&lt;br&gt;
并且，值得注意的是，110B 的模型是 Dense 的模型，不是虚胖的 MOE 模型&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/K4nwMGcgftyAo_h8JoBaQg&#34;&gt;中国人自己的 Llama：Qwen1.5 开源 110B 参数模型&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; Blog 地址:&lt;/strong&gt; &lt;a href=&#34;https://qwenlm.github.io/blog/qwen1.5-110b&#34;&gt;https://qwenlm.github.io/blog/qwen1.5-110b&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;HuggingFace 地址:&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen1.5-110B-Chat&#34;&gt;https://huggingface.co/Qwen/Qwen1.5-110B-Chat&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 体验地址:&lt;/strong&gt; &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen1.5-110B-Chat-demo&#34;&gt;https://huggingface.co/spaces/Qwen/Qwen1.5-110B-Chat-demo&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;49-vidu国产-sora&#34;&gt; (49) Vidu（国产 Sora）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.27&lt;/strong&gt;&lt;/em&gt;  国产 AI 视频大模型 Vidu 在中关村论坛未来人工智能先锋论坛上发布。&lt;br&gt;
　　“中国首个长时长、高一致性、高动态性视频” 是 Vidu 的代名词，Vidu 模型由清华大学和生数科技联合开发，具有以下 6 大特征：模拟真实物理世界、富有想象力、具有多镜头语言、出色的视频时长、时空一致性高、理解中国元素。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模拟真实物理世界：展现复杂、细节丰富的场景，模拟真实世界的物理特性，符合物理规律，例如具有光影效果和人物表情等。&lt;/li&gt;
&lt;li&gt;富有想象力：具备虚构场景能力，创造超现实主义画面&lt;/li&gt;
&lt;li&gt;具有多镜头语言：支持实现复杂动态镜头，体现远、近、中景、特写，长镜头、追焦等效果&lt;/li&gt;
&lt;li&gt;出色视频时长：一镜到底 16s 视频生成，单一大模型，端到端生成&lt;/li&gt;
&lt;li&gt;时空一致性高：不同镜头之间的视频连贯，人物和场景在时空中保持一致&lt;/li&gt;
&lt;li&gt;理解中国元素：可以理解、生成熊猫、龙等中国元素。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/PBC_ZTsV70HEv18-hHlxOQ?version=4.1.22.6014&amp;amp;platform=win&amp;amp;nwr_flag=1#wechat_redirect&#34;&gt;国产版 Sora 横空出世，清华大学与生数科技联合发布 Vidu&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 申请链接地址:&lt;/strong&gt; &lt;a href=&#34;https://shengshu.feishu.cn/share/base/form/shrcnybSDE4Id1JnA5EQ0scv1Ph&#34;&gt;https://shengshu.feishu.cn/share/base/form/shrcnybSDE4Id1JnA5EQ0scv1Ph&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;50-video-mamba-suitemamba-视频领域应用&#34;&gt; (50) Video Mamba Suite（Mamba 视频领域应用）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.05.01&lt;/strong&gt;&lt;/em&gt;  来自南京大学、上海人工智能实验室、复旦大学、浙江大学的研究团队发布了一项开创性工作。他们全面审视了 Mamba 在视频建模中的多重角色，提出了针对 14 种模型 / 模块的 Video Mamba Suite，在 12 项视频理解任务中对其进行了深入评估。结果令人振奋：Mamba 在视频专用和视频 - 语言任务中均展现出强劲的潜力，实现了效率与性能的理想平衡。这不仅是技术上的飞跃，更是对未来视频理解研究的有力推动。&lt;br&gt;
　　研究团队精心打造了 video-mamba-suite（视频 Mamba 套件）。该套件旨在补充现有研究的不足，通过一系列深入的实验和分析，探索 Mamba 在视频理解中的多样化角色和潜在优势。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/269KtCu1rcPTSMiAvCL0Qg?version=4.1.22.6014&amp;amp;platform=win&amp;amp;nwr_flag=1#wechat_redirect&#34;&gt;Mamba 再次击败 Transformer！在视频理解任务中杀疯了！&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文链接：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.09626&#34;&gt;https://arxiv.org/abs/2403.09626&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 代码链接：&lt;/strong&gt; &lt;a href=&#34;https://github.com/OpenGVLab/video-mamba-suite&#34;&gt;https://github.com/OpenGVLab/video-mamba-suite&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;51-kan全新神经网络架构&#34;&gt; (51) KAN（全新神经网络架构）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.05.02&lt;/strong&gt;&lt;/em&gt;  一种全新的神经网络架构 KAN，诞生了！与传统的 MLP 架构截然不同，且能用更少的参数在数学、物理问题上取得更高精度。&lt;br&gt;
　　在函数拟合、偏微分方程求解，甚至处理凝聚态物理方面的任务都比 MLP 效果要好。&lt;br&gt;
　　而在大模型问题的解决上，KAN 天然就能规避掉灾难性遗忘问题，并且注入人类的习惯偏差或领域知识非常容易。&lt;br&gt;
　　来自 MIT、加州理工学院、东北大学等团队的研究一出，瞬间引爆一整个科技圈：Yes We KAN！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/5WFJMPJvtaofeGDxFQ9aDw?version=4.1.22.6014&amp;amp;platform=win&amp;amp;nwr_flag=1#wechat_redirect&#34;&gt;全新神经网络架构 KAN 一夜爆火！200 参数顶 30 万，MIT 华人一作，轻松复现 Nature 封面 AI 数学研究&lt;/a&gt; version=4.1.22.6014&amp;amp;platform=win&amp;amp;nwr_flag=1#wechat_redirect)&lt;br&gt;
&lt;strong&gt; 项目链接：&lt;/strong&gt; &lt;a href=&#34;https://kindxiaoming.github.io/pykan/&#34;&gt;https://kindxiaoming.github.io/pykan/&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 论文链接：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.19756&#34;&gt;https://arxiv.org/abs/2404.19756&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;52-meshy-3文本生成-3d-模型&#34;&gt; (52) Meshy 3（文本生成 3D 模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.05.01&lt;/strong&gt;&lt;/em&gt;  文本生成 3D 模型 Meshy 3 重磅发布，目前可免费试用，UI、提示词都支持中文。&lt;br&gt;
　　本次，Meshy 3 生成的 3D 模型更加细腻逼真，支持 360 度全景观超分辨率贴图、纹理、位移、法线、曲率以及物理光照渲染效果。&lt;br&gt;
　　也就是说，用户可以像雕塑那样去生成 3D 模型，并且可下载 fbx、obj、glb、usdz 等文件格式放在不同场景中使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/gLOMqvlje2v0mdUlruca0Q?version=4.1.22.6014&amp;amp;platform=win&amp;amp;nwr_flag=1#wechat_redirect&#34;&gt;支持中文，免费试用！文本生成 360 度，物理光照 3D 模型&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;项目链接：&lt;/strong&gt; /&lt;br&gt;
&lt;strong&gt; 论文链接：&lt;/strong&gt; /&lt;br&gt;
&lt;strong&gt; 免费体验地址：&lt;/strong&gt; &lt;a href=&#34;https://app.meshy.ai/zh/discover&#34;&gt;https://app.meshy.ai/zh/discover&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;53-memgptllm-记忆管理框架&#34;&gt; (53) MemGPT（LLM 记忆管理框架）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.05.02&lt;/strong&gt;&lt;/em&gt;  根据《MemGPT：将大语言模型作为操作系统》论文，其研发灵感来自于操作系统的分层内存系统，通过在快速和慢速内存之间移动数据来提供大内存资源的外观。MemGPT 系统，智能地管理不同的内存层，以有效地在 LLM 的有限上下文窗口内提供扩展上下文，并利用中断来管理自身与用户之间的控制流。&lt;br&gt;
　　MemGPT 的研究者写道：“大型语言模型 彻底改变了人工智能，但受到有限的上下文窗口的限制，阻碍了它们在扩展对话和文档分析等任务中的实用性。为了能够在有限的上下文窗口之外使用上下文，我们提出了虚拟上下文管理，这是一种从传统操作系统中的分层内存系统中汲取灵感的技术，该技术通过快速内存和慢速内存之间的数据移动提供大内存资源的外观。使用这种技术，我们引入了 MemGPT，这是一个智能管理不同内存层的系统，以便在 LLM 有限的上下文窗口内有效地提供扩展上下文，并利用中断来管理其自身和用户之间的控制流。我们在两个领域评估了受操作系统启发的设计，现代 LLM 的有限上下文窗口严重影响了其性能：文档分析，MemGPT 能够分析远远超出底层 LLM 上下文窗口的大型文档，以及多会话聊天，其中 MemGPT 能够分析远远超出底层 LLM 上下文窗口的大型文档。MemGPT 可以创建会话代理，通过与用户的长期交互来记忆、反映和动态发展。”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/holcsXlfNQ9ZYBX5xEECNw&#34;&gt;GitHub 8.9K Star，伯克利大学开源 LLM 记忆管理框架 MemGPT&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 项目链接：&lt;/strong&gt; &lt;a href=&#34;https://github.com/cpacker/MemGPT&#34;&gt;https://github.com/cpacker/MemGPT&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 论文链接：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.08560&#34;&gt;https://arxiv.org/abs/2310.08560&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 免费体验地址：&lt;/strong&gt; &lt;a href=&#34;https://app.meshy.ai/zh/discover&#34;&gt;https://app.meshy.ai/zh/discover&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;54-vimvision-mamba-mamba-高性能视觉版&#34;&gt; (54) Vim（Vision Mamba (Mamba 高性能视觉版)）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.05.03&lt;/strong&gt;&lt;/em&gt;  来自华中科技大学、地平线、智源人工智能研究院等机构的研究者提出了 Vision Mamba（Vim）。&lt;br&gt;
　　在 ImageNet 分类任务、COCO 对象检测任务和 ADE20k 语义分割任务上，与  DeiT 等成熟的视觉 Transformers 相比，Vim 实现了更高的性能，同时还显著提高了计算和内存效率。例如，在对分辨率为 1248×1248 的图像进行批量推理提取特征时，Vim 比 DeiT 快 2.8 倍，并节省 86.8% 的 GPU 内存。结果表明，Vim 能够克服对高分辨率图像执行 Transformer 式理解时的计算和内存限制，并且具有成为视觉基础模型的下一代骨干的巨大潜力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/r7AFR6x44H4DTQBfbQwU8w&#34;&gt;重磅！视觉 Mamba 正式收录顶会 ICML 2024！&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2401.09417.pdf&#34;&gt;https://arxiv.org/pdf/2401.09417.pdf&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 项目地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/hustvl/Vim&#34;&gt;https://github.com/hustvl/Vim&lt;/a&gt;&lt;/p&gt;
</content>
        <category term="人工智能" />
        <category term="AIGC前沿" />
        <updated>2024-03-15T08:16:00.000Z</updated>
    </entry>
</feed>
