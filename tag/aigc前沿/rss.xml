<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>且听风吟，御剑于心！ • Posts by &#34;aigc前沿&#34; tag</title>
        <link>https://leezhao415.github.io</link>
        <description></description>
        <language>zh-CN</language>
        <pubDate>Fri, 15 Mar 2024 16:16:00 +0800</pubDate>
        <lastBuildDate>Fri, 15 Mar 2024 16:16:00 +0800</lastBuildDate>
        <category>人工智能/CV</category>
        <category>Transformer/DETR(CV)</category>
        <category>人工智能</category>
        <category>数据集</category>
        <category>大数据框架</category>
        <category>编程工具</category>
        <category>NLP</category>
        <category>模型部署</category>
        <category>数据结构与算法</category>
        <category>Python数据分析</category>
        <category>网络通信</category>
        <category>YOLOX</category>
        <category>CV算法</category>
        <category>VSLAM</category>
        <category>NCNN部署</category>
        <category>YOLOX目标检测</category>
        <category>多模态</category>
        <category>目标检测（人脸检测）</category>
        <category>目标跟踪</category>
        <category>深度学习</category>
        <category>CV未来</category>
        <category>且读文摘</category>
        <category>NLP-BERT</category>
        <category>自然语言处理NLP</category>
        <category>IOU</category>
        <category>OpenCV之DNN模块</category>
        <category>深度模型</category>
        <category>激活函数</category>
        <category>NLP-模型优化</category>
        <category>梯度更新</category>
        <category>概述</category>
        <category>人脸识别</category>
        <category>名人名言</category>
        <category>寒窑赋</category>
        <category>NLP/评估指标</category>
        <category>度量学习</category>
        <category>智能家居</category>
        <category>机器学习/损失函数</category>
        <category>机器学习</category>
        <category>CV/目标检测工具箱</category>
        <category>模型性能指标</category>
        <category>科研项目成果</category>
        <category>计算机顶会</category>
        <category>表面缺陷检测</category>
        <category>计算机视觉CV</category>
        <category>网络编程</category>
        <category>NLP/数据增强工具</category>
        <category>AIGC前沿</category>
        <category>计算机视觉</category>
        <category>模型优化</category>
        <category>三维建模</category>
        <category>计算机视觉库</category>
        <category>深度学习环境配置</category>
        <category>知识蒸馏</category>
        <category>多任务学习模型</category>
        <category>数据库原理</category>
        <category>算法</category>
        <category>操作系统</category>
        <category>深度模型（目标检测）</category>
        <category>视频理解</category>
        <category>ReID</category>
        <category>MOT</category>
        <category>NLP-发展史</category>
        <category>编程语言</category>
        <category>CV数据集</category>
        <category>Linux</category>
        <category>PaddlePaddle</category>
        <item>
            <guid isPermalink="true">https://leezhao415.github.io/2024/03/15/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91AIGC%E5%90%AF%E5%85%832024/</guid>
            <title>【精华】AIGC启元2024</title>
            <link>https://leezhao415.github.io/2024/03/15/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91AIGC%E5%90%AF%E5%85%832024/</link>
            <category>人工智能</category>
            <category>AIGC前沿</category>
            <pubDate>Fri, 15 Mar 2024 16:16:00 +0800</pubDate>
            <description><![CDATA[ &lt;meta name=&#34;referrer&#34; content=&#34;no-referrer&#34;&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;文章目录&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#aigc-%E5%89%8D%E6%B2%BF&#34;&gt;AIGC 前沿&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#1-gemini-15-pro%E8%B0%B7%E6%AD%8C%E6%96%B0%E4%B8%80%E4%BB%A3%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(1) Gemini 1.5 Pro（谷歌新一代多模态大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-sora%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(2) Sora（文本生成视频大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-emo%E9%98%BF%E9%87%8C%E7%94%9F%E6%88%90%E5%BC%8Fai%E6%A8%A1%E5%9E%8B&#34;&gt;(3) EMO（阿里生成式 AI 模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-playground-v25%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(4) Playground v2.5（文生图大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-vsp-llm%E5%94%87%E8%AF%AD%E8%AF%86%E5%88%AB&#34;&gt;(5) VSP-LLM（唇语识别）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#6-ideogram-ai-%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(6) Ideogram ai  （文生图大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#7-ltx-studio%E7%94%9F%E6%88%90%E5%BC%8Fai%E7%94%B5%E5%BD%B1%E5%88%B6%E4%BD%9C%E5%B9%B3%E5%8F%B0&#34;&gt;(7) LTX studio（生成式 AI 电影制作平台）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#8-claude3llm&#34;&gt;(8) Claude3（LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#9-open-sora%E6%96%87%E7%94%9F%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(9) Open Sora（文生视频大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#10-yi-9bllm&#34;&gt;(10) Yi-9B（LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#11-stable-diffusion-3lvm&#34;&gt;(11) Stable Diffusion 3（LVM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#12-cares-copilot10%E5%A4%9A%E6%A8%A1%E6%80%81%E6%89%8B%E6%9C%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(12) CARES Copilot1.0（多模态手术大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#13-figure-01%E9%80%9A%E7%94%A8%E6%9C%BA%E5%99%A8%E4%BA%BAfigure-ai-openai&#34;&gt;(13) Figure 01 通用机器人（Figure AI + OpenAI）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#14-devinai%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%8A%A9%E6%89%8B&#34;&gt;(14) Devin（AI 软件工程师助手）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#15-behavior-1k%E6%9D%8E%E9%A3%9E%E9%A3%9E%E5%9B%A2%E9%98%9F%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E5%9F%BA%E5%87%86&#34;&gt;(15) BEHAVIOR-1K（李飞飞团队 — 具身智能基准）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#16-mm1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%8B%B9%E6%9E%9C%E5%85%AC%E5%8F%B8%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(16) MM1 大模型（苹果公司多模态大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#17-aesopagent%E8%BE%BE%E6%91%A9%E9%99%A2%E6%99%BA%E8%83%BD%E4%BD%93%E9%A9%B1%E5%8A%A8%E7%9A%84%E8%BF%9B%E5%8C%96%E7%B3%BB%E7%BB%9F&#34;&gt;(17) AesopAgent（达摩院 — 智能体驱动的进化系统）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#18-cogview3%E6%96%87%E7%94%9F%E5%9B%BE%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(18) CogView3（文生图大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#19-autodev%E5%BE%AE%E8%BD%AF%E5%9B%A2%E9%98%9F%E5%85%A8%E8%87%AA%E5%8A%A8-ai-%E9%A9%B1%E5%8A%A8%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6&#34;&gt;(19) AutoDev（微软团队全自动 AI 驱动软件开发框架）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#20-vloggergoogle%E5%9B%BE%E7%94%9F%E9%9F%B3%E9%A2%91%E9%A9%B1%E5%8A%A8%E8%A7%86%E9%A2%91%E6%96%B9%E6%B3%95&#34;&gt;(20) VLOGGER（Google 图生音频驱动视频方法）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#21-textmonkeymonkey%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%96%87%E6%A1%A3%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8&#34;&gt;(21) TextMonkey（Monkey 多模态大模型在文档领域的应用）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#22-open-sora-10%E6%96%87%E7%94%9F%E8%A7%86%E9%A2%91%E5%A4%A7%E6%A8%A1%E5%9E%8B&#34;&gt;(22) Open-Sora 1.0（文生视频大模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#23-grok-1%E9%A9%AC%E6%96%AF%E5%85%8B%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&#34;&gt;(23) Grok-1（马斯克开源大语言模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#24-blackwell-gb200%E8%8B%B1%E4%BC%9F%E8%BE%BE%E6%96%B0%E4%B8%80%E4%BB%A3ai%E5%8A%A0%E9%80%9F%E5%8D%A1&#34;&gt;(24) Blackwell GB200（英伟达新一代 AI 加速卡）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#25-kimimoonshot-ai-%E6%99%BA%E8%83%BD%E5%8A%A9%E6%89%8B&#34;&gt;(25) Kimi（Moonshot AI 智能助手）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#26-suno-v3%E9%9F%B3%E4%B9%90chatgpt%E6%97%B6%E5%88%BB&#34;&gt;(26) Suno v3（音乐 ChatGPT 时刻）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#27-morasora%E7%9A%84%E9%80%9A%E6%89%8D%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B&#34;&gt;(27) Mora（Sora 的通才视频生成模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#28-mistral-7b-v02&#34;&gt;(28) Mistral 7B v0.2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#29-dbrxllm&#34;&gt;(29) DBRX（LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#30-grok-15llm&#34;&gt;(30) Grok-1.5（LLM）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#31-voice-engineopenai%E9%9F%B3%E9%A2%91%E6%A8%A1%E5%9E%8B&#34;&gt;(31) Voice Engine（OpenAI 音频模型）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#32-jambamamba-transformer&#34;&gt;(32) Jamba（Mamba + Transformer）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#33-streamingt2v%E6%96%87%E7%94%9F%E8%A7%86%E9%A2%91&#34;&gt;(33) StreamingT2V（文生视频）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#34-safe-longfactdeepmind%E6%A0%B9%E6%B2%BB%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%BB%E8%A7%89%E9%97%AE%E9%A2%98&#34;&gt;(34) SAFE + LongFact（DeepMind 根治大模型幻觉问题）&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
&lt;hr&gt;
&lt;h2&gt;&lt;span id=&#34;aigc-前沿&#34;&gt; AIGC 前沿&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;span id=&#34;1-gemini-15-pro谷歌新一代多模态大模型&#34;&gt; (1) Gemini 1.5 Pro（谷歌新一代多模态大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.16&lt;/strong&gt;&lt;/em&gt;  谷歌新一代多模态大模型 Gemini 1.5 Pro，在性能上超越 OpenAI 的 GPT-4 Turbo，堪称业界最强大模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzU5OTI0NTc3Mg==&amp;amp;mid=2247530889&amp;amp;idx=1&amp;amp;sn=0686f28b493fb61fdcd3e619b9a97517&#34;&gt;“打假” Sora，谷歌 Gemini 1.5 Pro 第一波评测出炉｜甲子光年&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://openai.com/sora&#34;&gt;https://openai.com/sora&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;2-sora文本生成视频大模型&#34;&gt; (2) Sora（文本生成视频大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.16&lt;/strong&gt;&lt;/em&gt;  Sora 文本生成视频的大模型。它所展现出来的能力几乎可以 “碾压” 目前全球能实现文本生成视频的大模型 包 括 Runway、Pika、Stable Video Diffusion 等 20 多个产品。&lt;br&gt;
　　用户仅需输入简短一句话，Sora 就可生成一段长达 60 秒的视频，远远超过市面上同类型级别的 AI 视频生成时长。在此之前，AI 视频模型生成时长几乎在 10 秒以内，而 “明星模型” Runway 和 Pika 等也仅有 3 到 4 秒。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.thepaper.cn/newsDetail_forward_26415514&#34;&gt;Sora 到底有多强？&lt;/a&gt; |  &lt;a href=&#34;https://mp.weixin.qq.com/s/zmKeNWrU6me3leNpdNH2wQ&#34;&gt;微软最新 Sora 综述&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://ai.google.dev/gemma&#34;&gt;Gemma Open Models&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;3-emo阿里生成式-ai-模型&#34;&gt; (3) EMO（阿里生成式 AI 模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.28&lt;/strong&gt;&lt;/em&gt;  生成式 AI 模型 EMO（Emote Portrait Alive）。EMO 仅需一张人物肖像照片和音频，就可以让照片中的人物按照音频内容 “张嘴” 唱歌、说话，且口型基本一致，面部表情和头部姿态非常自然。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1792189739510241856&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;阿里 EMO 模型，一张照片就能造谣&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://humanaigc.github.io/emote-portrait-alive/&#34;&gt;https://humanaigc.github.io/emote-portrait-alive/&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;4-playground-v25文生图大模型&#34;&gt; (4) Playground v2.5（文生图大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.28&lt;/strong&gt;&lt;/em&gt;  Playground 在去年发布 Playground v2.0 之后再次开源新的文生图模型 Playground v2.5。相比上一个版本，Playground v2.5 在美学质量，颜色和对比度，多尺度生成以及以人为中心的细节处理有比较大的提升。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/684287454&#34;&gt;超过 Midjourney v5.2 的开源文生图大模型 Playground v2.5 来了&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://playground.com/&#34;&gt;https://playground.com/&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;5-vsp-llm唇语识别&#34;&gt; (5) VSP-LLM（唇语识别）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.28&lt;/strong&gt;&lt;/em&gt;  一种通过观察视频中人的嘴型来理解和翻译说话内容的技术，也就是识别唇语。该技术能够将视频中的唇动转化为文本（视觉语音识别），并将这些唇动直接翻译成目标语言的文本 (视觉语音翻译)。不仅如此，VSP-LLM 还能智能识别和去除视频中不必要的重复信息，使处理过程更加快速和准确。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.chinaz.com/2024/0228/1599901.shtml&#34;&gt;VSP-LLM：可通过观察视频中人的嘴型来识别唇语&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://github.com/sally-sh/vsp-llm&#34;&gt;https://github.com/sally-sh/vsp-llm&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;6-ideogram-ai-文生图大模型&#34;&gt; (6) Ideogram ai  （文生图大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.29&lt;/strong&gt;&lt;/em&gt;  Ideogram 发布了最新的 Ideogram1.0 图像生成模型，该模型具有强大的文字生成能力和提示词理解能力。Ideogram1.0 在文本渲染准确性方面实现了飞跃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt;&lt;a href=&#34;https://www.chinaz.com/2024/0229/1599986.shtml&#34;&gt;Ideogram 1.0 图像生成模型发布 文字生成能力更强大了&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt;&lt;a href=&#34;https://top.aibase.com/tool/ideogram-ai&#34;&gt;https://top.aibase.com/tool/ideogram-ai&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;7-ltx-studio生成式-ai-电影制作平台&#34;&gt; (7) LTX studio（生成式 AI 电影制作平台）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.29&lt;/strong&gt;&lt;/em&gt;  生成式 AI 电影制作平台 —LTX Studio，用户只需要输入文本就能生成超 25 秒的微电影视频，同时可对镜头切换、角色、场景一致性、摄像机、灯光等进行可视化精准控制。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://new.qq.com/rain/a/20240229A02EA500&#34;&gt;效果比 Sora 惊艳，著名 AI 平台大动作！文本生成超 25 秒视频，带背景音乐、转场等效果&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://ltx.studio&#34;&gt;https://ltx.studio&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;8-claude3llm&#34;&gt; (8) Claude3（LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.04&lt;/strong&gt;&lt;/em&gt;  Claude3 是由 Anthropic 发布的最新的 AI 大模型系列，同时，Claude3 是多模态大模型 ，具有强大的 “视觉能力”。Claude3 Opus 已经在部分行业行为准则中的表现优于 OpenAI 的 GPT-4 和谷歌的 Gemini Ultra，如本科生水平知识（MMLU）、研究生级别专家推理（GPQA）和基础数学（GSM8K）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://new.qq.com/rain/a/20240307A09O1N00&#34;&gt;OpenAI 劲敌出现！Claude3 正式发布，超越 GTP-4?&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 官网链接：&lt;/strong&gt; &lt;a href=&#34;https://www.anthropic.com/claude&#34;&gt;https://www.anthropic.com/claude&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;9-open-sora文生视频大模型&#34;&gt; (9) Open Sora（文生视频大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.01&lt;/strong&gt;&lt;/em&gt;  北大团队联合兔展发起了一项 Sora 复现计划 ——Open Sora&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1792479658318662669&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;北大与兔展智能发起复现 Sora，框架已开源&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt;&lt;br&gt;
&lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html&#34;&gt;https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;https://github.com/PKU-YuanGroup/Open-Sora-Plan&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;10-yi-9bllm&#34;&gt; (10) Yi-9B（LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.06&lt;/strong&gt;&lt;/em&gt;  李开复旗下 AI 公司零一万物的最新力作 ——Yi-9B 大模型正式对外开源发布。这款具有 90 亿参数的大模型，在代码和数学能力上达到了前所未有的高度，同时保持了对消费级显卡的良好兼容性，为广大开发者和研究人员提供了前所未有的便利性和强大功能。&lt;br&gt;
　　Yi-9B 作为 Yi 系列中的新成员，被誉为 “理科状元”，特别加强了在代码和数学方面的学习能力。相较于市场上其他类似规模的开源模型，如 Mistral-7B、SOLAR-10.7B、Gemma-7B 等，Yi-9B 展现出了最佳的性能表现。特别值得一提的是，Yi-9B 既提供了浮点数版本（BF 16），也提供了整数版本（Int8），使其能够轻松部署在包括 RTX 4090 和 RTX 3090 在内的消费级显卡上，大大降低了使用门槛和成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1792881393208541121&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;零一万物开源 Yi-9B 大模型，消费级显卡可用，代码数学历史最强&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; &lt;a href=&#34;https://github.com/01-ai/Yi&#34;&gt;https://github.com/01-ai/Yi&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;11-stable-diffusion-3lvm&#34;&gt; (11) Stable Diffusion 3（LVM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.06&lt;/strong&gt;&lt;/em&gt;  Stable Diffusion 3 采用了与 Sora 相同的 DiT（Diffusion Transformer）架构，一经发布就引起了不小的轰动。与之前的版本相比，Stable Diffusion 3 生成的图在质量上实现了很大改进，支持多主题提示，文字书写效果也更好了（明显不再乱码）。&lt;br&gt;
　　Stability AI 表示，Stable Diffusion 3 是一个模型系列，参数量从 800M 到 8B 不等。这个参数量意味着，它可以在很多便携式设备上直接跑，大大降低了 AI 大模型的使用门槛。&lt;br&gt;
　　在最新发布的论文中，Stability AI 表示，在基于人类偏好的评估中，Stable Diffusion 3 优于当前最先进的文本到图像生成系统，如 DALL・E 3、Midjourney v6 和 Ideogram v1。不久之后，他们将公开该研究的实验数据、代码和模型权重。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/mH6IzExPPBpX8YTwxlP6dA?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;Stable Diffusion 3 论文终于发布，架构细节大揭秘，对复现 Sora 有帮助？&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文链接：&lt;/strong&gt; &lt;a href=&#34;https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf&#34;&gt;https://stabilityai-public-packages.s3.us-west-2.amazonaws.com/Stable+Diffusion+3+Paper.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;12-cares-copilot10多模态手术大模型&#34;&gt; (12) CARES Copilot1.0（多模态手术大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.11&lt;/strong&gt;&lt;/em&gt;  CARES Copilot 是由中国科学院香港创新院 AI 中心研发的一个可信赖、可解释、面向医疗垂直领域并能与智能医疗设备高度集成的大模型系统。CARES Copilot 1.0 实现了图像、文本、语音、视频、MRI、CT、超声等多模态的手术数据理解。支持超过 100K 上下文的长窗口理解和高效分析，能理解超过 3000 页的复杂手术教材，对于年轻医生的培训和教学具有极高的实用价值。此外，该系统能通过深度检索功能，快速精确地提取手术教材、专家指南、医学论文等专业文档的信息，确保其提供的答案具有高度的可信度和可追溯性。经测试，系统能在一秒钟内完成百万级数据的快速检索，同时保持 95% 的准确率。该系统已在多家医院的不同科室进行了内部测试和迭代优化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793375529838669070&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;CARES Copilot 1.0 多模态手术大模型发布，可实现轻量化部署&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;13-figure-01-通用机器人figure-ai-openai&#34;&gt; (13) Figure 01 通用机器人（Figure AI + OpenAI）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.13&lt;/strong&gt;&lt;/em&gt;  Figure 01 通用机器人由 Figure AI 和 OpenAI 合作完成。展示视频中，Figure AI 人形机器人具有视觉能力并能表述所见画面，它伸手拿起桌上的苹果，并解释了这么做的原因，人类的提问后，这台人形机器人 “思索” 2~3 秒后便能顺畅作答，手部动作速度则接近人类。据视频介绍，机器人采用了端到端神经网络。&lt;br&gt;
　　该人形机器人由 OpenAI 提供了视觉推理和语言理解，Figure AI 的神经网络则提供快速、灵巧的机器人动作。人形机器人将摄像机的图像输入和麦克风接收的语音文字输入 OpenAI 提供的视觉语言大模型（VLM）中，该模型可以理解图像和文字。Figure 机载相机以 10hz 的频率拍摄画面，随后神经网络以 200hz 的频率输出 24 个自由度动作。画面中的人形机器人不依赖远程操作，行为都是学习而得的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793472537987147749&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;与 OpenAI 合作 13 天后，Figure 人形机器人展示与人类对话能力&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;14-devinai-软件工程师助手&#34;&gt; (14) Devin（AI 软件工程师助手）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.13&lt;/strong&gt;&lt;/em&gt;  一家成立不到两个月但拥有十名天才工程师的初创公司 Cognition 推出了一款名为 Devin 的人工智能（AI）助手，可以协助人类软件工程师完成诸多开发任务。Devin 不同于现有其他 AI 编码者，它可以从零构建网站、自行部署应用、修复漏洞、学习新技术等，人类只需扮演一个下指令和监督的角色。&lt;br&gt;
　　这是第一个真正意义上完全自主的 AI 软件工程师，一亮相即掀起轩然大波，因为人们担心：人类程序员是不是真要失业了？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://finance.eastmoney.com/a/202403153013211460.html&#34;&gt;人类程序员真要失业？首位 “AI 软件工程师” 亮相引爆科技圈&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;官网链接：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;15-behavior-1k李飞飞团队-具身智能基准&#34;&gt; (15) BEHAVIOR-1K（李飞飞团队 — 具身智能基准）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.02.27&lt;/strong&gt;&lt;/em&gt;  来自斯坦福、得克萨斯大学奥斯汀分校等大学的研究团队推出了一项以人为本的机器人技术综合模拟基准 ——BEHAVIOR-1K。&lt;br&gt;
　　BEHAVIOR-1K 包括两个部分，由 “您希望机器人为您做什么？” 这一问题的广泛调查结果指导和推动。第一部分是对 1000 种日常活动的定义，以 50 个场景（房屋、花园、餐厅、办公室等）为基础，其中有 9000 多个标注了丰富物理和语义属性的物体。其次是 OMNIGIBSON，这是一个模拟环境，通过对刚体、可变形体和液体进行逼真的物理模拟和渲染来支持这些活动。&lt;br&gt;
　　实验表明，BEHAVIOR-1K 中的活动是长视距的，并且依赖于复杂的操作技能，这两点对于最先进的机器人学习解决方案来说仍然是一个挑战。为了校准 BEHAVIOR-1K 的模拟与现实之间的差距，研究团队进行了一项初步研究，将在模拟公寓中使用移动机械手学习到的解决方案转移到现实世界中。&lt;br&gt;
　　研究团队希望 BEHAVIOR-1K 以人为本的特性、多样性和现实性能使其在具身智能和机器人学习研究中发挥重要作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/669737780&#34;&gt;stanford Behavior-1k—— 包含一千种日常任务的具身智能 benchmark&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 官网链接：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;16-mm1-大模型苹果公司多模态大模型&#34;&gt; (16) MM1 大模型（苹果公司多模态大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.15&lt;/strong&gt;&lt;/em&gt;  苹果公司最新发布了一款名为 MM1 的大型多模态基础模型，拥有 300 亿参数，采用了 MoE 架构，并且超过一半的作者是华人。&lt;br&gt;
　　该模型采用了 MoE 变体，并且在预训练指标和多项多模态基准测试上表现出了领先水平。研究者通过多项消融试验，探讨了模型架构、预训练数据选择以及训练程序等方面的重要性。他们发现，图像分辨率、视觉编码器损失和预训练数据在建模设计中都起着关键作用。&lt;br&gt;
　　MM1 的发布标志着苹果在多模态领域的重要进展，也为未来苹果可能推出的相关产品奠定了技术基础。该研究的成果对于推动生成式人工智能领域的发展具有重要意义，值得业界密切关注。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.chinaz.com/2024/0315/1603636.shtml&#34;&gt;苹果大模型 MM1 入场：参数达到 300 亿 超半数作者是华人&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.09611.pdf&#34;&gt;https://arxiv.org/pdf/2403.09611.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;17-aesopagent达摩院-智能体驱动的进化系统&#34;&gt; (17) AesopAgent（达摩院 — 智能体驱动的进化系统）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.15&lt;/strong&gt;&lt;/em&gt;  阿里达摩院提出了一个关于故事到视频制作的智能体驱动进化系统 ——AesopAgent，它是智能体技术在多模态内容生成方面的实际应用。&lt;br&gt;
　　该系统在一个统一的框架内集成了多种生成功能，因此个人用户可以轻松利用这些模块。这一创新系统可将用户故事提案转化为脚本、图像和音频，然后将这些多模态内容整合到视频中。此外，动画单元（如 Gen-2 和 Sora）可以使视频更具感染力。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.zhihu.com/pin/1751650851838750720&#34;&gt;阿里达摩院提出 AesopAgent：从故事到视频制作，智能体驱动的进化系统&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.07952.pdf&#34;&gt;https://arxiv.org/pdf/2403.07952.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;18-cogview3文生图大模型&#34;&gt; (18) CogView3（文生图大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.10&lt;/strong&gt;&lt;/em&gt;  文生图系统的最新进展主要是由扩散模型推动的。然而，单级文本到图像扩散模型在计算效率和图像细节细化方面仍面临挑战。为了解决这个问题，来自清华大学和智谱 AI 的研究团队提出了 CogView3—— 一个能提高文本到图像扩散性能的创新级联框架。&lt;br&gt;
　　据介绍，CogView3 是第一个在文本到图像生成领域实现 relay diffusion 的模型，它通过首先创建低分辨率图像，然后应用基于中继（relay-based）的超分辨率来执行任务。这种方法不仅能产生有竞争力的文本到图像输出，还能大大降低训练和推理成本。&lt;br&gt;
　　实验结果表明，在人类评估中，CogView3 比目前最先进的开源文本到图像扩散模型 SDXL 高出 77.0%，而所需的推理时间仅为后者的 1/2。经过提炼（distilled）的 CogView3 变体性能与 SDXL 相当，而推理时间仅为后者的 1/10。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.zhihu.com/pin/1750532992102223872&#34;&gt;CogView3：更精细、更快速的文生图&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.05121.pdf&#34;&gt;https://arxiv.org/pdf/2403.05121.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;19-autodev微软团队全自动-ai-驱动软件开发框架&#34;&gt; (19) AutoDev（微软团队全自动 AI 驱动软件开发框架）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.10&lt;/strong&gt;&lt;/em&gt;  微软团队推出了全自动 AI 驱动软件开发框架 AutoDev，该框架专为自主规划和执行复杂的软件工程任务而设计。AutoDev 使用户能够定义复杂的软件工程目标，并将其分配给 AutoDev 的自主 AI 智能体来实现。这些 AI 智能体可以对代码库执行各种操作，包括文件编辑、检索、构建过程、执行、测试和 git 操作。它们还能访问文件、编译器输出、构建和测试日志、静态分析工具等。这使得 AI 智能体能够以完全自动化的方式执行任务并全面了解所需的上下文信息。&lt;br&gt;
　　此外，AutoDev 还将所有操作限制在 Docker 容器内，建立了一个安全的开发环境。该框架结合了防护栏以确保用户隐私和文件安全，允许用户在 AutoDev 中定义特定的允许或限制命令和操作。&lt;br&gt;
　　研究团队在 HumanEval 数据集上对 AutoDev 进行了测试，在代码生成和测试生成方面分别取得了 91.5% 和 87.8% 的 Pass@1 好成绩，证明了它在自动执行软件工程任务的同时维护安全和用户控制的开发环境方面的有效性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://blog.csdn.net/m0_67695717/article/details/135610166&#34;&gt;AutoDev 1.5.3：精准的自动化测试生成、本地模型强化与流程自动化优化&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; /&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;20-vloggergoogle-图生音频驱动视频方法&#34;&gt; (20) VLOGGER（Google 图生音频驱动视频方法）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.14&lt;/strong&gt;&lt;/em&gt;  Google Research 提出了一种从单张人物输入图像生成音频驱动人类视频的方法 ——VLOGGER，它建立在最近成功的生成扩散模型基础之上。&lt;br&gt;
　　VLOGGER 由两部分组成，一是随机人体到三维运动扩散模型，二是一种基于扩散的新型架构，它通过空间和时间控制来增强文本到图像模型。这有助于生成长度可变的高质量视频，并可通过人脸和身体的高级表示轻松控制。&lt;br&gt;
　　与之前的工作相比，这一方法不需要对每个人进行训练，不依赖于人脸检测和裁剪，能生成完整的图像（不仅仅是人脸或嘴唇），并能考虑广泛的情况（如可见躯干或不同的主体身份），这对于正确合成交流的人类至关重要。研究团队还提出了一个包含三维姿势和表情注释的全新多样化数据集 MENTOR，它比以前的数据集大一个数量级（800000 identities），并且包含动态手势。研究团队在其上训练并简化了他们的主要技术贡献。&lt;br&gt;
　　VLOGGER 在三个公共基准测试中的表现达到了 SOTA，考虑到图像质量、身份保留和时间一致性，同时还能生成上半身手势。VLOGGER 在多个多样性指标方面的表现都表明其架构选择和 MENTOR 的使用有利于大规模训练一个公平、无偏见的模型。最后，研究团队还展示了在视频编辑和个性化方面的应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://www.zhihu.com/pin/1751650384068771840&#34;&gt;VLOGGER：基于多模态扩散的具身虚拟形象合成&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.08764.pdf&#34;&gt;https://arxiv.org/pdf/2403.08764.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;21-textmonkeymonkey-多模态大模型在文档领域的应用&#34;&gt; (21) TextMonkey（Monkey 多模态大模型在文档领域的应用）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.15&lt;/strong&gt;&lt;/em&gt;  TextMonkey 是 Monkey 在文档领域的重要升级，突破了通用文档理解能力的边界，在场景文字识别、办公文档摘要生成、数学问题问答、文档版式分析，表格理解，图表问答，电子文档关键信息抽取等 12 项等文档权威数据集以及在国际上规模最全的文档图像智能数据集 OCRBench 上取得了显著突破，通用文档理解性能大幅超越现有方法。&lt;br&gt;
　　TextMonkey 能帮助我们结构化图表、表格以及文档数据，通过将图像内容转化为轻量级的数据交换格式，方便记录和提取。TextMonkey 也能作为智能手机代理，无需接触后端，仅需语音输入及屏幕截图，即能够模仿人类的点击手势，能够在手机上执行各种任务，自主操控手机应用程序。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt;&lt;br&gt;
&lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793666944109402323&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;华科大研发多模态大模型 “猴子” 升级&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://zhuanlan.zhihu.com/p/685937695&#34;&gt; [全网首发中文版] TextMonkey: An OCRFree Large Multimodal Model for Understanding Document&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;GitHub 仓库地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/Yuliang-Liu/Monkey&#34;&gt;https://github.com/Yuliang-Liu/Monkey&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/2311.06607.pdf&#34;&gt;https://arxiv.org/pdf/2311.06607.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;22-open-sora-10文生视频大模型&#34;&gt; (22) Open-Sora 1.0（文生视频大模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.17&lt;/strong&gt;&lt;/em&gt;  Colossal-AI 团队全面开源全球首个类 Sora 架构视频生成模型 「Open-Sora 1.0」，涵盖了整个训练流程，包括数据处理、所有训练细节和模型权重，携手全球 AI 热爱者共同推进视频创作的新纪元。&lt;br&gt;
　　Colossal-AI 团队深入解读 Sora 复现方案的多个关键维度，包括模型架构设计、训练复现方案、数据预处理、模型生成效果展示以及高效训练优化策略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793833243872165307&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;没等来 OpenAI，等来了 Open-Sora 全面开源&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; GitHub 仓库地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/hpcaitech/Open-Sora&#34;&gt;https://github.com/hpcaitech/Open-Sora&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;23-grok-1马斯克开源大语言模型&#34;&gt; (23) Grok-1（马斯克开源大语言模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.17&lt;/strong&gt;&lt;/em&gt;  马斯克宣布开源 Grok-1，这使得 Grok-1 成为当前参数量最大的开源大语言模型，拥有 3140 亿参数，远超 OpenAI GPT-3.5 的 1750 亿。有意思的是，Grok-1 宣布开源的封面图为 Midjourney 生成，可谓 “AI helps AI”。&lt;br&gt;
　　Grok-1 是一个规模较大（314B 参数）的模型，需要有足够 GPU 内存的机器才能使用示例代码测试模型。网友表示这可能需要一台拥有 628 GB GPU 内存的机器。此外，该存储库中 MoE 层的实现效率并不高，之所以选择该实现是为了避免需要自定义内核来验证模型的正确性。&lt;br&gt;
　　目前已开源的热门大模型包括 Meta 的 Llama2、法国的 Mistral 等。通常来说，发布开源模型有助于社区展开大规模的测试和反馈，意味着模型本身的迭代速度也能加快。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793830961836866098&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;马斯克用行动反击 开源自家顶级大模型 压力给到 OpenAI&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;GitHub 仓库地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/xai-org/grok-1&#34;&gt;https://github.com/xai-org/grok-1&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 官方博客：&lt;/strong&gt; &lt;a href=&#34;https://x.ai/blog/grok-os&#34;&gt;https://x.ai/blog/grok-os&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 模型磁力链接：&lt;/strong&gt; &lt;a href=&#34;https://academictorrents.com/details/5f96d43576e3d386c9ba65b883210a393b68210e&#34;&gt;https://academictorrents.com/details/5f96d43576e3d386c9ba65b883210a393b68210e&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;24-blackwell-gb200英伟达新一代-ai-加速卡&#34;&gt; (24) Blackwell GB200（英伟达新一代 AI 加速卡）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.18&lt;/strong&gt;&lt;/em&gt;  英伟达公司于 2024 年的 GTC 大会上宣布了下一代人工智能超级计算机的问世，同时推出了备受业界瞩目的 AI 加速卡 ——Blackwell GB200。这款加速卡的发布，标志着人工智能领域又迈出了坚实的一步，其强大的性能、成本及能耗的突破，预计将引领 AI 技术的全新发展。&lt;br&gt;
　　Blackwell GB200 采用了英伟达新一代 AI 图形处理器架构 Blackwell，相较于前一代 Hopper 架构，其性能实现了巨大的飞跃。GB200 由两个 B200 Blackwell GPU 和一个基于 Arm 的 Grace CPU 组成，这种独特的组合使得其在处理大语言模型推理任务时，性能比 H100 提升高达 30 倍。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793911137057474891&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;性能飙升 30 倍，能耗骤降 25 倍！英伟达发布 Blackwell GB200！&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;25-kimimoonshot-ai-智能助手&#34;&gt; (25) Kimi（Moonshot AI 智能助手）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.18&lt;/strong&gt;&lt;/em&gt;  国内 AI 创业公司月之暗面（Moonshot AI）宣布在大模型长上下文窗口技术上取得新的突破，Kimi 智能助手已支持 200 万字超长无损上下文，短短五个月内 “长文本” 输入量提升 10 倍，并于即日起开启产品 “内测”。&lt;br&gt;
　　月之暗面创始人杨植麟博士表示，通往通用人工智能（AGI）的话，无损的长上下文将会是一个很关键的基础技术。历史上所有的模型架构演进，本质上都是在提升有效的、无损的上下文长度。上下文长度可能存在摩尔定律，但需要同时优化长度和无损压缩水平两个指标，才是有意义的规模化。&lt;br&gt;
　　月之暗面联合创始人 周昕宇则向钛媒体 App 透露，月之暗面即将在今年内推出自研的多模态大模型。同时，商业化也在快速推进。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://baijiahao.baidu.com/s?id=1793878336231855457&amp;amp;wfr=spider&amp;amp;for=pc&#34;&gt;对话月之暗面：Kimi 智能助手支持 200 万字无损输入，年内将发布多模态模型｜钛媒体 AGI&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;26-suno-v3音乐-chatgpt-时刻&#34;&gt; (26) Suno v3（音乐 ChatGPT 时刻）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.24&lt;/strong&gt;&lt;/em&gt;  AI 初创公司 Suno AI 重磅推出了第一款可制作「广播级」的音乐生成模型 ——V3，一时间在网上掀起轩然大波。仅用几秒的时间，V3 便可以创作出 2 分钟的完整歌曲。为了激发人们的创作灵感，Suno v3 还新增了更丰富的音乐风格和流派选项，比如古典音乐、爵士乐、Hiphop、电子等新潮曲风。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/BRzmlw-uE2C6ROF2O2_-nw&#34;&gt;音乐 ChatGPT 时刻来临！Suno V3 秒生爆款歌曲，12 人团队创现象级 AI&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;27-morasora-的通才视频生成模型&#34;&gt; (27) Mora（Sora 的通才视频生成模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.24&lt;/strong&gt;&lt;/em&gt;  理海大学联手微软团队一种新型的多 AI 智能体框架 ———Mora。Mora 更像是 Sora 的通才视频生成。通过整合多个 SOTA 的视觉 AI 智能体，来复现 Sora 展示的通用视频生成能力。具体来说，Mora 能够利用多个视觉智能体，在多种任务中成功模拟 Sora 的视频生成能力，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本到视频生成&lt;/li&gt;
&lt;li&gt;基于文本条件的图像到视频生成&lt;/li&gt;
&lt;li&gt;扩展已生成视频&lt;/li&gt;
&lt;li&gt;视频到视频编辑&lt;/li&gt;
&lt;li&gt;拼接视频&lt;/li&gt;
&lt;li&gt;模拟数字世界&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/GkJwyVFVwxih-ZQWWBIuNg&#34;&gt;Sora 不开源，微软给你开源！全球最接近 Sora 视频模型诞生，12 秒生成效果逼真炸裂&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.13248&#34;&gt;https://arxiv.org/abs/2403.13248&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;28-mistral-7b-v02&#34;&gt; (28) Mistral 7B v0.2&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.24&lt;/strong&gt;&lt;/em&gt;  这次开源的 Mistral 7B v0.2 Base Model ，是 Mistral-7B-Instruct-v0.2 背后的原始预训练模型，后者属于该公司的「Mistral Tiny」系列。&lt;br&gt;
此次更新主要包括三个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将 8K 上下文提到了 32K；&lt;/li&gt;
&lt;li&gt;Rope Theta = 1e6；&lt;/li&gt;
&lt;li&gt;取消滑动窗口。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/R56Ob5dZjMh1alhMin8DZw&#34;&gt;32K 上下文，Mistral 7B v0.2 基模型突然开源了&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;29-dbrxllm&#34;&gt; (29) DBRX（LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.28&lt;/strong&gt;&lt;/em&gt;  超级独角兽 Databricks 重磅推出 1320 亿参数的开源模型 ——DBRX。全球最强开源大模型王座易主，超越了 Llama 2、Mixtral 和 Grok-1。MoE 又立大功！这个过程只用了 2 个月，1000 万美元，和 3100 块 H100。采用了细粒度 MoE 架构，而且每次输入仅使用 360 亿参数，实现了更快的每秒 token 吞吐量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/88zvF3vwtTJcGl__HR6hBg?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;全球最强开源模型一夜易主，1320 亿参数推理飙升 2 倍！&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;项目地址：&lt;/strong&gt;&lt;br&gt;
&lt;a href=&#34;https://github.com/databricks/dbrx&#34;&gt;https://github.com/databricks/dbrx&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://huggingface.co/databricks&#34;&gt;https://huggingface.co/databricks&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;30-grok-15llm&#34;&gt; (30) Grok-1.5（LLM）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.29&lt;/strong&gt;&lt;/em&gt;  马斯克发布 Grok-1.5，强化推理和上下文，HumanEval 得分超 GPT-4 继开源 Grok-1 后，xAI 刚刚官方发布了他们的最新模型 Grok-1.5。据介绍，Grok-1.5 能够进行长语境理解和高级推理，并将于近日在 xAI 平台上向早期测试者和现有 Grok 用户开放。&lt;br&gt;
Grok-1.5 最显著的改进之一是其在编码和数学相关任务中的表现。在给出的测试结果中，Grok-1.5 在 MATH 基准测试中取得了 50.6% 的得分，在 GSM8K 基准测试中取得了 90% 的得分。此外，在评估代码生成和解决问题能力的 HumanEval 基准测试中，Grok-1.5 获得了 74.1% 的高分，超过了 GPT-4。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/RJ_KBxX1eNzMOGiGQ1iPCg?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;马斯克发布 Grok-1.5，强化推理和上下文，HumanEval 得分超 GPT-4&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 项目主页：&lt;/strong&gt; &lt;a href=&#34;https://x.ai/blog/grok-1.5&#34;&gt;https://x.ai/blog/grok-1.5&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;31-voice-engineopenai-音频模型&#34;&gt; (31) Voice Engine（OpenAI 音频模型）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.30&lt;/strong&gt;&lt;/em&gt;  OpenAI 在官网首次展示了全新自定义音频模型 “Voice Engine”。用户只需要提供 15 秒左右的参考声音，通过 Voice Engine 就能生成几乎和原音一模一样的全新音频，在清晰度、语音连贯、音色、自然度等方面比市面上多数产品都强很多。&lt;br&gt;
除了能合成音频之外，OpenAI 还展示了 Voice Engine 很多其他际商业用途，例如，一位失去声音表达能力的女孩，在 Voice Engine 帮助下能像以前一样正常发音说话。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/ErMhYBEjjDMpJfPlj9NiIw?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;OpenAI 首次展示音频模型 Voice Engine，生成的声音太逼真了！&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;32-jambamamba-transformer&#34;&gt; (32) Jamba（Mamba + Transformer）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.03.29&lt;/strong&gt;&lt;/em&gt;  AI21 Labs 推出并开源了一种名为「Jamba」的新方法，在多个基准上超越了 transformer。&lt;br&gt;
　　Mamba 的 SSM 架构可以很好地解决 transformer 的内存资源和上下文问题。然而，Mamba 方法很难提供与 transformer 模型相同的输出水平。&lt;br&gt;
　　Jamba 将基于结构化状态空间模型 (SSM) 的 Mamba 模型与 transformer 架构相结合，旨在将 SSM 和 transformer 的最佳属性结合在一起。&lt;br&gt;
　　Jamba 模型具有以下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一个基于 Mamba 的生产级模型，采用新颖的 SSM-Transformer 混合架构；&lt;/li&gt;
&lt;li&gt;与 Mixtral 8x7B 相比，长上下文上的吞吐量提高了 3 倍；&lt;/li&gt;
&lt;li&gt;提供对 256K 上下文窗口的访问；&lt;/li&gt;
&lt;li&gt;公开了模型权重；&lt;/li&gt;
&lt;li&gt;同等参数规模中唯一能够在单个 GPU 上容纳高达 140K 上下文的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/DPga3zigenLkPG587QCxbQ?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;打败 Transformer，Mamba 时代来了？&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;33-streamingt2v文生视频&#34;&gt; (33) StreamingT2V（文生视频）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.01&lt;/strong&gt;&lt;/em&gt;  Picsart 人工智能研究所、德克萨斯大学和 SHI 实验室的研究人员联合推出了 StreamingT2V 视频模型。通过文本就能直接生成 2 分钟、1 分钟等不同时间，动作一致、连贯、没有卡顿的高质量视频。&lt;br&gt;
　　虽然 StreamingT2V 在视频质量、多元化等还无法与 Sora 媲美，但在高速运动方面非常优秀，这为开发长视频模型提供了技术思路。&lt;br&gt;
　　研究人员表示，理论上，StreamingT2V 可以无限扩展视频的长度，并正在准备开源该视频模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/CXPpQaPL5wiog5C0U1MXxQ?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;文本直接生成 2 分钟视频，即将开源模型 StreamingT2V&lt;/a&gt;&lt;br&gt;
&lt;strong&gt; 论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.14773&#34;&gt;https://arxiv.org/abs/2403.14773&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Github 地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/Picsart-AI-Research/StreamingT2V&#34;&gt;https://github.com/Picsart-AI-Research/StreamingT2V&lt;/a&gt;（即将开源）&lt;/p&gt;
&lt;h3&gt;&lt;span id=&#34;34-safe-longfactdeepmind-根治大模型幻觉问题&#34;&gt; (34) SAFE + LongFact（DeepMind 根治大模型幻觉问题）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2024.04.03&lt;/strong&gt;&lt;/em&gt;  Google DeepMind 的人工智能专家团队和斯坦福大学的研究者发布了一篇名为《衡量大型语言模型长篇事实性》（Long-form factuality in large language models）的研究论文，研究者们对长篇事实性问题进行了深度探究，并对语言模型在长篇事实性上的表现进行了全面评估。&lt;br&gt;
　　他们推出了一套新的数据集 ——LongFact，其中包含了 2,280 个涵盖 38 个不同话题的引导问题；同时，提出了一个新颖的评估方法 ——SAFE（Self-contained Accuracy with Google Evidence），该方法运用语言模型代理人和 Google 搜索查询技术来进行评估。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;推荐文章：&lt;/strong&gt; &lt;a href=&#34;https://mp.weixin.qq.com/s/jjzmO6vN3sDLYYLH2YMTzg?version=4.1.20.8013&amp;amp;platform=win&#34;&gt;人类标注的时代已经结束？DeepMind 开源 SAFE 根治大模型幻觉问题&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;论文地址：&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.18802&#34;&gt;https://arxiv.org/abs/2403.18802&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Github 地址：&lt;/strong&gt; &lt;a href=&#34;https://github.com/google-deepmind/long-form-factuality&#34;&gt;https://github.com/google-deepmind/long-form-factuality&lt;/a&gt;&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
