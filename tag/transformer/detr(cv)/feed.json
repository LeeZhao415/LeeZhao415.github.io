{
    "version": "https://jsonfeed.org/version/1",
    "title": "且听风吟，御剑于心！ • All posts by \"transformer/detr(cv)\" tag",
    "description": "",
    "home_page_url": "https://leezhao415.github.io",
    "items": [
        {
            "id": "https://leezhao415.github.io/2021/08/07/DETR%EF%BC%9A%E5%9F%BA%E4%BA%8E-Transformers-%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/",
            "url": "https://leezhao415.github.io/2021/08/07/DETR%EF%BC%9A%E5%9F%BA%E4%BA%8E-Transformers-%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/",
            "title": "DETR：基于 Transformers 的目标检测",
            "date_published": "2021-08-07T10:48:23.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#detr%E5%9F%BA%E4%BA%8E-transformers-%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B\">DETR：基于 Transformers 的目标检测</a>\n<ul>\n<li><a href=\"#%E5%89%8D%E8%A8%80\">前言</a></li>\n<li><a href=\"#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C\">相关工作</a></li>\n<li><a href=\"#detr-%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86\">DETR 的实现原理</a>\n<ul>\n<li><a href=\"#cnn\">CNN</a></li>\n</ul>\n</li>\n<li><a href=\"#transformers-encoder-decoder\">Transformers encoder-decoder</a></li>\n<li><a href=\"#ffn\">FFN</a></li>\n<li><a href=\"#%E5%8C%88%E7%89%99%E5%88%A9%E5%8C%B9%E9%85%8D\">匈牙利匹配</a></li>\n<li><a href=\"#%E9%80%9A%E8%BF%87%E5%AF%B9%E6%AF%94-vit-%E6%80%9D%E8%80%83-detr\">通过对比 ViT 思考 DETR</a></li>\n<li><a href=\"#detr-%E8%BF%98%E8%83%BD%E5%81%9A%E5%88%86%E5%89%B2\">DETR 还能做分割</a></li>\n<li><a href=\"#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90\">实验结果分析</a>\n<ul>\n<li><a href=\"#comparison-study\">Comparison Study</a></li>\n<li><a href=\"#ablation-study\">Ablation Study</a></li>\n</ul>\n</li>\n<li><a href=\"#%E7%AE%80%E6%98%93%E4%BB%A3%E7%A0%81\">简易代码</a></li>\n<li><a href=\"#%E7%BB%93%E8%AE%BA\">结论</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h2><span id=\"detr基于-transformers-的目标检测\"> DETR：基于 Transformers 的目标检测</span></h2>\n<h3><span id=\"前言\"> 前言</span></h3>\n<p>最近可以说是随着 ViT 的大火，几乎可以说是一天就能看到一篇基于 Transformers 的 CV 论文，今天给大家介绍的是另一篇由 Facebook 在 ECCV2020 上发表的一篇基于 Transformers 的目标检测论文，这篇论文也是后续相当多的 Transformers 检测 / 分割的 baseline, 透过这篇论文我们来了解其套路.</p>\n<h3><span id=\"相关工作\"> 相关工作</span></h3>\n<p>提到目标检测，我们先来简要回顾一下最基础的一个工作 Faster R-CNN</p>\n<center><img src=\"https://filescdn.proginn.com/1d6c50abbc38128ab241c904721fd40a/20f49d71d223862824251e9f618d8324.webp\" alt=\"img\" style=\"zoom:50%;\"></center>\n<p>Faster R-CNN 第一步是用 CNN 给图像提特征，再通过非极大值抑制算法提取出候选框，最后预测每个候选框的位置和类别，</p>\n<h3><span id=\"detr-的实现原理\"> DETR 的实现原理</span></h3>\n<p>DETR 这篇文章就极大的简化了这个过程，他把候选框提取的过程通过一个标准的 Transformers encoder-decoder 架构代替，在 decoder 部分直接预测出来物体的位置和类别.</p>\n<center><img src=\"https://filescdn.proginn.com/45d878d32204d9a7ced473158b1391ee/19e6480cf1c51b1a692c58a616128bc9.webp\" alt=\"img\" style=\"zoom: 67%;\"></center>\n<p>流程分为三步:</p>\n<ol>\n<li>CNN 提特征</li>\n<li>Transformers 的 encoder-decoder 进行信息的融合</li>\n<li>FFN 预测 class 和 box</li>\n</ol>\n<h4><span id=\"cnn\"> CNN</span></h4>\n<p>利用 resnet-50 网络，将输入的图像 3 X <em>W</em><sub>0</sub> X <em>H</em><sub>0</sub> 变成尺度为 2048 X <em>W</em><sub>0</sub>/32 X <em>H</em><sub>0</sub>/32 的特征，再通过一个 1X1 卷积，将 channel 从 2048 变为更小 (通常 512)</p>\n<h3><span id=\"transformers-encoder-decoder\"> Transformers encoder-decoder</span></h3>\n<p>Transformer encoder 部分首先将输入的特征图降维并 flatten 成 d 个 <em>H</em> X <em>W</em> 维的向量，每个向量作为输入的 token, 由于 Self-attention 是置换不变形的，所以为了体现每个 token 在原图中的顺序关系，我们给每个 token 加上一个 positional encodings. 输出这是对应 Decoder 部分的 V 和 K.</p>\n<p>比如说我们一开始输入的图片是 512*512, 那么 d 应该是 256.</p>\n<p>Transformers decoder 部分是输入是 100 个 Object queries, 比如说我们数据集总共有 100 个类别的物体需要预测，那么这 100 object queries 经过 Transformers decoder 之后会预测出若干类别的物体和位置信息.</p>\n<center><img src=\"https://filescdn.proginn.com/7c8222d0fa35c09590751d12a0cb60b1/8a3a853c4037a1e1ea3927e14a252369.webp\" alt=\"img\" style=\"zoom:67%;\"></center>\n<p>作者发现在训练过程中在 decoder 中使用 auxiliary losses 很有帮助，特别是有助于模型输出正确数量的每个类的对象。</p>\n<h3><span id=\"ffn\"> FFN</span></h3>\n<p>DETR 在每个解码器层之后添加预测 FFN 和 Hungarian loss，所有预测 FFN 共享其参数。我们使用附加的共享层范数来标准化来自不同解码器层的预测 FFN 的输入，FFN 是一个最简单的多层感知机模块，对 Transformers decoder 的输出预测每个 object query 的类别和位置信息。在实际训练的过程中，通过匈牙利算法匹配预测和标签最小的损失，仅适用配对上的 query 计算 loss 回传梯度.</p>\n<p>loss 包括 Box loss 和 class loss</p>\n<p>Box Loss 包括 IOUloss 和 L1loss, 这个原理很简单.</p>\n<p>where  are hyperparameters and  is the generalized IoU [38]:</p>\n<p>class loss 就是最简单的交叉熵了.</p>\n<h3><span id=\"匈牙利匹配\"> 匈牙利匹配</span></h3>\n<p>匈牙利匹配算法是离散数学中图论部分的一个经典算法，描述的问题是一个二分图的最大匹配。换成人话来说就是这个二分图分成两部分，一部分是我们对 100 种 object query 预测的结果，另一部分是实际的标签，由于我们一开始是不知道这 100 个 object query 输入的时候应该预测那些类别的物体，有可能一开始第一个 token 预测的是 A 物体，第二个 token 预测的是 C 物体，总而言之是无序的，我们就要根据实际的 label, 找到预测结果中和他最接近的计算 loss. 其他没匹配上的则不计算 loss 回传梯度。下面这张图一目了然:</p>\n<center><img src=\"https://filescdn.proginn.com/c543cb1abdfeacf431fae3d0c23f5067/75f00e1c21f1f9f8aafb922475704ca3.webp\" alt=\"img\" style=\"zoom:67%;\"></center>\n<h3><span id=\"通过对比-vit-思考-detr\"> 通过对比 ViT 思考 DETR</span></h3>\n<p>其实笔者在阅读这篇文章的时候更加重点的是对比 ViT 在一些实现细节上的不同之处，</p>\n<center><img src=\"https://filescdn.proginn.com/4f698e89fac1c0f6306728900f821dde/1ddbec9e3a7cda74289d8d094b6d3d4d.webp\" alt=\"img\" style=\"zoom:67%;\"></center>\n<ol>\n<li>首先 ViT 是没有使用 CNN 的，而 DETR 是先用 CNN 提取了图像的特征</li>\n<li>ViT 只使用了 Transformers-encoder, 在 encoder 的时候额外添加了一个 Class token 来预测图像类型，而 DETR 的 object token 则是通过 Decoder 学习的.</li>\n<li>DETR 和 VIT 中的 Transformers 在 encoder 部分都使用了 Position Embedding, 但是使用的并不一样，而 VIT 在使用的 Position Embedding 也是笔者一开始阅读文献的疑惑所在.</li>\n<li>DETR 的 Transformers encoder 使用的 feature 的每一个 pixel 作为 token embeddings 输入，而 ViT 则是直接把图像切成 16*16 个 Patch, 每个 patch 直接拉平作为 token embeddings</li>\n<li>相比较 VIT,DETR 更接近原始的 Transformers 架构.</li>\n</ol>\n<h3><span id=\"detr-还能做分割\"> DETR 还能做分割</span></h3>\n<center><img src=\"https://filescdn.proginn.com/b24f3678c5314f42eece7d97d5a8113e/07033f9208979fc87463922a1b880492.webp\" alt=\"img\" style=\"zoom: 90%;\"></center>\n<ol>\n<li>首先检测 box</li>\n<li>对每个 box 做分割</li>\n<li>为每个像素的类别投票</li>\n</ol>\n<p>作者在这篇论文在并没有详细讲实现细节，但是今年 CVPR2021 上发表的 SETR 则是重点讲如何利用 Transformers 做分割，我们下次细讲.</p>\n<h3><span id=\"实验结果分析\"> 实验结果分析</span></h3>\n<h4><span id=\"comparison-study\"> Comparison Study</span></h4>\n<center><img src=\"https://filescdn.proginn.com/be267a0d4f97e447624fd4a309d6cfda/11c96a44231d180d6eb6fd98d1c23fd3.webp\" alt=\"img\" style=\"zoom:50%;\"></center>\n<p>对比的是检测领域最经典的 Faster R-CNN, 可以看得出来了在同等参数两的情况下，在大目标物体的检测结果优于 Faster R-CNN, 道理嘛作者说是 Transformers 可以更关注全局信息.</p>\n<h4><span id=\"ablation-study\"> Ablation Study</span></h4>\n<center><img src=\"https://filescdn.proginn.com/71775ca96643ecd9b0102a86503fa333/cc9864649b6ca5eb89a580b205c645bd.webp\" alt=\"img\" style=\"zoom: 60%;\"></center>\n<ol>\n<li>Decoder 比 Encoder 重要</li>\n<li>Decoder 具有隐含的 “锚”，这对检测至关重要</li>\n<li>Encoder 仅帮助聚合同一对象的像素，减轻 decoder 的负担</li>\n</ol>\n<center><img src=\"https://filescdn.proginn.com/e9b2f4326a970e9db04c311e1efc7d3c/3519e6a3afba1f87dce95e9d901e5aa6.webp\" alt=\"img\" style=\"zoom:55%;\"></center>\n<p>在位置编码部分，作者对比了可学习的位置编码和基于 Sincos 函数的位置编码方法 (也就是原始 Transformers 的位置编码方法) 可以看得出来效果是 Sincos 的更好，但是都显著好于不加位置编码，因为作者也在原文中 Self-Attention 是并行的，他如果不加位置编码的话是置换不变性的 (这个看 Attention is All you Need 原文)</p>\n<center><img src=\"https://filescdn.proginn.com/7a815e66a7ef4c06a9acc9002b89be62/d5cccf07b43347dcbd1d047eb343959c.webp\" alt=\"img\" style=\"zoom: 60%;\"></center>\n<p>这个嘛，就是很简单的实验了，验证一下 loss 每个部分的作用，基本上就是格式化的东西.</p>\n<h3><span id=\"简易代码\"> 简易代码</span></h3>\n<p>作者最后在附录部分贴上了简易的代码实现细节</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch </span><br><span class=\"line\"><span class=\"keyword\">from</span> torch </span><br><span class=\"line\"><span class=\"keyword\">import</span> nn </span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision.models <span class=\"keyword\">import</span> resnet50</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DETR</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, num_classes, hidden_dim, nheads,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               num_encoder_layers, num_decoder_layers</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__() </span><br><span class=\"line\">        <span class=\"comment\"># We take only convolutional layers from ResNet-50 model</span></span><br><span class=\"line\">        self.backbone=nn.Sequential(</span><br><span class=\"line\">          *<span class=\"built_in\">list</span>(resnet50(pretrained=<span class=\"literal\">True</span>).children())[:-<span class=\"number\">2</span>])</span><br><span class=\"line\">        self.conv = nn.Conv2d(<span class=\"number\">2048</span>, hidden_dim, <span class=\"number\">1</span>) </span><br><span class=\"line\">        self.transformer = nn.Transformer(hidden_dim, nheads,</span><br><span class=\"line\">                                          num_encoder_layers,</span><br><span class=\"line\">                                          num_decoder_layers)</span><br><span class=\"line\">        self.linear_class = nn.Linear(hidden_dim, num_classes + <span class=\"number\">1</span>)</span><br><span class=\"line\">        self.linear_bbox = nn.Linear(hidden_dim, <span class=\"number\">4</span>) </span><br><span class=\"line\">        self.query_pos =nn.Parameter(torch.rand(<span class=\"number\">100</span>, hidden_dim))</span><br><span class=\"line\">        self.row_embed = nn.Parameter(torch.rand(<span class=\"number\">50</span>, hidden_dim // <span class=\"number\">2</span>))</span><br><span class=\"line\">        self.col_embed = nn.Parameter(torch.rand(<span class=\"number\">50</span>, hidden_dim // <span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, inputs</span>):</span></span><br><span class=\"line\">        x = self.backbone(inputs) h = self.conv(x)</span><br><span class=\"line\">        H,W=h.shape[-<span class=\"number\">2</span>:]</span><br><span class=\"line\">        pos = torch.cat([</span><br><span class=\"line\">          self.col_embed[:W].unsqueeze(<span class=\"number\">0</span>).repeat(H, <span class=\"number\">1</span>, <span class=\"number\">1</span>),</span><br><span class=\"line\">          self.row_embed[:H].unsqueeze(<span class=\"number\">1</span>).repeat(<span class=\"number\">1</span>, W, <span class=\"number\">1</span>), ],</span><br><span class=\"line\">          dim=-<span class=\"number\">1</span>).flatten(<span class=\"number\">0</span>, <span class=\"number\">1</span>).unsqueeze(<span class=\"number\">1</span>) </span><br><span class=\"line\">        h = self.transformer(pos + </span><br><span class=\"line\">                             h.flatten(<span class=\"number\">2</span>).permute(<span class=\"number\">2</span>, <span class=\"number\">0</span>,<span class=\"number\">1</span>),</span><br><span class=\"line\">                             self.query_pos.unsqueeze(<span class=\"number\">1</span>)) </span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.linear_class(h), self.linear_bbox(h).sigmoid()</span><br><span class=\"line\"></span><br><span class=\"line\">detr = DETR(num_classes=<span class=\"number\">91</span>, hidden_dim=<span class=\"number\">256</span>, nheads=<span class=\"number\">8</span>, num_encoder_layers=<span class=\"number\">6</span>, num_decoder_layers=<span class=\"number\">6</span>)</span><br><span class=\"line\">detr.<span class=\"built_in\">eval</span>() </span><br><span class=\"line\">inputs = torch.randn(<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">800</span>, <span class=\"number\">1200</span>) </span><br><span class=\"line\">logits, bboxes = detr(inputs)</span><br></pre></td></tr></table></figure>\n<h3><span id=\"结论\"> 结论</span></h3>\n<p>一篇很简单的 Transformers 在目标检测上的应用，也是最近大火的 Transformers 系列必引的一篇论文，我觉得他和 VIT 代表了 CV 对 Transformers 架构的两种看法吧，VIT 是只用 Encoder, 这也是目前最主流的做法，而 DETR 则是运用了 CNN 和 Transformers encoder-decoder 的结合，从 motivation 上来说我个人更喜欢 DETR, 这段时间也基本上把 Transformers 一系列都读完了，会以一个系列调几篇好的论文讲解 (水文实在是太多了)。</p>\n",
            "tags": [
                "人工智能/CV",
                "Transformer/DETR(CV)"
            ]
        }
    ]
}