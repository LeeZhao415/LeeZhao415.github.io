{
    "version": "https://jsonfeed.org/version/1",
    "title": "且听风吟，御剑于心！ • All posts by \"模型部署\" tag",
    "description": "",
    "home_page_url": "https://leezhao415.github.io",
    "items": [
        {
            "id": "https://leezhao415.github.io/2022/07/26/ONNX%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0%E4%B8%8E%E4%BB%A3%E7%A0%81%E6%A3%80%E6%9F%A5/",
            "url": "https://leezhao415.github.io/2022/07/26/ONNX%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0%E4%B8%8E%E4%BB%A3%E7%A0%81%E6%A3%80%E6%9F%A5/",
            "title": "ONNX模型构造与代码检查",
            "date_published": "2022-07-26T15:51:31.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#onnx%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0%E4%B8%8E%E4%BB%A3%E7%A0%81%E6%A3%80%E6%9F%A5\">ONNX 模型构造与代码检查</a>\n<ul>\n<li><a href=\"#1-%E6%9E%84%E9%80%A0%E6%8F%8F%E8%BF%B0%E5%BC%A0%E9%87%8F%E4%BF%A1%E6%81%AF%E7%9A%84%E5%AF%B9%E8%B1%A1valueinfoproto\">1 构造描述张量信息的对象 <code>ValueInfoProto</code> </a></li>\n<li><a href=\"#2-%E6%9E%84%E9%80%A0%E7%AE%97%E5%AD%90%E8%8A%82%E7%82%B9%E4%BF%A1%E6%81%AFnodeproto\">2 构造算子节点信息 <code>NodeProto</code> </a></li>\n<li><a href=\"#3-%E6%9E%84%E9%80%A0%E8%AE%A1%E7%AE%97%E5%9B%BEgraphproto\">3 构造计算图 <code>GraphProto</code> </a></li>\n<li><a href=\"#4-%E5%B0%81%E8%A3%85%E8%AE%A1%E7%AE%97%E5%9B%BE\">4 封装计算图</a></li>\n<li><a href=\"#5-%E6%A3%80%E6%9F%A5%E4%BB%A3%E7%A0%81\">5 检查代码</a></li>\n</ul>\n</li>\n<li><a href=\"#onnx-python-api-%E6%9E%84%E9%80%A0%E6%A8%A1%E5%9E%8B%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81\">ONNX Python API 构造模型完整代码</a></li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h4><span id=\"onnx-模型构造与代码检查\"> ONNX 模型构造与代码检查</span></h4>\n<p>参考博客:<a href=\"https://zhuanlan.zhihu.com/p/516920606\">https://zhuanlan.zhihu.com/p/516920606</a></p>\n<h5><span id=\"1-构造描述张量信息的对象-valueinfoproto\"> 1 构造描述张量信息的对象 <code>ValueInfoProto</code></span></h5>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import onnx </span><br><span class=\"line\">from onnx import helper </span><br><span class=\"line\">from onnx import TensorProto </span><br><span class=\"line\"> </span><br><span class=\"line\">a = helper.make_tensor_value_info(<span class=\"string\">&#x27;a&#x27;</span>, TensorProto.FLOAT, [10, 10]) </span><br><span class=\"line\">x = helper.make_tensor_value_info(<span class=\"string\">&#x27;x&#x27;</span>, TensorProto.FLOAT, [10, 10]) </span><br><span class=\"line\">b = helper.make_tensor_value_info(<span class=\"string\">&#x27;b&#x27;</span>, TensorProto.FLOAT, [10, 10]) </span><br><span class=\"line\">output = helper.make_tensor_value_info(<span class=\"string\">&#x27;output&#x27;</span>, TensorProto.FLOAT, [10, 10]) </span><br></pre></td></tr></table></figure>\n<h5><span id=\"2-构造算子节点信息-nodeproto\"> 2 构造算子节点信息 <code>NodeProto</code></span></h5>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mul = helper.make_node(<span class=\"string\">&#x27;Mul&#x27;</span>, [<span class=\"string\">&#x27;a&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>], [<span class=\"string\">&#x27;c&#x27;</span>]) </span><br><span class=\"line\">add = helper.make_node(<span class=\"string\">&#x27;Add&#x27;</span>, [<span class=\"string\">&#x27;c&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>], [<span class=\"string\">&#x27;output&#x27;</span>]) </span><br></pre></td></tr></table></figure>\n<h5><span id=\"3-构造计算图-graphproto\"> 3 构造计算图 <code>GraphProto</code></span></h5>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graph = helper.make_graph([mul, add], <span class=\"string\">&#x27;linear_func&#x27;</span>, [a, x, b], [output]) </span><br></pre></td></tr></table></figure>\n<h5><span id=\"4-封装计算图\"> 4 封装计算图</span></h5>\n<p>用  <code>helper.make_model</code>  把计算图  <code>GraphProto</code>  封装进模型  <code>ModelProto</code></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model = helper.make_model(graph) </span><br></pre></td></tr></table></figure>\n<h5><span id=\"5-检查代码\"> 5 检查代码</span></h5>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onnx.checker.check_model(model) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(model) </span><br><span class=\"line\">onnx.save(model, <span class=\"string\">&#x27;linear_func.onnx&#x27;</span>) </span><br></pre></td></tr></table></figure>\n<h4><span id=\"onnx-python-api-构造模型完整代码\"> ONNX Python API 构造模型完整代码</span></h4>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import onnx </span><br><span class=\"line\">from onnx import helper </span><br><span class=\"line\">from onnx import TensorProto </span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># input and output </span></span><br><span class=\"line\">a = helper.make_tensor_value_info(<span class=\"string\">&#x27;a&#x27;</span>, TensorProto.FLOAT, [10, 10]) </span><br><span class=\"line\">x = helper.make_tensor_value_info(<span class=\"string\">&#x27;x&#x27;</span>, TensorProto.FLOAT, [10, 10]) </span><br><span class=\"line\">b = helper.make_tensor_value_info(<span class=\"string\">&#x27;b&#x27;</span>, TensorProto.FLOAT, [10, 10]) </span><br><span class=\"line\">output = helper.make_tensor_value_info(<span class=\"string\">&#x27;output&#x27;</span>, TensorProto.FLOAT, [10, 10]) </span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># Mul </span></span><br><span class=\"line\">mul = helper.make_node(<span class=\"string\">&#x27;Mul&#x27;</span>, [<span class=\"string\">&#x27;a&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>], [<span class=\"string\">&#x27;c&#x27;</span>]) </span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># Add </span></span><br><span class=\"line\">add = helper.make_node(<span class=\"string\">&#x27;Add&#x27;</span>, [<span class=\"string\">&#x27;c&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>], [<span class=\"string\">&#x27;output&#x27;</span>]) </span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># graph and model </span></span><br><span class=\"line\">graph = helper.make_graph([mul, add], <span class=\"string\">&#x27;linear_func&#x27;</span>, [a, x, b], [output]) </span><br><span class=\"line\">model = helper.make_model(graph) </span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># save model </span></span><br><span class=\"line\">onnx.checker.check_model(model) </span><br><span class=\"line\"><span class=\"built_in\">print</span>(model) </span><br><span class=\"line\">onnx.save(model, <span class=\"string\">&#x27;linear_func.onnx&#x27;</span>) </span><br></pre></td></tr></table></figure>\n",
            "tags": [
                "人工智能",
                "模型部署"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2022/03/06/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E4%B8%BB%E6%B5%81%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E7%90%86%E6%9E%B6%E6%9E%84/",
            "url": "https://leezhao415.github.io/2022/03/06/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E4%B8%BB%E6%B5%81%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E7%90%86%E6%9E%B6%E6%9E%84/",
            "title": "【精华】主流的深度学习推理架构",
            "date_published": "2022-03-06T07:15:30.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#%E4%B8%BB%E6%B5%81%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E7%90%86%E6%9E%B6%E6%9E%84\">主流的深度学习推理架构</a>\n<ul>\n<li><a href=\"#1ncnn\">（1）NCNN</a></li>\n<li><a href=\"#2openvino\">（2）OpenVino</a></li>\n<li><a href=\"#3tensorrt\">（3）TensorRT</a></li>\n<li><a href=\"#4mediapipe\">（4）MediaPipe</a></li>\n<li><a href=\"#5onnx\">（5）ONNX</a></li>\n<li><a href=\"#6mnn\">（6）MNN</a></li>\n<li><a href=\"#7mace\">（7）MACE</a></li>\n<li><a href=\"#8tnn\">（8）TNN</a></li>\n<li><a href=\"#9tvm\">（9）TVM</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h4><span id=\"主流的深度学习推理架构\"> 主流的深度学习推理架构</span></h4>\n<p>发布时间: 2021-10-20 12:29</p>\n<p>以深度学习为主的人工智能算法模型在日常 AI 应用中逐渐占据主流方向，相关的各类产品也是层出不穷。我们平时所看到的 AI 产品，像刷脸支付、智能语音、银行的客服机器人等，都是 AI 算法的具体落地应用。AI 技术在具体落地应用方面，和其他软件技术一样，也需要具体的部署和实施的。既然要做部署，那就会有不同平台设备上的各种不同的部署方法和相关的部署架构工具，目前在人工智能的落地部署方面，各大平台机构也都是大展身手，纷纷推出自家的部署平台。</p>\n<p>目前市场上应用最广泛的部署工具主要有以下几种：</p>\n<ul>\n<li>\n<p>腾讯公司开发的移动端平台部署工具 —— <code>NCNN</code></p>\n</li>\n<li>\n<p>Intel 公司针对自家设备开开发的部署工具 —— <code>OpenVino</code></p>\n</li>\n<li>\n<p>NVIDIA 公司针对自家 GPU 开发的部署工具 —— <code>TensorRT</code></p>\n</li>\n<li>\n<p>Google 针对自家硬件设备和深度学习框架开发的部署工具 —— <code>MediaPipe</code></p>\n</li>\n<li>\n<p>由微软、亚马逊、Facebook 和 IBM 等公司共同开发的开放神经网络交换格式 —— <code>ONNX</code> (Open Neural Network Exchange)</p>\n</li>\n<li>\n<p>阿里巴巴公司开发的移动端部署工具 ——MNN</p>\n</li>\n<li>\n<p>小米公司开发的移动端平台部署工具 ——MACE</p>\n</li>\n<li>\n<p>腾讯公司基于 Rapidnet、ncnn 开发的平台部署工具 ——TNN</p>\n</li>\n<li>\n<p>华盛顿大学的 SAMPL 组开发的平台部署工具 ——TVM</p>\n</li>\n</ul>\n<p>除此之外，还有一些深度学习框架有自己的专用部署服务：</p>\n<ul>\n<li>\n<p>TensorFlow 自己提供的部署服务： <code>TensorFlow Serving</code> 、 <code>TensorFlow Lite</code></p>\n</li>\n<li>\n<p>Pytorch 自己提供的部署服务： <code>libtorch</code></p>\n</li>\n</ul>\n<center><img src=\"https://pics4.baidu.com/feed/6f061d950a7b02089df539b2dc3cb9da562cc890.jpeg?token=8a2b1fa318f2ae76490ea6bd4c360c36\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>本文主要是针对这些不同的部署工具做一个简单的分析，对比一下各家不同的部署工具到底有哪些优势和不足之处，方便大家在做部署的时候能够找到适合自己的项目的部署方法。具体的各种不同的部署工具的下载安装和使用方法会在后续的文章中做出详细的教程，关注深度人工智能学院，了解最实用的人工智能干货知识。</p>\n<h5><span id=\"1ncnn\"> （1）NCNN</span></h5>\n<p><a href=\"https://github.com/Tencent/ncnn\">Github 地址</a></p>\n<p>NCNN 是腾讯优图实验室首个开源项目，是一个为手机端极致优化的高性能神经网络前向计算框架。并在 2017 年 7 月正式开源。NCNN 做为腾讯优图最 “火” 的开源项目之一，是一个为手机端极致优化的高性能神经网络前向计算框架，在设计之初便将手机端的特殊场景融入核心理念，是业界首个为移动端优化的开源神经网络推断库。能实现无第三方依赖，跨平台操作，在手机端 CPU 运算速度在开源框架中处于领先水平。基于该平台，开发者能够轻松将深度学习算法移植到手机端，输出高效的执行，进而产出人工智能 APP，将 AI 技术带到用户指尖。</p>\n<p>NCNN 从设计之初深刻考虑手机端的部署和使用。无第三方依赖，跨平台，手机端 CPU 的速度快于目前所有已知的开源框架。基于 NCNN，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP，将 AI 带到你的指尖。NCNN 目前已在腾讯多款应用中使用，如 QQ，Qzone，微信，天天 P 图等。</p>\n<p>下面是 NCNN 在各大系统平台的应用发展状态情况：</p>\n<center><img src=\"https://pics0.baidu.com/feed/d50735fae6cd7b89012233e4b1c109aed8330e7d.jpeg?token=e956710da06ddaa90696d2d0a71dda12\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>从 NCNN 的发展矩阵可以看出，NCNN 覆盖了几乎所有常用的系统平台，尤其是在移动平台上的适用性更好，在 Linux、Windows 和 Android、以及 iOS、macOS 平台上都可以使用 GPU 来部署模型。</p>\n<p>根据官方的功能描述，NCNN 在各方面的性能都比较优良：</p>\n<ul>\n<li>\n<p>支持卷积神经网络，支持多输入和多分支结构，可计算部分分支</p>\n</li>\n<li>\n<p>无任何第三方库依赖，不依赖 BLAS/NNPACK 等计算框架</p>\n</li>\n<li>\n<p>纯 C++ 实现，跨平台，支持 android ios 等</p>\n</li>\n<li>\n<p>ARM NEON 汇编级良心优化，计算速度极快</p>\n</li>\n<li>\n<p>精细的内存管理和数据结构设计，内存占用极低</p>\n</li>\n<li>\n<p>支持多核并行计算加速，ARM big.LITTLE cpu 调度优化</p>\n</li>\n<li>\n<p>支持基于全新低消耗的 vulkan api GPU 加速</p>\n</li>\n<li>\n<p>整体库体积小于 700K，并可轻松精简到小于 300K</p>\n</li>\n<li>\n<p>可扩展的模型设计，支持 8bit 量化 和半精度浮点存储，可导入 caffe/pytorch/mxnet/onnx/darknet/keras/tensorflow (mlir) 模型</p>\n</li>\n<li>\n<p>支持直接内存零拷贝引用加载网络模型</p>\n</li>\n<li>\n<p>可注册自定义层实现并扩展</p>\n</li>\n</ul>\n<p>除此之外，NCNN 在对各种硬件设备的支持上也非常给力：</p>\n<center><img src=\"https://pics2.baidu.com/feed/bf096b63f6246b60d6fee1c5561d5145530fa2fa.jpeg?token=d7c9a1ea55b036d1da0cb1f7600f8986\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>NCNN 的官方代码地址：<a href=\"https://github.com/Tencent/ncnn\">https://github.com/Tencent/ncnn</a></p>\n<p>移动端的部署工具除了 NCNN，还有<strong>华盛顿大学的 TVM、阿里的 MNN、小米的 MACE、腾讯优图基于 NCNN 开发的 TNN</strong> 等推理部署工具。</p>\n<h5><span id=\"2openvino\"> （2）OpenVino</span></h5>\n<p><a href=\"https://github.com/openvinotoolkit/openvino\">Github 地址</a></p>\n<p>OpenVINO 工具套件全称是<strong> O</strong>pen <strong>V</strong>isual <strong>I</strong>nference &amp; <strong>N</strong>eural Network <strong>O</strong>ptimization，是 Intel 于 2018 年发布的，<strong>开源、商用免费</strong>、主要应用于计算机视觉、实现神经网络模型优化和推理计算 (Inference) 加速的软件工具套件。由于其商用免费，且可以把深度学习模型部署在英尔特 CPU 和集成 GPU 上，大大节约了显卡费用，所以越来越多的深度学习应用都使用 OpenVINO 工具套件做深度学习模型部署。</p>\n<p>OpenVINO 是一个 Pipeline 工具集，同时可以兼容各种开源框架训练好的模型，拥有算法模型上线部署的各种能力，只要掌握了该工具，你可以轻松的将预训练模型在 Intel 的 CPU 上快速部署起来。</p>\n<p>对于 AI 工作负载来说，OpenVINO 提供了深度学习推理套件（DLDT)，该套件可以将各种开源框架训练好的模型进行线上部署，除此之外，还包含了图片处理工具包 OpenCV，视频处理工具包 Media SDK，用于处理图像视频解码，前处理和推理结果后处理等。</p>\n<p>在做推理的时候，大多数情况需要前处理和后处理，前处理如通道变换，取均值，归一化，Resize 等，后处理是推理后，需要将检测框等特征叠加至原图等，都可以使用 OpenVINO 工具套件里的 API 接口完成。</p>\n<p>OpenVino 目前支持 Linux、Windows、macOS、Raspbian 等系统平台。</p>\n<center><img src=\"https://pics0.baidu.com/feed/242dd42a2834349b323b4f890d0f5ec737d3beef.jpeg?token=aec96dcaf69a1e9c03e87883d6ae1098\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p><strong>OpenVINO 工具套件</strong>主要包括：Model Optimizer (模型优化器)—— 用于优化神经网络模型的工具，Inference Engine (推理引擎)—— 用于加速推理计算的软件包。</p>\n<center><img src=\"https://pics4.baidu.com/feed/00e93901213fb80e43b8685289346427b838943c.jpeg?token=078fc24b610382ff85ef46331734a2ab\" alt=\"img\"></center>\n<p>模型优化器是一个 python 脚本工具，用于将开源框架训练好的模型转化为推理引擎可以识别的中间表达，其实就是两个文件，xml 和 bin 文件，前者是网络结构的描述，后者是权重文件。模型优化器的作用包括压缩模型和加速，比如，去掉推理无用的操作 (Dropout)，层的融合 (Conv + BN + Relu)，以及内存优化。</p>\n<p>推理引擎是一个支持 C\\C++ 和 python 的一套 API 接口，需要开发人员自己实现推理过程的开发，开发流程其实非常的简单，核心流程如下：</p>\n<ul>\n<li>装载处理器的插件库</li>\n<li>读取网络结构和权重</li>\n<li>配置输入和输出参数</li>\n<li>装载模型</li>\n<li>创建推理请求</li>\n<li>准备输入 Data</li>\n<li>推理</li>\n<li>结果处理</li>\n</ul>\n<p>OpenVino 工具套件的工作流程图：</p>\n<center><img src=\"https://pics0.baidu.com/feed/d000baa1cd11728b87604edf0c1988c7c1fd2ce9.jpeg?token=63445328b92e67bc8d2a9a170e4b2c25\" alt=\"img\"></center>\n<p>OpenVino 的官方地址：<a href=\"https://docs.openvinotoolkit.org/latest/index.html\">https://docs.openvinotoolkit.org/latest/index.html</a></p>\n<p>提醒一下，大家不要去下面这个网站，因为这个网站是一个酿酒厂的网站：</p>\n<p><a href=\"https://openvino.org/\">https://openvino.org/</a></p>\n<h5><span id=\"3tensorrt\"> （3）TensorRT</span></h5>\n<p><a href=\"https://github.com/NVIDIA/TensorRT\">Github 地址</a></p>\n<p>TensorRT 是 NVIDIA 开发的一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT 可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT 现已能支持 TensorFlow、Caffe、Mxnet、Pytorch 等几乎所有的深度学习框架，将 TensorRT 和 NVIDIA 的 GPU 结合起来，能在几乎所有的框架中进行快速和高效的部署推理。</p>\n<p>TensorRT 是一个 C<ins> 库，从 TensorRT 3 开始提供 C</ins> API 和 Python API，主要用来针对 NVIDIA GPU 进行高性能推理（Inference）加速，它可为深度学习推理应用提供低延迟和高吞吐量。在推理期间，基于 TensorRT 的应用比仅 CPU 平台的执行速度快 40 倍。</p>\n<center><img src=\"https://pics3.baidu.com/feed/08f790529822720ec0468fffab2e414ff31fab1e.jpeg?token=a7ba6e4e9e0bef3fc90c2f4d12d6de1d\" alt=\"img\" style=\"zoom:70%;\"></center>\n<p>一般的深度学习项目，训练时为了加快速度，会使用多 GPU 分布式训练。但在部署推理时，为了降低成本，往往使用单个 GPU 机器甚至嵌入式平台（比如 NVIDIA Jetson）进行部署，部署端也要有与训练时相同的深度学习环境，如 caffe，TensorFlow 等。由于训练的网络模型可能会很大（比如，inception，resnet 等），参数很多，而且部署端的机器性能存在差异，就会导致推理速度慢，延迟高。这对于那些高实时性的应用场合是致命的，比如自动驾驶要求实时目标检测，目标追踪等。所以为了提高部署推理的速度，出现了很多轻量级神经网络，比如 squeezenet，mobilenet，shufflenet 等。基本做法都是基于现有的经典模型提出一种新的模型结构，然后用这些改造过的模型重新训练，再重新部署。</p>\n<p>而 TensorRT 则是对训练好的模型进行优化。TensorRT 就只是推理优化器。当你的网络训练完之后，可以将训练模型文件直接丢进 TensorRT 中，而不再需要依赖深度学习框架（Caffe，TensorFlow 等）</p>\n<center><img src=\"https://pics6.baidu.com/feed/a2cc7cd98d1001e9731508d205eb30e555e79734.jpeg?token=ee687e606a9c8ac3a55277dcf2bed804\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>可以认为 TensorRT 是一个只有前向传播的深度学习推理框架，这个框架可以将 Caffe，TensorFlow，PyTorch 等网络模型解析，然后与 TensorRT 中对应的层进行一一映射，把其他框架的模型统一全部转换到 TensorRT 中，然后在 TensorRT 中可以针对 NVIDIA 自家 GPU 实施优化策略，并进行部署加速。</p>\n<center><img src=\"https://pics4.baidu.com/feed/9c16fdfaaf51f3de2f9367d7540bbb163b29794b.jpeg?token=e99e9155d9ea0bee44a6ae020a9c8537\" alt=\"img\"></center>\n<p>TensorRT 依赖于 Nvidia 的深度学习硬件环境，可以是 GPU 也可以是 DLA，如果没有的话则无法使用。TensorRT 支持目前大部分的神经网络 Layer 的定义，同时提供了 API 让开发者自己实现特殊 Layer 的操作。</p>\n<p>TensorRT 基于 CUDA，NVIDIA 的并行编程模型，能够利用 CUDA-X AI 中的库、开发工具和技术，为人工智能、自动机器、高性能计算和图形优化所有深度学习框架的推理。</p>\n<p>TensorRT 的部署分为两个部分：</p>\n<ol>\n<li>\n<p>优化训练好的模型并生成计算流图</p>\n</li>\n<li>\n<p>使用 TensorRT Runtime 部署计算流图</p>\n</li>\n</ol>\n<p><strong>TensorRT 的部署流程</strong>：</p>\n<center><img src=\"https://pics0.baidu.com/feed/3b87e950352ac65cd5e1c02b4617f91891138af5.jpeg?token=30908a1c4e1e018e68b4b5fe68d913eb\" alt=\"img\"></center>\n<p><strong>TensorRT 的模型导入流程</strong>：</p>\n<center><img src=\"https://pics6.baidu.com/feed/b3fb43166d224f4aa822e448cd12db5b9922d125.jpeg?token=5ebfd55ec0ce398da0190460f29f21cd\" alt=\"img\" style=\"zoom: 67%;\"></center>\n<p><strong>TensorRT 的优化过程</strong>：</p>\n<center><img src=\"https://pics1.baidu.com/feed/4e4a20a4462309f72aafe27acfeb47fad5cad681.jpeg?token=2048eb6e71d6a01d744bccf83dc96a10\" alt=\"img\"></center>\n<p>网络模型在导入至 TensorRT 后会进行一系列的优化，主要优化内容如下图所示</p>\n<p>TensorRT 官网下载地址：<a href=\"https://developer.nvidia.com/zh-cn/tensorrt\">https://developer.nvidia.com/zh-cn/tensorrt</a></p>\n<p>开发者指南：<a href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html\">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html</a></p>\n<p>Github 地址：<a href=\"https://github.com/NVIDIA/TensorRT\">https://github.com/NVIDIA/TensorRT</a></p>\n<h5><span id=\"4mediapipe\"> （4）MediaPipe</span></h5>\n<p><a href=\"https://github.com/google/mediapipe\">Github 地址</a></p>\n<p>MediaPipe 是一款由 Google Research 开发并开源的多媒体机器学习模型应用框架。在谷歌，一系列重要产品，如 YouTube、Google Lens、ARCore、Google Home 以及 Nest，都已深度整合了 MediaPipe。</p>\n<p>MediaPipe 是一个基于图形的跨平台框架，用于构建多模式（视频，音频和传感器）应用的机器学习管道。MediaPipe 可在移动设备、工作站和服务器上跨平台运行，并支持移动 GPU 加速。使用 MediaPipe，可以将应用的机器学习管道构建为模块化组件的图形。MediaPipe 不仅可以被部署在服务器端，更可以在多个移动端 （安卓和苹果 iOS）和嵌入式平台（Google Coral 和树莓派）中作为设备端机器学习推理 （On-device Machine Learning Inference）框架。</p>\n<center><img src=\"https://pics0.baidu.com/feed/377adab44aed2e73a34652fe3be4ea8285d6faa0.jpeg?token=8e65a91d6b3cdde3d34cd6b1985c12d5\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>一款多媒体机器学习应用的成败除了依赖于模型本身的好坏，还取决于设备资源的有效调配、多个输入流之间的高效同步、跨平台部署上的便捷程度、以及应用搭建的快速与否。</p>\n<p>基于这些需求，谷歌开发并开源了 MediaPipe 项目。除了上述的特性，MediaPipe 还支持 TensorFlow 和 TF Lite 的推理引擎（Inference Engine），任何 TensorFlow 和 TF Lite 的模型都可以在 MediaPipe 上使用。同时，在移动端和嵌入式平台，MediaPipe 也支持设备本身的 GPU 加速。</p>\n<p>MediaPipe 专为机器学习（ML）从业者而设计，包括研究人员，学生和软件开发人员，他们实施生产就绪的 ML 应用程序，发布伴随研究工作的代码，以及构建技术原型。MediaPipe 的主要用例是使用推理模型和其他可重用组件对应用机器学习管道进行快速原型设计。MediaPipe 还有助于将机器学习技术部署到各种不同硬件平台上的演示和应用程序中。</p>\n<p>MediaPipe 的核心框架由 C++ 实现，并提供 Java 以及 Objective C 等语言的支持。MediaPipe 的主要概念包括数据包（Packet）、数据流（Stream）、计算单元（Calculator）、图（Graph）以及子图（Subgraph）。数据包是最基础的数据单位，一个数据包代表了在某一特定时间节点的数据，例如一帧图像或一小段音频信号；数据流是由按时间顺序升序排列的多个数据包组成，一个数据流的某一特定时间戳（Timestamp）只允许至多一个数据包的存在；而数据流则是在多个计算单元构成的图中流动。MediaPipe 的图是有向的 —— 数据包从数据源（Source Calculator 或者 Graph Input Stream）流入图直至在汇聚结点（Sink Calculator 或者 Graph Output Stream） 离开。</p>\n<center><img src=\"https://pics1.baidu.com/feed/d0c8a786c9177f3e5217be81b52a70ce9e3d560d.jpeg?token=a9eaa07c36f8e834d24c104df4d110bf\" alt=\"img\" style=\"zoom:90%;\"></center>\n<p>MediaPipe 在开源了多个由谷歌内部团队实现的计算单元（Calculator）的同时，也向用户提供定制新计算单元的接口。创建一个新的 Calculator，需要用户实现 Open ()，Process ()，Close () 去分别定义 Calculator 的初始化，针对数据流的处理方法，以及 Calculator 在完成所有运算后的关闭步骤。为了方便用户在多个图中复用已有的通用组件，例如图像数据的预处理、模型的推理以及图像的渲染等， MediaPipe 引入了子图（Subgraph）的概念。因此，一个 MediaPipe 图中的节点既可以是计算单元，亦可以是子图。子图在不同图内的复用，方便了大规模模块化的应用搭建。</p>\n<p>MediaPipe 不支持除了 tensorflow 之外的其他深度学习框架，但是对各种系统平台和语言的支持非常友好：</p>\n<center><img src=\"https://pics4.baidu.com/feed/8644ebf81a4c510f70bd4c1bdebc6e24d52aa5a1.jpeg?token=5c3ed7fa6a2c0bfa4eeba204a30aaa75\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>MediaPipe 的官方地址：<a href=\"https://google.github.io/mediapipe/\">https://google.github.io/mediapipe/</a></p>\n<p>GitHub 地址：<a href=\"https://github.com/google/mediapipe\">https://github.com/google/mediapipe</a></p>\n<h5><span id=\"5onnx\"> （5）ONNX</span></h5>\n<p><a href=\"https://github.com/onnx/onnx\">Github 地址</a></p>\n<p>Open Neural Network Exchange（ONNX，开放神经网络交换）格式，是一个用于表示深度学习模型的标准，可使模型在不同框架之间进行转移。ONNX 是一种针对机器学习所设计的开放式的文件格式，用于存储训练好的模型。它使得不同的人工智能框架（如 Pytorch, MXNet）可以采用相同格式存储模型数据并交互。ONNX 的规范及代码主要由微软，亚马逊 ，Facebook 和 IBM 等公司共同开发，以开放源代码的方式托管在 Github 上。目前官方支持加载 ONNX 模型并进行推理的深度学习框架有：Caffe2, PyTorch, MXNet，<a href=\"http://ML.NET\">ML.NET</a>，TensorRT 和 Microsoft CNTK，并且 TensorFlow 也非官方的支持 ONNX。</p>\n<p>比方说现在某组织因为主要开发用 TensorFlow 为基础的框架，现在有一个深度算法，需要将其部署在移动设备上，以观测变现。传统地我们需要用 caffe2 重新将模型写好，然后再训练参数；试想下这将是一个多么耗时耗力的过程。</p>\n<p>此时，ONNX 便应运而生，Caffe2，PyTorch，Microsoft Cognitive Toolkit，Apache MXNet 等主流框架都对 ONNX 有着不同程度的支持。这就便于了我们的算法及模型在不同的框架之间的迁移。无论你使用何种训练框架训练模型（比如 TensorFlow/Pytorch/OneFlow/Paddle），在训练完毕后你都可以将这些框架的模型统一转换为 ONNX 这种统一的格式进行存储。</p>\n<p>开放式神经网络交换（ONNX）是迈向开放式生态系统的第一步，它使 AI 开发人员能够随着项目的发展选择合适的工具。ONNX 为 AI 模型提供开源格式。它定义了可扩展的计算图模型，以及内置运算符和标准数据类型的定义。最初的 ONNX 专注于推理（评估）所需的功能。ONNX 解释计算图的可移植，它使用 graph 的序列化格式。它不一定是框架选择在内部使用和操作计算的形式。例如，如果在优化过程中操作更有效，则实现可以在存储器中以不同方式表示模型。</p>\n<p>在获得 ONNX 模型之后，模型部署人员自然就可以将这个模型部署到兼容 ONNX 的运行环境中去。这里一般还会设计到额外的模型转换工作，典型的比如在 Android 端利用 NCNN 部署 ONNX 格式模型，那么就需要将 ONNX 利用 NCNN 的转换工具转换到 NCNN 所支持的 bin 和 param 格式。</p>\n<p>ONNX 作为一个文件格式，我们自然需要一定的规则去读取我们想要的信息或者是写入我们需要保存信息。ONNX 使用的是 Protobuf 这个序列化数据结构去存储神经网络的权重信息。熟悉 Caffe 或者 Caffe2 的同学应该知道，它们的模型存储数据结构协议也是 Protobuf。</p>\n<p>Protobuf 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API（摘自官方介绍）。</p>\n<p>Protobuf 协议是一个以 *.proto 后缀文件为基础的，这个文件描述了用户自定义的数据结构。如果需要了解更多细节请参考 0x7 节的资料 3，这里只是想表达 ONNX 是基于 Protobuf 来做数据存储和传输，那么自然 onnx.proto 就是 ONNX 格式文件了。</p>\n<p>ONNX 作为框架共用的一种模型交换格式，使用 protobuf 二进制格式来序列化模型，可以提供更好的传输性能我们可能会在某一任务中将 Pytorch 或者 TensorFlow 模型转化为 ONNX 模型 (ONNX 模型一般用于中间部署阶段)，然后再拿转化后的 ONNX 模型进而转化为我们使用不同框架部署需要的类型，ONNX 相当于一个翻译的作用。</p>\n<center><img src=\"https://pics4.baidu.com/feed/5882b2b7d0a20cf4cd20a6bbcbec003fadaf9915.jpeg?token=131436ea2d13a4af3d07a01a427b2184\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>ONNX 将每一个网络的每一层或者说是每一个算子当作节点 Node，再由这些 Node 去构建一个 Graph，相当于是一个网络。最后将 Graph 和这个 onnx 模型的其他信息结合在一起，生成一个 model，也就是最终的.onnx 的模型。</p>\n<p>构建一个简单的 onnx 模型，实质上，只要构建好每一个 node，然后将它们和输入输出超参数一起塞到 graph，最后转成 model 就可以了。</p>\n<p>在计算方面，虽然更高级的表达不同，但不同框架产生的最终结果都是非常接近。因此实时跟踪某一个神经网络是如何在这些框架上生成的，接着使用这些信息创建一个通用的计算图，即符合 ONNX 标准的计算图。</p>\n<p>ONNX 为可扩展的计算图模型、内部运算器（Operator）以及标准数据类型提供了定义。在初始阶段，每个计算数据流图以节点列表的形式组织起来，构成一个非循环的图。节点有一个或多个的输入与输出。每个节点都是对一个运算器的调用。图还会包含协助记录其目的、作者等信息的元数据。运算器在图的外部实现，但那些内置的运算器可移植到不同的框架上，每个支持 ONNX 的框架将在匹配的数据类型上提供这些运算器的实现。</p>\n<p>Microsoft 和合作伙伴社区创建了 ONNX 作为表示机器学习模型的开放标准。 许多框架（包括 TensorFlow、PyTorch、SciKit-Learn、Keras、Chainer、MXNet、MATLAB 和 SparkML）中的模型都可以导出或转换为标准 ONNX 格式。模型采用 ONNX 格式后，可在各种平台和设备上运行。</p>\n<p>ONNX 运行时是一种用于将 ONNX 模型部署到生产环境的高性能推理引擎。它针对云和 Edge 进行了优化，适用于 Linux、Windows 和 Mac。它使用 C++ 编写，还包含 C、Python、C#、Java 和 Javascript (Node.js) API，可在各种环境中使用。ONNX 运行时同时支持 DNN 和传统 ML 模型，并与不同硬件上的加速器（例如，NVidia GPU 上的 TensorRT、Intel 处理器上的 OpenVINO、Windows 上的 DirectML 等）集成。通过使用 ONNX 运行时，可以从大量的生产级优化、测试和不断改进中受益。</p>\n<p>ONNX 运行时用于大规模 Microsoft 服务，如必应、Office 和 Azure 认知服务。性能提升取决于许多因素，但这些 Microsoft 服务的 CPU 平均起来可实现 2 倍的性能提升。除了 Azure 机器学习服务外，ONNX 运行时还在支持机器学习工作负荷的其他产品中运行，包括：</p>\n<ul>\n<li>\n<p>Windows: 该运行时作为 Windows 机器学习的一部分内置于 Windows 中，在数亿台设备上运行。</p>\n</li>\n<li>\n<p>Azure SQL 产品系列：针对 Azure SQL Edge 和 Azure SQL 托管实例中的数据运行本机评分。</p>\n</li>\n<li>\n<p><a href=\"http://ML.NET\">ML.NET</a>：在 <a href=\"http://ML.NET\">ML.NET</a> 中运行 ONNX 模型。</p>\n</li>\n</ul>\n<center><img src=\"https://pics2.baidu.com/feed/aa18972bd40735fa6cac32875ab444ba0e24089a.jpeg?token=0f2931f539d3ddfd5e3759f24d93aeea\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>ONNX 的官方网站：<a href=\"https://onnx.ai/\">https://onnx.ai/</a></p>\n<p>ONXX 的 GitHub 地址：<a href=\"https://github.com/onnx/onnx\">https://github.com/onnx/onnx</a></p>\n<h5><span id=\"6mnn\"> （6）MNN</span></h5>\n<p><a href=\"https://github.com/alibaba/MNN\">Github 地址</a></p>\n<p>MNN 是一个高效、轻量的深度学习框架。由阿里巴巴开源，它支持深度模型推理与训练，尤其在端侧的推理与训练性能在业界处于领先地位。目前，MNN 已经在阿里巴巴的手机淘宝、手机天猫、优酷、钉钉、闲鱼等 20 多个 App 中使用，覆盖直播、短视频、搜索推荐、商品图像搜索、互动营销、权益发放、安全风控等 70 多个场景。此外，IoT 等场景下也有若干应用。</p>\n<p>MNN 的架构设计理念与性能数据在 MLSys 2020 上面发表。Paper <a href=\"https://proceedings.mlsys.org/static/paper_files/mlsys/2020/7-Paper.pdf\">在此处</a>。如果 MNN 对你的研究有所助益，欢迎引用 MNN 的论文：</p>\n<p><strong>（1）整体特点</strong></p>\n<p>1&gt;  <code>轻量性</code></p>\n<ul>\n<li>针对端侧设备特点深度定制和裁剪，无任何依赖，可以方便地部署到移动设备和各种嵌入式设备中。</li>\n<li>iOS 平台：armv7+arm64 静态库大小 5MB 左右，链接生成可执行文件增加大小 620KB 左右，metallib 文件 600KB 左右。</li>\n<li>Android 平台：so 大小 400KB 左右，OpenCL 库 400KB 左右，Vulkan 库 400KB 左右。</li>\n</ul>\n<p>2&gt;  <code>通用性</code></p>\n<ul>\n<li>支持 <code>Tensorflow</code> 、 <code>Caffe</code> 、 <code>ONNX</code>  等主流模型文件格式，支持 <code>CNN</code> 、 <code>RNN</code> 、 <code>GAN</code>  等常用网络。</li>\n<li>支持 86 个 <code>Tensorflow</code> Op、34 个 <code>Caffe</code> Op；各计算设备支持的 MNN Op 数：CPU 71 个，Metal 55 个，OpenCL 29 个，Vulkan 31 个。</li>\n<li>支持 iOS 8.0+、Android 4.3 + 和具有 POSIX 接口的嵌入式设备。</li>\n<li>支持异构设备混合计算，目前支持 CPU 和 GPU，可以动态导入 GPU Op 插件，替代 CPU Op 的实现。</li>\n</ul>\n<p>3&gt;  <code>高性能</code></p>\n<ul>\n<li>不依赖任何第三方计算库，依靠大量手写汇编实现核心运算，充分发挥 ARM CPU 的算力。</li>\n<li>iOS 设备上可以开启 GPU 加速（Metal），常用模型上快于苹果原生的 CoreML。</li>\n<li>Android 上提供了 <code>OpenCL</code> 、 <code>Vulkan</code> 、 <code>OpenGL</code>  三套方案，尽可能多地满足设备需求，针对主流 GPU（ <code>Adreno</code>  和 <code>Mali</code> ）做了深度调优。</li>\n<li>卷积、转置卷积算法高效稳定，对于任意形状的卷积均能高效运行，广泛运用了 Winograd 卷积算法，对 3x3 -&gt; 7x7 之类的对称卷积有高效的实现。</li>\n<li>针对 ARM v8.2 的新架构额外作了优化，新设备可利用半精度计算的特性进一步提速。</li>\n</ul>\n<p>4&gt;  <code>易用性</code></p>\n<ul>\n<li>有高效的图像处理模块，覆盖常见的形变、转换等需求，一般情况下，无需额外引入 libyuv 或 opencv 库处理图像。</li>\n<li>支持回调机制，可以在网络运行中插入回调，提取数据或者控制运行走向。</li>\n<li>支持只运行网络中的一部分，或者指定 CPU 和 GPU 间并行运行。</li>\n</ul>\n<p><strong>（2）架构设计</strong></p>\n<center><img src=\"https://static.oschina.net/uploads/space/2019/0507/114930_QiJ8_4062684.png\" alt=\"img\" style=\"zoom: 50%;\"></center>\n<p>MNN 可以分为 Converter 和 Interpreter 两部分。</p>\n<p>Converter 由 Frontends 和 Graph Optimize 构成。前者负责支持不同的训练框架，MNN 当前支持 Tensorflow (Lite)、Caffe 和 ONNX (PyTorch/MXNet 的模型可先转为 ONNX 模型再转到 MNN)；后者通过算子融合、算子替代、布局调整等方式优化图。</p>\n<p>Interpreter 由 Engine 和 Backends 构成。前者负责模型的加载、计算图的调度；后者包含各计算设备下的内存分配、Op 实现。在 Engine 和 Backends 中，MNN 应用了多种优化方案，包括在卷积和反卷积中应用 Winograd 算法、在矩阵乘法中应用 Strassen 算法、低精度计算、Neon 优化、手写汇编、多线程优化、内存复用、异构计算等。</p>\n<h5><span id=\"7mace\"> （7）MACE</span></h5>\n<p><a href=\"https://github.com/XiaoMi/mace\">Github 地址</a></p>\n<p><code>Mobile AI Compute Engine (MACE)</code>  是小米开发的一个专为移动端异构计算平台 (支持 Android, iOS, Linux, Windows) 优化的神经网络计算框架。 主要从以下的角度做了专门的优化：</p>\n<ul>\n<li>性能\n<ul>\n<li>代码经过 NEON 指令，OpenCL 以及 Hexagon HVX 专门优化，并且采用 <a href=\"https://arxiv.org/abs/1509.09308\">Winograd 算法</a>来进行卷积操作的加速。 此外，还对启动速度进行了专门的优化。</li>\n</ul>\n</li>\n<li>功耗\n<ul>\n<li>支持芯片的功耗管理，例如 ARM 的 big.LITTLE 调度，以及高通 Adreno GPU 功耗选项。</li>\n</ul>\n</li>\n<li>系统响应\n<ul>\n<li>支持自动拆解长时间的 OpenCL 计算任务，来保证 UI 渲染任务能够做到较好的抢占调度， 从而保证系统 UI 的相应和用户体验。</li>\n</ul>\n</li>\n<li>内存占用\n<ul>\n<li>通过运用内存依赖分析技术，以及内存复用，减少内存的占用。另外，保持尽量少的外部 依赖，保证代码尺寸精简。</li>\n</ul>\n</li>\n<li>模型加密与保护\n<ul>\n<li>模型保护是重要设计目标之一。支持将模型转换成 C++ 代码，以及关键常量字符混淆，增加逆向的难度。</li>\n</ul>\n</li>\n<li>硬件支持范围\n<ul>\n<li>支持高通，联发科，以及松果等系列芯片的 CPU，GPU 与 DSP (目前仅支持 Hexagon) 计算加速。CPU 模式支持 Android, iOS, Linux 等系统。</li>\n</ul>\n</li>\n<li>模型格式支持\n<ul>\n<li>支持<a href=\"https://github.com/tensorflow/tensorflow\"> TensorFlow</a>， <a href=\"https://github.com/BVLC/caffe\">Caffe</a> 和<a href=\"https://github.com/onnx/onnx\"> ONNX</a> 等模型格式。</li>\n</ul>\n</li>\n</ul>\n<h5><span id=\"8tnn\"> （8）TNN</span></h5>\n<p><a href=\"https://github.com/Tencent/TNN\">Github 地址</a></p>\n<p>TNN 是由腾讯优图实验室开源的高性能、轻量级神经网络推理框架，同时拥有跨平台、高性能、模型压缩、代码裁剪等众多突出优势。TNN 框架在原有 Rapidnet、ncnn 框架的基础上进一步加强了移动端设备的支持以及性能优化，同时借鉴了业界主流开源框架高性能和良好拓展性的特性，拓展了对于后台 X86, NV GPU 的支持。手机端 TNN 已经在手 Q、微视、P 图等众多应用中落地，服务端 TNN 作为腾讯云 AI 基础加速框架已为众多业务落地提供加速支持。欢迎大家参与协同共建，促进 TNN 推理框架进一步完善。</p>\n<h5><span id=\"9tvm\"> （9）TVM</span></h5>\n<p><a href=\"https://github.com/apache/tvm\">Github 地址</a></p>\n<p>TVM 是一款开源项目，主要由华盛顿大学的 SAMPL 组贡献开发。目前深度学习社区十分活跃，每天都有研究者提出新的 operation 以期望更好的提升模型的准确率。同时，随着越来越多的厂商开始做硬件（比如寒武纪，商汤科技等等），运行神经网络的时候会有越来越多的后端设备可供选择。</p>\n<p>而这对于做框架的人来说就比较头疼，既要尝试为新出现的各种 operation 提供支持，又要在新出现的后端设备上实现现有的 operation。TVM 项目因此应运而生，希望达到的目标就是研究人员只用写一次 operation，然后 TVM 自动对各种后端设备生成性能可观的代码。</p>\n<p>按照官方的定义，TVM 是一套完整的 stack，包括神经网络图优化（比如 op fusion）和单个 operation 优化等部分。我习惯于将图优化的部分归类做 Relay 项目，而仅仅把单个 operation 优化看做 TVM，因此文章之后提到的 TVM 基本是指单个算子优化这部分。</p>\n<center><img src=\"https://pic2.zhimg.com/80/v2-0ab891db0248c2c6f12badf0ba31efdd_720w.jpg\" alt=\"img\"></center>\n<p>上面这张摘自 tvm 的官网 (<a href=\"https://link.zhihu.com/?target=https%3A//tvm.ai/about\">https://tvm.ai/about</a>) 的图片说明了 TVM 处于深度学习框架的位置。TVM 位于神经网络图（High-Level Differentiable IR）的下方，底层硬件（LLVM, CUDA, Metal）的上方。</p>\n<p>图片右边的 AutoTVM 我认为比较独立。这个目的是自动调整 TVM 生成的代码的一些参数，试图让 TVM 生成的代码尽可能快。做自动代码优化的优秀项目除了 AutoTVM，还有<a href=\"https://halide-lang.org/papers/autoscheduler2019.html\"> Halide</a>，个人认为目前 Halide 做代码自动优化做的更好。TVM 的基本思路参考自 Halide，其中的数据结构也引用了很多 Halide 的实现，强烈推荐感兴趣的朋友研究一下 Halide</p>\n",
            "tags": [
                "人工智能",
                "模型部署"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2022/03/03/%E3%80%90Linux%E3%80%91NanoDet-Plus%E4%B9%8Bncnn%E9%83%A8%E7%BD%B2/",
            "url": "https://leezhao415.github.io/2022/03/03/%E3%80%90Linux%E3%80%91NanoDet-Plus%E4%B9%8Bncnn%E9%83%A8%E7%BD%B2/",
            "title": "【Linux】NanoDet Plus之ncnn部署",
            "date_published": "2022-03-03T14:56:48.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#1%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C\">（1）准备工作：</a></li>\n<li><a href=\"#2%E6%93%8D%E4%BD%9C%E6%AD%A5%E9%AA%A4\">（2）操作步骤：</a>\n<ul>\n<li><a href=\"#step1\">step1</a></li>\n<li><a href=\"#step2\">step2</a></li>\n<li><a href=\"#step3\">step3</a></li>\n</ul>\n</li>\n<li><a href=\"#3%E5%8F%82%E8%80%83%E6%95%99%E7%A8%8B\">（3）参考教程</a></li>\n</ul>\n<!-- tocstop -->\n<hr>\n<p>本项目实现在 linux 平台使用 ncnn 部署 NanoDet Plus 模型的功能，内容亲测有效，可作为通用 ncnn 模型部署参考项目。</p>\n<h4><span id=\"1准备工作\"> （1）准备工作：</span></h4>\n<ul>\n<li>下载安卓的项目包：<a href=\"https://github.com/nihui/ncnn-android-nanodet\">https://github.com/nihui/ncnn-android-nanodet</a></li>\n</ul>\n<h4><span id=\"2操作步骤\"> （2）操作步骤：</span></h4>\n<h5><span id=\"step1\"> step1</span></h5>\n<p><a href=\"https://github.com/Tencent/ncnn/releases\">https://github.com/Tencent/ncnn/releases</a></p>\n<ul>\n<li>Download ncnn-YYYYMMDD-android-vulkan.zip or build ncnn for android yourself</li>\n<li>Extract ncnn-YYYYMMDD-android-vulkan.zip into <strong>app/src/main/jni</strong> and change the <strong>ncnn_DIR</strong> path to yours in <strong>app/src/main/jni/CMakeLists.txt</strong></li>\n</ul>\n<h5><span id=\"step2\"> step2</span></h5>\n<p><a href=\"https://github.com/nihui/opencv-mobile\">https://github.com/nihui/opencv-mobile</a></p>\n<ul>\n<li>Download opencv-mobile-XYZ-android.zip</li>\n<li>Extract opencv-mobile-XYZ-android.zip into <strong>app/src/main/jni</strong> and change the <strong>OpenCV_DIR</strong> path to yours in <strong>app/src/main/jni/CMakeLists.txt</strong></li>\n</ul>\n<h5><span id=\"step3\"> step3</span></h5>\n<ul>\n<li>Open this project with Android Studio, build it and enjoy!</li>\n</ul>\n<h4><span id=\"3参考教程\"> （3）参考教程</span></h4>\n<ul>\n<li>\n<p><a href=\"https://blog.csdn.net/Jianyuemou/article/details/120763091?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164272662116780271983928%2522%252C%2522scm%2522%253A%252220140713.130102334%E2%80%A6%2522%257D&amp;request_id=164272662116780271983928&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2blogsobaiduend~default-1-120763091.nonecase&amp;utm_term=nanodet%E9%83%A8%E7%BD%B2%E5%9C%A8android+studio&amp;spm=1018.2226.3001.4450\">安卓部署：手机端 Anchor-free 的目标检测模型 Nanodet</a></p>\n</li>\n<li>\n<p><a href=\"https://blog.csdn.net/qq_33596242/article/details/122377654?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164272662116780271954464%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=164272662116780271954464&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2blogfirst_rank_ecpm_v1~rank_v31_ecpm-3-122377654.nonecase&amp;utm_term=nanodet%E9%83%A8%E7%BD%B2%E5%9C%A8android+studio&amp;spm=1018.2226.3001.4450\">安卓手机部署 nanodet</a></p>\n</li>\n</ul>\n",
            "tags": [
                "人工智能",
                "模型部署"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/",
            "url": "https://leezhao415.github.io/2022/03/03/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AE%9E%E4%BE%8B%EF%BC%88OpenVINO%E3%80%81TensorRT%EF%BC%89/",
            "title": "【精华】模型部署实例（OpenVINO、TensorRT）",
            "date_published": "2022-03-03T14:56:11.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#1openvino%E9%83%A8%E7%BD%B2nanodet%E6%A8%A1%E5%9E%8B\">（1）OpenVINO 部署 NanoDet 模型</a>\n<ul>\n<li><a href=\"#1-nanodet%E7%AE%80%E4%BB%8B\">1&gt; nanodet 简介</a></li>\n<li><a href=\"#2-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE\">2&gt; 环境配置</a></li>\n<li><a href=\"#3-nanodet%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E8%BD%AC%E6%8D%A2onnx\">3&gt; NanoDet 模型训练和转换 ONNX</a></li>\n<li><a href=\"#4-nanodet%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2\">4&gt; NanoDet 模型部署</a></li>\n<li><a href=\"#5-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81%E4%B8%80%E8%A7%88\">5&gt; 核心代码一览</a></li>\n<li><a href=\"#6-%E6%8E%A8%E7%90%86%E6%97%B6%E9%97%B4%E5%B1%95%E7%A4%BA%E5%8F%8A%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA\">6&gt; 推理时间展示及预测结果展示</a></li>\n</ul>\n</li>\n<li><a href=\"#2tensorrt%E9%83%A8%E7%BD%B2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B\">（2）TensorRT 部署深度学习模型</a>\n<ul>\n<li><a href=\"#1-%E8%83%8C%E6%99%AF\">1&gt; 背景</a></li>\n<li><a href=\"#2-%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF\">2&gt; 相关技术</a></li>\n<li><a href=\"#3-tensorflow%E6%A8%A1%E5%9E%8Btensorrt%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B\">3&gt; tensorflow 模型 tensorRT 部署教程</a></li>\n<li><a href=\"#4-caffe%E6%A8%A1%E5%9E%8Btensorrt%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B\">4&gt; Caffe 模型 tensorRT 部署教程</a></li>\n<li><a href=\"#5-%E4%B8%BAtensorrt%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82\">5&gt; 为 tensorRT 添加自定义层</a></li>\n<li><a href=\"#6-%E4%B8%BAcaffeparser%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82%E6%94%AF%E6%8C%81\">6&gt; 为 CaffeParser 添加自定义层支持</a></li>\n<li><a href=\"#7-%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95\">7&gt; 心得体会（踩坑记录）</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h4><span id=\"1openvino-部署-nanodet-模型\"> （1）OpenVINO 部署 NanoDet 模型</span></h4>\n<h5><span id=\"1gt-nanodet-简介\"> 1&gt; nanodet 简介</span></h5>\n<p>NanoDet （<a href=\"https://github.com/RangiLyu/nanodet%EF%BC%89%E6%98%AF%E4%B8%80%E4%B8%AA%E9%80%9F%E5%BA%A6%E8%B6%85%E5%BF%AB%E5%92%8C%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%9A%84Anchor-free\">https://github.com/RangiLyu/nanodet）是一个速度超快和轻量级的 Anchor-free</a> 目标检测模型。想了解算法本身的可以去搜一搜之前机器之心的介绍。</p>\n<h5><span id=\"2gt-环境配置\"> 2&gt; 环境配置</span></h5>\n<ul>\n<li><a href=\"https://so.csdn.net/so/search?q=Ubuntu&amp;spm=1001.2101.3001.7020\">Ubuntu</a>：18.04</li>\n<li><a href=\"https://so.csdn.net/so/search?q=OpenVINO&amp;spm=1001.2101.3001.7020\">OpenVINO</a>：2020.4</li>\n<li><a href=\"https://so.csdn.net/so/search?q=OpenCV&amp;spm=1001.2101.3001.7020\">OpenCV</a>：3.4.2</li>\n<li>OpenVINO 和 OpenCV 安装包（编译好了，也可以自己从官网下载自己编译）可以从链接: <a href=\"https://pan.baidu.com/s/1zxtPKm-Q48Is5mzKbjGHeg\">https://pan.baidu.com/s/1zxtPKm-Q48Is5mzKbjGHeg</a> 密码: gw5c 下载</li>\n<li>OpenVINO 安装</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvzf l_openvino_toolkit_p_2020.4.287.tgz</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">cd</span> l_openvino_toolkit_p_2020.4.287</span><br><span class=\"line\">sudo ./install_GUI.sh 一路next安装</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">cd</span> /opt/intel/openvino/install_dependencies</span><br><span class=\"line\">sudo ./install_openvino_dependencies.sh</span><br><span class=\"line\"></span><br><span class=\"line\">vi ~/.bashrc</span><br></pre></td></tr></table></figure>\n<ul>\n<li>把如下两行放置到 bashrc 文件尾</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">source</span> /opt/intel/openvino/bin/setupvars.sh</span><br><span class=\"line\"><span class=\"built_in\">source</span> /opt/intel/openvino/opencv/setupvars.sh</span><br></pre></td></tr></table></figure>\n<ul>\n<li>source ~/.bashrc 激活环境</li>\n<li>模型优化配置步骤</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites</span><br><span class=\"line\">sudo ./install_prerequisites_onnx.sh（模型是从onnx转为IR文件，只需配置onnx依赖）</span><br></pre></td></tr></table></figure>\n<ul>\n<li>OpenCV 配置</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -xvzf opencv-3.4.2.zip <span class=\"comment\"># 解压OpenCV到用户根目录即可，以便后续调用。（这是我编译好的版本，有需要可以自己编译）</span></span><br></pre></td></tr></table></figure>\n<h5><span id=\"3gt-nanodet-模型训练和转换-onnx\"> 3&gt; NanoDet 模型训练和转换 ONNX</span></h5>\n<ul>\n<li>git clone <a href=\"https://github.com/Wulingtian/nanodet.git\">https://github.com/Wulingtian/nanodet.git</a></li>\n<li>cd nanodet</li>\n<li>cd config 配置模型文件，训练模型</li>\n<li>定位到 nanodet 目录，进入 tools 目录，打开 export.py 文件，配置 cfg_path model_path out_path 三个参数</li>\n<li>定位到 nanodet 目录，运行 python tools/export.py 得到转换后的 onnx 模型</li>\n<li>python /opt/intel/openvino/deployment_tools/model_optimizer/mo_onnx.py --input_model onnx 模型 --output_dir 期望模型输出的路径。得到 IR 文件</li>\n</ul>\n<h5><span id=\"4gt-nanodet-模型部署\"> 4&gt; NanoDet 模型部署</span></h5>\n<ul>\n<li>sudo apt install cmake 安装 cmake</li>\n<li>git clone <a href=\"https://github.com/Wulingtian/nanodet_openvino.git\">https://github.com/Wulingtian/nanodet_openvino.git</a> （求 star！）</li>\n<li>cd nanodet_openvino 打开 CMakeLists.txt 文件，修改 OpenCV_INCLUDE_DIRS 和 OpenCV_LIBS_DIR，之前已经把 OpenCV 解压到根目录了，所以按照你自己的路径指定</li>\n<li>定位到 nanodet_openvino，cd models 把之前生成的 IR 模型（包括 bin 和 xml 文件）文件放到该目录下</li>\n<li>定位到 nanodet_openvino， cd test_imgs 把需要测试的图片放到该目录下</li>\n<li>定位到 nanodet_openvino，编辑 main.cpp，xml_path 参数修改为 &quot;…/models/ 你的模型名称.xml&quot;</li>\n<li>编辑 num_class 设置类别数，例如：我训练的模型是安全帽检测，只有 1 类，那么设置为 1</li>\n<li>编辑 src 设置测试图片路径，src 参数修改为 &quot;…/test_imgs/ 你的测试图片&quot;</li>\n<li>定位到 nanodet_openvino</li>\n<li>mkdir build; cd build; cmake … ;make</li>\n<li>./detect_test 输出平均推理时间，以及保存预测图片到当前目录下，至此，部署完成！</li>\n</ul>\n<h5><span id=\"5gt-核心代码一览\"> 5&gt; 核心代码一览</span></h5>\n<figure class=\"highlight php\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//主要对图片进行预处理，包括resize和归一化</span></span><br><span class=\"line\">std::vector&lt;<span class=\"keyword\">float</span>&gt; Detector::prepareImage(cv::Mat &amp;src_img)&#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">    std::vector&lt;<span class=\"keyword\">float</span>&gt; result(INPUT_W * INPUT_H * <span class=\"number\">3</span>);</span><br><span class=\"line\">    <span class=\"keyword\">float</span> *data = result.data();</span><br><span class=\"line\">    <span class=\"keyword\">float</span> ratio = <span class=\"keyword\">float</span>(INPUT_W) / <span class=\"keyword\">float</span>(src_img.cols) &lt; <span class=\"keyword\">float</span>(INPUT_H) / <span class=\"keyword\">float</span>(src_img.rows) ? <span class=\"keyword\">float</span>(INPUT_W) / <span class=\"keyword\">float</span>(src_img.cols) : <span class=\"keyword\">float</span>(INPUT_H) / <span class=\"keyword\">float</span>(src_img.rows);</span><br><span class=\"line\">    cv::Mat flt_img = cv::Mat::zeros(cv::Size(INPUT_W, INPUT_H), CV_8UC3);</span><br><span class=\"line\">    cv::Mat rsz_img = cv::Mat::zeros(cv::Size(src_img.cols*ratio, src_img.rows*ratio), CV_8UC3);</span><br><span class=\"line\">    cv::resize(src_img, rsz_img, cv::Size(), ratio, ratio);</span><br><span class=\"line\"> </span><br><span class=\"line\">    rsz_img.copyTo(flt_img(cv::Rect(<span class=\"number\">0</span>, <span class=\"number\">0</span>, rsz_img.cols, rsz_img.rows)));</span><br><span class=\"line\">    flt_img.convertTo(flt_img, CV_32FC3);</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">int</span> channelLength = INPUT_W * INPUT_H;</span><br><span class=\"line\">    std::vector&lt;cv::Mat&gt; split_img = &#123;</span><br><span class=\"line\">            cv::Mat(INPUT_W, INPUT_H, CV_32FC1, data + channelLength * <span class=\"number\">2</span>),</span><br><span class=\"line\">            cv::Mat(INPUT_W, INPUT_H, CV_32FC1, data + channelLength),</span><br><span class=\"line\">            cv::Mat(INPUT_W, INPUT_H, CV_32FC1, data)</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\"> </span><br><span class=\"line\">    cv::split(flt_img, split_img);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">3</span>; i++) &#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">        split_img[i] = (split_img[i] - img_mean[i]) / img_std[i];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\">//加载IR模型，初始化网络</span></span><br><span class=\"line\"><span class=\"keyword\">bool</span> Detector::init(<span class=\"keyword\">string</span> xml_path,<span class=\"keyword\">double</span> cof_threshold,<span class=\"keyword\">double</span> nms_area_threshold,<span class=\"keyword\">int</span> input_w, <span class=\"keyword\">int</span> input_h, <span class=\"keyword\">int</span> num_class, <span class=\"keyword\">int</span> r_rows, <span class=\"keyword\">int</span> r_cols, std::vector&lt;<span class=\"keyword\">int</span>&gt; s, std::vector&lt;<span class=\"keyword\">float</span>&gt; i_mean,std::vector&lt;<span class=\"keyword\">float</span>&gt; i_std)&#123;</span><br><span class=\"line\">    _xml_path = xml_path;</span><br><span class=\"line\">    _cof_threshold = cof_threshold;</span><br><span class=\"line\">    _nms_area_threshold = nms_area_threshold;</span><br><span class=\"line\">    INPUT_W = input_w;</span><br><span class=\"line\">    INPUT_H = input_h;</span><br><span class=\"line\">    NUM_CLASS = num_class;</span><br><span class=\"line\">    refer_rows = r_rows;</span><br><span class=\"line\">    refer_cols = r_cols;</span><br><span class=\"line\">    strides = s;</span><br><span class=\"line\">    img_mean = i_mean;</span><br><span class=\"line\">    img_std = i_std;</span><br><span class=\"line\">    Core ie;</span><br><span class=\"line\">    auto cnnNetwork = ie.ReadNetwork(_xml_path); </span><br><span class=\"line\"> </span><br><span class=\"line\">    InputsDataMap inputInfo(cnnNetwork.getInputsInfo());</span><br><span class=\"line\">    InputInfo::Ptr&amp; input = inputInfo.begin()-&gt;second;</span><br><span class=\"line\">    _input_name = inputInfo.begin()-&gt;first;</span><br><span class=\"line\">    input-&gt;setPrecision(Precision::FP32);</span><br><span class=\"line\">    input-&gt;getInputData()-&gt;setLayout(Layout::NCHW);</span><br><span class=\"line\">    ICNNNetwork::InputShapes inputShapes = cnnNetwork.getInputShapes();</span><br><span class=\"line\">    SizeVector&amp; inSizeVector = inputShapes.begin()-&gt;second;</span><br><span class=\"line\">    cnnNetwork.reshape(inputShapes);</span><br><span class=\"line\">    _outputinfo = OutputsDataMap(cnnNetwork.getOutputsInfo());</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (auto &amp;output : _outputinfo) &#123;</span><br><span class=\"line\">        output.second-&gt;setPrecision(Precision::FP32);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    _network =  ie.LoadNetwork(cnnNetwork, <span class=\"string\">&quot;CPU&quot;</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//模型推理及获取输出结果</span></span><br><span class=\"line\">vector&lt;Detector::Bbox&gt; Detector::process_frame(Mat&amp; inframe)&#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">    cv::Mat showImage = inframe.<span class=\"keyword\">clone</span>();</span><br><span class=\"line\">    std::vector&lt;<span class=\"keyword\">float</span>&gt; pr_img = prepareImage(inframe);</span><br><span class=\"line\">    InferRequest::Ptr infer_request = _network.CreateInferRequestPtr();</span><br><span class=\"line\">    Blob::Ptr frameBlob = infer_request-&gt;GetBlob(_input_name);</span><br><span class=\"line\">    InferenceEngine::LockedMemory&lt;<span class=\"keyword\">void</span>&gt; blobMapped = InferenceEngine::as&lt;InferenceEngine::MemoryBlob&gt;(frameBlob)-&gt;wmap();</span><br><span class=\"line\">    <span class=\"keyword\">float</span>* blob_data = blobMapped.<span class=\"keyword\">as</span>&lt;<span class=\"keyword\">float</span>*&gt;();</span><br><span class=\"line\"> </span><br><span class=\"line\">    memcpy(blob_data, pr_img.data(), <span class=\"number\">3</span> * INPUT_H * INPUT_W * sizeof(<span class=\"keyword\">float</span>));</span><br><span class=\"line\"> </span><br><span class=\"line\">    infer_request-&gt;Infer();</span><br><span class=\"line\">    vector&lt;Rect&gt; origin_rect;</span><br><span class=\"line\">    vector&lt;<span class=\"keyword\">float</span>&gt; origin_rect_cof;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i=<span class=\"number\">0</span>;</span><br><span class=\"line\">    vector&lt;Bbox&gt; bboxes;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (auto &amp;output : _outputinfo) &#123;</span><br><span class=\"line\">        auto output_name = output.first;</span><br><span class=\"line\">        Blob::Ptr blob = infer_request-&gt;GetBlob(output_name);</span><br><span class=\"line\">        LockedMemory&lt;<span class=\"keyword\">const</span> <span class=\"keyword\">void</span>&gt; blobMapped = <span class=\"keyword\">as</span>&lt;MemoryBlob&gt;(blob)-&gt;rmap();</span><br><span class=\"line\">        <span class=\"keyword\">float</span> *output_blob = blobMapped.<span class=\"keyword\">as</span>&lt;<span class=\"keyword\">float</span> *&gt;();</span><br><span class=\"line\">        bboxes = postProcess(showImage,output_blob);</span><br><span class=\"line\">        ++i;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> bboxes;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//对模型输出结果进行解码及nms</span></span><br><span class=\"line\">std::vector&lt;Detector::Bbox&gt; Detector::postProcess(<span class=\"keyword\">const</span> cv::Mat &amp;src_img,</span><br><span class=\"line\">                              <span class=\"keyword\">float</span> *output) &#123;</span><br><span class=\"line\">    GenerateReferMatrix();</span><br><span class=\"line\">    std::vector&lt;Detector::Bbox&gt; result;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> *out = output;</span><br><span class=\"line\">    <span class=\"keyword\">float</span> ratio = std::max(<span class=\"keyword\">float</span>(src_img.cols) / <span class=\"keyword\">float</span>(INPUT_W), <span class=\"keyword\">float</span>(src_img.rows) / <span class=\"keyword\">float</span>(INPUT_H));</span><br><span class=\"line\">    cv::Mat result_matrix = cv::Mat(refer_rows, NUM_CLASS + <span class=\"number\">4</span>, CV_32FC1, out);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> row_num = <span class=\"number\">0</span>; row_num &lt; refer_rows; row_num++) &#123;</span><br><span class=\"line\">        Detector::Bbox box;</span><br><span class=\"line\">        auto *row = result_matrix.ptr&lt;<span class=\"keyword\">float</span>&gt;(row_num);</span><br><span class=\"line\">        auto max_pos = std::max_element(row + <span class=\"number\">4</span>, row + NUM_CLASS + <span class=\"number\">4</span>);</span><br><span class=\"line\">        box.prob = row[max_pos - row];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (box.prob &lt; _cof_threshold)</span><br><span class=\"line\">            <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        box.classes = max_pos - row - <span class=\"number\">4</span>;</span><br><span class=\"line\">        auto *anchor = refer_matrix.ptr&lt;<span class=\"keyword\">float</span>&gt;(row_num);</span><br><span class=\"line\">        box.x = (anchor[<span class=\"number\">0</span>] - row[<span class=\"number\">0</span>] * anchor[<span class=\"number\">2</span>] + anchor[<span class=\"number\">0</span>] + row[<span class=\"number\">2</span>] * anchor[<span class=\"number\">2</span>]) / <span class=\"number\">2</span> * ratio;</span><br><span class=\"line\">        box.y = (anchor[<span class=\"number\">1</span>] - row[<span class=\"number\">1</span>] * anchor[<span class=\"number\">2</span>] + anchor[<span class=\"number\">1</span>] + row[<span class=\"number\">3</span>] * anchor[<span class=\"number\">2</span>]) / <span class=\"number\">2</span> * ratio;</span><br><span class=\"line\">        box.w = (row[<span class=\"number\">2</span>] + row[<span class=\"number\">0</span>]) * anchor[<span class=\"number\">2</span>] * ratio;</span><br><span class=\"line\">        box.h = (row[<span class=\"number\">3</span>] + row[<span class=\"number\">1</span>]) * anchor[<span class=\"number\">2</span>] * ratio;</span><br><span class=\"line\">        result.push_back(box);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    NmsDetect(result);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5><span id=\"6gt-推理时间展示及预测结果展示\"> 6&gt; 推理时间展示及预测结果展示</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210130130455826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MDU0MTUx,size_16,color_FFFFFF,t_70\" alt=\"img\" style=\"zoom:60%;\"></center>\n<p>我的老笔记本平均推理时间 15ms 左右，CPU 下实时推理</p>\n<p>安全帽检测结果</p>\n<p>至此完成了 NanoDet 在 X86 CPU 上的部署，希望有帮助到大家。</p>\n<h4><span id=\"2tensorrt-部署深度学习模型\"> （2）TensorRT 部署深度学习模型</span></h4>\n<p>原帖：<a href=\"https://zhuanlan.zhihu.com/p/84125533\">https://zhuanlan.zhihu.com/p/84125533</a></p>\n<h5><span id=\"1gt-背景\"> 1&gt; 背景</span></h5>\n<p>目前主流的<a href=\"https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;spm=1001.2101.3001.7020\">深度学习</a>框架（caffe，mxnet，tensorflow，pytorch 等）进行模型推断的速度都并不优秀，在实际工程中用上述的框架进行模型部署往往是比较低效的。而通过 Nvidia 推出的 tensorRT 工具来部署主流框架上训练的模型能够极大的提高模型推断的速度，往往相比与原本的框架能够有至少 1 倍以上的速度提升，同时占用的设备内存也会更加的少。因此对是所有需要部署模型的同志来说，掌握用 tensorRT 来部署深度学习模型的方法是非常有用的。</p>\n<h5><span id=\"2gt-相关技术\"> 2&gt; 相关技术</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/c961b6b959233494f133799097940297.png\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>上面的图片取自 TensorRT 的官网，里面列出了 tensorRT 使用的一些技术。可以看到比较成熟的深度学习落地技术：模型量化、动态内存优化、层的融合等技术均已经在 tensorRT 中集成了，这也是它能够极大提高模型推断速度的原因。总体来说 tensorRT 将训练好的模型通过一系列的优化技术转化为了能够在特定平台（GPU）上以高性能运行的代码，也就是最后图中生成的 Inference engine。目前也有一些其他的工具能够实现类似 tensorRT 的功能，例如<a href=\"https://link.zhihu.com/?target=https%3A//github.com/dmlc/tvm\"> TVM</a>，<a href=\"https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/TensorComprehensions\">TensorComprehensions</a> 也能有效的提高模型在特定平台上的推断速度，但是由于目前企业主流使用的都是 Nvidia 生产的计算设备，在这些设备上 nvidia 推出的 tensorRT 性能相比其他工具会更有优势一些。而且 tensorRT 依赖的代码库仅仅包括 C++ 和 cuda，相对与其他工具要更为精简一些。</p>\n<h5><span id=\"3gt-tensorflow-模型-tensorrt-部署教程\"> 3&gt; tensorflow 模型 tensorRT 部署教程</span></h5>\n<p>实际工程部署中多采用 c<ins> 进行部署，因此在本教程中也使用的是 tensorRT 的 C</ins>API，tensorRT 版本为 5.1.5。具体 tensorRT 安装可参考教程 [<a href=\"https://zhuanlan.zhihu.com/p/64053177\">深度学习] TensorRT 安装</a>，以及官网的安装说明。</p>\n<h6><span id=\"1模型持久化\"> （1）模型持久化</span></h6>\n<p>部署 tensorflow 模型的第一步是模型持久化，将模型结构和权重保存到一个.pb 文件当中。</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pb_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), [v.op.name <span class=\"keyword\">for</span> v <span class=\"keyword\">in</span> outputs])</span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.gfile.FastGFile(<span class=\"string\">&#x27;./pbmodel_name.pb&#x27;</span>, mode=<span class=\"string\">&#x27;wb&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    f.write(pb_graph.SerializeToString())</span><br></pre></td></tr></table></figure>\n<p>具体只需在模型定义和权重读取之后执行以上代码，调用 tf.graph_util.convert_variables_to_constants 函数将权重转为常量，其中 outputs 是需要作为输出的<a href=\"https://so.csdn.net/so/search?q=tensor&amp;spm=1001.2101.3001.7020\"> tensor</a> 的列表，最后用 pb_graph.SerializeToString () 将 graph 序列化并写入到 pb 文件当中，这样就生成了 pb 模型。</p>\n<h6><span id=\"2生成-uff-模型\"> （2）生成 uff 模型</span></h6>\n<p>有了 pb 模型，需要将其转换为 tensorRT 可用的 uff 模型，只需调用 uff 包自带的 convert 脚本即可</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python <span class=\"regexp\">/usr/</span>lib<span class=\"regexp\">/python2.7/</span>site-packages<span class=\"regexp\">/uff/</span>bin/convert_to_uff.py   pbmodel_name.pb</span><br></pre></td></tr></table></figure>\n<p>如转换成功会输出如下信息，包含图中总结点的个数以及推断出的输入输出节点的信息</p>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/9e81d7152cbe9e4dbd45f14eb340d704.png\" alt=\"img\" style=\"zoom:80%;\"></center>\n<h6><span id=\"3tensorrt-c-api-部署模型\"> （3）tensorRT c++ API 部署模型</span></h6>\n<p>使用 tensorRT 部署生成好的 uff 模型需要先讲 uff 中保存的模型权值以及网络结构导入进来，然后执行优化算法生成对应的 inference engine。具体代码如下，首先需要定义一个 IBuilder* builder，一个用来解析 uff 文件的 parser 以及 builder 创建的 network，parser 会将 uff 文件中的模型参数和网络结构解析出来存到 network，解析前要预先告诉 parser 网络输入输出输出的节点。解析后 builder 就能根据 network 中定义的网络结构创建 engine。在创建 engine 前会需要指定最大的 batchsize 大小，之后使用 engine 时输入的 batchsize 不能超过这个数值否则就会出错。推断时如果 batchsize 和设定最大值一样时效率最高。举个例子，如果设定最大 batchsize 为 10，实际推理输入一个 batch 10 张图的时候平均每张推断时间是 4ms 的话，输入一个 batch 少于 10 张图的时候平均每张图推断时间会高于 4ms。</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">IBuilder* builder = createInferBuilder(gLogger.getTRTLogger());</span><br><span class=\"line\">auto parser = createUffParser();</span><br><span class=\"line\">parser-&gt;registerInput(inputtensor_name, Dims3(INPUT_C, INPUT_H, INPUT_W), UffInputOrder::kNCHW);</span><br><span class=\"line\">parser-&gt;registerOutput(outputtensor_name);</span><br><span class=\"line\">    INetworkDefinition* network = builder-&gt;createNetwork();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!parser-&gt;parse(uffFile, *network, nvinfer1::DataType::kFLOAT))</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        gLogError &lt;&lt; <span class=\"string\">&quot;Failure while parsing UFF file&quot;</span> &lt;&lt; std::endl;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> nullptr;</span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    builder-&gt;setMaxBatchSize(maxBatchSize);</span><br><span class=\"line\">    builder-&gt;setMaxWorkspaceSize(MAX_WORKSPACE);</span><br><span class=\"line\">    ICudaEngine* engine = builder-&gt;buildCudaEngine(*network);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!engine)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        gLogError &lt;&lt; <span class=\"string\">&quot;Unable to create engine&quot;</span> &lt;&lt; std::endl;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> nullptr;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>生成 engine 之后就可以进行推断了，执行推断时需要有一个上下文执行上下文 IExecutionContext* context，可以通过 engine-&gt;createExecutionContext () 获得。执行推断的核心代码是</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">context-&gt;execute(batchSize, &amp;buffers<span class=\"selector-attr\">[0]</span>);  </span><br></pre></td></tr></table></figure>\n<p>其中 buffer 是一个 void * 数组对应的是模型输入输出 tensor 的设备地址，通过 cudaMalloc 开辟输入输出所需要的设备空间（显存）将对应指针存到 buffer 数组中，在执行 execute 操作前通过 cudaMemcpy 把输入数据（输入图像）拷贝到对应输入的设备空间，执行 execute 之后还是通过 cudaMemcpy 把输出的结果从设备上拷贝出来。</p>\n<p>更为详细的例程可以参考 TensorRT 官方的 samples 中的<a href=\"https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT/tree/master/samples/opensource/sampleUffMNIST\"> sampleUffMNIST 代码</a></p>\n<h6><span id=\"4加速比情况\"> （4）加速比情况</span></h6>\n<p>实际工程中我在 Tesla M40 上用 tensorRT 来加速过 Resnet-50，Inception-resnet-v2，谷歌图像检索模型 Delf（DEep Local Features），加速前后单张图推断用时比较如下图（单位 ms）</p>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/b7a75ea584e9dd89ef2867e4f2259648.png\" alt=\"img\"></center>\n<h5><span id=\"4gt-caffe-模型-tensorrt-部署教程\"> 4&gt; Caffe 模型 tensorRT 部署教程</span></h5>\n<p>相比与 tensorflow 模型 caffe 模型的转换更加简单，不需要有 tensorflow 模型转 uff 模型这类的操作，tensorRT 能够直接解析 prototxt 和 caffemodel 文件获取模型的网络结构和权重。具体解析流程和上文描述的一致，不同的是 caffe 模型的 parser 不需要预先指定输入层，这是因为 prototxt 已经进行了输入层的定义，parser 能够自动解析出输入，另外 caffeparser 解析网络后返回一个 IBlobNameToTensor *blobNameToTensor 记录了网络中 tensor 和 pototxt 中名字的对应关系，在解析之后就需要通过这个对应关系，按照输出 tensor 的名字列表 outputs 依次找到对应的 tensor 并通过 network-&gt;markOutput 函数将其标记为输出，之后就可以生成 engine 了。</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">IBuilder* builder = createInferBuilder(gLogger);</span><br><span class=\"line\">    INetworkDefinition* network = builder-&gt;createNetwork();</span><br><span class=\"line\">    ICaffeParser* parser = createCaffeParser();</span><br><span class=\"line\">    DataType modelDataType = DataType::kFLOAT;</span><br><span class=\"line\">    <span class=\"keyword\">const</span> IBlobNameToTensor *blobNameToTensor =\tparser-&gt;parse(deployFile.c_str(),</span><br><span class=\"line\">                                                              modelFile.c_str(),</span><br><span class=\"line\">                                                              *network,</span><br><span class=\"line\">                                                              modelDataType);</span><br><span class=\"line\">    assert(blobNameToTensor != nullptr);</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (auto&amp; s : outputs) network-&gt;markOutput(*blobNameToTensor-&gt;find(s.c_str()));</span><br><span class=\"line\"> </span><br><span class=\"line\">    builder-&gt;setMaxBatchSize(maxBatchSize);</span><br><span class=\"line\">    builder-&gt;setMaxWorkspaceSize(<span class=\"number\">1</span> &lt;&lt; <span class=\"number\">30</span>);</span><br><span class=\"line\">    engine = builder-&gt;buildCudaEngine(*network);</span><br></pre></td></tr></table></figure>\n<p>生成 engine 后执行的方式和上一节描述的一致，详细的例程可以参考<a href=\"https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT/tree/master/samples/opensource/sampleMNIST\"> SampleMNIST</a></p>\n<h6><span id=\"1加速比情况\"> （1）加速比情况</span></h6>\n<p>实际工程中我在 Tesla M40 上用 tensorRT 加速过 caffe 的 VGG19，SSD 速度变为 1.6 倍，ResNet50，MobileNetV2 加速前后单张图推断用时比较如下图（单位 ms）</p>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/641c21accf80edb33a2254ef22cac8c8.png\" alt=\"img\"></center>\n<h5><span id=\"5gt-为-tensorrt-添加自定义层\"> 5&gt; 为 tensorRT 添加自定义层</span></h5>\n<p>tensorRT 目前只支持一些非常常见的操作，有很多操作它并不支持比如上采样 Upsample 操作，这时候就需要我们自行将其编写为 tensorRT 的插件层，从而使得这些不能支持的操作能在 tensorRT 中使用。以定义 Upsample 层为例，我们首先要定义一个继承自 tensorRT 插件基类的 Upsample 类</p>\n<figure class=\"highlight kotlin\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Upsample</span>: <span class=\"type\">public IPluginExt</span></span></span><br></pre></td></tr></table></figure>\n<p>然后要实现该类的一些必要方法，首先是 2 个构造函数，一个是传参数构建，另一个是从序列化后的比特流构建。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"built_in\">Upsample</span>(<span class=\"keyword\">int</span> scale = <span class=\"number\">2</span>) : <span class=\"built_in\">mScale</span>(scale) &#123;</span><br><span class=\"line\">        <span class=\"built_in\">assert</span>(mScale &gt; <span class=\"number\">0</span>);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">//定义上采样倍数</span></span><br><span class=\"line\"> <span class=\"built_in\">Upsmaple</span>(<span class=\"keyword\">const</span> <span class=\"keyword\">void</span> *data, <span class=\"keyword\">size_t</span> length) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *d = <span class=\"keyword\">reinterpret_cast</span>&lt;<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *&gt;(data), *a = d;</span><br><span class=\"line\">        mScale = read&lt;<span class=\"keyword\">int</span>&gt;(d);</span><br><span class=\"line\">        mDtype = read&lt;DataType&gt;(d);</span><br><span class=\"line\">        mCHW = read&lt;DimsCHW&gt;(d);</span><br><span class=\"line\">        <span class=\"built_in\">assert</span>(mScale &gt; <span class=\"number\">0</span>);</span><br><span class=\"line\">        <span class=\"built_in\">assert</span>(d == a + length);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">~<span class=\"built_in\">Upsample</span>()</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>一些定义层输出信息的方法</p>\n<figure class=\"highlight csharp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">   <span class=\"function\"><span class=\"built_in\">int</span> <span class=\"title\">getNbOutputs</span>(<span class=\"params\"></span>) <span class=\"keyword\">const</span> <span class=\"keyword\">override</span></span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">//模型的输出个数</span></span><br><span class=\"line\">    <span class=\"function\">Dims <span class=\"title\">getOutputDimensions</span>(<span class=\"params\"><span class=\"built_in\">int</span> index, <span class=\"keyword\">const</span> Dims *inputs, <span class=\"built_in\">int</span> nbInputDims</span>) <span class=\"keyword\">override</span></span> &#123;</span><br><span class=\"line\">       <span class=\"comment\">// std::cout &lt;&lt; &quot;Get ouputdims!!!&quot; &lt;&lt; std::endl;</span></span><br><span class=\"line\">        assert(nbInputDims == <span class=\"number\">1</span>);</span><br><span class=\"line\">        assert(inputs[<span class=\"number\">0</span>].nbDims == <span class=\"number\">3</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> DimsCHW(inputs[<span class=\"number\">0</span>].d[<span class=\"number\">0</span>], inputs[<span class=\"number\">0</span>].d[<span class=\"number\">1</span>] * mScale, inputs[<span class=\"number\">0</span>].d[<span class=\"number\">2</span>] * mScale);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">//获取模型输出的形状</span></span><br></pre></td></tr></table></figure>\n<p>根据输入的形状个数以及采用的数据类型检查合法性以及配置层参数的方法</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">    bool supportsFormat(DataType type, PluginFormat format) const override &#123;</span><br><span class=\"line\">        return (type == DataType::kFLOAT || type == DataType::kHALF || type == DataType::kINT8)</span><br><span class=\"line\">               &amp;&amp; format == PluginFormat::kNCHW;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">//检查层是否支持当前的数据类型和格式</span></span><br><span class=\"line\">    void configureWithFormat(<span class=\"keyword\">const</span> Dims *inputDims, int nbInputs, <span class=\"keyword\">const</span> Dims *outputDims, int nbOutputs,</span><br><span class=\"line\">                             DataType type, PluginFormat format, int maxBatchSize) override</span><br><span class=\"line\">       &#123;</span><br><span class=\"line\">         mDtype = <span class=\"class\"><span class=\"keyword\">type</span>;</span></span><br><span class=\"line\">         mCHW.c() = inputDims[<span class=\"number\">0</span>].d[<span class=\"number\">0</span>];</span><br><span class=\"line\">         mCHW.h() = inputDims[<span class=\"number\">0</span>].d[<span class=\"number\">1</span>];</span><br><span class=\"line\">         mCHW.w() = inputDims[<span class=\"number\">0</span>].d[<span class=\"number\">2</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"><span class=\"comment\">//配置层的参数</span></span><br></pre></td></tr></table></figure>\n<p>层的序列化方法</p>\n<figure class=\"highlight csharp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\">size_t <span class=\"title\">getSerializationSize</span>(<span class=\"params\"></span>) <span class=\"keyword\">override</span></span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">sizeof</span>(mScale) + <span class=\"keyword\">sizeof</span>(mDtype) + <span class=\"keyword\">sizeof</span>(mCHW);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">//输出序列化层所需的长度</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">serialize</span>(<span class=\"params\"><span class=\"keyword\">void</span> *buffer</span>) <span class=\"keyword\">override</span></span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">char</span> *d = reinterpret_cast&lt;<span class=\"built_in\">char</span> *&gt;(buffer), *a = d;</span><br><span class=\"line\">        write(d, mScale);</span><br><span class=\"line\">        write(d, mDtype);</span><br><span class=\"line\">        write(d, mCHW);</span><br><span class=\"line\">        assert(d == a + getSerializationSize());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">//将层参数序列化为比特流</span></span><br></pre></td></tr></table></figure>\n<p>层的运算方法</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">size_t</span> <span class=\"title\">getWorkspaceSize</span><span class=\"params\">(<span class=\"keyword\">int</span> maxBatchSize)</span> <span class=\"keyword\">const</span> <span class=\"keyword\">override</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"comment\">//层运算需要的临时工作空间大小</span></span><br><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">enqueue</span><span class=\"params\">(<span class=\"keyword\">int</span> batchSize, <span class=\"keyword\">const</span> <span class=\"keyword\">void</span> *<span class=\"keyword\">const</span> *inputs, <span class=\"keyword\">void</span> **outputs, <span class=\"keyword\">void</span> *workspace,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                cudaStream_t stream)</span> <span class=\"keyword\">override</span></span>;</span><br><span class=\"line\"><span class=\"comment\">//层执行计算的具体操作</span></span><br></pre></td></tr></table></figure>\n<p>在 enqueue 中我们调用编写好的 cuda kenerl 来进行 Upsample 的计算</p>\n<p>完成了 Upsample 类的定义，我们就可以直接在网络中添加我们编写的插件了，通过如下语句我们就定义一个上采样 2 倍的上采样层。addPluginExt 的第一个输入是 ITensor** 类别，这是为了支持多输出的情况，第二个参数就是输入个数，第三个参数就是需要创建的插件类对象。</p>\n<figure class=\"highlight csharp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Upsample <span class=\"title\">up</span>(<span class=\"params\"><span class=\"number\">2</span></span>)；</span></span><br><span class=\"line\"><span class=\"function\">auto upsamplelayer</span>=network-&gt;addPluginExt(inputtensot,<span class=\"number\">1</span>,up)</span><br></pre></td></tr></table></figure>\n<h5><span id=\"6gt-为-caffeparser-添加自定义层支持\"> 6&gt; 为 CaffeParser 添加自定义层支持</span></h5>\n<p>对于我们自定义的层如果写到了 caffe prototxt 中，在部署模型时调用 caffeparser 来解析就会报错。</p>\n<p>还是以 Upsample 为例，如果在 prototxt 中有下面这段来添加了一个 upsample 的层</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layer &#123;</span><br><span class=\"line\">  name: <span class=\"string\">&quot;upsample0&quot;</span></span><br><span class=\"line\">  type: &quot;Upsample&quot;</span><br><span class=\"line\">  bottom: <span class=\"string\">&quot;ReLU11&quot;</span></span><br><span class=\"line\">  top: <span class=\"string\">&quot;Upsample1&quot;</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这时再调用</p>\n<figure class=\"highlight rust\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">const</span> IBlobNameToTensor *blobNameToTensor =\tparser-&gt;parse(deployFile.c_str(),</span><br><span class=\"line\">                                                              modelFile.c_str(),</span><br><span class=\"line\">                                                              *network,</span><br><span class=\"line\">                                                              modelDataType);</span><br></pre></td></tr></table></figure>\n<p>就会出现错误</p>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/89445b8586b760aa23f31453e1137ad6.png\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>之前我们已经编写了 Upsample 的插件，怎么让 tensorRT 的 caffe parser 识别出 prototxt 中的 upsample 层自动构建我们自己编写的插件呢？这时我们就需要定义一个插件工程类继承基类 nvinfer1::IPluginFactory, nvcaffeparser1::IPluginFactoryExt。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PluginFactory</span> :</span> <span class=\"keyword\">public</span> nvinfer1::IPluginFactory, <span class=\"keyword\">public</span> nvcaffeparser1::IPluginFactoryExt</span><br></pre></td></tr></table></figure>\n<p>其中必须要的实现的方法有判断一个层是否是 plugin 的方法，输入的参数就是 prototxt 中 layer 的 name，通过 name 来判断一个层是否注册为插件</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">isPlugin</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *name)</span> <span class=\"keyword\">override</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"built_in\">isPluginExt</span>(name);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">bool</span> <span class=\"title\">isPluginExt</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *name)</span> <span class=\"keyword\">override</span> </span>&#123;</span><br><span class=\"line\"> </span><br><span class=\"line\">        <span class=\"keyword\">char</span> *aa = <span class=\"keyword\">new</span> <span class=\"keyword\">char</span>[<span class=\"number\">6</span>];</span><br><span class=\"line\">        <span class=\"built_in\">memcpy</span>(aa, name, <span class=\"number\">5</span>);</span><br><span class=\"line\">        aa[<span class=\"number\">5</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> res = !<span class=\"built_in\">strcmp</span>(aa, <span class=\"string\">&quot;upsam&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//判断层名字是否是upsample层的名字</span></span><br></pre></td></tr></table></figure>\n<p>根据名字创建插件的方法，有两中方式一个是由权重构建，另一个是由序列化后的比特流创建，对应了插件的两种构造函数，Upsample 没有权重，对于其他有权重的插件就能够用传入的 weights 初始化层。mplugin 是一个 vector 用来存储所有创建的插件层。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">IPlugin *<span class=\"title\">createPlugin</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *layerName, <span class=\"keyword\">const</span> nvinfer1::Weights *weights, <span class=\"keyword\">int</span> nbWeights)</span> <span class=\"keyword\">override</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"built_in\">assert</span>(<span class=\"built_in\">isPlugin</span>(layerName));</span><br><span class=\"line\">        mPlugin.<span class=\"built_in\">push_back</span>(std::unique_ptr&lt;Upsample&gt;(<span class=\"keyword\">new</span> <span class=\"built_in\">Upsample</span>(<span class=\"number\">2</span>)));</span><br><span class=\"line\">        <span class=\"keyword\">return</span> mPlugin[mPlugin.<span class=\"built_in\">size</span>() - <span class=\"number\">1</span>].<span class=\"built_in\">get</span>();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"><span class=\"function\">IPlugin *<span class=\"title\">createPlugin</span><span class=\"params\">(<span class=\"keyword\">const</span> <span class=\"keyword\">char</span> *layerName, <span class=\"keyword\">const</span> <span class=\"keyword\">void</span> *serialData, <span class=\"keyword\">size_t</span> serialLength)</span> <span class=\"keyword\">override</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"built_in\">assert</span>(<span class=\"built_in\">isPlugin</span>(layerName));</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> <span class=\"built_in\">Upsample</span>(serialData, serialLength);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"> std::vector &lt;std::unique_ptr&lt;Upsample&gt;&gt; mPlugin;</span><br></pre></td></tr></table></figure>\n<p>最后需要定义一个 destroy 方法来释放所有创建的插件层。</p>\n<figure class=\"highlight csharp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">destroyPlugin</span>(<span class=\"params\"></span>)</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (unsigned <span class=\"built_in\">int</span> i = <span class=\"number\">0</span>; i &lt; mPlugin.size(); i++) &#123;</span><br><span class=\"line\">            mPlugin[i].reset();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>对于 prototxt 存在多个多种插件的情况，可以在 isPlugin，createPlugin 方法中添加新的条件分支，根据层的名字创建对应的插件层。</p>\n<p>实现了 PluginFactory 之后在调用 caffeparser 的时候需要设置使用它，在调用 parser-&gt;parser 之前加入如下代码</p>\n<figure class=\"highlight haskell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">PluginFactory</span> pluginFactory;</span><br><span class=\"line\"><span class=\"title\">parser</span>-&gt;setPluginFactoryExt(&amp;pluginFactory);</span><br></pre></td></tr></table></figure>\n<p>就可以设置 parser 按照 pluginFactory 里面定义的规则来创建插件层，这样之前出现的不能解析 Upsample 层的错误就不会再出现了。</p>\n<p>官方添加插件层的样例<a href=\"https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT/tree/master/samples/opensource/samplePlugin\"> samplePlugin</a> 可以作为参考</p>\n<h5><span id=\"7gt-心得体会踩坑记录\"> 7&gt; 心得体会（踩坑记录）</span></h5>\n<ol>\n<li>\n<p>转 tensorflow 模型时，生成 pb 模型、转换 uff 模型以及调用 uffparser 时 register Input，output，这三个过程中输入输出节点的名字一定要注意保持一致，否则最终在 parser 进行解析时会出现错误，找不到输入输出节点。</p>\n</li>\n<li>\n<p>除了本文中列举的 pluginExt，tensorRT 中插件基类还有 IPlugin，IPluginV2，继承这些基类所需要实现的类方法有细微区别，具体情况可自行查看 tensorRT 安装文件夹下的 include/NvInfer.h 文件。同时添加自己写的层到网络时的函数有 addPlugin，addPluginExt，addPluginV2 这几种和 IPlugin，IPluginExt，IPluginV2 一一对应，不能够混用，否则有些默认调用的类方法不会调用的，比如用 addPlugin 添加的 PluginExt 层是不会调用 configureWithFormat 方法的，因为 IPlugin 类没有该方法。同样的在还有 caffeparser 的 setPluginFactory 和 setPluginFactoryExt 也是不能混用的。</p>\n</li>\n<li>\n<p>运行程序出现 cuda failure 一般情况下是由于将内存数据拷贝到磁盘时出现了非法内存访问，注意检查 buffer 开辟的空间大小和拷贝过去数据的大小是否一致.</p>\n</li>\n<li>\n<p>有一些操作在 tensorRT 中不支持但是可以通过一些支持的操作进行组合替代，比如 ，这样可以省去一些编写自定义层的时间。</p>\n</li>\n<li>\n<p>tensorflow 中的 flatten 操作默认时 keepdims=False 的，但是在转化 uff 文时会默认按照 keepdims=True 转换，因此在 tensorflow 中对 flatten 后的向量进行 transpose、expanddims 等等操作，在转换到 uff 后用 tensorRT 解析时容易出现错误，比如 “Order size is not matching the number dimensions of TensorRT” 。最好设置 tensorflow 的 reduce，flatten 操作的 keepdims=True，保持层的输出始终为 4 维形式，能够有效避免转到 tensorRT 时出现各种奇怪的错误。</p>\n</li>\n<li>\n<p>tensorRT 中的 slice 层存在一定问题，我用 network-&gt;addSlice 给网络添加 slice 层后，在执行 buildengine 这一步时就会出错 nvinfer1::builder::checkSanity (const nvinfer1::builder::Graph&amp;): Assertion `tensors.size () == g.tensors.size ()’ failed.，构建网络时最好避开使用 slice 层，或者自己实现自定层来执行 slice 操作。</p>\n</li>\n<li>\n<p>tensorRT 的 github 中有着部分的开源代码以及丰富的示例代码，多多学习能够帮助更快的掌握 tensorRT 的使用</p>\n</li>\n</ol>\n",
            "tags": [
                "人工智能",
                "模型部署"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2022/02/21/%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88%E4%B9%8B%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%A6%82%E8%BF%B0/",
            "url": "https://leezhao415.github.io/2022/02/21/%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88%E4%B9%8B%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%A6%82%E8%BF%B0/",
            "title": "部署方案之模型部署概述",
            "date_published": "2022-02-21T14:00:10.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88%E4%B9%8B%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%A6%82%E8%BF%B0\">部署方案之模型部署概述</a>\n<ul>\n<li><a href=\"#1-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%9C%BA%E6%99%AF\">1 模型部署场景</a></li>\n<li><a href=\"#2-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F\">2 模型部署方式</a></li>\n<li><a href=\"#3-%E9%83%A8%E7%BD%B2%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BC%98%E5%8C%96%E6%8C%87%E6%A0%87\">3 部署的核心优化指标</a></li>\n<li><a href=\"#4-%E9%83%A8%E7%BD%B2%E6%B5%81%E7%A8%8B\">4 部署流程</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h4><span id=\"部署方案之模型部署概述\"> 部署方案之模型部署概述</span></h4>\n<p>模型训练重点关注的是如何通过训练策略来得到一个性能更好的模型，其过程似乎包含着各种 “玄学”，被戏称为 “炼丹”。整个流程包含从训练样本的获取（包括数据采集与标注），模型结构的确定，<a href=\"https://so.csdn.net/so/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020\">损失函数</a>和评价指标的确定，到模型参数的训练，这部分更多是业务方去承接相关工作。一旦 “炼丹” 完成（即训练得到了一个指标不错的模型），如何将这颗 “丹药” 赋能到实际业务中，充分发挥其能力，这就是部署方需要承接的工作。</p>\n<p>因此，一般来说，学术界负责各种 SOTA (State of the Art) 模型的训练和结构探索，而工业界负责将这些 SOTA 模型应用落地，赋能百业。本文将要讲述的是，在 CV 场景中，如何实现模型的快速落地，赋能到产业应用中。模型部署一般无需再考虑如何修改训练方式或者修改网络结构以提高模型精度，更多的是需要明确部署的场景、部署方式（中心服务化还是本地终端部署）、模型的优化指标，以及如何提高吞吐率和减少延迟等，接下来将逐一进行介绍。</p>\n<h5><span id=\"1-模型部署场景\"> 1 模型部署场景</span></h5>\n<p>这个问题主要源于中心服务器云端部署和边缘部署两种方式的差异 云端部署常见的模式是，模型部署在云端服务器，用户通过网页访问或者 API 接口调用等形式向云端服务器发出请求，云端收到请求后处理并返回结果。边缘部署则主要用于嵌入式设备，主要通过将模型打包封装到 SDK，集成到嵌入式设备，数据的处理和模型推理都在终端设备上执行。</p>\n<h5><span id=\"2-模型部署方式\"> 2 模型部署方式</span></h5>\n<p>针对上面提到的两种场景，分别有两种不同的部署方案，Service 部署和 SDK 部署。 Service 部署：主要用于中心服务器云端部署，一般直接以训练的引擎库作为推理服务模式。SDK 部署：主要用于嵌入式端部署场景，以 C++ 等语言实现一套高效的前后处理和推理引擎库（高效推理模式下的 Operation/Layer/Module 的实现），用于提供高性能推理能力。此种方式一般需要考虑模型转换（动态图静态化）、模型联合编译等进行深度优化</p>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/c7e460b912162c9b9a74d7f0c65d455c.png\" alt=\"Image\" style=\"zoom:80%;\"></center>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/b549771d283e4eab7d5e8d27fe614011.png\" alt=\"Image\" style=\"zoom: 67%;\"></center>\n<h5><span id=\"3-部署的核心优化指标\"> 3 部署的核心优化指标</span></h5>\n<p>部署的核心目标是合理把控成本、功耗、性价比三大要素。</p>\n<p>成本问题是部署硬件的重中之重，AI 模型部署到硬件上的成本将极大限制用户的业务承受能力。成本问题主要聚焦于芯片的选型，比如，对比寒武纪 MLU220 和 MLU270，MLU270 主要用作数据中心级的加速卡，其算力和功耗都相对于边缘端的人工智能加速卡 MLU220 要低。至于 Nvida 推出的 Jetson 和 Tesla T4 也是类似思路，Tesla T4 是主打数据中心的推理加速卡，而 Jetson 则是嵌入式设备的加速卡。对于终端场景，还会根据对算力的需求进一步细分，比如表中给出的高通骁龙芯片，除 GPU 的浮点算力外，还会增加 DSP 以增加定点算力，篇幅有限，不再赘述，主要还是根据成本和业务需求来进行权衡。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/210c28ce9ca2b9017cbbd51ab504db55.png\" alt=\"Image\" style=\"zoom:80%;\"></center>\n<p>在数据中心服务场景，对于功耗的约束要求相对较低；在边缘终端设备场景，硬件的功耗会影响边缘设备的电池使用时长。因此，对于功耗要求相对较高，一般来说，利用 NPU 等专用优化的加速器单元来处理神经网络等高密度计算，能节省大量功耗。</p>\n<p>不同的业务场景对于芯片的选择有所不同，以达到更高的性价比。从公司业务来看，云端相对更加关注是多路的吞吐量优化需求，而终端场景则更关注单路的延时需要。在目前主流的 CV 领域，低比特模型相对成熟，且 INT8/INT4 芯片因成本低，且算力比高的原因已被广泛使用；但在 NLP 或者语音等领域，对于精度的要求较高，低比特模型精度可能会存在难以接受的精度损失，因此 FP16 是相对更优的选择。在 CV 领域的芯片性价比选型上，在有 INT8/INT4 计算精度的芯片里，主打低精度算力的产品是追求高性价比的主要选择之一，但这也为平衡精度和性价比提出了巨大的挑战。</p>\n<h5><span id=\"4-部署流程\"> 4 部署流程</span></h5>\n<p>上面简要介绍了部署的主要方式和场景，以及部署芯片的选型考量指标，接下来以 SDK 部署为例，给大家概括介绍一下 SenseParrots 在部署中的整体流程。SenseParrots 部署流程大致分为以下几个步骤：模型转换、模型量化压缩、模型打包封装 SDK。</p>\n<h6><span id=\"41-模型转换\"> 4.1 模型转换</span></h6>\n<p>模型转换主要用于模型在不同框架之间的流转，常用于训练和推理场景的连接。目前主流的框架都以 ONNX 或者 caffe 为模型的交换格式，SenseParrots 也不例外。SenseParrots 的模型转换主要分为计算图生成和计算图转换两大步骤，另外，根据需要，还可以在中间插入计算图优化，对计算机进行推理加速（诸如常见的 CONV/BN 的算子融合）。</p>\n<p>计算图生成是通过一次 inference 并追踪记录的方式，将用户的模型完整地翻译成静态的表达。在模型 inference 的过程中，框架会记录执行算子的类型、输入输出、超参、参数和调用该算子的模型层次，最后把 inference 过程中得到的算子信息和模型信息结合得到最终的静态计算图。</p>\n<p>在计算图生成之后与计算图转换之前，可以进行计算图优化，例如去除冗余 op，计算合并等。SenseParrots 原生实现了一批计算图的精简优化 pass，也开放接口鼓励用户对计算图进行自定义的处理和优化操作。</p>\n<p>计算图转换是指分析静态计算图的算子，对应转换到目标格式。SenseParrots 支持了多后端的转换，能够转换到各个 opset 的 ONNX、原生 caffe 和多种第三方版本的 caffe。框架通过算子转换器继承或重写的方式，让 ONNX 和 caffe 的不同版本的转换开发变得更加简单。同时，框架开放了自定义算子生成和自定义算子转换器的接口，让第三方框架开发者也能够轻松地自主开发实现 SenseParrots 到第三方框架的转换。</p>\n<h6><span id=\"42-模型量化压缩\"> 4.2 模型量化压缩</span></h6>\n<p>终端场景中，一般会有内存和速度的考虑，因此会要求模型尽量小，同时保证较高的吞吐率。除了人工针对嵌入式设备设计合适的模型，如 MobileNet 系列，通过 NAS (Neural Architecture Search) 自动搜索小模型，以及通过蒸馏 / 剪枝的方式压缩模型外，一般还会使用量化来达到减小模型规模和加速的目的。</p>\n<p>量化的过程主要是将原始浮点 FP32 训练出来的模型压缩到定点 INT8 (或者 INT4/INT1) 的模型，由于 INT8 只需要 8 比特来表示，因此相对于 32 比特的浮点，其模型规模理论上可以直接降为原来的 1/4，这种压缩率是非常直观的。另外，大部分终端设备都会有专用的定点计算单元，通过低比特指令实现的低精度算子，速度上会有很大的提升，当然，这部分还依赖协同体系结构和算法来获得更大的加速。</p>\n<p>量化的技术栈主要分为<strong>量化训练（QAT, Quantization Aware Training）<strong>和</strong>离线量化（PTQ, Post Training Quantization）</strong>, 两者的主要区别在于，<strong>量化训练</strong>是通过对模型插入伪量化算子（这些算子用来模拟低精度运算的逻辑），通过梯度下降等优化方式在原始浮点模型上进行微调，从来调整参数得到精度符合预期的模型。<strong>离线量化</strong>主要是通过少量校准数据集（从原始数据集中挑选 100-1000 张图，不需要训练样本的标签）获得网络的 activation 分布，通过统计手段或者优化浮点和定点输出的分布来获得量化参数，从而获取最终部署的模型。两者各有优劣，量化训练基于原始浮点模型的训练逻辑进行训练，理论上更能保证收敛到原始模型的精度，但需要精细调参且生产周期较长；离线量化只需要基于少量校准数据，因此生产周期短且更加灵活，缺点是精度可能略逊于量化训练。实际落地过程中，发现大部分模型通过离线量化就可以获得不错的模型精度（1% 以内的精度损失，当然这部分精度的提升也得益于优化策略的加持），剩下少部分模型可能需要通过量化训练来弥补精度损失，因此实际业务中会结合两者的优劣来应用。</p>\n<p>量化主要有两大难点：一是如何平衡模型的吞吐率和精度，二是如何结合推理引擎充分挖掘芯片的能力。比特数越低其吞吐率可能会越大，但其精度损失可能也会越大，因此，如何通过算法提升精度至关重要，这也是组内的主要工作之一。另外，压缩到低比特，某些情况下吞吐率未必会提升，还需要结合推理引擎优化一起对模型进行图优化，甚至有时候会反馈如何进行网络设计，因此会是一个算法与工程迭代的过程。</p>\n<h6><span id=\"43-模型打包封装-sdk\"> 4.3 模型打包封装 SDK</span></h6>\n<p>实际业务落地过程中，模型可能只是产品流程中的一环，用于实现某些特定功能，其输出可能会用于流程的下一环。因此，模型打包会将模型的前后处理，一个或者多个模型整合到一起，再加入描述性的文件（前后处理的参数、模型相关参数、模型格式和版本等）来实现一个完整的功能。因此，SDK 除了需要一些通用前后处理的高效实现，对齐训练时的前后处理逻辑，还需要具有足够好的扩展性来应对不同的场景，方便业务线同学扩展新的功能。可以看到，模型打包过程更多是模型的进一步组装，将不同模型组装在一起，当需要使用的时候将这些内容解析成整个流程（pipeline）的不同阶段（stage），从而实现整个产品功能。</p>\n<p>另外，考虑到模型很大程度是研究员的研究成果，对外涉及保密问题，因此会对模型进行加密，以保证其安全性。加密算法的选择需要根据实际业务需求来决定，诸如不同加密算法其加解密效率不一样，加解密是否有中心验证服务器，其核心都是为了保护研究成果。</p>\n",
            "tags": [
                "人工智能",
                "模型部署"
            ]
        }
    ]
}