{
    "version": "https://jsonfeed.org/version/1",
    "title": "且听风吟，御剑于心！ • All posts by \"计算机视觉cv\" tag",
    "description": "",
    "home_page_url": "https://leezhao415.github.io",
    "items": [
        {
            "id": "https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/",
            "url": "https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2/",
            "title": "计算机视觉之目标分割",
            "date_published": "2021-06-24T15:14:56.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2\"><strong>目标分割</strong></a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89\"><strong>1 定义</strong></a></li>\n<li><a href=\"#2-%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0%E5%92%8C%E7%B1%BB%E5%9E%8B\"><strong>2 任务描述和类型</strong></a>\n<ul>\n<li><a href=\"#21-%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0\"><strong>2.1 任务描述</strong></a></li>\n<li><a href=\"#22-%E4%BB%BB%E5%8A%A1%E7%B1%BB%E5%9E%8B\"><strong>2.2 任务类型</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#3-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87\"><strong>3 评价指标</strong></a>\n<ul>\n<li><a href=\"#31-%E5%83%8F%E7%B4%A0%E7%B2%BE%E5%BA%A6\"><strong>3.1 像素精度</strong></a></li>\n<li><a href=\"#32-%E5%B9%B3%E5%9D%87%E5%83%8F%E7%B4%A0%E7%B2%BE%E5%BA%A6\"><strong>3.2 平均像素精度</strong></a></li>\n<li><a href=\"#33-%E5%B9%B3%E5%9D%87%E4%BA%A4%E5%B9%B6%E6%AF%94\"><strong>3.3 平均交并比</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#4-%E7%BB%8F%E5%85%B8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C\"><strong>4 经典深度学习网络</strong></a>\n<ul>\n<li><a href=\"#41-fcn\"><strong>4.1 FCN</strong></a></li>\n<li><a href=\"#42-unet\"><strong>4.2 UNet</strong></a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h3><span id=\"目标分割\"> <strong>目标分割</strong></span></h3>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130714683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<ul>\n<li>图像分割是目标检测更进阶的任务，目标检测只需要框出每个目标的包围盒，语义分割需要进一步判断图像中哪些像素属于哪个目标。但是，语义分割不区分属于相同类别的不同实例。如上图所示，当图像中有多个 cube 时，语义分割会将所有立方体整体的所有像素预测为 “cube” 这个类别。与此不同的是，<strong>实例分割</strong>需要区分出哪些像素属于第一个 cube、哪些像素属于第二个 cube……。</li>\n</ul>\n<h4><span id=\"1-定义\"> <strong>1 定义</strong></span></h4>\n<p>在计算机视觉领域，图像分割（Object Segmentation）指的是将数字图像细分为多个图像子区域（像素的集合）的过程，并且同一个子区域内的特征具有一定相似性，不同子区域的特征呈现较为明显的差异。</p>\n<p>图像分割的目标就是为图像中的每个像素分类。应用领域非常的广泛：自动驾驶、医疗影像，图像美化、三维重建等等。</p>\n<ul>\n<li>自动驾驶（Autonomous vehicles）：汽车需要安装必要的感知系统以了解它们的环境，这样自动驾驶汽车才能够安全地驶入现有的道路</li>\n<li>医疗影像诊断（Medical image diagnostics）：机器在分析能力上比放射科医生更强，而且可以大大减少诊断所需时间。</li>\n</ul>\n<h4><span id=\"2-任务描述和类型\"> <strong>2 任务描述和类型</strong></span></h4>\n<h5><span id=\"21-任务描述\"> <strong>2.1 任务描述</strong></span></h5>\n<p>简单来说，我们的目标是输入一个 RGB 彩色图片（<em>h<strong>e</strong>i<strong>g</strong>h**t</em>×<em>w<strong>i</strong>d<strong>t</strong>h</em>×3）或者一个灰度图（<em>h<strong>e</strong>i<strong>g</strong>h**t</em>×<em>w<strong>i</strong>d<strong>t</strong>h</em>×1），然后输出一个包含各个像素类别标签的分割图（<em>h<strong>e</strong>i<strong>g</strong>h**t</em>×<em>w<strong>i</strong>d<strong>t</strong>h</em>×1）。如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130937190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<p>与我们处理分类值的方式类似，预测目标可以采用 one-hot 编码，即为每一个可能的类创建一个输出通道。通过取每个像素点在各个 channel 的 argmax 可以得到最终的预测分割图，（如下图所示）：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131006773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<p>比如：person 的编码为：10000，而 Grass 的编码为：00100</p>\n<p>当将预测结果叠加到单个 channel 时，称这为一个掩膜 mask，它可以给出一张图像中某个特定类的所在区域：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131035900.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<h5><span id=\"22-任务类型\"> <strong>2.2 任务类型</strong></span></h5>\n<p>目前的图像分割任务主要有两类： 语义分割和实例分割</p>\n<p>我们以下图为例，来介绍这两种分割方式：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131129200.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>语义分割就是把图像中每个像素赋予一个类别标签，如下图我们将图像中的像素分类为人，羊，狗，草地即可。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131156944.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<ul>\n<li>实例分割，相对于语义分割来讲，不仅要区分不同类别的像素，还需要需要对同一类别的不同个体进行区分。如下图所示，不仅需要进行类别的划分，还要将各个个体划分出来：羊 1，羊 2，羊 3，羊 4，羊 5 等。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131225421.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<p>目前图像分割的任务主要集中在语义分割，而目前的难点也在于 “语义”，表达某一语义的同一物体并不总是以相同的形象出现，如包含不同的颜色、纹理等，这对精确分割带来了很大的挑战。而且以目前的模型表现来看，在准确率上还有很大的提升空间。而实例分割的思路主要是目标检测 + 语义分割，即用目标检测方法将图像中的不同实例框出，再用语义分割方法在不同检测结果内进行逐像素标记。</p>\n<h4><span id=\"3-评价指标\"> <strong>3 评价指标</strong></span></h4>\n<p>图像分割中通常使用许多标准来衡量算法的精度。这些标准通常是像素精度及 IoU 的变种，以下我们将会介绍常用的几种逐像素标记的精度标准。</p>\n<p>为了便于解释，假设如下：共有<em> k</em>+1 个类（从<em> L</em>0 到<em> L**k</em>，其中包含一个背景类），<em>p<strong>i</strong>j</em> 表示本属于类<em> i</em> 但被预测为类<em> j</em> 的像素。即<em> p<strong>i</strong>i</em> 表示预测正确的像素。</p>\n<h5><span id=\"31-像素精度\"> <strong>3.1 像素精度</strong></span></h5>\n<p>Pixel Accuracy (PA，像素精度)：这是最简单的度量，为预测正确的像素占总像素的比例。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131257252.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p>对于样本不均衡的情况，例如医学图像分割中，背景与标记样本之间的比例往往严重失衡。因此并不适合使用这种方法进行度量。</p>\n<h5><span id=\"32-平均像素精度\"> <strong>3.2 平均像素精度</strong></span></h5>\n<p>Mean Pixel Accuracy (MPA，平均像素精度)：是 PA 的一种简单提升，计算每个<strong>类内</strong>被正确分类像素数的比例，之后求所有类的平均。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131411320.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<h5><span id=\"33-平均交并比\"> <strong>3.3 平均交并比</strong></span></h5>\n<p>Mean Intersection over Union (MIoU，平均交并比)：为语义分割的标准度量，其计算两个集合的交集和并集之比，在语义分割的问题中，这两个集合为真实值（ground truth）和预测值（predicted segmentation）。交集为预测正确的像素数（intersection），并集为预测或真实值为<em> i</em> 类的和减去预测正确的像素，在每个类上计算 IoU，之后求平均即可。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131442793.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p>那么，如何理解这里的公式呢？如下图所示，红色圆代表真实值，黄色圆代表预测值。橙色部分红色圆与黄色圆的交集，即预测正确的部分，红色部分表示假负（真实值为该类预测错误）的部分，黄色表示假正（预测值为 i 类，真实值为其他）的部分。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131512176.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 33%;\"></center>\n<p>MIoU 计算的是计算 A 与 B 的交集（橙色部分）与 A 与 B 的并集（红色 + 橙色 + 黄色）之间的比例，在理想状态下 A 与 B 重合，两者比例为 1 。</p>\n<p><strong>在以上所有的度量标准中，MIoU 由于其简洁、代表性强而成为最常用的度量标准，大多数研究人员都使用该标准报告其结果。PA 对于样本不均衡的情况不适用。</strong></p>\n<h4><span id=\"4-经典深度学习网络\"> <strong>4 经典深度学习网络</strong></span></h4>\n<h5><span id=\"41-fcn\"> <strong>4.1 FCN</strong></span></h5>\n<p><strong>FCN（Fully Convolutional Networks）</strong> 用于图像语义分割，自从该网络提出后，就成为语义分割的基本框架，后续算法基本都是在该网络框架中改进而来。</p>\n<p>对于一般的分类 CNN 网络，如 VGG 和 Resnet，都会在网络的最后加入一些全连接层，经过 softmax 后就可以获得类别概率信息。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624131828712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<p>但是这个概率只能标识整个图片的类别，不能标识每个像素点的类别，所以这种全连接方法不适用于图像分割。</p>\n<p>而 FCN 提出可以把后面几个全连接都换成卷积，这样就可以获得一张 2 维的 feature map，后接 softmax 获得每个像素点的分类信息，从而解决了分割问题，如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624132313797.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>简而言之，FCN 和 CNN 的区别就是：CNN 卷积层之后连接的是全连接层；FCN 卷积层之后仍连接卷积层，输出的是与输入大小相同的特征图。</p>\n<p><strong>1 网络结构</strong></p>\n<p>FCN 是一个端到端，像素对像素的全卷积网络，用于进行图像的语义分割。整体的网络结构分为两个部分：全卷积部分和上采样部分。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624132416744.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 60%;\"></center>\n<p><strong>1.1 全卷积部分</strong></p>\n<p>全卷积部分使用经典的 CNN 网络（以 AlexNet 网络为例），并把最后的全连接层换成 [外链图片转存失败，源站可能有防盗链机制，建议将图片保存下来直接上传 (img-OlWEDzDo-1624508143015)(<a href=\"https://math.jianshu.com/math?formula=1%5Ctimes%201\">https://math.jianshu.com/math?formula=1%5Ctimes%201</a>)] 卷积，用于提取特征。</p>\n<ul>\n<li>在传统的 Alex 结构中，前 5 层是卷积层，第 6 层和第 7 层分别是一个长度为 4096 的一维向量，第 8 层是长度为 1000 的一维向量，分别对应 1000 个不同类别的概率。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624133315447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:60%;\"></center>\n<ul>\n<li>FCN 将最后的 3 层转换为卷积层，卷积核的大小 (通道数，宽，高) 分别为 (4096,1,1)、(4096,1,1)、(1000,1,1)，虽然参数数目相同，但是计算方法就不一样了，这时还可使用预训练模型的参数。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624133402965.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>CNN 中输入的图像固定成 227x227 大小，第一层 pooling 后为 55x55，第二层 pooling 后图像大小为 27x27，第五层 pooling 后的图像大小为 13x13, 而 FCN 输入的图像是 H*W 大小，第一层 pooling 后变为原图大小的 ½，第二层变为原图大小的 ¼，第五层变为原图大小的 ⅛，第八层变为原图大小的 1/16，如下所示：</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624133618160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>经过多次卷积和 pooling 以后，得到的图像越来越小，分辨率越来越低。对最终的特征图进行 upsampling，把图像进行放大到原图像的大小，就得到原图像的分割结果。</li>\n</ul>\n<p><strong>1.2 上采样部分</strong></p>\n<p>上采样部分将最终得到的特征图上采样得到原图像大小的语义分割结果。</p>\n<p>在这里采用的上采样方法是反卷积（Deconvolution），也叫做转置卷积（Transposed Convolution）：</p>\n<ul>\n<li>反卷积是一种特殊的正向卷积</li>\n<li>通俗的讲，就是输入补 0 + 卷积。先按照一定的比例通过补 0 来扩大输入图像的尺寸，再进行正向卷积即可。</li>\n</ul>\n<p>如下图所示：输入图像尺寸为 3x3，卷积核 kernel 为 3x3，步长 strides=2，填充 padding=1</p>\n<center><img src=\"https://img-blog.csdnimg.cn/2021062413365985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>假设反卷积的输入是 n x n ，反卷积的输出为 mxm ，padding=p，stride=s，kernel_size = k。</p>\n<p>那么此时反卷积的输出就为：</p>\n<p>m=s(n−1)+k−2pm=s(n−1)+k−2p</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624133733565.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 33%;\"></center>\n<p>与正向卷积不同的是，要先根据步长 strides 对输入的内部进行填充，这里 strides 可以理解成输入放大的倍数，而不能理解成卷积移动的步长。</p>\n<p>这样我们就可以通过反卷积实现上采样。</p>\n<p><strong>1.3 跳层连接</strong></p>\n<p>如果只利用反卷积对最后一层的特征图进行上采样的到原图大小的分割，由于最后一层的特征图太小，会损失很多细节。因而提出增加 Skips 结构将最后一层的预测（有更富的全局信息）和更浅层（有更多的局部细节）的预测结合起来。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624133817314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>那么：</p>\n<ul>\n<li>对于 FCN-32s，直接对 pool5 feature 进行 32 倍上采样获得 32x upsampled feature，再对 32x upsampled feature 每个点做 softmax prediction 获得 32x upsampled feature prediction（即分割图）。</li>\n<li>对于 FCN-16s，首先对 pool5 feature 进行 2 倍上采样获得 2x upsampled feature，再把 pool4 feature 和 2x upsampled feature 逐点相加，然后对相加的 feature 进行 16 倍上采样，并 softmax prediction，获得 16x upsampled feature prediction。</li>\n<li>对于 FCN-8s，首先进行 pool4+2x upsampled feature 逐点相加，然后又进行 pool3+2x upsampled 逐点相加，即进行更多次特征融合。具体过程与 16s 类似，不再赘述。</li>\n</ul>\n<p>下面有一张 32 倍，16 倍和 8 倍上采样得到的结果图对比：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624133910198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>可以看到随着上采样做得越多，分割结果越来越精细。</p>\n<h5><span id=\"42-unet\"> <strong>4.2 UNet</strong></span></h5>\n<p>Unet 网络是建立在 FCN 网络基础上的，它的网络架构如下图所示，总体来说与 FCN 思路非常类似。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/2021062413393221.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>整个网络由编码部分（左） 和 解码部分（右）组成，类似于一个大大的 U 字母，具体介绍如下：</p>\n<p>1、编码部分是典型的卷积网络架构：</p>\n<ul>\n<li>架构中含有着一种重复结构，每次重复中都有 2 个 3 x 3 卷积层、非线性 ReLU 层和一个 2 x 2 max pooling 层（stride 为 2）。（图中的蓝箭头、红箭头，没画 ReLu）</li>\n<li>每一次下采样后我们都把特征通道的数量加倍</li>\n</ul>\n<p>2、解码部分也使用了类似的模式：</p>\n<ul>\n<li>每一步都首先使用反卷积 (up-convolution)，每次使用反卷积都将特征通道数量减半，特征图大小加倍。（图中绿箭头）</li>\n<li>反卷积过后，将反卷积的结果与编码部分中对应步骤的特征图拼接起来。（白 / 蓝块）</li>\n<li>编码部分中的特征图尺寸稍大，将其修剪过后进行拼接。（左边深蓝虚线）</li>\n<li>对拼接后的 map 再进行 2 次 3 x 3 的卷积。（右侧蓝箭头）</li>\n<li>最后一层的卷积核大小为 1 x 1，将 64 通道的特征图转化为特定类别数量（分类数量）的结果。（图中青色箭头）</li>\n</ul>\n<p>数，而不能理解成卷积移动的步长。</p>\n<p>这样我们就可以通过反卷积实现上采样。</p>\n",
            "tags": [
                "人工智能",
                "计算机视觉CV"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/",
            "url": "https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/",
            "title": "计算机视觉之目标检测",
            "date_published": "2021-06-24T15:11:09.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B\"><strong>目标检测</strong></a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89\"><strong>1 定义</strong></a></li>\n<li><a href=\"#2-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86\"><strong>2 常用数据集</strong></a>\n<ul>\n<li><a href=\"#21-pascal-voc%E6%95%B0%E6%8D%AE%E9%9B%86\"><strong>2.1 PASCAL VOC 数据集</strong></a></li>\n<li><a href=\"#22-ms-coco%E6%95%B0%E6%8D%AE%E9%9B%86\"><strong>2.2 MS COCO 数据集</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#3-%E5%B8%B8%E7%94%A8%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87\"><strong>3 常用的评价指标</strong></a>\n<ul>\n<li><a href=\"#31-iou\"><strong>3.1 IOU</strong></a></li>\n<li><a href=\"#32-mapmean-average-precision\"><strong>3.2 mAP（Mean Average Precision）</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#4-nms%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6\"><strong>4 NMS（非极大值抑制）</strong></a></li>\n<li><a href=\"#5-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95%E5%88%86%E7%B1%BB\"><strong>5 目标检测方法分类</strong></a>\n<ul>\n<li><a href=\"#51-two-stage%E7%9A%84%E7%AE%97%E6%B3%95\"><strong>5.1 two-stage 的算法</strong></a></li>\n<li><a href=\"#52-one-stage%E7%9A%84%E7%AE%97%E6%B3%95\"><strong>5.2 One-stage 的算法</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#6-%E7%BB%8F%E5%85%B8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C\"><strong>6 经典深度学习网络</strong></a>\n<ul>\n<li><a href=\"#61-rcnn\"><strong>6.1 RCNN</strong></a></li>\n<li><a href=\"#62-fast-rcnn\"><strong>6.2 Fast RCNN</strong></a></li>\n<li><a href=\"#63-faster-rcnn\"><strong>6.3 Faster RCNN</strong></a></li>\n<li><a href=\"#64-yolo\"><strong>6.4 YOLO</strong></a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h3><span id=\"目标检测\"> <strong>目标检测</strong></span></h3>\n<h4><span id=\"1-定义\"> <strong>1 定义</strong></span></h4>\n<p>目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标，并确定它们的类别和位置。</p>\n<p>目标检测中能检测出来的物体取决于当前任务（数据集）需要检测的物体有哪些。假设我们的目标检测模型定位是检测动物（牛、羊、猪、狗、猫五种结果），那么模型对任何一张图片输出结果不会输出鸭子、书籍等其它类型结果。</p>\n<p>目标检测的位置信息一般由两种格式（以图片左上角为原点 (0,0)）：</p>\n<p>1、极坐标表示：(xmin, ymin, xmax, ymax)</p>\n<ul>\n<li>xmin,ymin:x,y 坐标的最小值</li>\n<li>xmin,ymin:x,y 坐标的最大值</li>\n</ul>\n<p>2、中心点坐标：(x_center, y_center, w, h)</p>\n<ul>\n<li>x_center, y_center: 目标检测框的中心点坐标</li>\n<li>w,h: 目标检测框的宽、高</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124423469.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<h4><span id=\"2-常用数据集\"> <strong>2 常用数据集</strong></span></h4>\n<p>经典的目标检测数据集有两种，<strong>PASCAL VOC 数据集</strong> 和 <strong>MS COCO 数据集</strong>。</p>\n<h5><span id=\"21-pascal-voc-数据集\"> <strong>2.1 PASCAL VOC 数据集</strong></span></h5>\n<p>PASCAL VOC 是目标检测领域的经典数据集。PASCAL VOC 包含约 10,000 张带有边界框的图片用于训练和验证。PASCAL VOC 数据集是目标检测问题的一个基准数据集，很多模型都是在此数据集上得到的，常用的是 VOC2007 和 VOC2012 两个版本数据，共 20 个类别，分别是：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/2021062412450965.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<p>也就是：</p>\n<p>1. 人：人</p>\n<p>2. 动物：鸟，猫，牛，狗，马，羊</p>\n<p>3. 交通工具：飞机，自行车，船，公共汽车，汽车，摩托车，火车</p>\n<p>4. 室内：瓶子，椅子，餐桌，盆栽，沙发，电视 / 显示器</p>\n<p><strong>下载地址</strong>：<a href=\"https://pjreddie.com/projects/pascal-voc-dataset-mirror/\">https://pjreddie.com/projects/pascal-voc-dataset-mirror/</a></p>\n<p>整个数据的目录结构如下所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124534899.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>其中：</p>\n<ol>\n<li>JPEGImages 存放图片文件</li>\n<li>Annotations 下存放的是 xml 文件，描述了图片信息，如下图所示，需要关注的就是节点下的数据，尤其是 bndbox 下的数据.xmin,ymin 构成了 boundingbox 的左上角，xmax,ymax 构成了 boundingbox 的右下角，也就是图像中的目标位置信息</li>\n</ol>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124614809.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ol>\n<li>ImageSets 包含以下 4 个文件夹：</li>\n<li>Action 下存放的是人的动作（例如 running、jumping 等等）</li>\n<li>Layout 下存放的是具有人体部位的数据（人的 head、hand、feet 等等）</li>\n<li>Segmentation 下存放的是可用于分割的数据。</li>\n<li>Main 下存放的是图像物体识别的数据，总共分为 20 类，这是进行目标检测的重点。该文件夹中的数据对负样本文件进行了描述。</li>\n</ol>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124715162.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h5><span id=\"22-ms-coco-数据集\"> <strong>2.2 MS COCO 数据集</strong></span></h5>\n<p>MS COCO 的全称是 Microsoft Common Objects in Context，微软于 2014 年出资标注的 Microsoft COCO 数据集，与 ImageNet 竞赛一样，被视为是计算机视觉领域最受关注和最权威的比赛之一。</p>\n<p>COCO 数据集是一个大型的、丰富的物体检测，分割和字幕数据集。这个数据集以场景理解为目标，主要从复杂的日常场景中截取，图像中的目标通过精确的分割进行位置的标定。图像包括 91 类目标，328,000 影像和 2,500,000 个 label。目前为止目标检测的最大数据集，提供的类别有 80 类，有超过 33 万张图片，其中 20 万张有标注，整个数据集中个体的数目超过 150 万个。</p>\n<p>图像示例：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/2021062412483086.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>coco 数据集的标签文件标记了每个 segmentation+bounding box 的精确坐标，其精度均为小数点后两位一个目标的标签示意如下：</p>\n<p>{“segmentation”:[[392.87, 275.77, 402.24, 284.2, 382.54, 342.36, 375.99, 356.43, 372.23, 357.37, 372.23, 397.7, 383.48, 419.27,407.87, 439.91, 427.57, 389.25, 447.26, 346.11, 447.26, 328.29, 468.84, 290.77,472.59, 266.38], [429.44,465.23, 453.83, 473.67, 636.73, 474.61, 636.73, 392.07, 571.07, 364.88, 546.69,363.0]], “area”: 28458.996150000003, “iscrowd”: 0,“image_id”: 503837, <strong>“bbox”: [372.23, 266.38, 264.5,208.23]</strong>, “category_id”: 4, “id”: 151109},</p>\n<h4><span id=\"3-常用的评价指标\"> <strong>3 常用的评价指标</strong></span></h4>\n<h5><span id=\"31-iou\"> <strong>3.1 IOU</strong></span></h5>\n<p>在目标检测算法中，IoU（intersection over union，交并比）是目标检测算法中用来评价 2 个矩形框之间相似度的指标：</p>\n<p><strong>IoU = 两个矩形框相交的面积 / 两个矩形框相并的面积</strong>，</p>\n<p>如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124908127.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>通过一个例子看下在目标检测中的应用：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124946931.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 33%;\"></center>\n<p>其中上图蓝色框框为检测结果，红色框框为真实标注。</p>\n<p>那我们就可以通过预测结果与真实结果之间的交并比来衡量两者之间的相似度。一般情况下对于检测框的判定都会存在一个阈值，也就是 <code>IoU</code>  的阈值，一般可以设置当 <code>IoU</code>  的值大于 <code>0.5</code>  的时候，则可认为检测到目标物体。</p>\n<p>实现方法：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\"># 定义方法计算IOU</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Iou</span>(<span class=\"params\">box1, box2, wh=<span class=\"literal\">False</span></span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 判断bbox的表示形式</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> wh == <span class=\"literal\">False</span>:</span><br><span class=\"line\">        <span class=\"comment\"># 使用极坐标形式表示：直接获取两个bbox的坐标</span></span><br><span class=\"line\">        xmin1, ymin1, xmax1, ymax1 = box1</span><br><span class=\"line\">        xmin2, ymin2, xmax2, ymax2 = box2</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"comment\"># 使用中心点形式表示： 获取两个两个bbox的极坐标表示形式</span></span><br><span class=\"line\">        <span class=\"comment\"># 第一个框左上角坐标</span></span><br><span class=\"line\">        xmin1, ymin1 = <span class=\"built_in\">int</span>(box1[<span class=\"number\">0</span>]-box1[<span class=\"number\">2</span>]/<span class=\"number\">2.0</span>), <span class=\"built_in\">int</span>(box1[<span class=\"number\">1</span>]-box1[<span class=\"number\">3</span>]/<span class=\"number\">2.0</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 第一个框右下角坐标</span></span><br><span class=\"line\">        xmax1, ymax1 = <span class=\"built_in\">int</span>(box1[<span class=\"number\">0</span>]+box1[<span class=\"number\">2</span>]/<span class=\"number\">2.0</span>), <span class=\"built_in\">int</span>(box1[<span class=\"number\">1</span>]+box1[<span class=\"number\">3</span>]/<span class=\"number\">2.0</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 第二个框左上角坐标</span></span><br><span class=\"line\">        xmin2, ymin2 = <span class=\"built_in\">int</span>(box2[<span class=\"number\">0</span>]-box2[<span class=\"number\">2</span>]/<span class=\"number\">2.0</span>), <span class=\"built_in\">int</span>(box2[<span class=\"number\">1</span>]-box2[<span class=\"number\">3</span>]/<span class=\"number\">2.0</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 第二个框右下角坐标</span></span><br><span class=\"line\">        xmax2, ymax2 = <span class=\"built_in\">int</span>(box2[<span class=\"number\">0</span>]+box2[<span class=\"number\">2</span>]/<span class=\"number\">2.0</span>), <span class=\"built_in\">int</span>(box2[<span class=\"number\">1</span>]+box2[<span class=\"number\">3</span>]/<span class=\"number\">2.0</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 获取矩形框交集对应的左上角和右下角的坐标（intersection）</span></span><br><span class=\"line\">    xx1 = np.<span class=\"built_in\">max</span>([xmin1, xmin2])</span><br><span class=\"line\">    yy1 = np.<span class=\"built_in\">max</span>([ymin1, ymin2])</span><br><span class=\"line\">    xx2 = np.<span class=\"built_in\">min</span>([xmax1, xmax2])</span><br><span class=\"line\">    yy2 = np.<span class=\"built_in\">min</span>([ymax1, ymax2])</span><br><span class=\"line\">    <span class=\"comment\"># 计算两个矩形框面积</span></span><br><span class=\"line\">    area1 = (xmax1-xmin1) * (ymax1-ymin1) </span><br><span class=\"line\">    area2 = (xmax2-xmin2) * (ymax2-ymin2)</span><br><span class=\"line\">    <span class=\"comment\">#计算交集面积</span></span><br><span class=\"line\">    inter_area = (np.<span class=\"built_in\">max</span>([<span class=\"number\">0</span>, xx2-xx1])) * (np.<span class=\"built_in\">max</span>([<span class=\"number\">0</span>, yy2-yy1]))</span><br><span class=\"line\">    <span class=\"comment\">#计算交并比</span></span><br><span class=\"line\">    iou = inter_area / (area1+area2-inter_area+<span class=\"number\">1e-6</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> iou</span><br></pre></td></tr></table></figure>\n<p>假设我们检测结果如下所示，并展示在图像上：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.patches <span class=\"keyword\">as</span> patches</span><br><span class=\"line\"><span class=\"comment\"># 真实框与预测框</span></span><br><span class=\"line\">True_bbox, predict_bbox = [<span class=\"number\">100</span>, <span class=\"number\">35</span>, <span class=\"number\">398</span>, <span class=\"number\">400</span>], [<span class=\"number\">40</span>, <span class=\"number\">150</span>, <span class=\"number\">355</span>, <span class=\"number\">398</span>]</span><br><span class=\"line\"><span class=\"comment\"># bbox是bounding box的缩写</span></span><br><span class=\"line\">img = plt.imread(<span class=\"string\">&#x27;dog.jpeg&#x27;</span>)</span><br><span class=\"line\">fig = plt.imshow(img)</span><br><span class=\"line\"><span class=\"comment\"># 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：((左上x, 左上y), 宽, 高)</span></span><br><span class=\"line\"><span class=\"comment\"># 真实框绘制</span></span><br><span class=\"line\">fig.axes.add_patch(plt.Rectangle(</span><br><span class=\"line\">    xy=(True_bbox[<span class=\"number\">0</span>], True_bbox[<span class=\"number\">1</span>]), width=True_bbox[<span class=\"number\">2</span>]-True_bbox[<span class=\"number\">0</span>], height=True_bbox[<span class=\"number\">3</span>]-True_bbox[<span class=\"number\">1</span>],</span><br><span class=\"line\">    fill=<span class=\"literal\">False</span>, edgecolor=<span class=\"string\">&quot;blue&quot;</span>, linewidth=<span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"comment\"># 预测框绘制</span></span><br><span class=\"line\">fig.axes.add_patch(plt.Rectangle(</span><br><span class=\"line\">    xy=(predict_bbox[<span class=\"number\">0</span>], predict_bbox[<span class=\"number\">1</span>]), width=predict_bbox[<span class=\"number\">2</span>]-predict_bbox[<span class=\"number\">0</span>], height=predict_bbox[<span class=\"number\">3</span>]-predict_bbox[<span class=\"number\">1</span>],</span><br><span class=\"line\">    fill=<span class=\"literal\">False</span>, edgecolor=<span class=\"string\">&quot;red&quot;</span>, linewidth=<span class=\"number\">2</span>))</span><br></pre></td></tr></table></figure>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125017817.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p>计算 IoU：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Iou(True_bbox,predict_bbox)</span><br></pre></td></tr></table></figure>\n<p>结果为：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">0.5114435907762924</span></span><br></pre></td></tr></table></figure>\n<h5><span id=\"32-mapmean-average-precision\"> <strong>3.2 mAP（Mean Average Precision）</strong></span></h5>\n<p>目标检测问题中的每个图片都可能包含一些不同类别的物体，需要评估模型的物体分类和定位性能。因此，用于图像分类问题的标准指标 precision 不能直接应用于此。 在目标检测中，mAP 是主要的衡量指标。</p>\n<p>mAP 是多个分类任务的 AP 的平均值，而 AP（average precision）是 PR 曲线下的面积，所以在介绍 mAP 之前我们要先得到 PR 曲线。</p>\n<p><strong>TP、FP、FN、TN</strong></p>\n<p><strong>查准率、查全率</strong></p>\n<ul>\n<li>查准率（Precision）: TP/(TP + FP)</li>\n<li>查全率（Recall）: TP/(TP + FN)</li>\n</ul>\n<p>二者绘制的曲线称为 P-R 曲线</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125045167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p>先定义两个公式，一个是 Precision，一个是 Recall，与上面的公式相同，扩展开来，用另外一种形式进行展示，其中  <code>all detctions</code>  代表所有预测框的数量，  <code>all ground truths</code>  代表所有 GT 的数量。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125114930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p>AP 是计算某一类 P-R 曲线下的面积，mAP 则是计算所有类别 P-R 曲线下面积的平均值。</p>\n<p>假设我们有 7 张图片（Images1-Image7），这些图片有 15 个目标（绿色的框，GT 的数量，上文提及的  <code>all ground truths</code> ）以及 24 个预测边框（红色的框，A-Y 编号表示，并且有一个置信度值）：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125140694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<p>根据上图以及说明，我们可以列出以下表格，其中 Images 代表图片的编号，Detections 代表预测边框的编号，Confidences 代表预测边框的置信度，TP or FP 代表预测的边框是标记为 TP 还是 FP（认为预测边框与 GT 的 IOU 值大于等于 0.3 就标记为 TP；若一个 GT 有多个预测边框，则认为 IOU 最大且大于等于 0.3 的预测框标记为 TP，其他的标记为 FP，即一个 GT 只能有一个预测框标记为 TP），这里的 0.3 是随机取的一个值。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125202722.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>通过上表，我们可以绘制出 P-R 曲线（因为 AP 就是 P-R 曲线下面的面积），但是在此之前我们需要计算出 P-R 曲线上各个点的坐标，根据置信度从大到小排序所有的预测框，然后就可以计算 Precision 和 Recall 的值，见下表。（需要记住一个叫累加的概念，就是下图的 ACC TP 和 ACC FP）</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125230581.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>标号为 1 的 Precision 和 Recall 的计算方式：Precision=TP/(TP+FP)=1/(1+0)=1，Recall=TP/(TP+FN)=TP/( <code>all ground truths</code> )=1/15=0.0666 （ <code>all ground truths 上面有定义过了</code> ）</li>\n<li>标号 2：Precision=TP/(TP+FP)=1/(1+1)=0.5，Recall=TP/(TP+FN)=TP/( <code>all ground truths</code> )=1/15=0.0666</li>\n<li>标号 3：Precision=TP/(TP+FP)=2/(2+1)=0.6666，Recall=TP/(TP+FN)=TP/( <code>all ground truths</code> )=2/15=0.1333</li>\n<li>其他的依次类推</li>\n</ul>\n<p>然后就可以绘制出 P-R 曲线</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624140627754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>得到 P-R 曲线就可以计算 AP（P-R 曲线下的面积），要计算 P-R 下方的面积，有两种方法：</p>\n<ul>\n<li>在 VOC2010 以前，只需要选取当 Recall &gt;= 0, 0.1, 0.2, …, 1 共 11 个点时的 Precision 最大值，然后 AP 就是这 11 个 Precision 的平均值，取 11 个点 [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] 的插值所得</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125253497.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>得到一个类别的 AP 结果如下：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/202106241253136.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>要计算 mAP，就把所有类别的 AP 计算出来，然后求取平均即可。</p>\n<ul>\n<li>在 VOC2010 及以后，需要针对每一个不同的 Recall 值（包括 0 和 1），选取其大于等于这些 Recall 值时的 Precision 最大值，如下图所示：</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125342592.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>然后计算 PR 曲线下面积作为 AP 值：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125422648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>计算方法如下所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125447429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h4><span id=\"4-nms非极大值抑制\"> <strong>4 NMS（非极大值抑制）</strong></span></h4>\n<p>非极大值抑制（Non-Maximum Suppression，NMS），顾名思义就是抑制不是极大值的元素。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到 NMS 来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。 NMS 在计算机视觉领域有着非常重要的应用，如视频目标跟踪、数据挖掘、3D 重建、目标识别以及纹理分析等 。</p>\n<p>在目标检测中，NMS 的目的就是要去除冗余的检测框，保留最好的一个，如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125532753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>NMS 的原理是对于预测框的列表 B 及其对应的置信度 S, 选择具有最大 score 的检测框 M, 将其从 B 集合中移除并加入到最终的检测结果 D 中。通常将 B 中剩余检测框中与 M 的 IoU 大于阈值 Nt 的框从 B 中移除。重复这个过程，直到 B 为空。</p>\n<p>使用流程如下图所示：</p>\n<ul>\n<li>首先是检测出一系列的检测框</li>\n<li>将检测框按照类别进行分类</li>\n<li>对同一类别的检测框应用 NMS 获取最终的检测结果</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125612981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>通过一个例子看些 NMS 的使用方法，假设定位车辆，算法就找出了一系列的矩形框，我们需要判别哪些矩形框是没用的，需要使用 NMS 的方法来实现。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125636223.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>假设现在检测窗口有：A、B、C、D、E 5 个候选框，接下来进行迭代计算：</p>\n<ul>\n<li>第一轮：因为 B 是得分最高的，与 B 的 IoU＞0.5 删除。A，CDE 中现在与 B 计算 IoU，DE 结果＞0.5，剔除 DE，B 作为一个预测结果，有个检测框留下 B，放入集合</li>\n<li>第二轮：A 的得分最高，与 A 计算 IoU，C 的结果＞0.5，剔除 C，A 作为一个结果</li>\n</ul>\n<p>最终结果为在这个 5 个中检测出了两个目标为 A 和 B。</p>\n<p>单类别的 NMS 的实现方法如下所示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">nms</span>(<span class=\"params\">bboxes, confidence_score, threshold</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;非极大抑制过程</span></span><br><span class=\"line\"><span class=\"string\">    :param bboxes: 同类别候选框坐标</span></span><br><span class=\"line\"><span class=\"string\">    :param confidence: 同类别候选框分数</span></span><br><span class=\"line\"><span class=\"string\">    :param threshold: iou阈值</span></span><br><span class=\"line\"><span class=\"string\">    :return:</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 1、传入无候选框返回空</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(bboxes) == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> [], []</span><br><span class=\"line\">    <span class=\"comment\"># 强转数组</span></span><br><span class=\"line\">    bboxes = np.array(bboxes)</span><br><span class=\"line\">    score = np.array(confidence_score)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 取出n个的极坐标点</span></span><br><span class=\"line\">    x1 = bboxes[:, <span class=\"number\">0</span>]</span><br><span class=\"line\">    y1 = bboxes[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">    x2 = bboxes[:, <span class=\"number\">2</span>]</span><br><span class=\"line\">    y2 = bboxes[:, <span class=\"number\">3</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2、对候选框进行NMS筛选</span></span><br><span class=\"line\">    <span class=\"comment\"># 返回的框坐标和分数</span></span><br><span class=\"line\">    picked_boxes = []</span><br><span class=\"line\">    picked_score = []</span><br><span class=\"line\">    <span class=\"comment\"># 对置信度进行排序, 获取排序后的下标序号, argsort默认从小到大排序</span></span><br><span class=\"line\">    order = np.argsort(score)</span><br><span class=\"line\">    areas = (x2 - x1) * (y2 - y1)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> order.size &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"comment\"># 将当前置信度最大的框加入返回值列表中</span></span><br><span class=\"line\">        index = order[-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"comment\">#保留该类剩余box中得分最高的一个</span></span><br><span class=\"line\">        picked_boxes.append(bboxes[index])</span><br><span class=\"line\">        picked_score.append(confidence_score[index])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 获取当前置信度最大的候选框与其他任意候选框的相交面积</span></span><br><span class=\"line\">        x11 = np.maximum(x1[index], x1[order[:-<span class=\"number\">1</span>]])</span><br><span class=\"line\">        y11 = np.maximum(y1[index], y1[order[:-<span class=\"number\">1</span>]])</span><br><span class=\"line\">        x22 = np.minimum(x2[index], x2[order[:-<span class=\"number\">1</span>]])</span><br><span class=\"line\">        y22 = np.minimum(y2[index], y2[order[:-<span class=\"number\">1</span>]])</span><br><span class=\"line\">        <span class=\"comment\"># 计算相交的面积,不重叠时面积为0</span></span><br><span class=\"line\">        w = np.maximum(<span class=\"number\">0.0</span>, x22 - x11)</span><br><span class=\"line\">        h = np.maximum(<span class=\"number\">0.0</span>, y22 - y11)</span><br><span class=\"line\">        intersection = w * h</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 利用相交的面积和两个框自身的面积计算框的交并比</span></span><br><span class=\"line\">        ratio = intersection / (areas[index] + areas[order[:-<span class=\"number\">1</span>]] - intersection)</span><br><span class=\"line\">        <span class=\"comment\"># 保留IoU小于阈值的box</span></span><br><span class=\"line\">        keep_boxes_indics = np.where(ratio &lt; threshold)</span><br><span class=\"line\">        <span class=\"comment\"># 保留剩余的框</span></span><br><span class=\"line\">        order = order[keep_boxes_indics]</span><br><span class=\"line\">    <span class=\"comment\"># 返回NMS后的框及分类结果   </span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> picked_boxes, picked_score</span><br></pre></td></tr></table></figure>\n<p>假设有检测结果如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bounding = [(<span class=\"number\">187</span>, <span class=\"number\">82</span>, <span class=\"number\">337</span>, <span class=\"number\">317</span>), (<span class=\"number\">150</span>, <span class=\"number\">67</span>, <span class=\"number\">305</span>, <span class=\"number\">282</span>), (<span class=\"number\">246</span>, <span class=\"number\">121</span>, <span class=\"number\">368</span>, <span class=\"number\">304</span>)]</span><br><span class=\"line\">confidence_score = [<span class=\"number\">0.9</span>, <span class=\"number\">0.65</span>, <span class=\"number\">0.8</span>]</span><br><span class=\"line\">threshold = <span class=\"number\">0.3</span></span><br><span class=\"line\">picked_boxes, picked_score = nms(bounding, confidence_score, threshold)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;阈值threshold为:&#x27;</span>, threshold)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;NMS后得到的bbox是：&#x27;</span>, picked_boxes)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;NMS后得到的bbox的confidences是：&#x27;</span>, picked_score)</span><br></pre></td></tr></table></figure>\n<p>返回结果：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">阈值threshold为: <span class=\"number\">0.3</span></span><br><span class=\"line\">NMS后得到的bbox是： [array([<span class=\"number\">187</span>,  <span class=\"number\">82</span>, <span class=\"number\">337</span>, <span class=\"number\">317</span>])]</span><br><span class=\"line\">NMS后得到的bbox的confidences是： [<span class=\"number\">0.9</span>]</span><br></pre></td></tr></table></figure>\n<h4><span id=\"5-目标检测方法分类\"> <strong>5 目标检测方法分类</strong></span></h4>\n<p>目标检测算法主要分为 two-stage（两阶段）和 one-stage（单阶段）两类：</p>\n<h5><span id=\"51-two-stage-的算法\"> <strong>5.1 two-stage 的算法</strong></span></h5>\n<p>先由算法生成一系列作为样本的候选框，再通过卷积神经网络进行样本分类。如下图所示，主要通过一个卷积神经网络来完成目标检测过程，其提取的是 CNN 卷积特征，进行候选区域的筛选和目标检测两部分。网络的准确度高、速度相对较慢。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125705803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>two-stages 算法的代表是 RCNN 系列：R-CNN 到 Faster R-CNN 网络</p>\n<h5><span id=\"52-one-stage-的算法\"> <strong>5.2 One-stage 的算法</strong></span></h5>\n<p>直接通过主干网络给出目标的类别和位置信息，没有使用候选区域的筛选网路，这种算法速度快，但是精度相对 Two-stage 目标检测网络降低了很多。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125742618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>one-stage 算法的代表是： YOLO 系列：YOLOv1、YOLOv2、YOLOv3、 SSD 等</p>\n<h4><span id=\"6-经典深度学习网络\"> <strong>6 经典深度学习网络</strong></span></h4>\n<h5><span id=\"61-rcnn\"> <strong>6.1 RCNN</strong></span></h5>\n<p>2014 年提出 R-CNN 网络，该网络不再使用暴力穷举的方法，而是使用候选区域方法（region proposal method）创建目标检测的区域来完成目标检测的任务，R-CNN 是以深度神经网络为基础的目标检测的模型 ，以 R-CNN 为基点，后续的 Fast R-CNN、Faster R-CNN 模型都延续了这种目标检测思路。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125814125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>步骤是：</p>\n<ol>\n<li><strong>候选区域生成</strong>：使用选择性搜索（Selective Search）的方法找出图片中可能存在目标的侯选区域</li>\n<li><strong>CNN 网络提取特征</strong>：选取预训练卷积神经网网络（AlexNet 或 VGG）用于进行特征提取。</li>\n<li><strong>目标分类</strong>：训练支持向量机（SVM）来辨别目标物体和背景，对每个类别，都要训练一个二元 SVM。</li>\n<li><strong>目标定位</strong>：训练一个线性回归模型，为每个辨识到的物体生成更精确的边界框。</li>\n</ol>\n<h6><span id=\"1-候选区域生成\"> <strong>1 候选区域生成</strong></span></h6>\n<p>在<strong>选择性搜索（SelectiveSearch，SS）中</strong>，使用语义分割的方法，它将颜色、边界、纹理等信息作为合并条件，采用多尺度的综合方法，将图像在像素级上划分出一系列的区域，这些区域要远远少于传统的滑动窗口的穷举法产生的候选区域。</p>\n<p>SelectiveSearch 在一张图片上提取出来约 2000 个侯选区域，<strong>需要注意的是这些候选区域的长宽不固定</strong>。 而使用 CNN 提取候选区域的特征向量，需要接受固定长度的输入，所以需要对候选区域做一些尺寸上的修改。</p>\n<h6><span id=\"2-cnn-网络提取特征\"> <strong>2 CNN 网络提取特征</strong></span></h6>\n<p>采用预训练模型 (AlexNet 或 VGG) 在生成的候选区域上进行特征提取，将提取好的特征保存在磁盘中，用于后续步骤的分类和回归。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125849971.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>1. 全连接层的输入数据的尺寸是固定的，因此在将候选区域送入 CNN 网络中时，需进行裁剪或变形为固定的尺寸，在进行特征提取。</p>\n<p>2. 预训练模型在 ImageNet 数据集上获得，最后的全连接层是 1000，在这里我们需要将其改为 N+1 (N 为目标类别的数目，例如 VOC 数据集中 N=20，coco 数据集中 N=80，1 是加一个背景) 后，进行微调即可。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125911535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>3. 利用微调后的 CNN 网络，提取每一个候选区域的特征，获取一个 4096 维的特征，一幅图像就是 2000x4096 维特征存储到磁盘中。</p>\n<h6><span id=\"3-目标分类svm\"> <strong>3 目标分类（SVM）</strong></span></h6>\n<p>假设我们要检测猫狗两个类别，那我们需要训练猫和狗两个不同类别的 SVM 分类器，然后使用训练好的分类器对一幅图像中 2000 个候选区域的特征向量分别判断一次，这样得出 [2000, 2] 的得分矩阵，如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125933826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>对于 N 个类别的检测任务，需要训练 N（目标类别数目）个 SVM 分类器，对候选区域的特征向量（4096 维）进行二分类，判断其是某一类别的目标，还是背景来完成目标分类。</p>\n<h6><span id=\"4-目标定位\"> <strong>4 目标定位</strong></span></h6>\n<p>通过选择性搜索获取的目标位置不是非常的准确，实验证明，训练一个线性回归模型在给定的候选区域的结果上去预测一个新的检测窗口，能够获得更精确的位置。修正过程如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624125955589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>通过训练一个回归器来对候选区域的范围进行一个调整，这些候选区域最开始只是用选择性搜索的方法粗略得到的，通过调整之后得到更精确的位置，如下所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130024151.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h6><span id=\"5-预测过程\"> <strong>5 预测过程</strong></span></h6>\n<p>使用选择性搜索的方法从一张图片中提取 2000 个候选区域，将每个区域送入 CNN 网络中进行特征提取，然后送入到 SVM 中进行分类，并使用候选框回归器，计算出每个候选区域的位置。 候选区域较多，有 2000 个，需要剔除掉部分检测结果。 针对每个类，通过计算 IOU, 采取非最大值抑制 NMS 的方法，保留比较好的检测结果。</p>\n<h6><span id=\"算法总结\"> <strong>算法总结</strong></span></h6>\n<p>1、训练阶段多，训练耗时： 微调 CNN 网络 + 训练 SVM + 训练边框回归器。</p>\n<p>2、预测速度慢：使用 GPU, <strong>VGG16 模型处理一张图像需要 47s</strong>。</p>\n<p>3、占用磁盘空间大：5000 张图像产生几百 G 的特征文件。</p>\n<p>4、数据的形状变化：候选区域要经过缩放来固定大小，无法保证目标的不变形</p>\n<h5><span id=\"62-fast-rcnn\"> <strong>6.2 Fast RCNN</strong></span></h5>\n<h6><span id=\"621-模型特点\"> <strong>6.2.1 模型特点</strong></span></h6>\n<p>相比于 R-CNN, Fast R-CNN 主要在以下三个方面进行了改进：</p>\n<p>1、提高训练和预测的速度</p>\n<p>R-CNN 首先从测试图中提取 2000 个候选区域，然后将这 2000 个候选区域分别输入到预训练好的 CNN 中提取特征。由于候选区域有大量的重叠，这种提取特征的方法，就会重复的计算重叠区域的特征。在 Fast-RCNN 中，将整张图输入到 CNN 中提取特征，将候选区域映射到特征图上，这样就避免了对图像区域进行重复处理，提高效率减少时间。</p>\n<p>2、不需要额外的空间保存 CNN 网络提取的特征向量</p>\n<p>RCNN 中需要将提取到的特征保存下来，用于为每个类训练单独的 SVM 分类器和边框回归器。在 Fast-RCNN 中，将类别判断和边框回归统一使用 CNN 实现，不需要在额外的空间存储特征。</p>\n<p>3、不在直接对候选区域进行缩放</p>\n<p>RCNN 中需要对候选区域进行缩放送入 CNN 中进行特征提取，在 Fast-RCNN 中使用 ROIpooling 的方法进行尺寸的调整。</p>\n<h6><span id=\"622-算法流程\"> <strong>6.2.2 算法流程</strong></span></h6>\n<p>Fast_RCNN 的流程如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130052839.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>步骤是：</p>\n<p>1、<strong>候选区域生成</strong>：使用选择性搜索（Selective Search）的方法找出图片中可能存在目标的侯选区域，只需要候选区域的位置信息</p>\n<p>2、<strong>CNN 网络特征提取</strong>：将整张图像输入到 CNN 网络中，得到整副图的特征图，并将上一步获取的候选区域位置从原图映射到该特征图上</p>\n<p>3、<strong>ROIPooling</strong>: 对于每个特征图上候选框，RoI pooling 层从特征图中提取固定长度的特征向量每个特征向量被送入一系列全连接（fc）层中。</p>\n<p>4、<strong>目标检测</strong>：分两部分完成，一个输出各类别加上 1 个背景类别的 Softmax 概率估计，另一个为各类别的每一个类别输出四个实数值，来确定目标的位置信息。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130120663.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h5><span id=\"63-faster-rcnn\"> <strong>6.3 Faster RCNN</strong></span></h5>\n<p>在 R-CNN 和 Fast RCNN 的基础上，在 2016 年提出了 Faster RCNN 网络模型，在结构上，Faster RCNN 已经将候选区域的生成，特征提取，目标分类及目标框的回归都整合在了一个网络中，综合性能有较大提高，在检测速度方面尤为明显。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130141698.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>1、<strong>特征提取</strong>：将整个图像缩放至固定的大小输入到 CNN 网络中进行特征提取，得到特征图。</p>\n<p>2、<strong>候选区域提取</strong>：输入特征图，使用区域生成网络 RPN，产生一些列的候选区域</p>\n<p>3、<strong>ROIPooling</strong>: 与 Fast RCNN 网络中一样，使用最大池化固定候选区域的尺寸，送入后续网络中进行处理</p>\n<p>4、<strong>目标分类和回归</strong>：与 Fast RCNN 网络中一样，使用两个同级层：K+1 个类别的 SoftMax 分类层和边框的回归层，来完成目标的分类和回归。</p>\n<p>Faster R-CNN 的流程与 Fast R-CNN 的区别不是很大，重要的改进是使用 RPN 网络来替代选择性搜索获取候选区域，所以我们可以将 Faster R-CNN 网络看做 RPN 和 Fast R-CNN 网络的结合。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130214226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:60%;\"></center>\n<p>将网络分为四部分：</p>\n<ul>\n<li><strong>Backbone</strong>：Backbone 由 CNN 卷积神经网络构成，常用的是 VGG 和 resnet, 用来提取图像中的特征，获取图像的特征图。该特征图被共享用于后续 RPN 层生成候选区域和 ROIPooling 层中。</li>\n<li><strong>RPN 网络</strong>：RPN 网络用于生成候选区域，用于后续的目标检测。</li>\n<li><strong>Roi Pooling</strong>: 该部分收集图像的特征图和 RPN 网络提取的候选区域位置，综合信息后获取固定尺寸的特征，送入后续全连接层判定目标类别和确定目标位置。</li>\n<li><strong>目标分类与回归</strong>：该部分利用 ROIpooling 输出特征向量计算候选区域的类别，并通过回归获得检测框最终的精确位置。</li>\n</ul>\n<h5><span id=\"64-yolo\"> <strong>6.4 YOLO</strong></span></h5>\n<p>Yolo 意思是 You Only Look Once，它并没有真正的去掉候选区域，而是创造性的将候选区和目标分类合二为一，看一眼图片就能知道有哪些对象以及它们的位置。</p>\n<p>Yolo 模型采用预定义预测区域的方法来完成目标检测，具体而言是将原始图像划分为 7x7=49 个网格（grid），每个网格允许预测出 2 个边框（bounding box，包含某个对象的矩形框），总共 49x2=98 个 bounding box。我们将其理解为 98 个预测区，很粗略的覆盖了图片的整个区域，就在这 98 个预测区中进行目标检测。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130250191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<p>YOLO 的结构非常简单，就是单纯的卷积、池化最后加了两层全连接，从网络结构上看，与前面介绍的 CNN 分类网络没有本质的区别，最大的差异是输出层用线性函数做激活函数，因为需要预测 bounding box 的位置（数值型），而不仅仅是对象的概率。所以粗略来说，YOLO 的整个结构就是输入图片经过神经网络的变换得到一个输出的张量，如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130311213.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<p>网络结构比较简单，重点是我们要理解网络输入与输出之间的关系。</p>\n<h6><span id=\"41-网络输入\"> <strong>4.1 网络输入</strong></span></h6>\n<p>网络的输入是原始图像，唯一的要求是缩放到 448x448 的大小。主要是因为 Yolo 的网络中，卷积层最后接了两个全连接层，全连接层是要求固定大小的向量作为输入，所以 Yolo 的输入图像的大小固定为 448x448。</p>\n<h6><span id=\"42-网络输出\"> <strong>4.2 网络输出</strong></span></h6>\n<p>网络的输出就是一个 7x7x30 的张量（tensor）。那这个输出结果我们要怎么理解那？</p>\n<p><strong>4.2.1 7X7 网格</strong></p>\n<p>根据 YOLO 的设计，输入图像被划分为 7x7 的网格（grid），输出张量中的 7x7 就对应着输入图像的 7x7 网格。或者我们把 7x7x30 的张量看作 7x7=49 个 30 维的向量，也就是输入图像中的每个网格对应输出一个 30 维的向量。如下图所示，比如输入图像左上角的网格对应到输出张量中左上角的向量。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130336324.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<p><strong>4.2.2 30 维向量</strong></p>\n<p>30 维的向量包含：2 个 bbox 的位置和置信度以及该网格属于 20 个类别的概率</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130629189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li><strong>2 个 bounding box 的位置</strong> 每个 bounding box 需要 4 个数值来表示其位置，(Center_x,Center_y,width,height)，即 (bounding box 的中心点的 x 坐标，y 坐标，bounding box 的宽度，高度)，2 个 bounding box 共需要 8 个数值来表示其位置。</li>\n<li><strong>2 个 bounding box 的置信度</strong> bounding box 的置信度 = 该 bounding box 内存在对象的概率 * 该 bounding box 与该对象实际 bounding box 的 IOU，用公式表示就是：</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624130650450.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 33%;\"></center>\n<p>Pr (Object) 是 bounding box 内存在对象的概率</p>\n<ul>\n<li><strong>20 个对象分类的概率</strong></li>\n</ul>\n<p>Yolo 支持识别 20 种不同的对象（人、鸟、猫、汽车、椅子等），所以这里有 20 个值表示该网格位置存在任一种对象的概率.</p>\n",
            "tags": [
                "人工智能",
                "计算机视觉CV"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/",
            "url": "https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/",
            "title": "计算机视觉之图像分类",
            "date_published": "2021-06-24T15:08:20.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB\"><strong>图像分类</strong></a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89\"><strong>1 定义</strong></a></li>\n<li><a href=\"#2-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86\"><strong>2 常用数据集</strong></a>\n<ul>\n<li><a href=\"#21-mnist%E6%95%B0%E6%8D%AE%E9%9B%86\"><strong>2.1 mnist 数据集</strong></a></li>\n<li><a href=\"#22-cifar-10%E5%92%8Ccifar-100\"><strong>2.2 CIFAR-10 和 CIFAR-100</strong></a></li>\n<li><a href=\"#23-imagenet\"><strong>2.3 ImageNet</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#3-%E7%BB%8F%E5%85%B8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C\"><strong>3 经典深度学习网络</strong></a>\n<ul>\n<li><a href=\"#31-alexnet\"><strong>3.1 AlexNet</strong></a></li>\n<li><a href=\"#32-vgg\"><strong>3.2 VGG</strong></a></li>\n<li><a href=\"#33-googlenet\"><strong>3.3 GoogLeNet</strong></a></li>\n<li><a href=\"#34-resnet\"><strong>3.4 ResNet</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#4-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95\"><strong>4 图像增强方法</strong></a>\n<ul>\n<li><a href=\"#41-tfimage%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA\"><strong>4.1 tf.image 进行图像增强</strong></a></li>\n<li><a href=\"#42-%E4%BD%BF%E7%94%A8imagedatagenerator%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA\"><strong>4.2 使用 ImageDataGenerator () 进行图像增强</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#5-%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83\"><strong>5 模型微调</strong></a>\n<ul>\n<li><a href=\"#51-%E5%BE%AE%E8%B0%83\"><strong>5.1 微调</strong></a></li>\n<li><a href=\"#52-%E7%83%AD%E7%8B%97%E8%AF%86%E5%88%AB\"><strong>5.2 热狗识别</strong></a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h3><span id=\"图像分类\"> <strong>图像分类</strong></span></h3>\n<h4><span id=\"1-定义\"> <strong>1 定义</strong></span></h4>\n<p>从给定的类别集合中为图像分配对应标签的任务</p>\n<h4><span id=\"2-常用数据集\"> <strong>2 常用数据集</strong></span></h4>\n<h5><span id=\"21-mnist-数据集\"> <strong>2.1 mnist 数据集</strong></span></h5>\n<p>该数据集是手写数字 0-9 的集合，共有 60k 训练图像、10k 测试图像、10 个类别、图像大小 28×28×1. 我们可以通过 tf.keras 直接加载该数据集：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.datasets <span class=\"keyword\">import</span> mnist</span><br><span class=\"line\"><span class=\"comment\"># 加载mnist数据集</span></span><br><span class=\"line\">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br></pre></td></tr></table></figure>\n<p>随机选择图像展示结果如下所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624122656512.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<h5><span id=\"22-cifar-10-和-cifar-100\"> <strong>2.2 CIFAR-10 和 CIFAR-100</strong></span></h5>\n<ul>\n<li>CIFAR-10 数据集 5 万张训练图像、1 万张测试图像、10 个类别、每个类别有 6k 个图像，图像大小 32×32×3。下图列举了 10 个类，每一类随机展示了 10 张图片：</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624122725728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<ul>\n<li>CIFAR-100 数据集也是有 5 万张训练图像、1 万张测试图像、包含 100 个类别、图像大小 32×32×3。</li>\n</ul>\n<p>在 tf.keras 中加载数据集时：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensorflow.keras.datasets <span class=\"keyword\">import</span> cifar10,cifar100</span><br><span class=\"line\"><span class=\"comment\"># 加载Cifar10数据集</span></span><br><span class=\"line\">(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()</span><br><span class=\"line\"><span class=\"comment\"># 加载Cifar100数据集</span></span><br><span class=\"line\">(train_images, train_labels), (test_images, test_labels)= cifar100.load_data()</span><br></pre></td></tr></table></figure>\n<h5><span id=\"23-imagenet\"> <strong>2.3 ImageNet</strong></span></h5>\n<p>ImageNet 数据集是 ILSVRC 竞赛使用的是数据集，由斯坦福大学李飞飞教授主导，包含了超过 1400 万张全尺寸的有标记图片，大约有 22000 个类别的数据。ILSVRC 全称 ImageNet Large-Scale Visual Recognition Challenge，是视觉领域最受追捧也是最具权威的学术竞赛之一，代表了图像领域的最高水平。从 2010 年开始举办到 2017 年最后一届，使用 ImageNet 数据集的一个子集，总共有 1000 类。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624122823869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>该比赛的获胜者从 2012 年开始都是使用的深度学习的方法：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624122755551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>2012 年冠军是 <code>AlexNet</code> , 由于准确率远超传统方法的第二名（top5 错误率为 15.3%，第二名为 26.2%），引起了很大的轰动。自此之后，CNN 成为在图像识别分类的核心算法模型，带来了深度学习的大爆发。</li>\n<li>2013 年冠军是 <code>ZFNet</code> ，结构和 AlexNet 区别不大，分类效果也差不多。</li>\n<li>2014 年亚军是 <code>VGG</code>  网络，网络结构十分简单，因此至今 VGG-16 仍在广泛使用。</li>\n<li>2014 年的冠军网络是 <code>GooLeNet</code>  ，核心模块是 Inception Module。Inception 历经了 V1、V2、V3、V4 等多个版本的发展，不断趋于完善。GoogLeNet 取名中 L 大写是为了向 LeNet 致敬，而 Inception 的名字来源于盗梦空间中的 &quot;we need to go deeper&quot; 梗。</li>\n<li>2015 年冠军网络是 <code>ResNet</code> 。核心是带短连接的残差模块，其中主路径有两层卷积核（Res34），短连接把模块的输入信息直接和经过两次卷积之后的信息融合，相当于加了一个恒等变换。短连接是深度学习又一重要思想，除计算机视觉外，短连接思想也被用到了机器翻译、语音识别 / 合成领域</li>\n<li>2017 年冠军 <code>SENet</code>  是一个模块，可以和其他的网络架构结合，比如 GoogLeNet、ResNet 等。</li>\n</ul>\n<h4><span id=\"3-经典深度学习网络\"> <strong>3 经典深度学习网络</strong></span></h4>\n<h5><span id=\"31-alexnet\"> <strong>3.1 AlexNet</strong></span></h5>\n<p>2012 年，AlexNet 横空出世，该模型的名字源于论文第一作者的姓名 Alex Krizhevsky 。AlexNet 使用了 8 层卷积神经网络，以很大的优势赢得了 ImageNet 2012 图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的方向。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123016360.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>该网络的特点是：</p>\n<ul>\n<li>AlexNet 包含 8 层变换，有 5 层卷积和 2 层全连接隐藏层，以及 1 个全连接输出层</li>\n<li>AlexNet 第一层中的卷积核形状是 11\\times11。第二层中的卷积核形状减小到 5\\times5，之后全采用 3\\times3。所有的池化层窗口大小为 3\\times3、步幅为 2 的最大池化。</li>\n<li>AlexNet 将 sigmoid 激活函数改成了 ReLU 激活函数，使计算更简单，网络更容易训练</li>\n<li>AlexNet 通过 dropOut 来控制全连接层的模型复杂度。</li>\n<li>AlexNet 引入了大量的图像增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</li>\n</ul>\n<p>在 tf.keras 中实现 AlexNet 模型：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 构建AlexNet模型</span></span><br><span class=\"line\">net = tf.keras.models.Sequential([</span><br><span class=\"line\">    <span class=\"comment\"># 卷积层：96个卷积核，卷积核为11*11，步幅为4，激活函数relu</span></span><br><span class=\"line\">    tf.keras.layers.Conv2D(filters=<span class=\"number\">96</span>,kernel_size=<span class=\"number\">11</span>,strides=<span class=\"number\">4</span>,activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 池化:窗口大小为3*3、步幅为2</span></span><br><span class=\"line\">    tf.keras.layers.MaxPool2D(pool_size=<span class=\"number\">3</span>, strides=<span class=\"number\">2</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 卷积层：256个卷积核，卷积核为5*5，步幅为1，padding为same，激活函数relu</span></span><br><span class=\"line\">    tf.keras.layers.Conv2D(filters=<span class=\"number\">256</span>,kernel_size=<span class=\"number\">5</span>,padding=<span class=\"string\">&#x27;same&#x27;</span>,activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 池化:窗口大小为3*3、步幅为2</span></span><br><span class=\"line\">    tf.keras.layers.MaxPool2D(pool_size=<span class=\"number\">3</span>, strides=<span class=\"number\">2</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 卷积层：384个卷积核，卷积核为3*3，步幅为1，padding为same，激活函数relu</span></span><br><span class=\"line\">    tf.keras.layers.Conv2D(filters=<span class=\"number\">384</span>,kernel_size=<span class=\"number\">3</span>,padding=<span class=\"string\">&#x27;same&#x27;</span>,activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 卷积层：384个卷积核，卷积核为3*3，步幅为1，padding为same，激活函数relu</span></span><br><span class=\"line\">    tf.keras.layers.Conv2D(filters=<span class=\"number\">384</span>,kernel_size=<span class=\"number\">3</span>,padding=<span class=\"string\">&#x27;same&#x27;</span>,activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 卷积层：256个卷积核，卷积核为3*3，步幅为1，padding为same，激活函数relu</span></span><br><span class=\"line\">    tf.keras.layers.Conv2D(filters=<span class=\"number\">256</span>,kernel_size=<span class=\"number\">3</span>,padding=<span class=\"string\">&#x27;same&#x27;</span>,activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 池化:窗口大小为3*3、步幅为2</span></span><br><span class=\"line\">    tf.keras.layers.MaxPool2D(pool_size=<span class=\"number\">3</span>, strides=<span class=\"number\">2</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 伸展为1维向量</span></span><br><span class=\"line\">    tf.keras.layers.Flatten(),</span><br><span class=\"line\">    <span class=\"comment\"># 全连接层:4096个神经元，激活函数relu</span></span><br><span class=\"line\">    tf.keras.layers.Dense(<span class=\"number\">4096</span>,activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 随机失活</span></span><br><span class=\"line\">    tf.keras.layers.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 全链接层：4096个神经元，激活函数relu</span></span><br><span class=\"line\">    tf.keras.layers.Dense(<span class=\"number\">4096</span>,activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 随机失活</span></span><br><span class=\"line\">    tf.keras.layers.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">    <span class=\"comment\"># 输出层：10个神经元，激活函数softmax</span></span><br><span class=\"line\">    tf.keras.layers.Dense(<span class=\"number\">10</span>,activation=<span class=\"string\">&#x27;softmax&#x27;</span>)</span><br><span class=\"line\">])</span><br></pre></td></tr></table></figure>\n<p>我们构造一个高和宽均为 227 的单通道数据样本来看一下模型的架构：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 构造输入X，并将其送入到net网络中</span></span><br><span class=\"line\">X = tf.random.uniform((<span class=\"number\">1</span>,<span class=\"number\">227</span>,<span class=\"number\">227</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">y = net(X)</span><br><span class=\"line\"><span class=\"comment\"># 通过net.summay()查看网络的形状</span></span><br><span class=\"line\">net.summay()</span><br></pre></td></tr></table></figure>\n<p>网络架构如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Model: <span class=\"string\">&quot;sequential&quot;</span></span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (<span class=\"built_in\">type</span>)                 Output Shape              Param <span class=\"comment\">#   </span></span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">conv2d (Conv2D)              (<span class=\"number\">1</span>, <span class=\"number\">55</span>, <span class=\"number\">55</span>, <span class=\"number\">96</span>)           <span class=\"number\">11712</span>     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d (MaxPooling2D) (<span class=\"number\">1</span>, <span class=\"number\">27</span>, <span class=\"number\">27</span>, <span class=\"number\">96</span>)           <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_1 (Conv2D)            (<span class=\"number\">1</span>, <span class=\"number\">27</span>, <span class=\"number\">27</span>, <span class=\"number\">256</span>)          <span class=\"number\">614656</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_1 (MaxPooling2 (<span class=\"number\">1</span>, <span class=\"number\">13</span>, <span class=\"number\">13</span>, <span class=\"number\">256</span>)          <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_2 (Conv2D)            (<span class=\"number\">1</span>, <span class=\"number\">13</span>, <span class=\"number\">13</span>, <span class=\"number\">384</span>)          <span class=\"number\">885120</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_3 (Conv2D)            (<span class=\"number\">1</span>, <span class=\"number\">13</span>, <span class=\"number\">13</span>, <span class=\"number\">384</span>)          <span class=\"number\">1327488</span>   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_4 (Conv2D)            (<span class=\"number\">1</span>, <span class=\"number\">13</span>, <span class=\"number\">13</span>, <span class=\"number\">256</span>)          <span class=\"number\">884992</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_2 (MaxPooling2 (<span class=\"number\">1</span>, <span class=\"number\">6</span>, <span class=\"number\">6</span>, <span class=\"number\">256</span>)            <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">flatten (Flatten)            (<span class=\"number\">1</span>, <span class=\"number\">9216</span>)                 <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense (Dense)                (<span class=\"number\">1</span>, <span class=\"number\">4096</span>)                 <span class=\"number\">37752832</span>  </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout (Dropout)            (<span class=\"number\">1</span>, <span class=\"number\">4096</span>)                 <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_1 (Dense)              (<span class=\"number\">1</span>, <span class=\"number\">4096</span>)                 <span class=\"number\">16781312</span>  </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dropout_1 (Dropout)          (<span class=\"number\">1</span>, <span class=\"number\">4096</span>)                 <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_2 (Dense)              (<span class=\"number\">1</span>, <span class=\"number\">10</span>)                   <span class=\"number\">40970</span>     </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: <span class=\"number\">58</span>,<span class=\"number\">299</span>,082</span><br><span class=\"line\">Trainable params: <span class=\"number\">58</span>,<span class=\"number\">299</span>,082</span><br><span class=\"line\">Non-trainable params: <span class=\"number\">0</span></span><br><span class=\"line\">_________________________________________________________________</span><br></pre></td></tr></table></figure>\n<h5><span id=\"32-vgg\"> <strong>3.2 VGG</strong></span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123100322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>2014 年，牛津大学计算机视觉组（Visual Geometry Group）和 Google DeepMind 公司的研究员一起研发出了新的深度卷积神经网络：VGGNet，并取得了 ILSVRC2014 比赛分类项目的第二名，主要贡献是使用很小的卷积核 (3×3) 构建卷积神经网络结构，能够取得较好的识别精度，常用来提取图像特征的 VGG-16 和 VGG-19。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123126392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>VGGNet 使用的全部都是 3x3 的小卷积核和 2x2 的池化核，通过不断加深网络来提升性能。VGG 可以通过重复使用简单的基础块来构建深度模型。</p>\n<p>在 tf.keras 中实现 VGG 模型，首先来实现 VGG 块，它的组成规律是：连续使用多个相同的填充为 1、卷积核大小为 3\\times 3 的卷积层后接上一个步幅为 2、窗口形状为 2\\times 2 的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用 <code>vgg_block</code>  函数来实现这个基础的 VGG 块，它可以指定卷积层的数量 <code>num_convs</code>  和每层的卷积核个数 num_filters：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义VGG网络中的卷积块：卷积层的个数，卷积层中卷积核的个数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">vgg_block</span>(<span class=\"params\">num_convs, num_filters</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 构建序列模型</span></span><br><span class=\"line\">    blk = tf.keras.models.Sequential()</span><br><span class=\"line\">    <span class=\"comment\"># 遍历所有的卷积层</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_convs):</span><br><span class=\"line\">        <span class=\"comment\"># 每个卷积层：num_filter个卷积核，卷积核大小为3*3，padding是same，激活函数是relu</span></span><br><span class=\"line\">        blk.add(tf.keras.layers.Conv2D(num_filters,kernel_size=<span class=\"number\">3</span>,</span><br><span class=\"line\">                                    padding=<span class=\"string\">&#x27;same&#x27;</span>,activation=<span class=\"string\">&#x27;relu&#x27;</span>))</span><br><span class=\"line\">    <span class=\"comment\"># 卷积块最后是一个最大池化，窗口大小为2*2，步长为2</span></span><br><span class=\"line\">    blk.add(tf.keras.layers.MaxPool2D(pool_size=<span class=\"number\">2</span>, strides=<span class=\"number\">2</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> blk</span><br></pre></td></tr></table></figure>\n<p>VGG16 网络有 5 个卷积块，前 2 块使用两个卷积层，而后 3 块使用三个卷积层。第一块的输出通道是 64，之后每次对输出通道数翻倍，直到变为 512。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义5个卷积块，指明每个卷积块中的卷积层个数及相应的卷积核个数</span></span><br><span class=\"line\">conv_arch = ((<span class=\"number\">2</span>, <span class=\"number\">64</span>), (<span class=\"number\">2</span>, <span class=\"number\">128</span>), (<span class=\"number\">3</span>, <span class=\"number\">256</span>), (<span class=\"number\">3</span>, <span class=\"number\">512</span>), (<span class=\"number\">3</span>, <span class=\"number\">512</span>))</span><br></pre></td></tr></table></figure>\n<p>因为这个网络使用了 13 个卷积层和 3 个全连接层，所以经常被称为 VGG-16, 通过制定 conv_arch 得到模型架构后构建 VGG16：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义VGG网络</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">vgg</span>(<span class=\"params\">conv_arch</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 构建序列模型</span></span><br><span class=\"line\">    net = tf.keras.models.Sequential()</span><br><span class=\"line\">    <span class=\"comment\"># 根据conv_arch生成卷积部分</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (num_convs, num_filters) <span class=\"keyword\">in</span> conv_arch:</span><br><span class=\"line\">        net.add(vgg_block(num_convs, num_filters))</span><br><span class=\"line\">    <span class=\"comment\"># 卷积块序列后添加全连接层</span></span><br><span class=\"line\">    net.add(tf.keras.models.Sequential([</span><br><span class=\"line\">        <span class=\"comment\"># 将特征图展成一维向量</span></span><br><span class=\"line\">        tf.keras.layers.Flatten(),</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层：4096个神经元，激活函数是relu</span></span><br><span class=\"line\">        tf.keras.layers.Dense(<span class=\"number\">4096</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 随机失活</span></span><br><span class=\"line\">        tf.keras.layers.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层：4096个神经元，激活函数是relu</span></span><br><span class=\"line\">        tf.keras.layers.Dense(<span class=\"number\">4096</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 随机失活</span></span><br><span class=\"line\">        tf.keras.layers.Dropout(<span class=\"number\">0.5</span>),</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层：10个神经元，激活函数是softmax</span></span><br><span class=\"line\">        tf.keras.layers.Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;softmax&#x27;</span>)]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> net</span><br><span class=\"line\"><span class=\"comment\"># 网络实例化</span></span><br><span class=\"line\">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>\n<p>我们构造一个高和宽均为 224 的单通道数据样本来看一下模型的架构：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 构造输入X，并将其送入到net网络中</span></span><br><span class=\"line\">X = tf.random.uniform((<span class=\"number\">1</span>,<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">y = net(X)</span><br><span class=\"line\"><span class=\"comment\"># 通过net.summay()查看网络的形状</span></span><br><span class=\"line\">net.summay()</span><br></pre></td></tr></table></figure>\n<p>网络架构如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Model: <span class=\"string\">&quot;sequential_15&quot;</span></span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (<span class=\"built_in\">type</span>)                 Output Shape              Param <span class=\"comment\">#   </span></span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">sequential_16 (Sequential)   (<span class=\"number\">1</span>, <span class=\"number\">112</span>, <span class=\"number\">112</span>, <span class=\"number\">64</span>)         <span class=\"number\">37568</span>     </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">sequential_17 (Sequential)   (<span class=\"number\">1</span>, <span class=\"number\">56</span>, <span class=\"number\">56</span>, <span class=\"number\">128</span>)          <span class=\"number\">221440</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">sequential_18 (Sequential)   (<span class=\"number\">1</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">256</span>)          <span class=\"number\">1475328</span>   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">sequential_19 (Sequential)   (<span class=\"number\">1</span>, <span class=\"number\">14</span>, <span class=\"number\">14</span>, <span class=\"number\">512</span>)          <span class=\"number\">5899776</span>   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">sequential_20 (Sequential)   (<span class=\"number\">1</span>, <span class=\"number\">7</span>, <span class=\"number\">7</span>, <span class=\"number\">512</span>)            <span class=\"number\">7079424</span>   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">sequential_21 (Sequential)   (<span class=\"number\">1</span>, <span class=\"number\">10</span>)                   <span class=\"number\">119586826</span> </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: <span class=\"number\">134</span>,<span class=\"number\">300</span>,<span class=\"number\">362</span></span><br><span class=\"line\">Trainable params: <span class=\"number\">134</span>,<span class=\"number\">300</span>,<span class=\"number\">362</span></span><br><span class=\"line\">Non-trainable params: <span class=\"number\">0</span></span><br><span class=\"line\">__________________________________________________________________</span><br></pre></td></tr></table></figure>\n<h5><span id=\"33-googlenet\"> <strong>3.3 GoogLeNet</strong></span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123157142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>GoogLeNet 的名字不是 GoogleNet，而是 GoogLeNet，这是为了致敬 LeNet。GoogLeNet 和 AlexNet/VGGNet 这类依靠加深网络结构的深度的思想不完全一样。GoogLeNet 在加深度的同时做了结构上的创新，引入了一个叫做 Inception 的结构来代替之前的卷积加激活的经典组件。GoogLeNet 在 ImageNet 分类比赛上的 Top-5 错误率降低到了 6.7%。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123223267.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>整个网络架构我们分为五个模块，每个模块之间使用步幅为 2 的 3×33×3 最大池化层来减小输出高宽。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123306825.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123338652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>代码实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">## B1模块</span></span><br><span class=\"line\"><span class=\"comment\"># 定义模型的输入</span></span><br><span class=\"line\">inputs = tf.keras.Input(shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>),name = <span class=\"string\">&quot;input&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># b1 模块</span></span><br><span class=\"line\"><span class=\"comment\"># 卷积层7*7的卷积核，步长为2，pad是same，激活函数RELU</span></span><br><span class=\"line\">x = tf.keras.layers.Conv2D(<span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, strides=<span class=\"number\">2</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>)(inputs)</span><br><span class=\"line\"><span class=\"comment\"># 最大池化：窗口大小为3*3，步长为2，pad是same</span></span><br><span class=\"line\">x = tf.keras.layers.MaxPool2D(pool_size=<span class=\"number\">3</span>, strides=<span class=\"number\">2</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## B2模块</span></span><br><span class=\"line\"><span class=\"comment\"># 卷积层1*1的卷积核，步长为2，pad是same，激活函数RELU</span></span><br><span class=\"line\">x = tf.keras.layers.Conv2D(<span class=\"number\">64</span>, kernel_size=<span class=\"number\">1</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># 卷积层3*3的卷积核，步长为2，pad是same，激活函数RELU</span></span><br><span class=\"line\">x = tf.keras.layers.Conv2D(<span class=\"number\">192</span>, kernel_size=<span class=\"number\">3</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># 最大池化：窗口大小为3*3，步长为2，pad是same</span></span><br><span class=\"line\">x = tf.keras.layers.MaxPool2D(pool_size=<span class=\"number\">3</span>, strides=<span class=\"number\">2</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## B3模块</span></span><br><span class=\"line\"><span class=\"comment\"># Inception</span></span><br><span class=\"line\">x = Inception(<span class=\"number\">64</span>, (<span class=\"number\">96</span>, <span class=\"number\">128</span>), (<span class=\"number\">16</span>, <span class=\"number\">32</span>), <span class=\"number\">32</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># Inception</span></span><br><span class=\"line\">x = Inception(<span class=\"number\">128</span>, (<span class=\"number\">128</span>, <span class=\"number\">192</span>), (<span class=\"number\">32</span>, <span class=\"number\">96</span>), <span class=\"number\">64</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># 最大池化：窗口大小为3*3，步长为2，pad是same</span></span><br><span class=\"line\">x = tf.keras.layers.MaxPool2D(pool_size=<span class=\"number\">3</span>, strides=<span class=\"number\">2</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## B4模块</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">aux_classifier</span>(<span class=\"params\">x, filter_size</span>):</span></span><br><span class=\"line\">    <span class=\"comment\">#x:输入数据，filter_size:卷积层卷积核个数，全连接层神经元个数</span></span><br><span class=\"line\">    <span class=\"comment\"># 池化层</span></span><br><span class=\"line\">    x = tf.keras.layers.AveragePooling2D(</span><br><span class=\"line\">        pool_size=<span class=\"number\">5</span>, strides=<span class=\"number\">3</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>)(x)</span><br><span class=\"line\">    <span class=\"comment\"># 1x1 卷积层</span></span><br><span class=\"line\">    x = tf.keras.layers.Conv2D(filters=filter_size[<span class=\"number\">0</span>], kernel_size=<span class=\"number\">1</span>, strides=<span class=\"number\">1</span>,</span><br><span class=\"line\">                               padding=<span class=\"string\">&#x27;valid&#x27;</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>)(x)</span><br><span class=\"line\">    <span class=\"comment\"># 展平</span></span><br><span class=\"line\">    x = tf.keras.layers.Flatten()(x)</span><br><span class=\"line\">    <span class=\"comment\"># 全连接层1</span></span><br><span class=\"line\">    x = tf.keras.layers.Dense(units=filter_size[<span class=\"number\">1</span>], activation=<span class=\"string\">&#x27;relu&#x27;</span>)(x)</span><br><span class=\"line\">    <span class=\"comment\"># softmax输出层</span></span><br><span class=\"line\">    x = tf.keras.layers.Dense(units=<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;softmax&#x27;</span>)(x)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Inception</span></span><br><span class=\"line\">x = Inception(<span class=\"number\">192</span>, (<span class=\"number\">96</span>, <span class=\"number\">208</span>), (<span class=\"number\">16</span>, <span class=\"number\">48</span>), <span class=\"number\">64</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># 辅助输出1</span></span><br><span class=\"line\">aux_output_1 = aux_classifier(x, [<span class=\"number\">128</span>, <span class=\"number\">1024</span>])</span><br><span class=\"line\"><span class=\"comment\"># Inception</span></span><br><span class=\"line\">x = Inception(<span class=\"number\">160</span>, (<span class=\"number\">112</span>, <span class=\"number\">224</span>), (<span class=\"number\">24</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># Inception</span></span><br><span class=\"line\">x = Inception(<span class=\"number\">128</span>, (<span class=\"number\">128</span>, <span class=\"number\">256</span>), (<span class=\"number\">24</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># Inception</span></span><br><span class=\"line\">x = Inception(<span class=\"number\">112</span>, (<span class=\"number\">144</span>, <span class=\"number\">288</span>), (<span class=\"number\">32</span>, <span class=\"number\">64</span>), <span class=\"number\">64</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># 辅助输出2</span></span><br><span class=\"line\">aux_output_2 = aux_classifier(x, [<span class=\"number\">128</span>, <span class=\"number\">1024</span>])</span><br><span class=\"line\"><span class=\"comment\"># Inception</span></span><br><span class=\"line\">x = Inception(<span class=\"number\">256</span>, (<span class=\"number\">160</span>, <span class=\"number\">320</span>), (<span class=\"number\">32</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># 最大池化</span></span><br><span class=\"line\">x = tf.keras.layers.MaxPool2D(pool_size=<span class=\"number\">3</span>, strides=<span class=\"number\">2</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## B5模块</span></span><br><span class=\"line\"><span class=\"comment\"># Inception</span></span><br><span class=\"line\">x = Inception(<span class=\"number\">256</span>, (<span class=\"number\">160</span>, <span class=\"number\">320</span>), (<span class=\"number\">32</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># Inception</span></span><br><span class=\"line\">x = Inception(<span class=\"number\">384</span>, (<span class=\"number\">192</span>, <span class=\"number\">384</span>), (<span class=\"number\">48</span>, <span class=\"number\">128</span>), <span class=\"number\">128</span>)(x)</span><br><span class=\"line\"><span class=\"comment\"># GAP</span></span><br><span class=\"line\">x = tf.keras.layers.GlobalAvgPool2D()(x)</span><br><span class=\"line\"><span class=\"comment\"># 输出层</span></span><br><span class=\"line\">main_outputs = tf.keras.layers.Dense(<span class=\"number\">10</span>,activation=<span class=\"string\">&#x27;softmax&#x27;</span>)(x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用Model来创建模型，指明输入和输出</span></span><br><span class=\"line\">model = tf.keras.Model(inputs=inputs, outputs=[main_outputs,aux_output_1，aux_output_2]) </span><br><span class=\"line\">model.summary()</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Model: <span class=\"string\">&quot;functional_3&quot;</span></span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (<span class=\"built_in\">type</span>)                 Output Shape              Param <span class=\"comment\">#   </span></span><br><span class=\"line\">=================================================================</span><br><span class=\"line\"><span class=\"built_in\">input</span> (InputLayer)           [(<span class=\"literal\">None</span>, <span class=\"number\">224</span>, <span class=\"number\">224</span>, <span class=\"number\">3</span>)]     <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_122 (Conv2D)          (<span class=\"literal\">None</span>, <span class=\"number\">112</span>, <span class=\"number\">112</span>, <span class=\"number\">64</span>)      <span class=\"number\">9472</span>      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_27 (MaxPooling (<span class=\"literal\">None</span>, <span class=\"number\">56</span>, <span class=\"number\">56</span>, <span class=\"number\">64</span>)        <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_123 (Conv2D)          (<span class=\"literal\">None</span>, <span class=\"number\">56</span>, <span class=\"number\">56</span>, <span class=\"number\">64</span>)        <span class=\"number\">4160</span>      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">conv2d_124 (Conv2D)          (<span class=\"literal\">None</span>, <span class=\"number\">56</span>, <span class=\"number\">56</span>, <span class=\"number\">192</span>)       <span class=\"number\">110784</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_28 (MaxPooling (<span class=\"literal\">None</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">192</span>)       <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">inception_19 (Inception)     (<span class=\"literal\">None</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">256</span>)       <span class=\"number\">163696</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">inception_20 (Inception)     (<span class=\"literal\">None</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">480</span>)       <span class=\"number\">388736</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_31 (MaxPooling (<span class=\"literal\">None</span>, <span class=\"number\">14</span>, <span class=\"number\">14</span>, <span class=\"number\">480</span>)       <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">inception_21 (Inception)     (<span class=\"literal\">None</span>, <span class=\"number\">14</span>, <span class=\"number\">14</span>, <span class=\"number\">512</span>)       <span class=\"number\">376176</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">inception_22 (Inception)     (<span class=\"literal\">None</span>, <span class=\"number\">14</span>, <span class=\"number\">14</span>, <span class=\"number\">512</span>)       <span class=\"number\">449160</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">inception_23 (Inception)     (<span class=\"literal\">None</span>, <span class=\"number\">14</span>, <span class=\"number\">14</span>, <span class=\"number\">512</span>)       <span class=\"number\">510104</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">inception_24 (Inception)     (<span class=\"literal\">None</span>, <span class=\"number\">14</span>, <span class=\"number\">14</span>, <span class=\"number\">528</span>)       <span class=\"number\">605376</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">inception_25 (Inception)     (<span class=\"literal\">None</span>, <span class=\"number\">14</span>, <span class=\"number\">14</span>, <span class=\"number\">832</span>)       <span class=\"number\">868352</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d_37 (MaxPooling (<span class=\"literal\">None</span>, <span class=\"number\">7</span>, <span class=\"number\">7</span>, <span class=\"number\">832</span>)         <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">inception_26 (Inception)     (<span class=\"literal\">None</span>, <span class=\"number\">7</span>, <span class=\"number\">7</span>, <span class=\"number\">832</span>)         <span class=\"number\">1043456</span>   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">inception_27 (Inception)     (<span class=\"literal\">None</span>, <span class=\"number\">7</span>, <span class=\"number\">7</span>, <span class=\"number\">1024</span>)        <span class=\"number\">1444080</span>   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">global_average_pooling2d_2 ( (<span class=\"literal\">None</span>, <span class=\"number\">1024</span>)              <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense_10 (Dense)             (<span class=\"literal\">None</span>, <span class=\"number\">10</span>)                <span class=\"number\">10250</span>     </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: <span class=\"number\">5</span>,<span class=\"number\">983</span>,<span class=\"number\">802</span></span><br><span class=\"line\">Trainable params: <span class=\"number\">5</span>,<span class=\"number\">983</span>,<span class=\"number\">802</span></span><br><span class=\"line\">Non-trainable params: <span class=\"number\">0</span></span><br><span class=\"line\">___________________________________________________________</span><br></pre></td></tr></table></figure>\n<h5><span id=\"34-resnet\"> <strong>3.4 ResNet</strong></span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123432304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>网络越深，获取的信息就越多，特征也越丰富。但是在实践中，随着网络的加深，优化效果反而越差，测试数据和训练数据的准确率反而降低了。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/2021062412345640.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>针对这一问题，何恺明等人提出了残差网络（ResNet）在 2015 年的 ImageNet 图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123528400.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>ResNet 网络中按照残差块的通道数分为不同的模块。第一个模块前使用了步幅为 2 的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p>\n<p>下面我们来实现这些模块。注意，这里对第一个模块做了特别处理。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># ResNet网络中模块的构成</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ResnetBlock</span>(<span class=\"params\">tf.keras.layers.Layer</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 网络层的定义：输出通道数（卷积核个数），模块中包含的残差块个数，是否为第一个模块</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self,num_channels, num_residuals, first_block=<span class=\"literal\">False</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ResnetBlock, self).__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 模块中的网络层</span></span><br><span class=\"line\">        self.listLayers=[]</span><br><span class=\"line\">        <span class=\"comment\"># 遍历模块中所有的层</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_residuals):</span><br><span class=\"line\">            <span class=\"comment\"># 若为第一个残差块并且不是第一个模块，则使用1*1卷积，步长为2（目的是减小特征图，并增大通道数）</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> i == <span class=\"number\">0</span> <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> first_block:</span><br><span class=\"line\">                self.listLayers.append(Residual(num_channels, use_1x1conv=<span class=\"literal\">True</span>, strides=<span class=\"number\">2</span>))</span><br><span class=\"line\">            <span class=\"comment\"># 否则不使用1*1卷积，步长为1 </span></span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                self.listLayers.append(Residual(num_channels))      </span><br><span class=\"line\">    <span class=\"comment\"># 定义前向传播过程</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span>(<span class=\"params\">self, X</span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># 所有层依次向前传播即可</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> self.listLayers.layers:</span><br><span class=\"line\">            X = layer(X)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> X</span><br></pre></td></tr></table></figure>\n<p>ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样：在输出通道数为 64、步幅为 2 的 7×77×7 卷积层后接步幅为 2 的 3×33×3 的最大池化层。不同之处在于 ResNet 每个卷积层后增加了 BN 层，接着是所有残差模块，最后，与 GoogLeNet 一样，加入全局平均池化层（GAP）后接上全连接层输出。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 构建ResNet网络</span></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ResNet</span>(<span class=\"params\">tf.keras.Model</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># 初始化：指定每个模块中的残差快的个数</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self,num_blocks</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ResNet, self).__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 输入层：7*7卷积，步长为2</span></span><br><span class=\"line\">        self.conv=layers.Conv2D(<span class=\"number\">64</span>, kernel_size=<span class=\"number\">7</span>, strides=<span class=\"number\">2</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>)</span><br><span class=\"line\">        <span class=\"comment\"># BN层</span></span><br><span class=\"line\">        self.bn=layers.BatchNormalization()</span><br><span class=\"line\">        <span class=\"comment\"># 激活层</span></span><br><span class=\"line\">        self.relu=layers.Activation(<span class=\"string\">&#x27;relu&#x27;</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 最大池化层</span></span><br><span class=\"line\">        self.mp=layers.MaxPool2D(pool_size=<span class=\"number\">3</span>, strides=<span class=\"number\">2</span>, padding=<span class=\"string\">&#x27;same&#x27;</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 第一个block，通道数为64</span></span><br><span class=\"line\">        self.resnet_block1=ResnetBlock(<span class=\"number\">64</span>,num_blocks[<span class=\"number\">0</span>], first_block=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"comment\"># 第二个block，通道数为128</span></span><br><span class=\"line\">        self.resnet_block2=ResnetBlock(<span class=\"number\">128</span>,num_blocks[<span class=\"number\">1</span>])</span><br><span class=\"line\">        <span class=\"comment\"># 第三个block，通道数为256</span></span><br><span class=\"line\">        self.resnet_block3=ResnetBlock(<span class=\"number\">256</span>,num_blocks[<span class=\"number\">2</span>])</span><br><span class=\"line\">        <span class=\"comment\"># 第四个block，通道数为512</span></span><br><span class=\"line\">        self.resnet_block4=ResnetBlock(<span class=\"number\">512</span>,num_blocks[<span class=\"number\">3</span>])</span><br><span class=\"line\">        <span class=\"comment\"># 全局平均池化</span></span><br><span class=\"line\">        self.gap=layers.GlobalAvgPool2D()</span><br><span class=\"line\">        <span class=\"comment\"># 全连接层：分类</span></span><br><span class=\"line\">        self.fc=layers.Dense(units=<span class=\"number\">10</span>,activation=tf.keras.activations.softmax)</span><br><span class=\"line\">    <span class=\"comment\"># 前向传播过程</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">call</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># 卷积</span></span><br><span class=\"line\">        x=self.conv(x)</span><br><span class=\"line\">        <span class=\"comment\"># BN</span></span><br><span class=\"line\">        x=self.bn(x)</span><br><span class=\"line\">        <span class=\"comment\"># 激活</span></span><br><span class=\"line\">        x=self.relu(x)</span><br><span class=\"line\">        <span class=\"comment\"># 最大池化</span></span><br><span class=\"line\">        x=self.mp(x)</span><br><span class=\"line\">        <span class=\"comment\"># 残差模块</span></span><br><span class=\"line\">        x=self.resnet_block1(x)</span><br><span class=\"line\">        x=self.resnet_block2(x)</span><br><span class=\"line\">        x=self.resnet_block3(x)</span><br><span class=\"line\">        x=self.resnet_block4(x)</span><br><span class=\"line\">        <span class=\"comment\"># 全局平均池化</span></span><br><span class=\"line\">        x=self.gap(x)</span><br><span class=\"line\">        <span class=\"comment\"># 全链接层</span></span><br><span class=\"line\">        x=self.fc(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"><span class=\"comment\"># 模型实例化：指定每个block中的残差块个数 </span></span><br><span class=\"line\">mynet=ResNet([<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>])</span><br></pre></td></tr></table></figure>\n<p>这里每个模块里有 4 个卷积层（不计算 1×1 卷积层），加上最开始的卷积层和最后的全连接层，共计 18 层。这个模型被称为 ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的 ResNet 模型，例如更深的含 152 层的 ResNet-152。虽然 ResNet 的主体架构跟 GoogLeNet 的类似，但 ResNet 结构更简单，修改也更方便。这些因素都导致了 ResNet 迅速被广泛使用。 在训练 ResNet 之前，我们来观察一下输入形状在 ResNet 的架构：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = tf.random.uniform(shape=(<span class=\"number\">1</span>,  <span class=\"number\">224</span>, <span class=\"number\">224</span> , <span class=\"number\">1</span>))</span><br><span class=\"line\">y = mynet(X)</span><br><span class=\"line\">mynet.summary()</span><br><span class=\"line\">Model: <span class=\"string\">&quot;res_net&quot;</span></span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">Layer (<span class=\"built_in\">type</span>)                 Output Shape              Param <span class=\"comment\">#   </span></span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">conv2d_2 (Conv2D)            multiple                  <span class=\"number\">3200</span>      </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">batch_normalization_2 (Batch multiple                  <span class=\"number\">256</span>       </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">activation (Activation)      multiple                  <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">max_pooling2d (MaxPooling2D) multiple                  <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">resnet_block (ResnetBlock)   multiple                  <span class=\"number\">148736</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">resnet_block_1 (ResnetBlock) multiple                  <span class=\"number\">526976</span>    </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">resnet_block_2 (ResnetBlock) multiple                  <span class=\"number\">2102528</span>   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">resnet_block_3 (ResnetBlock) multiple                  <span class=\"number\">8399360</span>   </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">global_average_pooling2d (Gl multiple                  <span class=\"number\">0</span>         </span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">dense (Dense)                multiple                  <span class=\"number\">5130</span>      </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: <span class=\"number\">11</span>,<span class=\"number\">186</span>,<span class=\"number\">186</span></span><br><span class=\"line\">Trainable params: <span class=\"number\">11</span>,<span class=\"number\">178</span>,<span class=\"number\">378</span></span><br><span class=\"line\">Non-trainable params: <span class=\"number\">7</span>,<span class=\"number\">808</span></span><br><span class=\"line\">_________________________________________________________________</span><br></pre></td></tr></table></figure>\n<h4><span id=\"4-图像增强方法\"> <strong>4 图像增强方法</strong></span></h4>\n<p>图像增强（image augmentation）指通过剪切、旋转 / 反射 / 翻转变换、缩放变换、平移变换、尺度变换、对比度变换、噪声扰动、颜色变换等一种或多种组合数据增强变换的方式来增加数据集的大小。图像增强的意义是通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模，而且随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。</p>\n<p>常见的图像增强方式可以分为两类：几何变换类和颜色变换类</p>\n<ul>\n<li>几何变换类，主要是对图像进行几何变换操作，包括<strong>翻转，旋转，裁剪，变形，缩放</strong>等。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123612578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>颜色变换类，指通过模糊、颜色变换、擦除、填充等方式对图像进行处理</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123652449.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>实现图像增强可以通过 tf.image 来完成，也可以通过 tf.keras.imageGenerator 来完成。</p>\n<h5><span id=\"41-tfimage-进行图像增强\"> <strong>4.1 tf.image 进行图像增强</strong></span></h5>\n<p>导入所需的工具包并读取要处理的图像：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导入工具包</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\"># 读取图像并显示</span></span><br><span class=\"line\">cat = plt.imread(<span class=\"string\">&#x27;./cat.jpg&#x27;</span>)</span><br><span class=\"line\">plt.imshow(cat)</span><br></pre></td></tr></table></figure>\n<p><strong>1 翻转和裁剪</strong></p>\n<p>左右翻转图像是最早也是最广泛使用的一种图像增广方法。可以通过 <code>tf.image.random_flip_left_right</code>  来实现图像左右翻转。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 左右翻转并显示</span></span><br><span class=\"line\">cat1 = tf.image.random_flip_left_right(cat)</span><br><span class=\"line\">plt.imshow(cat1）</span><br></pre></td></tr></table></figure>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123745141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p>创建 <code>tf.image.random_flip_up_down</code>  实例来实现图像的上下翻转，上下翻转使用的较少。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 上下翻转</span></span><br><span class=\"line\">cat2 = tf.image.random_flip_up_down(cat)</span><br><span class=\"line\">plt.imshow(cat2)</span><br></pre></td></tr></table></figure>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123834552.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p>随机裁剪出一块面积为原面积 10%∼100%10%∼100% 的区域，且该区域的宽和高之比随机取自 0.5∼20.5∼2，然后再将该区域的宽和高分别缩放到 200 像素。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 随机裁剪</span></span><br><span class=\"line\">cat3 = tf.image.random_crop(cat,(<span class=\"number\">200</span>,<span class=\"number\">200</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">plt.imshow(cat3)</span><br></pre></td></tr></table></figure>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624123921882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p><strong>2 颜色变换</strong></p>\n<p>另一类增广方法是颜色变换。我们可以从 4 个方面改变图像的颜色：亮度、对比度、饱和度和色调。接下来将图像的亮度随机变化为原图亮度的 50%50%（即 1−0.51−0.5）∼150%∼150%（即 1+0.51+0.5）。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat4=tf.image.random_brightness(cat,<span class=\"number\">0.5</span>)</span><br><span class=\"line\">plt.imshow(cat4)</span><br></pre></td></tr></table></figure>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124001130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p>类似地，我们也可以随机变化图像的色调</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat5 = tf.image.random_hue(cat,<span class=\"number\">0.5</span>)</span><br><span class=\"line\">plt.imshow(cat5)</span><br></pre></td></tr></table></figure>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124033222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<h5><span id=\"42-使用-imagedatagenerator-进行图像增强\"> <strong>4.2 使用 ImageDataGenerator () 进行图像增强</strong></span></h5>\n<p>ImageDataGenerator () 是 keras.preprocessing.image 模块中的图片生成器，可以在 batch 中对数据进行增强，扩充数据集大小，增强模型的泛化能力。比如旋转，变形等，如下所示：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">keras.preprocessing.image.ImageDataGenerator(</span><br><span class=\"line\">               rotation_range=<span class=\"number\">0</span>, <span class=\"comment\">#整数。随机旋转的度数范围。</span></span><br><span class=\"line\">               width_shift_range=<span class=\"number\">0.0</span>, <span class=\"comment\">#浮点数、宽度平移</span></span><br><span class=\"line\">               height_shift_range=<span class=\"number\">0.0</span>, <span class=\"comment\">#浮点数、高度平移</span></span><br><span class=\"line\">               brightness_range=<span class=\"literal\">None</span>, <span class=\"comment\"># 亮度调整</span></span><br><span class=\"line\">               shear_range=<span class=\"number\">0.0</span>, <span class=\"comment\"># 裁剪</span></span><br><span class=\"line\">               zoom_range=<span class=\"number\">0.0</span>, <span class=\"comment\">#浮点数 或 [lower, upper]。随机缩放范围</span></span><br><span class=\"line\">               horizontal_flip=<span class=\"literal\">False</span>, <span class=\"comment\"># 左右翻转</span></span><br><span class=\"line\">               vertical_flip=<span class=\"literal\">False</span>, <span class=\"comment\"># 垂直翻转</span></span><br><span class=\"line\">               rescale=<span class=\"literal\">None</span> <span class=\"comment\"># 尺度调整</span></span><br><span class=\"line\">            )</span><br></pre></td></tr></table></figure>\n<p>来看下水平翻转的结果：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获取数据集</span></span><br><span class=\"line\">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class=\"line\"><span class=\"comment\"># 将数据转换为4维的形式</span></span><br><span class=\"line\">x_train = X_train.reshape(X_train.shape[<span class=\"number\">0</span>],<span class=\"number\">28</span>,<span class=\"number\">28</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">x_test = X_test.reshape(X_test.shape[<span class=\"number\">0</span>],<span class=\"number\">28</span>,<span class=\"number\">28</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 设置图像增强方式：水平翻转</span></span><br><span class=\"line\">datagen = ImageDataGenerator(horizontal_flip=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># 查看增强后的结果</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> X_batch,y_batch <span class=\"keyword\">in</span> datagen.flow(x_train,y_train,batch_size=<span class=\"number\">9</span>):</span><br><span class=\"line\">    plt.figure(figsize=(<span class=\"number\">8</span>,<span class=\"number\">8</span>)) <span class=\"comment\"># 设定每个图像显示的大小</span></span><br><span class=\"line\">    <span class=\"comment\"># 产生一个3*3网格的图像</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>,<span class=\"number\">9</span>):</span><br><span class=\"line\">        plt.subplot(<span class=\"number\">330</span>+<span class=\"number\">1</span>+i) </span><br><span class=\"line\">        plt.title(y_batch[i])</span><br><span class=\"line\">        plt.axis(<span class=\"string\">&#x27;off&#x27;</span>)</span><br><span class=\"line\">        plt.imshow(X_batch[i].reshape(<span class=\"number\">28</span>,<span class=\"number\">28</span>),cmap=<span class=\"string\">&#x27;gray&#x27;</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124125444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h4><span id=\"5-模型微调\"> <strong>5 模型微调</strong></span></h4>\n<h5><span id=\"51-微调\"> <strong>5.1 微调</strong></span></h5>\n<p>如何在只有 6 万张图像的 MNIST 训练数据集上训练模型。学术界当下使用最广泛的大规模图像数据集 ImageNet，它有超过 1,000 万的图像和 1,000 类的物体。然而，我们平常接触到数据集的规模通常在这两者之间。假设我们想从图像中识别出不同种类的椅子，然后将购买链接推荐给用户。一种可能的方法是先找出 100 种常见的椅子，为每种椅子拍摄 1,000 张不同角度的图像，然后在收集到的图像数据集上训练一个分类模型。另外一种解决办法是应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。例如，虽然 ImageNet 数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。</p>\n<p>微调由以下 4 步构成。</p>\n<ol>\n<li>在源数据集（如 ImageNet 数据集）上预训练一个神经网络模型，即源模型。</li>\n<li>创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。</li>\n<li>为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</li>\n<li>在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。</li>\n</ol>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124223925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。</p>\n<h5><span id=\"52-热狗识别\"> <strong>5.2 热狗识别</strong></span></h5>\n<p>接下来我们来实践一个具体的例子：热狗识别。将基于一个小数据集对在 ImageNet 数据集上训练好的 ResNet 模型进行微调。该小数据集含有数千张热狗或者其他事物的图像。我们将使用微调得到的模型来识别一张图像中是否包含热狗。</p>\n<p>首先，导入实验所需的工具包。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br></pre></td></tr></table></figure>\n<p><strong>5.2.1 获取数据集</strong></p>\n<p>我们首先将数据集放在路径 hotdog/data 之下:</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624124255387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>每个类别文件夹里面是图像文件。</p>\n<p>上一节中我们介绍了 ImageDataGenerator 进行图像增强，我们可以通过以下方法读取图像文件，该方法以文件夹路径为参数，生成经过图像增强后的结果，并产生 batch 数据：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">flow_from_directory(self, directory,</span><br><span class=\"line\">                            target_size=(<span class=\"number\">256</span>, <span class=\"number\">256</span>), color_mode=<span class=\"string\">&#x27;rgb&#x27;</span>,</span><br><span class=\"line\">                            classes=<span class=\"literal\">None</span>, class_mode=<span class=\"string\">&#x27;categorical&#x27;</span>,</span><br><span class=\"line\">                            batch_size=<span class=\"number\">32</span>, shuffle=<span class=\"literal\">True</span>, seed=<span class=\"literal\">None</span>,</span><br><span class=\"line\">                            save_to_dir=<span class=\"literal\">None</span>）</span><br></pre></td></tr></table></figure>\n<p>主要参数：</p>\n<ul>\n<li>directory: 目标文件夹路径，对于每一个类对应一个子文件夹，该子文件夹中任何 JPG、PNG、BNP、PPM 的图片都可以读取。</li>\n<li>target_size: 默认为 (256, 256)，图像将被 resize 成该尺寸。</li>\n<li>batch_size: batch 数据的大小，默认 32。</li>\n<li>shuffle: 是否打乱数据，默认为 True。</li>\n</ul>\n<p>我们创建两个 <code>tf.keras.preprocessing.image.ImageDataGenerator</code>  实例来分别读取训练数据集和测试数据集中的所有图像文件。将训练集图片全部处理为高和宽均为 224 像素的输入。此外，我们对 RGB（红、绿、蓝）三个颜色通道的数值做标准化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获取数据集</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pathlib</span><br><span class=\"line\">train_dir = <span class=\"string\">&#x27;transferdata/train&#x27;</span></span><br><span class=\"line\">test_dir = <span class=\"string\">&#x27;transferdata/test&#x27;</span></span><br><span class=\"line\"><span class=\"comment\"># 获取训练集数据</span></span><br><span class=\"line\">train_dir = pathlib.Path(train_dir)</span><br><span class=\"line\">train_count = <span class=\"built_in\">len</span>(<span class=\"built_in\">list</span>(train_dir.glob(<span class=\"string\">&#x27;*/*.jpg&#x27;</span>)))</span><br><span class=\"line\"><span class=\"comment\"># 获取测试集数据</span></span><br><span class=\"line\">test_dir = pathlib.Path(test_dir)</span><br><span class=\"line\">test_count = <span class=\"built_in\">len</span>(<span class=\"built_in\">list</span>(test_dir.glob(<span class=\"string\">&#x27;*/*.jpg&#x27;</span>)))</span><br><span class=\"line\"><span class=\"comment\"># 创建imageDataGenerator进行图像处理</span></span><br><span class=\"line\">image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=<span class=\"number\">1.</span>/<span class=\"number\">255</span>)</span><br><span class=\"line\"><span class=\"comment\"># 设置参数</span></span><br><span class=\"line\">BATCH_SIZE = <span class=\"number\">32</span></span><br><span class=\"line\">IMG_HEIGHT = <span class=\"number\">224</span></span><br><span class=\"line\">IMG_WIDTH = <span class=\"number\">224</span></span><br><span class=\"line\"><span class=\"comment\"># 获取训练数据</span></span><br><span class=\"line\">train_data_gen = image_generator.flow_from_directory(directory=<span class=\"built_in\">str</span>(train_dir),</span><br><span class=\"line\">                                                    batch_size=BATCH_SIZE,</span><br><span class=\"line\">                                                    target_size=(IMG_HEIGHT, IMG_WIDTH),</span><br><span class=\"line\">                                                    shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\"># 获取测试数据</span></span><br><span class=\"line\">test_data_gen = image_generator.flow_from_directory(directory=<span class=\"built_in\">str</span>(test_dir),</span><br><span class=\"line\">                                                    batch_size=BATCH_SIZE,</span><br><span class=\"line\">                                                    target_size=(IMG_HEIGHT, IMG_WIDTH),</span><br><span class=\"line\">                                                    shuffle=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>下面我们随机取 1 个 batch 的图片然后绘制出来。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\"># 显示图像</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">show_batch</span>(<span class=\"params\">image_batch, label_batch</span>):</span></span><br><span class=\"line\">    plt.figure(figsize=(<span class=\"number\">10</span>,<span class=\"number\">10</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> n <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">15</span>):</span><br><span class=\"line\">        ax = plt.subplot(<span class=\"number\">5</span>,<span class=\"number\">5</span>,n+<span class=\"number\">1</span>)</span><br><span class=\"line\">        plt.imshow(image_batch[n]）</span><br><span class=\"line\">        plt.axis(<span class=\"string\">&#x27;off&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 随机选择一个batch的图像        </span></span><br><span class=\"line\">image_batch, label_batch = <span class=\"built_in\">next</span>(train_data_gen)</span><br><span class=\"line\"><span class=\"comment\"># 图像显示</span></span><br><span class=\"line\">show_batch(image_batch, label_batch)</span><br></pre></td></tr></table></figure>\n<center><img src=\"C:\\Users\\14767\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210622215025389.png\" alt=\"image-20210622215025389\" style=\"zoom:50%;#pic_center\"></center>\n<p><strong>5.2.2 模型构建与训练</strong></p>\n<p>我们使用在 ImageNet 数据集上预训练的 ResNet-50 作为源模型。这里指定 <code>weights='imagenet'</code>  来自动下载并加载预训练的模型参数。在第一次使用时需要联网下载模型参数。</p>\n<p>Keras 应用程序（keras.applications）是具有预先训练权值的固定架构，该类封装了很多重量级的网络架构，如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/2021062412434130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n<p>实现时实例化模型架构：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tf.keras.applications.ResNet50(</span><br><span class=\"line\">    include_top=<span class=\"literal\">True</span>, weights=<span class=\"string\">&#x27;imagenet&#x27;</span>, input_tensor=<span class=\"literal\">None</span>, input_shape=<span class=\"literal\">None</span>,</span><br><span class=\"line\">    pooling=<span class=\"literal\">None</span>, classes=<span class=\"number\">1000</span>, **kwargs</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p>主要参数：</p>\n<ul>\n<li>include_top: 是否包括顶层的全连接层。</li>\n<li>weights: None 代表随机初始化， ‘imagenet’ 代表加载在 ImageNet 上预训练的权值。</li>\n<li>input_shape: 可选，输入尺寸元组，仅当 include_top=False 时有效，否则输入形状必须是 (224, 224, 3)（channels_last 格式）或 (3, 224, 224)（channels_first 格式）。它必须为 3 个输入通道，且宽高必须不小于 32，比如 (200, 200, 3) 是一个合法的输入尺寸。</li>\n</ul>\n<p>在该案例中我们使用 resNet50 预训练模型构建模型：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 加载预训练模型</span></span><br><span class=\"line\">ResNet50 = tf.keras.applications.ResNet50(weights=<span class=\"string\">&#x27;imagenet&#x27;</span>, input_shape=(<span class=\"number\">224</span>,<span class=\"number\">224</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\"><span class=\"comment\"># 设置所有层不可训练</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> layer <span class=\"keyword\">in</span> ResNet50.layers:</span><br><span class=\"line\">    layer.trainable = <span class=\"literal\">False</span></span><br><span class=\"line\"><span class=\"comment\"># 设置模型</span></span><br><span class=\"line\">net = tf.keras.models.Sequential()</span><br><span class=\"line\"><span class=\"comment\"># 预训练模型</span></span><br><span class=\"line\">net.add(ResNet50)</span><br><span class=\"line\"><span class=\"comment\"># 展开</span></span><br><span class=\"line\">net.add(tf.keras.layers.Flatten())</span><br><span class=\"line\"><span class=\"comment\"># 二分类的全连接层</span></span><br><span class=\"line\">net.add(tf.keras.layers.Dense(<span class=\"number\">2</span>, activation=<span class=\"string\">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure>\n<p>接下来我们使用之前定义好的 ImageGenerator 将训练集图片送入 ResNet50 进行训练。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 模型编译：指定优化器，损失函数和评价指标</span></span><br><span class=\"line\">net.<span class=\"built_in\">compile</span>(optimizer=<span class=\"string\">&#x27;adam&#x27;</span>,</span><br><span class=\"line\">            loss=<span class=\"string\">&#x27;categorical_crossentropy&#x27;</span>,</span><br><span class=\"line\">            metrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 模型训练：指定数据，每一个epoch中只运行10个迭代，指定验证数据集</span></span><br><span class=\"line\">history = net.fit(</span><br><span class=\"line\">                    train_data_gen,</span><br><span class=\"line\">                    steps_per_epoch=<span class=\"number\">10</span>,</span><br><span class=\"line\">                    epochs=<span class=\"number\">3</span>,</span><br><span class=\"line\">                    validation_data=test_data_gen,</span><br><span class=\"line\">                    validation_steps=<span class=\"number\">10</span></span><br><span class=\"line\">                    )</span><br><span class=\"line\">Epoch <span class=\"number\">1</span>/<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">10</span>/<span class=\"number\">10</span> [==============================] - 28s 3s/step - loss: <span class=\"number\">0.6931</span> - accuracy: <span class=\"number\">0.5031</span> - val_loss: <span class=\"number\">0.6930</span> - val_accuracy: <span class=\"number\">0.5094</span></span><br><span class=\"line\">Epoch <span class=\"number\">2</span>/<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">10</span>/<span class=\"number\">10</span> [==============================] - 29s 3s/step - loss: <span class=\"number\">0.6932</span> - accuracy: <span class=\"number\">0.5094</span> - val_loss: <span class=\"number\">0.6935</span> - val_accuracy: <span class=\"number\">0.4812</span></span><br><span class=\"line\">Epoch <span class=\"number\">3</span>/<span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"number\">10</span>/<span class=\"number\">10</span> [==============================] - 31s 3s/step - loss: <span class=\"number\">0.6935</span> - accuracy: <span class=\"number\">0.4844</span> - val_loss: <span class=\"number\">0.6933</span> - val_accuracy: <span class=\"number\">0.4875</span></span><br></pre></td></tr></table></figure>",
            "tags": [
                "人工智能",
                "计算机视觉CV"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AF%BB%E7%AF%87/",
            "url": "https://leezhao415.github.io/2021/06/24/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AF%BB%E7%AF%87/",
            "title": "计算机视觉算法导读篇",
            "date_published": "2021-06-24T15:05:36.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%E5%8F%B2\">1 深度学习发展史</a></li>\n<li><a href=\"#2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A6%82%E8%BF%B0\">2 计算机视觉概述</a>\n<ul>\n<li><a href=\"#21-%E5%AE%9A%E4%B9%89\">2.1 定义</a></li>\n<li><a href=\"#22-%E4%BB%BB%E5%8A%A1%E5%88%86%E8%A7%A3\">2.2 任务分解</a></li>\n<li><a href=\"#23-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF\">2.3 应用场景</a></li>\n<li><a href=\"#24-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%8F%91%E5%B1%95%E5%8F%B2\">2.4 计算机视觉发展史</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624121720507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<h3><span id=\"1-深度学习发展史\"> 1 深度学习发展史</span></h3>\n<ul>\n<li><strong>起源</strong>：深度学习所需要的神经网络技术起源于 20 世纪 50 年代，叫做感知机。当时也通常使用单层感知机，尽管结构简单，但是能够解决复杂的问题。后来感知机被证明存在严重的问题，因为只能学习线性可分函数，连简单的异或 (XOR) 等线性不可分问题都无能为力，1969 年 Marvin Minsky 写了一本叫做《Perceptrons》的书，他提出了著名的两个观点：1. 单层感知机没用，我们需要多层感知机来解决复杂问题 2. 没有有效的训练算法。</li>\n<li><strong>发展</strong>：20 世纪 90 年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机（SVM，Support Vector Machines）、 Boosting、最大熵方法（如 LR，Logistic Regression）等。这些模型的结构基本上可以看成带有一层隐层节点（如 SVM、Boosting），或没有隐层节点（如 LR）。这些模型无论是在理论分析还是应用中都获得了巨大的成功。相比之下，由于理论分析的难度大，训练方法又需要很多经验和技巧，这个时期浅层人工神经网络反而相对沉寂.</li>\n<li>2006 年，杰弗里・辛顿以及他的学生鲁斯兰・萨拉赫丁诺夫<strong>正式提出了深度学习</strong>的概念。他们在世界顶级学术期刊《科学》发表的一篇文章中详细的给出了 “梯度消失” 问题的解决方案 —— 通过无监督的学习方法逐层训练算法，再使用有监督的反向传播算法进行调优。该深度学习方法的提出，立即在学术圈引起了巨大的反响，以斯坦福大学、多伦多大学为代表的众多世界知名高校纷纷投入巨大的人力、财力进行深度学习领域的相关研究。而后又迅速蔓延到工业界中。</li>\n<li>2012 年，在著名的 ImageNet 图像识别大赛中，杰弗里・辛顿领导的小组采用深度学习模型 AlexNet 一举夺冠。AlexNet 采用 ReLU 激活函数，从根本上解决了梯度消失问题，并采用 GPU 极大的提高了模型的运算速度。同年，由斯坦福大学著名的吴恩达教授和世界顶尖计算机专家 Jeff Dean 共同主导的深度神经网络 ——DNN 技术在图像识别领域取得了惊人的成绩，在 ImageNet 评测中成功的把错误率从 26％降低到了 15％。深度学习算法在世界大赛的脱颖而出，也再一次吸引了学术界和工业界对于深度学习领域的关注。</li>\n<li>2016 年，随着谷歌公司基于深度学习开发的 AlphaGo 以 4:1 的比分战胜了国际顶尖围棋高手李世石，深度学习的热度一时无两。后来，AlphaGo 又接连和众多世界级围棋高手过招，均取得了完胜。这也证明了在围棋界，基于深度学习技术的机器人已经超越了人类。</li>\n<li>2017 年，基于强化学习算法的 AlphaGo 升级版 AlphaGo Zero 横空出世。其采用 “从零开始”、“无师自通” 的学习模式，以 100:0 的比分轻而易举打败了之前的 AlphaGo。除了围棋，它还精通国际象棋等其它棋类游戏，可以说是真正的棋类 “天才”。此外在这一年，深度学习的相关算法在医疗、金融、艺术、无人驾驶等多个领域均取得了显著的成果。所以，也有专家把<strong> 2017 年看作是深度学习甚至是人工智能发展最为突飞猛进的一年</strong>。</li>\n<li>2019 年，基于 Transformer 的自然语言模型的持续增长和扩散，这是一种语言建模神经网络模型，可以在几乎所有任务上提高 NLP 的质量。Google 甚至将其用作相关性的主要信号之一，这是多年来最重要的更新</li>\n<li>2020 年，深度学习扩展到更多的应用场景，比如积水识别，路面塌陷等，而且疫情期间，在智能外呼系统，人群测温系统，口罩人脸识别等都有深度学习的应用。</li>\n</ul>\n<h3><span id=\"2-计算机视觉概述\"> 2 计算机视觉概述</span></h3>\n<h4><span id=\"21-定义\"> 2.1 定义</span></h4>\n<p>计算机视觉是指用摄像机和电脑及其他相关设备，对生物视觉的一种模拟。它的主要任务让计算机理解图片或者视频中的内容，就像人类和许多其他生物每天所做的那样。</p>\n<h4><span id=\"22-任务分解\"> 2.2 任务分解</span></h4>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624121817382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<p>主要分为三大经典任务： <code>图像分类</code> 、 <code>目标检测</code> 、 <code>图像分割</code></p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624121853824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li><strong><a href=\"https://blog.csdn.net/qq_36722887/article/details/118186747?spm=1001.2014.3001.5502\">图像分类（Classification）</a></strong>：即是将图像结构化为某一类别的信息，用事先确定好的类别 (category) 来描述图片。</li>\n<li><strong><a href=\"https://blog.csdn.net/qq_36722887/article/details/118186918?spm=1001.2014.3001.5502\">目标检测（Detection）</a></strong>：分类任务关心整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求同时获得这一目标的类别信息和位置信息（classification + localization）。</li>\n<li><strong><a href=\"https://blog.csdn.net/qq_36722887/article/details/118187206?spm=1001.2014.3001.5502\">图像分割（Segmentation）</a></strong>：分割是对图像的像素级描述，它赋予每个像素类别（实例）意义，适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割。</li>\n</ul>\n<h4><span id=\"23-应用场景\"> 2.3 应用场景</span></h4>\n<center><img src=\"https://img-blog.csdnimg.cn/2021062412194040.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 60%;\"></center>\n<ul>\n<li>\n<p><strong>人脸识别</strong></p>\n<p>人脸识别技术目前已经广泛应用于金融、司法、军队、公安、边检、政府、航天、电力、工厂、教育、医疗等行业。据业内人士分析，我国的人脸识别产业的需求旺盛，需求推动导致企业敢于投入资金。</p>\n<p>代表企业：Face++ 旷视科技、依图科技、商汤科技、深醒科技、云从科技等。</p>\n</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624122009165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>\n<p><strong>视频监控</strong></p>\n<p>人工智能技术可以对结构化的人、车、物等视频内容信息进行快速检索、查询。这项应用使得让公安系统在繁杂的监控视频中搜寻到罪犯的有了可能。在大量人群流动的交通枢纽，该技术也被广泛用于人群分析、防控预警等。</p>\n<p>代表企业：SenseTime 商汤科技、DeepGlint 格灵深瞳、依图科技、云天励飞、深网视界等。</p>\n</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/2021062412212572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>\n<p><strong>图片识别分析</strong></p>\n<p>代表企业：Face++ 旷视科技、图普科技、码隆科技、酒咔嚓、YI + 陌上花科技等。</p>\n</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624122150694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 33%;\"></center>\n<ul>\n<li>\n<p><strong>辅助驾驶</strong></p>\n<p>随着汽车的普及，汽车已经成为人工智能技术非常大的应用投放方向，但就目前来说，想要完全实现自动驾驶 / 无人驾驶，距离技术成熟还有一段路要走。不过利用人工智能技术，汽车的驾驶辅助的功能及应用越来越多，这些应用多半是基于计算机视觉和图像处理技术来实现。</p>\n<p>代表企业：纵目科技、TuSimple 图森科技、驭势科技、MINIEYE 佑驾创新、中天安驰等。</p>\n</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624122516655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 33%;\"></center>\n<p>除了上述这些，计算机视觉在三维视觉，三维重建，工业仿真，地理信息系统，工业视觉，医疗影像诊断，文字识别（OCR），图像及视频编辑等领域也有广泛的应用。</p>\n<h4><span id=\"24-计算机视觉发展史\"> 2.4 计算机视觉发展史</span></h4>\n<ul>\n<li>1963 年， <code>Larry Roberts</code>  发表了 CV 领域的第一篇专业论文，用以对简单几何体进行边缘提取和三维重建。</li>\n<li>1966 年，麻省理工学院 (MIT) 发起了一个夏季项目，目标是搭建一个 <code>机器视觉系统</code> ，完成模式识别 (pattern recognition) 等工作。虽然未成功，但是计算机视觉作为一个科学领域的正式诞生的标志。</li>\n<li>1982 年，学者 David Marr 发表的著作《Vision》从严谨又长远的角度给出了 CV 的 <code>发展方向</code> 和一些 <code>基本算法</code> ，其中不乏现在为人熟知的 “图层” 的概念、边缘提取、三维重建等，标志着计算机视觉成为了一门独立学科。</li>\n<li>1999 年 David Lowe 提出了 <code>尺度不变特征变换</code> （SIFT, Scale-invariant feature transform）目标检测算法，用于匹配不同拍摄方向、纵深、光线等图片中的相同元素。</li>\n<li>2009 年，由 Felzenszwalb 教授在提出基于 <code>HOG</code>  的 deformable parts model，可变形零件模型开发，它是深度学习之前最好的最成功的 objectdetection &amp; recognition 算法。</li>\n<li>Everingham 等人在 2006 年至 2012 年间搭建了一个大型图片数据库，供机器识别和训练，称为 <code>PASCAL Visual Object Challenge</code> ，该数据库中有 20 种类别的图片，每种图片数量在一千至一万张不等。</li>\n<li>2009 年，李飞飞教授等在 CVPR2009 上发表了一篇名为《ImageNet: A Large-Scale Hierarchical Image Database》的论文，发布了 <code>ImageNet数据集</code> ，这是为了检测计算机视觉能否识别自然万物，回归机器学习，克服过拟合问题。</li>\n<li>2012 年，Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 创造了一个 “大型的深度卷积神经网络”，也即现在众所周知的  <code>AlexNet</code> ，赢得了当年的 ILSVRC。这是史上第一次有模型在 ImageNet 数据集表现如此出色。自那时起，CNN 才成了家喻户晓的名字。</li>\n<li>2014 年，蒙特利尔大学提出 <code>生成对抗网络（GAN）</code> ：拥有两个相互竞争的神经网络可以使机器学习得更快。一个网络尝试模仿真实数据生成假的数据，而另一个网络则试图将假数据区分出来。随着时间的推移，两个网络都会得到训练，生成对抗网络（GAN）被认为是计算机视觉领域的重大突破。</li>\n<li>2018 年末，英伟达发布的 <code>视频到视频生成（Video-to-Video synthesis）</code> ，它通过精心设计的发生器、鉴别器网络以及时空对抗物镜，合成高分辨率、照片级真实、时间一致的视频，实现了让 AI 更具物理意识，更强大，并能够推广到新的和看不见的更多场景。</li>\n<li>2019，更强大的 GAN， <code>BigGAN</code> ，是拥有了更聪明的学习技巧的 GAN，由它训练生成的图像连它自己都分辨不出真假，因为除非拿显微镜看，否则将无法判断该图像是否有任何问题，因而，它更被誉为史上最强的图像生成器.</li>\n</ul>\n",
            "tags": [
                "人工智能",
                "计算机视觉CV"
            ]
        }
    ]
}