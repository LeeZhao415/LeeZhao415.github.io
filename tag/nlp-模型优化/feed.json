{
    "version": "https://jsonfeed.org/version/1",
    "title": "且听风吟，御剑于心！ • All posts by \"nlp-模型优化\" tag",
    "description": "",
    "home_page_url": "https://leezhao415.github.io",
    "items": [
        {
            "id": "https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/",
            "url": "https://leezhao415.github.io/2021/07/09/%E3%80%90%E8%AF%A6%E8%A7%A3%E3%80%91%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B9%8B%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4/",
            "title": "【详解】模型优化技巧之优化器和学习率调整",
            "date_published": "2021-07-09T11:31:37.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#pytorch%E5%8D%81%E5%A4%A7%E4%BC%98%E5%8C%96%E5%99%A8\">PyTorch 十大优化器</a>\n<ul>\n<li><a href=\"#1-torchoptimsgd\">1 torch.optim.SGD</a></li>\n<li><a href=\"#2-torchoptimasgd\">2 torch.optim.ASGD</a></li>\n<li><a href=\"#3-torchoptimrprop\">3 torch.optim.Rprop</a></li>\n<li><a href=\"#4-torchoptimadagrad\">4 torch.optim.Adagrad</a></li>\n<li><a href=\"#5-torchoptimadadelta\">5 torch.optim.Adadelta</a></li>\n<li><a href=\"#6-torchoptimrmsprop\">6 torch.optim.RMSprop</a></li>\n<li><a href=\"#7-torchoptimadamamsgrad\">7 torch.optim.Adam(AMSGrad)</a></li>\n<li><a href=\"#8-torchoptimadamax\">8 torch.optim.Adamax</a></li>\n<li><a href=\"#9-torchoptimsparseadam\">9 torch.optim.SparseAdam</a></li>\n<li><a href=\"#10-torchoptimlbfgs\">10 torch.optim.LBFGS</a></li>\n</ul>\n</li>\n<li><a href=\"#pytorch-%E5%85%AD%E5%A4%A7%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95\">PyTorch 六大学习率调整方法</a>\n<ul>\n<li><a href=\"#1-lr_schedulersteplr\">1 lr_scheduler.StepLR</a></li>\n<li><a href=\"#2-lr_schedulermultisteplr\">2 lr_scheduler.MultiStepLR</a></li>\n<li><a href=\"#3-lr_schedulerexponentiallr\">3 lr_scheduler.ExponentialLR</a></li>\n<li><a href=\"#4-lr_schedulercosineannealinglr\">4 lr_scheduler.CosineAnnealingLR</a></li>\n<li><a href=\"#5-lr_schedulerreducelronplateau\">5 lr_scheduler.ReduceLROnPlateau</a></li>\n<li><a href=\"#6-lr_schedulerlambdalr\">6 lr_scheduler.LambdaLR</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h3><span id=\"pytorch-十大优化器\"> PyTorch 十大优化器</span></h3>\n<h4><span id=\"1-torchoptimsgd\"> 1 torch.optim.SGD</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">SGD</span>(<span class=\"params\">params, lr=, momentum=<span class=\"number\">0</span>, dampening=<span class=\"number\">0</span>, weight_decay=<span class=\"number\">0</span>, nesterov=<span class=\"literal\">False</span></span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n可实现 SGD 优化算法，带动量 SGD 优化算法，带 NAG (Nesterov accelerated gradient) 动量 SGD 优化算法，并且均可拥有 weight_decay 项。</p>\n<p><strong>参数：</strong><br>\n<strong>params(iterable)</strong>- 参数组 (参数组的概念请查看 3.2 优化器基类：Optimizer)，优化器要管理的那部分参数。<br>\n<strong>lr(float)</strong>- 初始学习率，可按需随着训练过程不断调整学习率。<br>\n<strong>momentum(float)</strong>- 动量，通常设置为 0.9，0.8<br>\n<strong>dampening(float)</strong>- dampening for momentum ，暂时不了其功能，在源码中是这样用的：buf.mul_(momentum).add_(1 - dampening, d_p)，值得注意的是，若采用 nesterov，dampening 必须为 0.<br>\n<strong>weight_decay(float)</strong>- 权值衰减系数，也就是 L2 正则项的系数<br>\n<strong> nesterov (bool)</strong>- bool 选项，是否使用 NAG (Nesterov accelerated gradient)</p>\n<p><strong>注意事项：</strong><br>\npytroch 中使用 SGD 十分需要注意的是，更新公式与其他框架略有不同！<br>\npytorch 中是这样的：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">v=ρ∗v+g</span><br><span class=\"line\">p=p−lr∗v = p - lr∗ρ∗v - lr∗g</span><br><span class=\"line\"><span class=\"number\">12</span></span><br></pre></td></tr></table></figure>\n<p>其他框架：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">v=ρ∗v+lr∗g</span><br><span class=\"line\">p=p−v = p - ρ∗v - lr∗g</span><br><span class=\"line\"><span class=\"number\">12</span></span><br></pre></td></tr></table></figure>\n<p>ρ 是动量，v 是速率，g 是梯度，p 是参数，其实差别就是在 ρ∗v 这一项，pytorch 中将此项也乘了一个学习率。</p>\n<h4><span id=\"2-torchoptimasgd\"> 2 torch.optim.ASGD</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">ASGD</span>(<span class=\"params\">params, lr=<span class=\"number\">0.01</span>, lambd=<span class=\"number\">0.0001</span>, alpha=<span class=\"number\">0.75</span>, t0=<span class=\"number\">1000000.0</span>, weight_decay=<span class=\"number\">0</span></span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\nASGD 也成为 SAG，均表示随机平均梯度下降 (Averaged Stochastic Gradient Descent)，简单地说 ASGD 就是用空间换时间的一种 SGD，详细可参看论文：<a href=\"http://riejohnson.com/rie/stograd_nips.pdf\">http://riejohnson.com/rie/stograd_nips.pdf</a></p>\n<p><strong>参数：</strong><br>\n<strong>params(iterable)</strong> - 参数组 (参数组的概念请查看 3.1 优化器基类：Optimizer)，优化器要优化的那些参数。<br>\n<strong>lr(float)</strong> - 初始学习率，可按需随着训练过程不断调整学习率。<br>\n<strong>lambd(float)</strong> - 衰减项，默认值 1e-4。<br>\n<strong>alpha(float)</strong> - power for eta update ，默认值 0.75。<br>\n<strong>t0(float)</strong> - point at which to start averaging，默认值 1e6。<br>\n<strong>weight_decay(float)</strong> - 权值衰减系数，也就是 L2 正则项的系数。</p>\n<h4><span id=\"3-torchoptimrprop\"> 3 torch.optim.Rprop</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">Rprop</span>(<span class=\"params\">params, lr=<span class=\"number\">0.01</span>, etas=(<span class=\"params\"><span class=\"number\">0.5</span>, <span class=\"number\">1.2</span></span>), step_sizes=(<span class=\"params\"><span class=\"number\">1e-06</span>, <span class=\"number\">50</span></span>)</span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n实现 Rprop 优化方法 (弹性反向传播)，优化方法原文《Martin Riedmiller und Heinrich Braun: Rprop - A Fast Adaptive Learning Algorithm. Proceedings of the International Symposium on Computer and Information Science VII, 1992》<br>\n该优化方法适用于 full-batch，不适用于 mini-batch，因而在 min-batch 大行其道的时代里，很少见到。</p>\n<h4><span id=\"4-torchoptimadagrad\"> 4 torch.optim.Adagrad</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">Adagrad</span>(<span class=\"params\">params, lr=<span class=\"number\">0.01</span>, lr_decay=<span class=\"number\">0</span>, weight_decay=<span class=\"number\">0</span>, initial_accumulator_value=<span class=\"number\">0</span></span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n实现 Adagrad 优化方法 (Adaptive Gradient)，Adagrad 是一种自适应优化方法，是自适应的为各个参数分配不同的学习率。这个学习率的变化，会受到梯度的大小和迭代次数的影响。梯度越大，学习率越小；梯度越小，学习率越大。缺点是训练后期，学习率过小，因为 Adagrad 累加之前所有的梯度平方作为分母。<br>\n详细公式请阅读：Adaptive Subgradient Methods for Online Learning and Stochastic Optimization<br>\nJohn Duchi, Elad Hazan, Yoram Singer; 12(Jul):2121−2159, 2011.(<a href=\"http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\">http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a>)</p>\n<h4><span id=\"5-torchoptimadadelta\"> 5 torch.optim.Adadelta</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">Adadelta</span>(<span class=\"params\">params, lr=<span class=\"number\">1.0</span>, rho=<span class=\"number\">0.9</span>, eps=<span class=\"number\">1e-06</span>, weight_decay=<span class=\"number\">0</span></span>)</span></span><br></pre></td></tr></table></figure>\n<p>功能：<br>\n实现 Adadelta 优化方法。Adadelta 是 Adagrad 的改进。Adadelta 分母中采用距离当前时间点比较近的累计项，这可以避免在训练后期，学习率过小。<br>\n详细公式请阅读:<a href=\"https://arxiv.org/pdf/1212.5701.pdf\">https://arxiv.org/pdf/1212.5701.pdf</a></p>\n<h4><span id=\"6-torchoptimrmsprop\"> 6 torch.optim.RMSprop</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">RMSprop</span>(<span class=\"params\">params, lr=<span class=\"number\">0.01</span>, alpha=<span class=\"number\">0.99</span>, eps=<span class=\"number\">1e-08</span>, weight_decay=<span class=\"number\">0</span>, momentum=<span class=\"number\">0</span>, centered=<span class=\"literal\">False</span></span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n实现 RMSprop 优化方法（Hinton 提出），RMS 是均方根（root meam square）的意思。RMSprop 和 Adadelta 一样，也是对 Adagrad 的一种改进。RMSprop 采用均方根作为分母，可缓解 Adagrad 学习率下降较快的问题。并且引入均方根，可以减少摆动，详细了解可读：<a href=\"http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a></p>\n<h4><span id=\"7-torchoptimadamamsgrad\"> 7 torch.optim.Adam(AMSGrad)</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">Adam</span>(<span class=\"params\">params, lr=<span class=\"number\">0.001</span>, betas=(<span class=\"params\"><span class=\"number\">0.9</span>, <span class=\"number\">0.999</span></span>), eps=<span class=\"number\">1e-08</span>, weight_decay=<span class=\"number\">0</span>, amsgrad=<span class=\"literal\">False</span></span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n实现 Adam (Adaptive Moment Estimation)) 优化方法。Adam 是一种自适应学习率的优化方法，Adam 利用梯度的一阶矩估计和二阶矩估计动态的调整学习率。吴老师课上说过，Adam 是结合了 Momentum 和 RMSprop，并进行了偏差修正。<br>\n<strong>参数：</strong><br>\namsgrad- 是否采用 AMSGrad 优化方法，asmgrad 优化方法是针对 Adam 的改进，通过添加额外的约束，使学习率始终为正值。(AMSGrad，ICLR-2018 Best-Pper 之一，《On the convergence of Adam and Beyond》)。<br>\n详细了解 Adam 可阅读，Adam: A Method for Stochastic Optimization (<a href=\"https://arxiv.org/abs/1412.6980\">https://arxiv.org/abs/1412.6980</a>)。</p>\n<h4><span id=\"8-torchoptimadamax\"> 8 torch.optim.Adamax</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">Adamax</span>(<span class=\"params\">params, lr=<span class=\"number\">0.002</span>, betas=(<span class=\"params\"><span class=\"number\">0.9</span>, <span class=\"number\">0.999</span></span>), eps=<span class=\"number\">1e-08</span>, weight_decay=<span class=\"number\">0</span></span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n实现 Adamax 优化方法。Adamax 是对 Adam 增加了一个学习率上限的概念，所以也称之为 Adamax。<br>\n详细了解可阅读，Adam: A Method for Stochastic Optimization (<a href=\"https://arxiv.org/abs/1412.6980\">https://arxiv.org/abs/1412.6980</a>)(没错，就是 Adam 论文中提出了 Adamax)。</p>\n<h4><span id=\"9-torchoptimsparseadam\"> 9 torch.optim.SparseAdam</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">SparseAdam</span>(<span class=\"params\">params, lr=<span class=\"number\">0.001</span>, betas=(<span class=\"params\"><span class=\"number\">0.9</span>, <span class=\"number\">0.999</span></span>), eps=<span class=\"number\">1e-08</span></span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n针对稀疏张量的一种 “阉割版” Adam 优化方法。<br>\nonly moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters</p>\n<h4><span id=\"10-torchoptimlbfgs\"> 10 torch.optim.LBFGS</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">LBFGS</span>(<span class=\"params\">params, lr=<span class=\"number\">1</span>, max_iter=<span class=\"number\">20</span>, max_eval=<span class=\"literal\">None</span>, tolerance_grad=<span class=\"number\">1e-05</span>, tolerance_change=<span class=\"number\">1e-09</span>, history_size=<span class=\"number\">100</span>, line_search_fn=<span class=\"literal\">None</span></span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n实现 L-BFGS（Limited-memory Broyden–Fletcher–Goldfarb–Shanno）优化方法。L-BFGS 属于拟牛顿算法。L-BFGS 是对 BFGS 的改进，特点就是节省内存。</p>\n<h3><span id=\"pytorch-六大学习率调整方法\"> PyTorch 六大学习率调整方法</span></h3>\n<p>优化器中最重要的一个参数就是学习率，合理的学习率可以使优化器快速收敛。一般在训练初期给予较大的学习率，随着训练的进行，学习率逐渐减小。学习率什么时候减小，减小多少，这就涉及到学习率调整方法。<br>\nPyTorch 中提供了六种方法供大家使用，下面将一一介绍，最后对学习率调整方法进行总结。</p>\n<h4><span id=\"1-lr_schedulersteplr\"> 1 lr_scheduler.StepLR</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">lr_scheduler</span>.<span class=\"title\">StepLR</span> (<span class=\"params\"> optimizer , step_size , gamma=<span class=\"number\">0.1</span> , last_epoch=-<span class=\"number\">1</span> </span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n等间隔调整学习率，调整倍数为 gamma 倍，调整间隔为 step_size。间隔单位是 step。需要注意的是，step 通常是指 epoch，不要弄成 iteration 了。<br>\n参数：<br>\n <code>step_size(int)</code> - 学习率下降间隔数，若为 30，则会在 30、60、90… 个 step 时，将学习率调整为 lr*gamma。<br>\n <code>gamma(float)</code> - 学习率调整倍数，默认为 0.1 倍，即下降 10 倍。<br>\n <code>last_epoch(int)</code> - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始 值。</p>\n<h4><span id=\"2-lr_schedulermultisteplr\"> 2 lr_scheduler.MultiStepLR</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">lr_scheduler</span>.<span class=\"title\">MultiStepLR</span> (<span class=\"params\"> optimizer , milestones , gamma=<span class=\"number\">0.1</span> , last_epoch=-<span class=\"number\">1</span> </span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n按设定的间隔调整学习率。这个方法适合后期调试使用，观察 loss 曲线，为每个实验定制学习率调整时机。<br>\n参数：<br>\n <code>milestones(list)</code> - 一个 list，每一个元素代表何时调整学习率，list 元素必须是递增的。如 milestones=[30,80,120]<br>\n <code>gamma(float)</code> - 学习率调整倍数，默认为 0.1 倍，即下降 10 倍。<br>\n <code>last_epoch(int)</code> - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始值。</p>\n<h4><span id=\"3-lr_schedulerexponentiallr\"> 3 lr_scheduler.ExponentialLR</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">lr_scheduler</span>.<span class=\"title\">ExponentialLR</span> (<span class=\"params\"> optimizer , gamma , last_epoch=-<span class=\"number\">1</span> </span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n按指数衰减调整学习率，调整公式: lr = lr * gammaepoch<br>\n 参数：<br>\n <code>gamma</code> - 学习率调整倍数的底，指数为 epoch，即 gammaepoch<br>\n <code>last_epoch(int)</code> - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始值。</p>\n<h4><span id=\"4-lr_schedulercosineannealinglr\"> 4 lr_scheduler.CosineAnnealingLR</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">lr_scheduler</span>.<span class=\"title\">CosineAnnealingLR</span> (<span class=\"params\"> optimizer , T_max , eta_min=<span class=\"number\">0</span> , last_epoch=-<span class=\"number\">1</span> </span>)</span></span><br></pre></td></tr></table></figure>\n<p>以余弦函数为周期，并在每个周期最大值时重新设置学习率。具体如下图所示</p>\n<p>详细请阅读论文《 SGDR: Stochastic Gradient Descent with Warm Restarts》(ICLR-2017)： <a href=\"https://arxiv.org/abs/1608.03983\">https://arxiv.org/abs/1608.03983</a><br>\n 参数：<br>\n <code>T_max(int)</code> - 一次学习率周期的迭代次数，即 T_max 个 epoch 之后重新设置学习率。<br>\n <code>eta_min(float)</code> - 最小学习率，即在一个周期中，学习率最小会下降到 eta_min，默认值为 0。<br>\n学习率调整公式为：</p>\n<p>可以看出是以初始学习率为最大学习率，以 2*Tmax 为周期，在一个周期内先下降，后上升。</p>\n<h4><span id=\"5-lr_schedulerreducelronplateau\"> 5 lr_scheduler.ReduceLROnPlateau</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">lr_scheduler</span>.<span class=\"title\">ReduceLROnPlateau</span> (<span class=\"params\"> optimizer , mode=<span class=\"string\">&#x27;min&#x27;</span> ,factor=<span class=\"number\">0.1</span> , patience=<span class=\"number\">10</span> , verbose=<span class=\"literal\">False</span> , threshold=<span class=\"number\">0.0001</span> , threshold_mode=<span class=\"string\">&#x27;rel&#x27;</span> , cooldown=<span class=\"number\">0</span> , min_lr=<span class=\"number\">0</span> , eps=<span class=\"number\">1e-08</span> </span>)</span></span><br></pre></td></tr></table></figure>\n<p><strong>功能：</strong><br>\n当某指标不再变化（下降或升高），调整学习率，这是非常实用的学习率调整策略。<br>\n例如，当验证集的 loss 不再下降时，进行学习率调整；或者监测验证集的 accuracy，当 accuracy 不再上升时，则调整学习率。<br>\n参数：<br>\n <code>mode(str)</code> - 模式选择，有 min 和 max 两种模式，min 表示当指标不再降低 (如监测 loss)，max 表示当指标不再升高 (如监测 accuracy)。<br>\n <code>factor(float)</code> - 学习率调整倍数 (等同于其它方法的 gamma)，即学习率更新为 lr = lr *factor<br>\n <code>patience(int)</code> - 直译 ——“耐心”，即忍受该指标多少个 step 不变化，当忍无可忍时，调整学习率。<br>\n <code>verbose(bool)</code> - 是否打印学习率信息， print (‘Epoch {:5d}: reducing learning rate’ ’ of group {} to {:.4e}.’.format (epoch, i, new_lr))<br>\n <code>threshold(float)</code> - Threshold for measuring the new optimum ，配合 threshold_mode 使用。<br>\n <code>threshold_mode(str)</code> - 选择判断指标是否达最优的模式，有两种模式，rel 和 abs。<br>\n当 threshold_mode==rel，并且 mode==max 时， dynamic_threshold = best * (1 +threshold) ；<br>\n当 threshold_mode==rel，并且 mode==min 时， dynamic_threshold = best * (1 -threshold) ；<br>\n当 threshold_mode==abs，并且 mode==max 时， dynamic_threshold = best + threshold ；<br>\n当 threshold_mode==abs，并且 mode==min 时， dynamic_threshold = best - threshold<br>\ncooldown (int)- “ 冷却时间 “ ，当调整学习率之后，让学习率调整策略冷静一下，让模型再训练一段时间，再重启监测模式。<br>\nmin_lr (float or list)- 学习率下限，可为 float ，或者 list ，当有多个参数组时，可用 list 进行设置。<br>\neps (float)- 学习率衰减的最小值，当学习率变化小于 eps 时，则不调整学习率。</p>\n<h4><span id=\"6-lr_schedulerlambdalr\"> 6 lr_scheduler.LambdaLR</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">torch</span>.<span class=\"title\">optim</span>.<span class=\"title\">lr_scheduler</span>.<span class=\"title\">LambdaLR</span> (<span class=\"params\"> optimizer , lr_lambda , last_epoch=-<span class=\"number\">1</span> </span>)</span></span><br></pre></td></tr></table></figure>\n<p>功能：<br>\n为不同参数组设定不同学习率调整策略。调整规则为，lr = base_lr *lmbda (self.last_epoch) 。<br>\n参数：<br>\n <code>lr_lambda(function or list)</code> - 一个计算学习率调整倍数的函数，输入通常为 step，当有多个参数组时，设为 list。<br>\n <code>last_epoch(int)</code> - 上一个 epoch 数，这个变量用来指示学习率是否需要调整。当 last_epoch 符合设定的间隔时，就会对学习率进行调整。当为 - 1 时，学习率设置为初始值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">例如： </span><br><span class=\"line\">ignored_params = <span class=\"built_in\">list</span>(<span class=\"built_in\">map</span>(<span class=\"built_in\">id</span>, net.fc3.parameters())) </span><br><span class=\"line\">base_params = <span class=\"built_in\">filter</span>(<span class=\"keyword\">lambda</span> p: <span class=\"built_in\">id</span>(p) <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> ignored_params, net.parameters()) </span><br><span class=\"line\">optimizer = optim.SGD([</span><br><span class=\"line\">&#123;<span class=\"string\">&#x27;params&#x27;</span>: base_params&#125;,</span><br><span class=\"line\">&#123;<span class=\"string\">&#x27;params&#x27;</span>: net.fc3.parameters(), <span class=\"string\">&#x27;lr&#x27;</span>: <span class=\"number\">0.001</span>*<span class=\"number\">100</span>&#125;], <span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>,weight_decay=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\">lambda1 = <span class=\"keyword\">lambda</span> epoch: epoch // <span class=\"number\">3</span></span><br><span class=\"line\">lambda2 = <span class=\"keyword\">lambda</span> epoch: <span class=\"number\">0.95</span> ** epoch</span><br><span class=\"line\"></span><br><span class=\"line\">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])</span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">100</span>):</span><br><span class=\"line\">scheduler.step()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;epoch: &#x27;</span>, i, <span class=\"string\">&#x27;lr: &#x27;</span>, scheduler.get_lr())</span><br><span class=\"line\">train(...)</span><br><span class=\"line\">validate(...)</span><br><span class=\"line\">输出： </span><br><span class=\"line\">epoch: <span class=\"number\">0</span> lr: [<span class=\"number\">0.0</span>, <span class=\"number\">0.1</span>]</span><br><span class=\"line\">epoch: <span class=\"number\">1</span> lr: [<span class=\"number\">0.0</span>, <span class=\"number\">0.095</span>]</span><br><span class=\"line\">epoch: <span class=\"number\">2</span> lr: [<span class=\"number\">0.0</span>, <span class=\"number\">0.09025</span>]</span><br><span class=\"line\">epoch: <span class=\"number\">3</span> lr: [<span class=\"number\">0.001</span>, <span class=\"number\">0.0857375</span>]</span><br><span class=\"line\">epoch: <span class=\"number\">4</span> lr: [<span class=\"number\">0.001</span>, <span class=\"number\">0.081450625</span>]</span><br><span class=\"line\">epoch: <span class=\"number\">5</span> lr: [<span class=\"number\">0.001</span>, <span class=\"number\">0.07737809374999999</span>]</span><br><span class=\"line\">epoch: <span class=\"number\">6</span> lr: [<span class=\"number\">0.002</span>, <span class=\"number\">0.07350918906249998</span>]</span><br><span class=\"line\">epoch: <span class=\"number\">7</span> lr: [<span class=\"number\">0.002</span>, <span class=\"number\">0.06983372960937498</span>]</span><br><span class=\"line\">epoch: <span class=\"number\">8</span> lr: [<span class=\"number\">0.002</span>, <span class=\"number\">0.06634204312890622</span>]</span><br><span class=\"line\">epoch: <span class=\"number\">9</span> lr: [<span class=\"number\">0.003</span>, <span class=\"number\">0.0630249409724609</span>]</span><br></pre></td></tr></table></figure>\n<p>为什么第一个参数组的学习率会是 0 呢？ 来看看学习率是如何计算的。<br>\n第一个参数组的初始学习率设置为 0.001, lambda1 = lambda epoch: epoch // 3,<br>\n 第 1 个 epoch 时，由 lr = base_lr * lmbda (self.last_epoch)，可知道 lr = 0.001 *<br>\n(0//3) ，又因为 1//3 等于 0，所以导致学习率为 0。<br>\n第二个参数组的学习率变化，就很容易看啦，初始为 0.1，lr = 0.1 * 0.95^epoch ，当 epoch 为 0 时，lr=0.1 ，epoch 为 1 时，lr=0.1*0.95。</p>\n<p><strong>学习率调整小结</strong><br>\n PyTorch 提供了六种学习率调整方法，可分为三大类，分别是</p>\n<ol>\n<li>有序调整；</li>\n<li>自适应调整；</li>\n<li>自定义调整。<br>\n第一类，依一定规律有序进行调整，这一类是最常用的，分别是等间隔下降 (Step)，按需设定下降间隔 (MultiStep)，指数下降 (Exponential) 和 CosineAnnealing。这四种方法的调整时机都是人为可控的，也是训练时常用到的。<br>\n第二类，依训练状况伺机调整，这就是 ReduceLROnPlateau 方法。该法通过监测某一指标的变化情况，当该指标不再怎么变化的时候，就是调整学习率的时机，因而属于自适应的调整。<br>\n第三类，自定义调整，Lambda。Lambda 方法提供的调整策略十分灵活，我们可以为不同的层设定不同的学习率调整方法，这在 fine-tune 中十分有用，我们不仅可为不同的层设定不同的学习率，还可以为其设定不同的学习率调整策略，简直不能更棒！</li>\n</ol>\n",
            "tags": [
                "人工智能",
                "NLP-模型优化"
            ]
        }
    ]
}