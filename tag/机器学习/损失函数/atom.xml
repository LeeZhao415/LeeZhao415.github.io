<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://leezhao415.github.io</id>
    <title>且听风吟，御剑于心！ • Posts by &#34;机器学习/损失函数&#34; tag</title>
    <link href="https://leezhao415.github.io" />
    <updated>2021-08-08T09:41:10.000Z</updated>
    <category term="人工智能/CV" />
    <category term="Transformer/DETR(CV)" />
    <category term="人工智能" />
    <category term="数据集" />
    <category term="大数据框架" />
    <category term="编程工具" />
    <category term="NLP" />
    <category term="模型部署" />
    <category term="数据结构与算法" />
    <category term="Python数据分析" />
    <category term="网络通信" />
    <category term="YOLOX" />
    <category term="CV算法" />
    <category term="AIGC前沿" />
    <category term="VSLAM" />
    <category term="NCNN部署" />
    <category term="YOLOX目标检测" />
    <category term="多模态" />
    <category term="目标跟踪" />
    <category term="目标检测（人脸检测）" />
    <category term="深度学习" />
    <category term="CV未来" />
    <category term="且读文摘" />
    <category term="NLP-BERT" />
    <category term="自然语言处理NLP" />
    <category term="IOU" />
    <category term="OpenCV之DNN模块" />
    <category term="深度模型" />
    <category term="NLP-模型优化" />
    <category term="激活函数" />
    <category term="梯度更新" />
    <category term="概述" />
    <category term="人脸识别" />
    <category term="名人名言" />
    <category term="寒窑赋" />
    <category term="NLP/评估指标" />
    <category term="度量学习" />
    <category term="智能家居" />
    <category term="机器学习/损失函数" />
    <category term="机器学习" />
    <category term="模型性能指标" />
    <category term="CV/目标检测工具箱" />
    <category term="科研项目成果" />
    <category term="表面缺陷检测" />
    <category term="计算机顶会" />
    <category term="计算机视觉CV" />
    <category term="网络编程" />
    <category term="NLP/数据增强工具" />
    <category term="计算机视觉" />
    <category term="模型优化" />
    <category term="三维建模" />
    <category term="计算机视觉库" />
    <category term="深度学习环境配置" />
    <category term="知识蒸馏" />
    <category term="多任务学习模型" />
    <category term="数据库原理" />
    <category term="算法" />
    <category term="操作系统" />
    <category term="深度模型（目标检测）" />
    <category term="视频理解" />
    <category term="ReID" />
    <category term="MOT" />
    <category term="NLP-发展史" />
    <category term="编程语言" />
    <category term="CV数据集" />
    <category term="Linux" />
    <category term="PaddlePaddle" />
    <entry>
        <id>https://leezhao415.github.io/2021/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/</id>
        <title>机器学习中的损失函数详解</title>
        <link rel="alternate" href="https://leezhao415.github.io/2021/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/"/>
        <content type="html">&lt;meta name=&#34;referrer&#34; content=&#34;no-referrer&#34;&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;文章目录&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&#34;&gt;什么是损失函数？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%B8%80-zero-one-loss0-1%E6%8D%9F%E5%A4%B1&#34;&gt;一、Zero-one Loss（0-1 损失）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%BA%8C-hinge-loss&#34;&gt;二、Hinge Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%B8%89-softmax-loss-%E5%A4%9A%E7%B1%BB%E5%88%AB&#34;&gt;三、softmax-loss （多类别）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E5%9B%9B-logistic-loss%E4%BA%8C%E5%88%86%E7%B1%BB%E7%9A%84%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&#34;&gt;四、Logistic-loss（二分类的交叉熵损失函数）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%BA%94-%E4%BA%A4%E5%8F%89%E7%86%B5cross-entropy%E5%A4%9A%E5%88%86%E7%B1%BB&#34;&gt;五、交叉熵，cross entropy（多分类）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E5%85%AD-softmax-cross-entropy&#34;&gt;六、softmax cross entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%B8%83-triplet-loss&#34;&gt;七、triplet loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E5%85%AB-%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AEmean-squared-errormse&#34;&gt;八、均方误差（mean squared error，MSE）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%B9%9D-%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AEmean-absolute-errormae&#34;&gt;九、平均绝对误差（Mean Absolute Error，MAE）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E5%8D%81-smooth-l1%E6%8D%9F%E5%A4%B1&#34;&gt;十、Smooth L1 损失&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E5%8D%81%E4%B8%80-center-loss&#34;&gt;十一、center loss&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
&lt;hr&gt;
&lt;h4&gt;&lt;span id=&#34;什么是损失函数&#34;&gt; 什么是损失函数？&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;损失函数 （Loss Function） 也可称为代价函数 （Cost Function）或误差函数（Error Function），用于衡量预测值与实际值的偏离程度。一般来说，我们在进行机器学习任务时，使用的每一个算法都有一个目标函数，算法便是对这个目标函数进行优化，特别是在分类或者回归任务中，便是使用损失函数（Loss Function）作为其目标函数。机器学习的目标就是希望预测值与实际值偏离较小，也就是希望损失函数较小，也就是所谓的最小化损失函数。&lt;/p&gt;
&lt;p&gt;损失函数是用来评价模型的预测值与真实值的不一致程度，它是一个非负实值函数。通常使用来表示，损失函数越小，模型的性能就越好。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;一-zero-one-loss0-1-损失&#34;&gt; 一、Zero-one Loss（0-1 损失）&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;0-1 loss 是最原始的 loss，它是一种较为简单的损失函数，如果预测值与目标值不相等，那么为 1，否则为 0，即：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic3.zhimg.com/v2-bfe56c54c57f8f9b6ce1fe9d19da1da2_r.jpg&#34; alt=&#34;preview&#34; style=&#34;zoom:50%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;0-1 损失可用于分类问题，但是由于该函数是非凸的，在最优化过程中求解不方便，有阶跃，不连续。0-1 loss 无法对 x 进行求导，在依赖于反向传播的深度学习任务中，无法被使用，所以使用不多。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;二-hinge-loss&#34;&gt; 二、Hinge Loss&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Hinge loss 主要用于支持向量机（SVM）中，它的称呼来源于损失的形状，定义如下：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic4.zhimg.com/v2-19173d16aa9b7d655bbdea7ed43fc6eb_r.jpg&#34; alt=&#34;preview&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;其中 y=+1 或−1，f (x)=wx+b，当为 SVM 的线性核时。如果分类正确，loss=0，如果错误则为 1-f (x)，所以它是一个分段不光滑的曲线。Hinge loss 被用来解 SVM 中的间隔最大化问题。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;三-softmax-loss-多类别&#34;&gt; 三、softmax-loss （多类别）&lt;/span&gt;&lt;/h4&gt;
&lt;center&gt;&lt;img src=&#34;https://pic2.zhimg.com/80/v2-c3db01467ac0e926f64ba819be71d079_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;其中主要是 softmax 函数计算的类别概率。softmax loss 被广泛用于分类问题中，而且发展出了很多的变种，有针对不平衡样本问题的 weighted softmax loss、focal loss，针对蒸馏学习的 soft softmax loss，促进类内更加紧凑的 L-softmax Loss 等一系列改进。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;强调一下&lt;/code&gt; ：softmax 函数与 softmax-loss 函数是不一样的，千万，千万别记混了。&lt;br&gt;
softmax 函数最常用作分类器的输出，来表示  个不同类上的概率分布。&lt;/p&gt;
&lt;p&gt;softmax 公式如下：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic1.zhimg.com/80/v2-9f12e1bf5d3d01a8dd3c7bf8e37f6fd8_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;使用 softmax 分类的前提：类别之间都是相互独立的。&lt;br&gt;
softmax 分类的本质：将特征向量做归一化处理（输出总是和为 1），将线性预测值转换为类别概率。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;四-logistic-loss二分类的交叉熵损失函数&#34;&gt; 四、Logistic-loss（二分类的交叉熵损失函数）&lt;/span&gt;&lt;/h4&gt;
&lt;center&gt;&lt;img src=&#34;https://pic3.zhimg.com/v2-d88c6c12d49c72bf99b2b5a23f98d012_r.jpg&#34; alt=&#34;preview&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;Logistic 不使用平方损失的原因：平方损失会导致损失函数是非凸的，不利于求解，因为非凸函数会存在许多的局部最优解。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;五-交叉熵cross-entropy多分类&#34;&gt; 五、交叉熵，cross entropy（多分类）&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;cross entropy loss 用于度量两个概率分布之间的相似性。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic3.zhimg.com/80/v2-9a41671bae385d60b4aec73450712f36_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;其中为样本的真实标签，取值只能为 0 或 1；为预测样本属于类别的概率；为类别的数量。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;六-softmax-cross-entropy&#34;&gt; 六、softmax cross entropy&lt;/span&gt;&lt;/h4&gt;
&lt;center&gt;&lt;img src=&#34;https://pic3.zhimg.com/80/v2-424a933cd6be7b04e8883da44156c95a_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;其中&lt;em&gt; P&lt;sub&gt;k,i&lt;/sub&gt;&lt;em&gt; 表示样本&lt;/em&gt; k&lt;/em&gt; 属于类别&lt;em&gt; i&lt;/em&gt; 的概率（真实标签，只能为 0 或 1）；&lt;em&gt;q&lt;sub&gt;k,i&lt;/sub&gt;&lt;em&gt; 表示 softmax 预测的样本&lt;/em&gt; k&lt;/em&gt; 属于类别&lt;em&gt; i&lt;/em&gt; 的概率；&lt;em&gt;c&lt;/em&gt; 是类别数；&lt;em&gt;n&lt;/em&gt; 是样本总数。如果概率是通过 softmax 计算得到的，那么就是 softmax cross entropy。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;七-triplet-loss&#34;&gt; 七、triplet loss&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;triplet-loss 是深度学习中的一种损失函数，用于训练差异性较小的样本，如人脸等。数据包括锚（Anchor）示例、正（Positive）示例、负（Negative）示例，通过优化锚示例与正示例的距离小于锚示例与负示例的距离，实现样本的相似性计算。也就是说通过学习后，使得同类样本的 positive 更靠近 Anchor，而不同类的样本 Negative 则远离 Anchor。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic1.zhimg.com/80/v2-7703ffa91736583843422f4fc529a8e8_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;如上图所示，triplet 是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为 Anchor，然后再随机选取一个与 Anchor (记为 x_a) 属于同一类的样本和不同类的样本，这两个样本对应的称为 Positive (记为 x_p) 和 Negative (记为 x_n)，由此构成一个（Anchor，Positive，Negative）三元组。&lt;/p&gt;
&lt;p&gt;有了上面的 triplet 的概念， triplet loss 就好理解了。针对三元组中的每个元素（样本），训练一个参数共享或者不共享的网络，得到三个元素（样本）的特征表达，分别记为：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic3.zhimg.com/80/v2-c479af8a7bf95b721beeaea0e59efa56_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:67%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;通过 Triplet Loss 的学习后，使得 Positive 和 Anchor（同类）特征表达之间的距离尽可能小，而 Anchor 和 Negative（不同类）特征表达之间的距离尽可能大，并且要让 x_a 与 x_n 之间的距离和 x_a 与 x_p 之间的距离之间有一个最小的间隔。公式表示就是：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic3.zhimg.com/80/v2-dca69f3e2f8a6594739aa0920f94700e_720w.png&#34; alt=&#34;img&#34; style=&#34;zoom: 60%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;其中距离用欧式距离度量，α 也称为 margin（间隔）参数。设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。&lt;/p&gt;
&lt;p&gt;对应的目标函数为：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic3.zhimg.com/80/v2-964e4767ace34c2e3661bd387984fcde_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:60%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;其中 + 表示 [ ] 内的值大于零的时候，取该值为损失；小于零的时候，损失为零。&lt;/p&gt;
&lt;p&gt;【triplet loss 梯度推导】&lt;br&gt;
我们将上述目标函数记为 L ，则有：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic1.zhimg.com/80/v2-983bcadd7ab8e5772874a55c6c247fb4_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom: 67%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;在训练 Triplet Loss 模型时，只需要输入样本，不需要输入标签，这样避免标签过多、同标签样本过少的问题，模型只关心样本编码，不关心样本类别。Triplet Loss 在相似性计算和检索中的效果较好，可以学习到样本与变换样本之间的关联，检索出与当前样本最相似的其他样本。Triplet Loss 通常应用于个体级别的细粒度识别，比如分类猫与狗等是大类别的识别，但是有些需求要精确至个体级别，比如识别不同种类不同颜色的猫等，所以 Triplet Loss 最主要的应用也是在细粒度检索领域中。&lt;/p&gt;
&lt;p&gt;Triplet Loss 的优点：&lt;br&gt;
如果把不同个体作为类别进行分类训练，Softmax 维度可能远大于 Feature 维度，精度无法保证。&lt;br&gt;
Triplet Loss 一般比分类能学习到更好的特征，在度量样本距离时，效果较好；&lt;br&gt;
Triplet Loss 支持调整阈值 Margin，控制正负样本的距离，当特征归一化之后，通过调节阈值提升置信度。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;八-均方误差mean-squared-errormse&#34;&gt; 八、均方误差（mean squared error，MSE）&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;也叫平方损失或 L2 损失，常用在最小二乘法中。它的思想是使得各个训练点到最优拟合线的距离最小（平方和最小）。均方误差损失函数也是我们最常见的损失函数了，相信大家都很熟悉了，常用于回归问题中。定义如下：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic1.zhimg.com/80/v2-ce83422099352b47b671598dc4cb8290_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;当预测值与目标值相差很大时，梯度容易爆炸，这既是 L2 loss 的最大问题。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;九-平均绝对误差mean-absolute-errormae&#34;&gt; 九、平均绝对误差（Mean Absolute Error，MAE）&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;所有单个观测值与算术平均值的绝对值的平均，也被称为 L1 loss，常用于回归问题中。与平均误差相比，平均绝对误差由于离差被绝对值化，不会出现正负相抵消的情况，因而，平均绝对误差能更好地反映预测值误差的实际情况。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic1.zhimg.com/80/v2-c660c2f5cc622a44189937b87f9f8e24_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;由于 L1 loss 具有稀疏性，为了惩罚较大的值，因此常常将其作为正则项添加到其他 loss 中作为约束。L1 loss 的最大问题是梯度在零点不平滑，导致会跳过极小值。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;十-smooth-l1-损失&#34;&gt; 十、Smooth L1 损失&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;原始的 L1 loss 和 L2 loss 都有缺陷，比如 L1 loss 的最大问题是梯度不平滑，而 L2 loss 的最大问题是容易梯度爆炸，所以研究者们对其提出了很多的改进。&lt;br&gt;
在 faster rcnn 框架中，使用了 smooth L1 loss 来综合 L1 与 L2 loss 的优点，定义如下：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic2.zhimg.com/80/v2-88f125f81ba05519c6ac92685713bf51_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom: 67%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;在比较小时，上式等价于 L2 loss，保持平滑。&lt;br&gt;
在比较大时，上式等价于 L1 loss，可以限制数值的大小。Smooth L1 损失能够解决梯度爆炸问题。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;十一-center-loss&#34;&gt; 十一、center loss&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;center loss 来自 ECCV2016 的一篇论文：A Discriminative Feature Learning Approach for Deep Face Recognition。&lt;/p&gt;
&lt;p&gt;论文链接：&lt;a href=&#34;http://ydwen.github.io/papers/WenECCV16.pdf&#34;&gt;http://ydwen.github.io/papers/WenECCV16.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码链接：&lt;a href=&#34;https://github.com/pangyupo/mxnet_center_loss&#34;&gt;https://github.com/pangyupo/mxnet_center_loss&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;什么是 center loss？一个 batch 中的每个样本的 feature 离 feature 的中心的距离的平方和要越小越好，也就是类内（intra-class）距离要越小越好。这就是 center loss。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic4.zhimg.com/80/v2-ecd3e6dde43482158dedc410d451ee43_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:87%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;其中&lt;em&gt; m&lt;/em&gt; 表示 mini-batch 的大小，&lt;em&gt;X&lt;sub&gt;i&lt;/sub&gt;&lt;em&gt; 表示第&lt;/em&gt; i&lt;/em&gt; 个样本的特征，&lt;em&gt;C&lt;sub&gt;yi&lt;/sub&gt;&lt;em&gt; 表示第&lt;/em&gt; i&lt;/em&gt; 个正确样本的特征中心。&lt;/p&gt;
&lt;p&gt;通常在用 CNN 做人脸识别等分类问题时，我们一般采用 softmax loss，在 close-set 测试中模型性能良好，但在遇到 unseen 数据情况下，模型性能会急剧下降。一个直观的感觉是：如果模型学到的特征判别度更高，那么再遇到 unseen 数据时，泛化性能会比较好。为了使得模型学到的特征判别度更高，论文提出了一种新的辅助损失函数，之说以说是辅助损失函数是因为新提出的损失函数需要结合 softmax loss，而非替代后者，在不同数据及上提高了识别准确率。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://pic1.zhimg.com/80/v2-023e6c6cadab5b75da150710d26e2c74_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:75%;&#34;&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&#34;https://pic4.zhimg.com/80/v2-ecd3e6dde43482158dedc410d451ee43_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:79%;&#34;&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&#34;https://pic1.zhimg.com/80/v2-9c02c49c93ec3bd91ee0d603ecaa3e3c_720w.jpg&#34; alt=&#34;img&#34; style=&#34;zoom:85%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;在结合使用这两种损失函数时，可以认为 softmax loss 负责增加 inter-class 距离，center-loss 负责减小 intra-class 距离，这样学习到的特征判别度会更高。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;缺点&lt;/code&gt; ：最麻烦的地方在于如何选择训练样本对。在论文中，作者也提到了，选取合适的样本对对于模型的性能至关重要，论文中采用的方法是每次选择比较难以分类的样本对重新训练，类似于 hard-mining。同时，合适的训练样本还可以加快收敛速度。&lt;/p&gt;
</content>
        <category term="人工智能" />
        <category term="机器学习/损失函数" />
        <updated>2021-08-08T09:41:10.000Z</updated>
    </entry>
</feed>
