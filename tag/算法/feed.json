{
    "version": "https://jsonfeed.org/version/1",
    "title": "且听风吟，御剑于心！ • All posts by \"算法\" tag",
    "description": "",
    "home_page_url": "https://leezhao415.github.io",
    "items": [
        {
            "id": "https://leezhao415.github.io/2021/05/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/",
            "url": "https://leezhao415.github.io/2021/05/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/",
            "title": "机器学习算法详解",
            "date_published": "2021-05-19T08:44:27.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D\">1 机器学习算法原理介绍</a>\n<ul>\n<li><a href=\"#11-k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95\">1.1 K - 近邻算法</a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89\">1 定义</a></li>\n<li><a href=\"#2-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B\">2 算法流程</a></li>\n<li><a href=\"#3-k-%E8%BF%91%E9%82%BB%E5%AE%9E%E7%8E%B0\">3 K - 近邻实现</a></li>\n<li><a href=\"#%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97\"><strong>距离计算</strong></a></li>\n<li><a href=\"#4-%E6%8B%93%E5%B1%95fit-tansform-fit_transform%E5%8C%BA%E5%88%AB\">4 拓展：fit ()、tansform ()、fit_transform () 区别</a></li>\n<li><a href=\"#5-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9\">5 K 近邻算法优缺点</a></li>\n</ul>\n</li>\n<li><a href=\"#12-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92\">1.2 线性回归</a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89-1\">1 定义</a></li>\n<li><a href=\"#2-api%E6%A1%88%E4%BE%8B\">2 API 案例</a></li>\n<li><a href=\"#3-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%92%8C%E4%BC%98%E5%8C%96\">3 线性回归的损失和优化</a></li>\n<li><a href=\"#4-%E6%A6%82%E5%BF%B5%E8%A7%A3%E9%87%8A\">4 概念解释</a></li>\n<li><a href=\"#5-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95\">5 梯度下降算法</a></li>\n<li><a href=\"#6-%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%E5%8E%9F%E5%9B%A0%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95\">6 欠拟合和过拟合原因及解决办法</a></li>\n<li><a href=\"#7-%E6%AD%A3%E5%88%99%E5%8C%96%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98\">7 正则化 (解决过拟合问题)</a></li>\n<li><a href=\"#8-sklearn%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BDapi\"><strong>8 sklearn 模型的保存和加载 API</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#13-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92\">1.3 逻辑回归</a>\n<ul>\n<li><a href=\"#1-%E6%80%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E6%8D%9F%E5%A4%B1\">1 总损失函数（对数似然损失）</a></li>\n<li><a href=\"#2-%E6%A6%82%E5%BF%B5%E8%A7%A3%E9%87%8A\">2 概念解释</a></li>\n<li><a href=\"#3-roc%E6%9B%B2%E7%BA%BF\">3 ROC 曲线</a></li>\n<li><a href=\"#4-%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98\">4 样本不均衡问题</a></li>\n</ul>\n</li>\n<li><a href=\"#14-%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95\">1.4 决策树算法</a>\n<ul>\n<li><a href=\"#1-%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A-%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87%E5%92%8C%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0\">1 信息增益、信息增益率和基尼系数</a></li>\n<li><a href=\"#2-%E4%BF%A1%E6%81%AF%E7%86%B5%E8%AE%A1%E7%AE%97%E6%A1%88%E4%BE%8B\">2 信息熵计算案例</a></li>\n</ul>\n</li>\n<li><a href=\"#15-%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95\">1.5 集成算法</a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89-2\">1 定义</a></li>\n<li><a href=\"#2-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%ADboosting%E5%92%8Cbagging\">2 集成学习中 boosting 和 Bagging</a></li>\n<li><a href=\"#3-bagging%E5%8F%8A%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97\">3 Bagging 及随机森林</a></li>\n<li><a href=\"#4-boosting\">4 boosting</a></li>\n</ul>\n</li>\n<li><a href=\"#16-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95\">1.6 聚类算法</a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89-3\">1 定义</a></li>\n<li><a href=\"#2-%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0\">2 算法学习</a></li>\n<li><a href=\"#3-api\">3 API</a></li>\n<li><a href=\"#4-%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90\">4 案例分析</a></li>\n<li><a href=\"#5-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0\">5 模型评估</a></li>\n<li><a href=\"#6-k-means\">6 K-Means</a></li>\n</ul>\n</li>\n<li><a href=\"#17-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF\">1.7 朴素贝叶斯</a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89-4\">1 定义</a></li>\n<li><a href=\"#2-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86\">2 算法原理</a></li>\n<li><a href=\"#3-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91\">3 拉普拉斯平滑</a></li>\n<li><a href=\"#4-%E6%A1%88%E4%BE%8B%E5%AE%9E%E7%8E%B0\">4 案例实现</a></li>\n</ul>\n</li>\n<li><a href=\"#18-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA\">1.8 支持向量机</a>\n<ul>\n<li><a href=\"#1-%E5%9F%BA%E6%9C%AC%E5%85%83%E7%B4%A0\">1 基本元素</a></li>\n<li><a href=\"#2-%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3\">2 基本思想</a></li>\n<li><a href=\"#3-%E7%94%A8%E9%80%94\">3 用途</a></li>\n<li><a href=\"#4-%E7%A1%AC%E9%97%B4%E9%9A%94%E5%92%8C%E8%BD%AF%E9%97%B4%E9%9A%94\">4 硬间隔和软间隔</a></li>\n<li><a href=\"#5-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%8E%A8%E5%AF%BC\">5 支持向量机推导</a></li>\n<li><a href=\"#6-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\">6 损失函数</a></li>\n<li><a href=\"#7-svm%E5%9B%9E%E5%BD%92\">7 SVM 回归</a></li>\n<li><a href=\"#8-svm%E4%BC%98%E7%BC%BA%E7%82%B9\">8 SVM 优缺点</a></li>\n</ul>\n</li>\n<li><a href=\"#19-em%E7%AE%97%E6%B3%95\">1.9 EM 算法</a>\n<ul>\n<li><a href=\"#1-%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3\">1 基本思想</a></li>\n<li><a href=\"#2-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B-1\">2 算法流程</a></li>\n</ul>\n</li>\n<li><a href=\"#110-hmm%E6%A8%A1%E5%9E%8B\">1.10 HMM 模型</a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89-5\">1 定义</a></li>\n<li><a href=\"#2-%E5%B8%B8%E8%A7%81%E6%9C%AF%E8%AF%AD\">2 常见术语</a></li>\n<li><a href=\"#3-hmm%E4%B8%A4%E4%B8%AA%E9%87%8D%E8%A6%81%E5%81%87%E8%AE%BE\">3 HMM 两个重要假设</a></li>\n<li><a href=\"#4-hmm%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86\">4 HMM 模型算法原理</a></li>\n<li><a href=\"#5-hmm%E6%A8%A1%E5%9E%8B%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98\">5 HMM 模型三个基本问题</a></li>\n<li><a href=\"#6-%E6%A1%88%E4%BE%8B%E5%AE%9E%E7%8E%B0\">6 案例实现</a></li>\n</ul>\n</li>\n<li><a href=\"#111-xgboost%E7%AE%97%E6%B3%95\">1.11 xgboost 算法</a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89-6\">1 定义</a></li>\n<li><a href=\"#2-%E6%9C%80%E4%BC%98%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95\">2 最优模型构建方法</a></li>\n<li><a href=\"#3-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0\">3 目标函数</a></li>\n<li><a href=\"#4-%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90-1\">4 案例分析</a></li>\n</ul>\n</li>\n<li><a href=\"#112-lightgbm%E7%AE%97%E6%B3%95\">1.12 lightGBM 算法</a>\n<ul>\n<li><a href=\"#1-%E5%AE%9A%E4%B9%89-7\">1 定义</a></li>\n<li><a href=\"#2-%E7%89%B9%E7%82%B9\">2 特点</a></li>\n<li><a href=\"#3-%E4%BC%98%E5%8C%96%E7%89%B9%E7%82%B9%E8%AF%A6%E8%A7%A3\">3 优化特点详解</a></li>\n<li><a href=\"#4-api%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D\">4 API 相关参数介绍</a></li>\n<li><a href=\"#5-%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90\">5 案例分析</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0\">2 机器学习算法实现</a>\n<ul>\n<li><a href=\"#1%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86\">1. 获取数据集</a></li>\n<li><a href=\"#2%E6%95%B0%E6%8D%AE%E5%9F%BA%E6%9C%AC%E5%A4%84%E7%90%86\">2. 数据基本处理</a></li>\n<li><a href=\"#3%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%A0%87%E5%87%86%E5%8C%96\">3. 特征工程：标准化</a></li>\n<li><a href=\"#4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83\">4. 机器学习 (模型训练)</a>\n<ul>\n<li><a href=\"#41-%E6%A8%A1%E5%9E%8B%E4%BC%B0%E8%AE%A1\">4.1 模型估计</a></li>\n<li><a href=\"#42-%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98\">4.2 模型调优</a></li>\n<li><a href=\"#43-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83\">4.3 模型训练</a></li>\n</ul>\n</li>\n<li><a href=\"#5%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0\">5. 模型评估</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h3><span id=\"1-机器学习算法原理介绍\"> 1 机器学习算法原理介绍</span></h3>\n<ul>\n<li>\n<p><strong>判别模型 (discriminative model)</strong></p>\n<p>已知输入变量 x，通过求解条件概率分布 P (y|x) 或者直接计算 y 的值来预测 y。</p>\n<p>例如：</p>\n<ul>\n<li>线性回归（Linear Regression）</li>\n<li>逻辑回归（Logistic Regression）</li>\n<li>支持向量机（SVM）</li>\n<li>传统神经网络（Traditional Neural Networks）</li>\n<li>线性判别分析（Linear Discriminative Analysis）</li>\n<li>条件随机场（Conditional Random Field）</li>\n</ul>\n</li>\n<li>\n<p><strong>生成模型（generative model）</strong></p>\n<p>已知输入变量 x，通过对观测值和标注数据计算联合概率分布 P (x,y) 来达到判定估算 y 的目的。</p>\n<p>例如：</p>\n<ul>\n<li>朴素贝叶斯（Naive Bayes）</li>\n<li>隐马尔科夫模型（HMM）</li>\n<li>贝叶斯网络（Bayesian Networks）</li>\n<li>隐含狄利克雷分布（Latent Dirichlet Allocation）</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"11-k-近邻算法\"> 1.1 K - 近邻算法</span></h4>\n<h5><span id=\"1-定义\"> 1 定义</span></h5>\n<p>K Nearest Neighbor 算法又叫 KNN 算法，如果一个样本在特征空间中的 k 个最相似 (即特征空间中最邻近) 的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p>\n<h5><span id=\"2-算法流程\"> 2 算法流程</span></h5>\n<p>1）计算已知类别数据集中的点与当前点之间的距离</p>\n<p>2）按距离递增次序排序</p>\n<p>3）选取与当前点距离最小的 k 个点</p>\n<p>4）统计前 k 个点所在的类别出现的频率（ <code>分类</code> ：样本出现最多个数         <code>回归</code> ：K 个杨样本的平均值）</p>\n<p>5）返回前 k 个点出现频率最高的类别作为当前点的预测分类</p>\n<blockquote>\n<p><strong>1 K 值选择</strong></p>\n<ul>\n<li>\n<p>K 值的减小就意味着整体模型变得复杂，容易发生过拟合；</p>\n</li>\n<li>\n<p>K 值的增大就意味着整体模型变得简单，容易发生欠拟合；</p>\n<p>注：实际应用中，K 值一般取一个比较小的数值，例如采用交叉验证来选择最优的 K 值。</p>\n</li>\n</ul>\n<p><strong>2 误差估计</strong></p>\n<ul>\n<li><strong>近似误差</strong>：对训练集的训练误差，关注训练集，近似误差小可能出现过拟合。</li>\n<li><strong>估计误差</strong>：对测试集的测试误差，关注测试集，估计误差小说明对未知数据的预测能力好。</li>\n</ul>\n</blockquote>\n<h5><span id=\"3-k-近邻实现\"> 3 K - 近邻实现</span></h5>\n<ul>\n<li>\n<p><strong>线性扫描（穷举搜索）</strong></p>\n<p>计算输入实例与每一个训练实例的距离。计算后再查找 K 近邻。当训练集很大时，计算非常耗时。</p>\n</li>\n<li>\n<p><strong>KD 树</strong></p>\n<ul>\n<li>\n<p>一种对 k 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构</p>\n</li>\n<li>\n<p>kd 树是一种二叉树，表示对 k 维空间的一个划分，构造 kd 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K 维超矩形区域。kd 树的每个结点对应于一个 k 维超矩形区域。</p>\n</li>\n<li>\n<p>利用 kd 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p>\n</li>\n</ul>\n</li>\n<li>\n<h5><span id=\"距离计算\"> <strong>距离计算</strong></span></h5>\n<ul>\n<li>\n<p><strong>欧式距离 (Euclidean Distance)</strong></p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154007259.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n</li>\n<li>\n<p><strong>曼哈顿距离 (Manhattan Distance)</strong></p>\n<center><img src=\"https://img-blog.csdnimg.cn/202105191540406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n</li>\n<li>\n<p><strong>切比雪夫距离 (Chebyshev Distance)</strong></p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154101437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n</li>\n<li>\n<p><strong>闵可夫斯基距离 (Minkowski Distance)</strong></p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154121877.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:33%;\"></center>\n</li>\n</ul>\n</li>\n</ul>\n<p>其中 p 是一个变参数：</p>\n<ul>\n<li>当 p=1 时，就是曼哈顿距离；</li>\n<li>当 p=2 时，就是欧氏距离；</li>\n<li>当 p→∞时，就是切比雪夫距离。</li>\n</ul>\n<p>根据 p 的不同，闵氏距离可以表示某一类 / 种的距离。</p>\n<p><strong>其它距离</strong>：标准化欧氏距离 (Standardized EuclideanDistance)、余弦距离 (Cosine Distance)、汉明距离 (Hamming Distance)、杰卡德距离 (Jaccard Distance)、马氏距离 (Mahalanobis Distance)。</p>\n<ul>\n<li>\n<p><strong>总结</strong></p>\n<p>闵氏距离的缺点：</p>\n<ul>\n<li>\n<p>将各个分量的量纲 (scale)，也就是 “单位” 相同的看待了；</p>\n</li>\n<li>\n<p>未考虑各个分量的分布（期望，方差等）可能是不同的。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h5><span id=\"4-拓展fit-tansform-fit_transform-区别\"> 4 拓展：fit ()、tansform ()、fit_transform () 区别</span></h5>\n<ul>\n<li><code>fit()</code> : Method calculates the parameters μ and σ and saves them as internal objects.<br>\n 解释：简单来说，就是求得训练集 X 的均值，方差，最大值，最小值，这些训练集 X 固有的属性。</li>\n<li><code>transform()</code> : Method using these calculated parameters apply the transformation to a particular dataset.<br>\n 解释：在 fit 的基础上，进行标准化，降维，归一化等操作（看具体用的是哪个工具，如 PCA，StandardScaler 等）。</li>\n<li><code>fit_transform()</code> : joins the fit() and transform() method for transformation of dataset.<br>\n 解释：fit_transform 是 fit 和 transform 的组合，既包括了训练又包含了转换。</li>\n<li><code>transform()</code>  和 <code>fit_transform()</code>  二者的功能都是对数据进行某种统一处理（比如标准化～N (0,1)，将数据缩放 (映射) 到某个固定区间，归一化，正则化等）</li>\n<li><code>fit_transform(trainData)</code>  对部分数据先拟合 fit，找到该 part 的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该 trainData 进行转换 transform，从而实现数据的标准化、归一化等等。</li>\n</ul>\n<h5><span id=\"5-k-近邻算法优缺点\"> 5 K 近邻算法优缺点</span></h5>\n<ul>\n<li><strong>优点：</strong>\n<ul>\n<li>简单有效</li>\n<li>重新训练的代价低</li>\n<li>适合类域交叉样本\n<ul>\n<li>KNN 方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN 方法较其他方法更为适合。</li>\n</ul>\n</li>\n<li>适合大样本自动分类\n<ul>\n<li>该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>缺点：</strong>\n<ul>\n<li>惰性学习\n<ul>\n<li>KNN 算法是懒散学习方法（lazy learning, 基本上不学习），一些积极学习的算法要快很多</li>\n</ul>\n</li>\n<li>类别评分不是规格化\n<ul>\n<li>不像一些通过概率评分的分类</li>\n</ul>\n</li>\n<li>输出可解释性不强\n<ul>\n<li>例如决策树的输出可解释性就较强</li>\n</ul>\n</li>\n<li>对不均衡的样本不擅长\n<ul>\n<li>当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的 K 个邻居中大容量类的样本占多数。该算法只计算 “最近的” 邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。</li>\n</ul>\n</li>\n<li>计算量较大\n<ul>\n<li>目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"12-线性回归\"> 1.2 线性回归</span></h4>\n<h5><span id=\"1-定义\"> 1 定义</span></h5>\n<p>利用回归方程 (函数) 对一个或多个自变量 (特征值) 和因变量 (目标值) 之间关系进行建模的一种分析方式。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154147901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h5><span id=\"2-api-案例\"> 2 API 案例</span></h5>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression</span><br><span class=\"line\">x = [[<span class=\"number\">80</span>, <span class=\"number\">86</span>],</span><br><span class=\"line\">[<span class=\"number\">82</span>, <span class=\"number\">80</span>],</span><br><span class=\"line\">[<span class=\"number\">85</span>, <span class=\"number\">78</span>],</span><br><span class=\"line\">[<span class=\"number\">90</span>, <span class=\"number\">90</span>],</span><br><span class=\"line\">[<span class=\"number\">86</span>, <span class=\"number\">82</span>],</span><br><span class=\"line\">[<span class=\"number\">82</span>, <span class=\"number\">90</span>],</span><br><span class=\"line\">[<span class=\"number\">78</span>, <span class=\"number\">80</span>],</span><br><span class=\"line\">[<span class=\"number\">92</span>, <span class=\"number\">94</span>]]</span><br><span class=\"line\">y = [<span class=\"number\">84.2</span>, <span class=\"number\">80.6</span>, <span class=\"number\">80.1</span>, <span class=\"number\">90</span>, <span class=\"number\">83.2</span>, <span class=\"number\">87.6</span>, <span class=\"number\">79.4</span>, <span class=\"number\">93.4</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 实例化API</span></span><br><span class=\"line\">estimator = LinearRegression()</span><br><span class=\"line\"><span class=\"comment\"># 使用fit方法进行训练</span></span><br><span class=\"line\">estimator.fit(x,y)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;回归系数：\\n&#x27;</span>, estimator.coef_)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;预测结果：\\n&#x27;</span>, estimator.predict([[<span class=\"number\">100</span>, <span class=\"number\">80</span>]]))</span><br></pre></td></tr></table></figure>\n<h5><span id=\"3-线性回归的损失和优化\"> 3 线性回归的损失和优化</span></h5>\n<ul>\n<li>\n<p><strong>1 总损失函数</strong>：（ <code>最小二乘法</code> ）</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154257917.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<ul>\n<li>yi 为第 i 个训练样本的真实值</li>\n<li>h (xi) 为第 i 个训练样本特征值组合预测函数</li>\n<li>又称最小二乘法</li>\n</ul>\n</li>\n<li>\n<p><strong>2 优化算法</strong>：</p>\n<ul>\n<li>\n<p><strong>2.1 正规方程</strong></p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154328694.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<ul>\n<li>X 为特征值矩阵</li>\n<li>y 为目标值矩阵</li>\n</ul>\n<blockquote>\n<p>缺点：当特征过多过复杂时，求解速度太慢并且得不到结果</p>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> mean_squared_error</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_model1</span>():</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    线性回归:正规方程</span></span><br><span class=\"line\"><span class=\"string\">    :return:None</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 1.获取数据</span></span><br><span class=\"line\">    data = load_boston()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.数据集划分</span></span><br><span class=\"line\">    x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3.特征工程-标准化</span></span><br><span class=\"line\">    transfer = StandardScaler()</span><br><span class=\"line\">    x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\">    x_test = transfer.fit_transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.机器学习-线性回归(正规方程)</span></span><br><span class=\"line\">    <span class=\"comment\"># fit_intercept：是否计算偏置; </span></span><br><span class=\"line\">    estimator = LinearRegression(fit_intercept=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    estimator.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.模型评估</span></span><br><span class=\"line\">    <span class=\"comment\"># 5.1 获取系数等值</span></span><br><span class=\"line\">    y_predict = estimator.predict(x_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;预测值为:\\n&quot;</span>, y_predict)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;模型中的回归系数为:\\n&quot;</span>, estimator.coef_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;模型中的偏置为:\\n&quot;</span>, estimator.intercept_)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.2 评价</span></span><br><span class=\"line\">    <span class=\"comment\"># 均方误差</span></span><br><span class=\"line\">    error = mean_squared_error(y_test, y_predict)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;误差为:\\n&quot;</span>, error)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>均方误差 (Mean Squared Error) MSE) 评价机制：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154352336.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n</blockquote>\n<ul>\n<li>\n<p><strong>2.2 梯度下降法 (Gradient Descent)</strong></p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154416207.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<blockquote>\n<p>α 为学习率或者步长</p>\n</blockquote>\n<ul>\n<li><strong>梯度</strong>\n<ul>\n<li>在 <code>单变量函数</code> 中，梯度其实就是函数的 <code>微分</code> ，代表着函数在某个给定点的 <code>切线的斜率</code> ；</li>\n<li>在 <code>多变量函数</code> 中，梯度是一个 <code>向量</code> ，向量有方向，梯度的方向就指出了函数在给定点的 <code>上升最快的方向</code> ；</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p><strong>算法选择依据：</strong></p>\n<ul>\n<li>小规模数据：\n<ul>\n<li>正规方程：LinearRegression (不能解决拟合问题)</li>\n<li>岭回归</li>\n</ul>\n</li>\n<li>大规模数据：\n<ul>\n<li>梯度下降法：SGDRegressor</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> SGDRegressor</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> mean_squared_error</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_model2</span>():</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    线性回归:梯度下降法</span></span><br><span class=\"line\"><span class=\"string\">    :return:None</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 1.获取数据</span></span><br><span class=\"line\">    data = load_boston()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.数据集划分</span></span><br><span class=\"line\">    x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3.特征工程-标准化</span></span><br><span class=\"line\">    transfer = StandardScaler()</span><br><span class=\"line\">    x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\">    x_test = transfer.fit_transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.机器学习-线性回归(特征方程)</span></span><br><span class=\"line\">    <span class=\"comment\"># loss:损失类型  loss=”squared_loss”: 普通最小二乘法</span></span><br><span class=\"line\">    <span class=\"comment\"># fit_intercept：是否计算偏置   </span></span><br><span class=\"line\">    <span class=\"comment\"># learning_rate: 学习率</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># &#x27;constant&#x27;: eta = eta0</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># &#x27;optimal&#x27;: eta = 1.0 / (alpha * (t + t0)) [default]</span></span><br><span class=\"line\">\t\t<span class=\"comment\"># &#x27;invscaling&#x27;: eta = eta0 / pow(t, power_t)   # power_t=0.25:存在父类当中</span></span><br><span class=\"line\">        <span class=\"comment\"># 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。</span></span><br><span class=\"line\">    estimator = SGDRegressor(loss=<span class=\"string\">&quot;squared_loss&quot;</span>,fit_intercept=<span class=\"literal\">True</span>,max_iter=<span class=\"number\">1000</span>,learning_rate=<span class=\"string\">&quot;constant&quot;</span>,eta0=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">    estimator.fit(x_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.模型评估</span></span><br><span class=\"line\">    <span class=\"comment\"># 5.1 获取系数等值</span></span><br><span class=\"line\">    y_predict = estimator.predict(x_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;预测值为:\\n&quot;</span>, y_predict)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;模型中的系数为:\\n&quot;</span>, estimator.coef_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;模型中的偏置为:\\n&quot;</span>, estimator.intercept_)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.2 评价</span></span><br><span class=\"line\">    <span class=\"comment\"># 均方误差</span></span><br><span class=\"line\">    error = mean_squared_error(y_test, y_predict)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;误差为:\\n&quot;</span>, error)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br></pre></td></tr></table></figure>\n<h5><span id=\"4-概念解释\"> 4 概念解释</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154448981.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<h5><span id=\"5-梯度下降算法\"> 5 梯度下降算法</span></h5>\n<ul>\n<li>\n<p><code>全梯度下降算法(Full gradient descent)</code></p>\n<ul>\n<li>\n<p>在更新参数时使用所有的样本来进行更新</p>\n</li>\n<li>\n<p>计算训练集所有样本误差，对其求和再取平均值作为目标函数。</p>\n</li>\n</ul>\n<blockquote>\n<p>缺点：</p>\n<ul>\n<li>\n<p>因为在执行每次更新时，我们需要在 <code>整个数据集</code> 上计算 <code>所有的梯度</code> ，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。</p>\n</li>\n<li>\n<p>批梯度下降法同样也不能在线更新模型，即在 <code>运行的过程中，不能增加新的样本</code> 。</p>\n</li>\n</ul>\n</blockquote>\n</li>\n<li>\n<p><code>随机梯度下降算法(Stochastic gradient descent)</code></p>\n<ul>\n<li>每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。</li>\n</ul>\n<blockquote>\n<p>优点：此过程简单，高效，通常可以较好地避免更新迭代收敛到局部最优解。</p>\n<p>缺点：由于 SG 每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。</p>\n</blockquote>\n</li>\n<li>\n<p><code>小批量梯度下降算法(Mini-batch gradient descent)</code></p>\n<ul>\n<li>每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用 FG 迭代更新权重。</li>\n</ul>\n<blockquote>\n<p>小批量梯度下降算法是 FG 和 SG 的折中方案，在一定程度上兼顾了以上两种方法的优点。</p>\n<p>batch_size：被抽出的小样本集所含样本点的个数，通常设置为 2 的幂次方，利于 GPU 加速处理。</p>\n<p>特别的，若 batch_size=1，则变成了 SG；若 batch_size=n，则变成了 FG.</p>\n</blockquote>\n</li>\n<li>\n<p><code>随机平均梯度下降算法(Stochastic average gradient descent)</code></p>\n<ul>\n<li>在内存中为每一个样本都维护一个旧的梯度，随机选择第 i 个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。</li>\n</ul>\n<blockquote>\n<p>在 SG 方法中，虽然避开了运算成本大的问题，但对于大数据训练而言，SG 效果常不尽如人意，因为每一轮梯度更新都完全与上一轮的数据和梯度无关。SAG 能克服该问题。</p>\n</blockquote>\n</li>\n</ul>\n<p>它们都是为了正确地调节权重向量，通过为每个权重计算一个梯度，从而更新权值，使目标函数尽可能最小化。其差别在于样本的使用方式不同。</p>\n<h5><span id=\"6-欠拟合和过拟合原因及解决办法\"> 6 欠拟合和过拟合原因及解决办法</span></h5>\n<ul>\n<li><strong>1 欠拟合原因以及解决办法</strong>\n<ul>\n<li>原因：学习到数据的特征过少</li>\n<li>解决办法：\n<ul>\n<li>1） <code>添加其他特征项</code> ，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性” 三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征” 等等，都可以作为特征添加的首选项。</li>\n<li>2） <code>添加多项式特征</code> ，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>2 过拟合原因以及解决办法</strong>\n<ul>\n<li>原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点</li>\n<li>解决办法：\n<ul>\n<li>1）重新 <code>清洗数据</code> ，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</li>\n<li>2） <code>增大数据的训练量</code> ，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</li>\n<li>3） <code>正则化</code></li>\n<li>4）减少特征维度，防止 <code>维灾难</code></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h5><span id=\"7-正则化-解决过拟合问题\"> 7 正则化 (解决过拟合问题)</span></h5>\n<ul>\n<li>\n<p>在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响）</p>\n</li>\n<li>\n<p><strong>正则化类别</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> Ridge, ElasticNet, Lasso</span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<p><code>L1正则化</code></p>\n<ul>\n<li>\n<p>作用：可以使得其中一些权重直接为 0，删除这个特征的影响</p>\n</li>\n<li>\n<p><code>LASSO回归</code>  (正则项为权值向量的 <code>ℓ1范数</code> )</p>\n<p>代价函数如下：</p>\n</li>\n</ul>\n</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154513865.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<blockquote>\n<p>Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。</p>\n</blockquote>\n<ul>\n<li>\n<p><code>L2正则化</code></p>\n<ul>\n<li>\n<p>作用：可以使得其中一些权重都很小，都接近于 0，削弱某个特征的影响</p>\n</li>\n<li>\n<p>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</p>\n</li>\n<li>\n<p><code>Ridge回归</code> （实现了 <code>SAG</code> ）</p>\n<p>代价函数如下：</p>\n</li>\n</ul>\n</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154546704.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># alpha:正则化力度，也叫 λ λ取值：0~1 1~10；</span></span><br><span class=\"line\"><span class=\"comment\"># solver:会根据数据自动选择优化方法  </span></span><br><span class=\"line\">     <span class=\"comment\"># sag:如果数据集、特征都比较大，选择该随机梯度下降优化</span></span><br><span class=\"line\"><span class=\"comment\"># normalize:数据是否进行标准化</span></span><br><span class=\"line\"><span class=\"comment\"># normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据</span></span><br><span class=\"line\"><span class=\"comment\"># Ridge.coef_:回归权重</span></span><br><span class=\"line\"><span class=\"comment\"># Ridge.intercept_:回归偏置</span></span><br><span class=\"line\">sklearn.linear_model.Ridge(alpha=<span class=\"number\">1.0</span>, fit_intercept=<span class=\"literal\">True</span>,solver=<span class=\"string\">&quot;auto&quot;</span>, normalize=<span class=\"literal\">False</span>)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>\n<p><code>Elastic Net (弹性网络)</code></p>\n<p>弹性网络在岭回归和 Lasso 回归中进行了折中，通过混合比 (mix ratio) r 进行控制：</p>\n<ul>\n<li>r=0：弹性网络变为岭回归</li>\n<li>r=1：弹性网络便为 Lasso 回归</li>\n</ul>\n<p>弹性网络的代价函数 ：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/2021051915461619.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n</li>\n</ul>\n<blockquote>\n<p>回归模型选择：</p>\n<ul>\n<li>常用：岭回归</li>\n<li>假设只有少部分特征是有用的：\n<ul>\n<li>弹性网络</li>\n<li>Lasso</li>\n<li>一般来说，弹性网络的使用更为广泛。因为在特征维度高于训练样本数，或者特征是强相关的情况下，Lasso 回归的表现不太稳定。</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<blockquote>\n<ul>\n<li>\n<p>在高维空间中，大多数训练数据驻留在定义特征空间的超立方体的角落中。</p>\n</li>\n<li>\n<p>所需的训练实例数量随着使用的维度数量呈指数增长。</p>\n</li>\n</ul>\n</blockquote>\n<ul>\n<li>\n<h5><span id=\"8-sklearn-模型的保存和加载-api\"> <strong>8 sklearn 模型的保存和加载 API</strong></span></h5>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.externals <span class=\"keyword\">import</span> joblib</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_dump_demo</span>():</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    模型保存和加载</span></span><br><span class=\"line\"><span class=\"string\">    :return:</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 1.获取数据</span></span><br><span class=\"line\">    data = load_boston()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 2.数据集划分</span></span><br><span class=\"line\">    x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 3.特征工程-标准化</span></span><br><span class=\"line\">    transfer = StandardScaler()</span><br><span class=\"line\">    x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\">    x_test = transfer.fit_transform(x_test)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.机器学习-线性回归(岭回归)</span></span><br><span class=\"line\">    <span class=\"comment\"># # 4.1 模型训练</span></span><br><span class=\"line\">    <span class=\"comment\"># estimator = Ridge(alpha=1)</span></span><br><span class=\"line\">    <span class=\"comment\"># estimator.fit(x_train, y_train)</span></span><br><span class=\"line\">    <span class=\"comment\">#</span></span><br><span class=\"line\">    <span class=\"comment\"># # 4.2 模型保存</span></span><br><span class=\"line\">    <span class=\"comment\"># joblib.dump(estimator, &quot;./data/test.pkl&quot;)</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 4.3 模型加载</span></span><br><span class=\"line\">    estimator = joblib.load(<span class=\"string\">&quot;./data/test.pkl&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.模型评估</span></span><br><span class=\"line\">    <span class=\"comment\"># 5.1 获取系数等值</span></span><br><span class=\"line\">    y_predict = estimator.predict(x_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;预测值为:\\n&quot;</span>, y_predict)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;模型中的系数为:\\n&quot;</span>, estimator.coef_)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;模型中的偏置为:\\n&quot;</span>, estimator.intercept_)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 5.2 评价</span></span><br><span class=\"line\">    <span class=\"comment\"># 均方误差</span></span><br><span class=\"line\">    error = mean_squared_error(y_test, y_predict)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;误差为:\\n&quot;</span>, error)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4><span id=\"13-逻辑回归\"> 1.3 逻辑回归</span></h4>\n<p>逻辑回归（Logistic Regression）是机器学习中的<strong>一种分类模型</strong>，逻辑回归是一种分类算法，逻辑回归就是解决二分类问题的利器。</p>\n<p><strong>算法原理</strong>：将线性回归的输出作为逻辑回归的输入，然后经过 sigmoid 函数变换将整体的值映射到 [0，1]，再设定阈值进行分类。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> klearn.linear_models <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"comment\"># solver可选参数:&#123;&#x27;liblinear&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;,&#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;&#125;，</span></span><br><span class=\"line\">\t<span class=\"comment\"># 对于小数据集来说，“liblinear”是个不错的选择，而“sag”和&#x27;saga&#x27;对于大型数据集会更快。</span></span><br><span class=\"line\">\t<span class=\"comment\"># 对于多类问题，只有&#x27;newton-cg&#x27;， &#x27;sag&#x27;， &#x27;saga&#x27;和&#x27;lbfgs&#x27;可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。</span></span><br><span class=\"line\"><span class=\"comment\"># penalty：正则化的种类</span></span><br><span class=\"line\"><span class=\"comment\"># C：正则化力度</span></span><br><span class=\"line\">estimator = LogisticRegression(solver=<span class=\"string\">&#x27;liblinear&#x27;</span>, penalty=‘l2’, C = <span class=\"number\">1.0</span>)</span><br></pre></td></tr></table></figure>\n<h5><span id=\"1-总损失函数对数似然损失\"> 1 总损失函数（对数似然损失）</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154639187.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<h5><span id=\"2-概念解释\"> 2 概念解释</span></h5>\n<p><code>准确率</code> ：所有样本中预测对的比例</p>\n<p><code>精确率</code> ：预测结果为正例样本中真实为正例的比例（预测为正的样本中的正样本）</p>\n<p><code>召回率</code> ：真实为正例的样本中预测结果为正例的比例（正样本中预测为正的样本）</p>\n<p><code>F1-score</code> ：反映模型的稳健性</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154656134.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> classification_report</span><br><span class=\"line\"><span class=\"comment\"># labels:指定类别对应的数字      target_names：目标类别名称          return：每个类别精确率与召回率</span></span><br><span class=\"line\">ret = classification_report(y_test, y_predict, labels=(<span class=\"number\">2</span>,<span class=\"number\">4</span>), target_names=(<span class=\"string\">&quot;良性&quot;</span>, <span class=\"string\">&quot;恶性&quot;</span>))</span><br></pre></td></tr></table></figure>\n<h5><span id=\"3-roc-曲线\"> 3 ROC 曲线</span></h5>\n<ul>\n<li><strong>TPR</strong> = TP / (TP + FN)     &lt; <code>击中率</code>  &gt;\n<ul>\n<li>所有真实类别为 1 的样本中，预测类别为 1 的比例</li>\n</ul>\n</li>\n<li><strong>FPR</strong> = FP / (FP + TN)    &lt; <code>虚惊率</code>  &gt;\n<ul>\n<li>所有真实类别为 0 的样本中，预测类别为 1 的比例</li>\n</ul>\n</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154713204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<ul>\n<li>\n<p><strong>AUC 指标</strong></p>\n<ul>\n<li>AUC 的概率意义是随机取一对正负样本，正样本得分大于负样本得分的概率</li>\n<li>AUC 的范围在 [0, 1] 之间，并且越接近 1 越好，越接近 0.5 属于乱猜</li>\n<li><strong>AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。</strong></li>\n<li><strong>0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</strong></li>\n</ul>\n</li>\n<li>\n<p><strong>API</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> roc_auc_score</span><br><span class=\"line\"><span class=\"comment\"># 计算所得为ROC曲线面积</span></span><br><span class=\"line\">roc_auc_score(y_test, y_predict)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h5><span id=\"4-样本不均衡问题\"> 4 样本不均衡问题</span></h5>\n<ul>\n<li>\n<p>增加一些少数类样本使得正、反例数目接近，然后再进行学习。</p>\n</li>\n<li>\n<p>关于类别不平衡的问题，主要有两种处理方式：</p>\n<ul>\n<li>\n<p><strong>1 过采样方法</strong></p>\n<ul>\n<li>增加数量较少那一类样本的数量，使得正负样本比例均衡。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用imblearn进行随机过采样</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> imblearn.over_sampling <span class=\"keyword\">import</span> RandomOverSampler</span><br><span class=\"line\">ros = RandomOverSampler(random_state=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>\n<p><strong>1.1 过采样经典方法</strong></p>\n<ul>\n<li>\n<p><strong>1 随机过采样法</strong></p>\n<p>通过复制所选择的样本生成样本集</p>\n<p>缺点：易产生模型过拟合问题</p>\n</li>\n<li>\n<p><strong>2 SMOTE 算法</strong></p>\n<p>(Synthetic Minority Oversampling，合成少数类过采样技术)</p>\n<ul>\n<li>\n<p>对每个少数类样本，从它的最近邻中随机选择一个样本，然后在两个样本之间的连线上随机选择一点作为新合成的少数类样本。</p>\n</li>\n<li>\n<p>SMOTE 算法摒弃了随机过采样复制样本的做法，可以防止随机过采样中容易过拟合的问题，实践证明此方法可以提高分类器的性能。</p>\n</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># SMOTE过采样</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> imblearn.over_sampling <span class=\"keyword\">import</span> SMOTE</span><br><span class=\"line\">X_resampled, y_resampled = SMOTE().fit_resample(X, y)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>2 欠采样方法</strong></p>\n<ul>\n<li>减少数量较多那一类样本的数量，使得正负样本比例均衡。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 随机欠采样</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> imblearn.under_sampling <span class=\"keyword\">import</span> RandomUnderSampler</span><br><span class=\"line\">rus = RandomUnderSampler(random_state=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>缺点：</p>\n<ul>\n<li>随机欠采样方法通过改变多数类样本比例以达到修改样本分布的目的，从而使样本分布较为均衡，但由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能会导致分类器丢失有关多数类的重要信息。</li>\n</ul>\n</blockquote>\n</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"14-决策树算法\"> 1.4 决策树算法</span></h4>\n<p>决策树思想的来源非常朴素，程序设计中的条件分支结构就是 if-else 结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法</p>\n<ul>\n<li><strong>是一种树形结构，本质是一颗由多个判断节点组成的树</strong></li>\n<li><strong>其中每个内部节点表示一个属性上的判断，</strong></li>\n<li><strong>每个分支代表一个判断结果的输出，</strong></li>\n<li><strong>最后每个叶节点代表一种分类结果</strong>。</li>\n</ul>\n<h5><span id=\"1-信息增益-信息增益率和基尼系数\"> 1 信息增益、信息增益率和基尼系数</span></h5>\n<p><code>信息熵</code>  (information entropy)：度量样本集合纯度最常用的一种指标。</p>\n<center><img src=\"C:\\Users\\14767\\AppData\\Roaming\\Typora\\typora-user-images\\image-20210507193435691.png\" alt=\"image-20210507193435691\" style=\"zoom:50%;#pic_center\"></center>\n<blockquote>\n<p>篮球比赛里，有 4 个球队 {A,B,C,D} ，获胜概率分别为 {1/2, 1/4, 1/8, 1/8}，求 Ent (D)</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154739610.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n</blockquote>\n<p><code>信息增益</code> （information gain）：以某特征划分数据集前后的熵的差值，用来衡量使用当前特征对于样本集合 D 划分效果的好坏。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154759873.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<h5><span id=\"2-信息熵计算案例\"> 2 信息熵计算案例</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154820650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154839727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154857235.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h4><span id=\"15-集成算法\"> 1.5 集成算法</span></h4>\n<h5><span id=\"1-定义\"> 1 定义</span></h5>\n<p>通过建立几个模型来解决单一预测问题。它的工作原理是生成多个分类器 / 模型，各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。</p>\n<h5><span id=\"2-集成学习中-boosting-和-bagging\"> 2 集成学习中 boosting 和 Bagging</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154922170.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h5><span id=\"3-bagging-及随机森林\"> 3 Bagging 及随机森林</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519154941212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<blockquote>\n<ol>\n<li>\n<p>采样不同数据集</p>\n</li>\n<li>\n<p>训练分类器</p>\n</li>\n<li>\n<p>平权投票，获取最终结果</p>\n</li>\n</ol>\n</blockquote>\n<blockquote>\n<p><strong>Bagging + 决策树 / 线性回归 / 逻辑回归 / 深度学习… = bagging 集成学习方法</strong></p>\n</blockquote>\n<ul>\n<li>\n<p><strong>随机森林</strong></p>\n<p><strong>随机森林</strong> <strong>= Bagging +</strong> <strong>决策树</strong></p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155016427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n</li>\n</ul>\n<blockquote>\n<p>包外数据：没有选择到的数据，称之为 Out-of-bag (OOB) 数据，当数据足够多，对于任意一组数据是包外数据的概率为 1/e。</p>\n<p>经验证，包外估计是对集成分类器泛化误差的<strong>无偏估计</strong>.</p>\n</blockquote>\n<ul>\n<li>\n<p><strong>API 实现</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># n_estimators：(default = 10)森林里的树木数量</span></span><br><span class=\"line\"><span class=\"comment\"># Criterion：(default =“gini”) 分割特征的测量方法</span></span><br><span class=\"line\"><span class=\"comment\"># max_features=&quot;auto”,每个决策树的最大特征数量</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">    If &quot;auto&quot;, then max_features=sqrt(n_features).</span></span><br><span class=\"line\"><span class=\"string\">    If &quot;sqrt&quot;, then max_features=sqrt(n_features)(same as &quot;auto&quot;).</span></span><br><span class=\"line\"><span class=\"string\">    If &quot;log2&quot;, then max_features=log2(n_features).</span></span><br><span class=\"line\"><span class=\"string\">    If None, then max_features=n_features.</span></span><br><span class=\"line\"><span class=\"string\">    &quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># bootstrap：(default = True) 是否在构建树时使用放回抽样</span></span><br><span class=\"line\"><span class=\"comment\"># min_samples_split 内部节点再划分所需最小样本数</span></span><br><span class=\"line\"><span class=\"comment\"># min_samples_leaf 叶子节点的最小样本数</span></span><br><span class=\"line\"><span class=\"comment\"># min_impurity_split: 节点划分最小不纯度</span></span><br><span class=\"line\">sklearn.ensemble.RandomForestClassifier(n_estimators=<span class=\"number\">10</span>, criterion=’gini’, max_depth=<span class=\"literal\">None</span>, bootstrap=<span class=\"literal\">True</span>, random_state=<span class=\"literal\">None</span>, min_samples_split=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h5><span id=\"4-boosting\"> 4 boosting</span></h5>\n<ul>\n<li>\n<h6><span id=\"定义\"> <strong>定义</strong></span></h6>\n<p>每新加入一个弱学习器，整体能力就会得到提升，代表算法有：Adaboost，GBDT，XGBoost，LightGBM</p>\n</li>\n<li>\n<h6><span id=\"算法步骤\"> <strong>算法步骤</strong></span></h6>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155041476.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>1）训练第一个学习器</li>\n<li>2）调整数据分布</li>\n<li>3）训练第二个学习器</li>\n<li>4）再次调整数据分布</li>\n<li>5）依次训练学习器，调整数据分布</li>\n</ul>\n</li>\n<li>\n<h6><span id=\"bagging-集成与-boosting-集成的区别\"> <strong>Bagging 集成与 Boosting 集成的区别</strong></span></h6>\n<ul>\n<li>区别一：数据方面\n<ul>\n<li>Bagging：对数据进行采样训练；</li>\n<li>Boosting：根据前一轮学习结果调整数据的重要性。</li>\n</ul>\n</li>\n<li>区别二：投票方面\n<ul>\n<li>Bagging：所有学习器平权投票；</li>\n<li>Boosting：对学习器进行加权投票。</li>\n</ul>\n</li>\n<li>区别三：学习顺序\n<ul>\n<li>Bagging 的学习是并行的，每个学习器没有依赖关系；</li>\n<li>Boosting 学习是串行，学习有先后顺序。</li>\n</ul>\n</li>\n<li>区别四：主要作用\n<ul>\n<li>Bagging 主要用于提高泛化性能（解决过拟合，也可以说降低方差）</li>\n<li>Boosting 主要用于提高训练精度 （解决欠拟合，也可以说降低偏差）</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155101240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<ul>\n<li>\n<h6><span id=\"adaboost\"> <strong>AdaBoost</strong></span></h6>\n</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155119234.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<ul>\n<li><strong>原理图</strong></li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155142253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> AdaBoostClassifier</span><br></pre></td></tr></table></figure>\n<ul>\n<li>\n<h6><span id=\"gbdt\"> <strong>GBDT</strong></span></h6>\n<ul>\n<li>\n<p>GBDT 的全称是 Gradient Boosting Decision Tree，梯度提升树，在传统机器学习算法中，GBDT 算的上 TOP3 的算法。</p>\n</li>\n<li>\n<p><strong>GBDT 使用的决策树是 CART 回归树</strong></p>\n</li>\n<li>\n<p>无论是处理回归问题还是二分类以及多分类，GBDT 使用的决策树通通都是都是 CART 回归树。</p>\n</li>\n</ul>\n</li>\n</ul>\n<p>在这里插入图片描述</p>\n<h4><span id=\"16-聚类算法\"> 1.6 聚类算法</span></h4>\n<h5><span id=\"1-定义\"> 1 定义</span></h5>\n<p>一种典型的无监督学习算法，主要用于将相似的样本自动归到一个类别中。</p>\n<p>在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。</p>\n<h5><span id=\"2-算法学习\"> 2 算法学习</span></h5>\n<ul>\n<li>1）随机设置 K 个特征空间内的点作为初始的聚类中心</li>\n<li>2）对于其他每个点计算到 K 个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</li>\n<li>3）接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</li>\n<li>4）如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程</li>\n</ul>\n<h5><span id=\"3-api\"> 3 API</span></h5>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># n_clusters:开始的聚类中心数量</span></span><br><span class=\"line\"><span class=\"comment\"># 参数</span></span><br><span class=\"line\">\t<span class=\"comment\"># estimator.fit(x)</span></span><br><span class=\"line\">\t<span class=\"comment\"># estimator.predict(x)</span></span><br><span class=\"line\">\t<span class=\"comment\"># estimator.fit_predict(x)</span></span><br><span class=\"line\">sklearn.cluster.KMeans(n_clusters=<span class=\"number\">8</span>)</span><br></pre></td></tr></table></figure>\n<h5><span id=\"4-案例分析\"> 4 案例分析</span></h5>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_blobs</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> calinski_harabaz_score</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建数据集</span></span><br><span class=\"line\"><span class=\"comment\"># X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共4个簇，</span></span><br><span class=\"line\"><span class=\"comment\"># 簇中心在[-1,-1], [0,0],[1,1], [2,2]， 簇方差分别为[0.4, 0.2, 0.2, 0.2]</span></span><br><span class=\"line\">X, y = make_blobs(n_samples=<span class=\"number\">1000</span>, n_features=<span class=\"number\">2</span>, centers=[[-<span class=\"number\">1</span>, -<span class=\"number\">1</span>], [<span class=\"number\">0</span>, <span class=\"number\">0</span>], [<span class=\"number\">1</span>, <span class=\"number\">1</span>], [<span class=\"number\">2</span>, <span class=\"number\">2</span>]],</span><br><span class=\"line\">                  cluster_std=[<span class=\"number\">0.4</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.2</span>],</span><br><span class=\"line\">                  random_state=<span class=\"number\">9</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据集可视化</span></span><br><span class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], marker=<span class=\"string\">&#x27;o&#x27;</span>)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">y_pred = KMeans(n_clusters=<span class=\"number\">2</span>, random_state=<span class=\"number\">9</span>).fit_predict(X)</span><br><span class=\"line\"><span class=\"comment\"># 分别尝试n_cluses=2\\3\\4,然后查看聚类效果</span></span><br><span class=\"line\">plt.scatter(X[:, <span class=\"number\">0</span>], X[:, <span class=\"number\">1</span>], c=y_pred)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 用Calinski-Harabasz Index评估的聚类分数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(calinski_harabaz_score(X, y_pred)) </span><br></pre></td></tr></table></figure>\n<h5><span id=\"5-模型评估\"> 5 模型评估</span></h5>\n<ul>\n<li>SSE\n<ul>\n<li>误差平方和的值越小越好</li>\n</ul>\n</li>\n<li>肘部法\n<ul>\n<li>下降率突然变缓时即认为是最佳的 k 值</li>\n</ul>\n</li>\n<li>SC 系数\n<ul>\n<li>取值为 [-1, 1]，其值越大越好</li>\n</ul>\n</li>\n<li>CH 系数\n<ul>\n<li>分数 s 高则聚类效果越好</li>\n<li>CH 需要达到的目的：<strong>用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。</strong></li>\n</ul>\n</li>\n</ul>\n<h5><span id=\"6-k-means\"> 6 K-Means</span></h5>\n<ul>\n<li>\n<p><strong>K-Means 算法优缺点总结</strong></p>\n<ul>\n<li>优点：\n<ul>\n<li>1. 原理简单（靠近中心点），实现容易</li>\n<li>2. 聚类效果中上（依赖 K 的选择）</li>\n<li>3. 空间复杂度 o (N)，时间复杂度 o (I<em>K</em>N)</li>\n</ul>\n</li>\n<li>缺点：\n<ul>\n<li>1. 对离群点，噪声敏感 （中心点易偏移）</li>\n<li>2. 很难发现大小差别很大的簇及进行增量计算</li>\n<li>3. 结果不一定是全局最优，只能保证局部最优（与 K 的个数及初值选取有关）</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>优化方法</strong></p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th><strong>优化方法</strong></th>\n<th><strong>思路</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Canopy+kmeans</td>\n<td>Canopy 粗聚类配合 kmeans</td>\n</tr>\n<tr>\n<td>kmeans++</td>\n<td>距离越远越容易成为新的质心</td>\n</tr>\n<tr>\n<td>二分 k-means</td>\n<td>拆除 SSE 最大的簇</td>\n</tr>\n<tr>\n<td>k-medoids</td>\n<td>和 kmeans 选取中心点的方式不同</td>\n</tr>\n<tr>\n<td>kernel kmeans</td>\n<td>映射到高维空间</td>\n</tr>\n<tr>\n<td>ISODATA</td>\n<td>动态聚类，可以更改 K 值大小</td>\n</tr>\n<tr>\n<td>Mini-batch K-Means</td>\n<td>大数据集分批聚类</td>\n</tr>\n</tbody>\n</table>\n<h4><span id=\"17-朴素贝叶斯\"> 1.7 朴素贝叶斯</span></h4>\n<h5><span id=\"1-定义\"> 1 定义</span></h5>\n<ul>\n<li>朴素贝叶斯：假定了特征与特征之间相互独立的贝叶斯公式</li>\n<li>朴素：假定了特征与特征相互独立</li>\n<li>如果一个事物在一些属性条件发生的情况下，事物属于 A 的概率 &gt; 属于 B 的概率，则判定事物属于 A。</li>\n</ul>\n<h5><span id=\"2-算法原理\"> 2 算法原理</span></h5>\n<ul>\n<li>分解各类先验样本数据中的特征；</li>\n<li>计算各类数据中，各特征的条件概率；(比如：特征 1 出现的情况下，属于 A 类的概率 p (A | 特征 1)，属于 B 类的概率 p (B | 特征 1)，属于 C 类的概率 p (C | 特征 1)…)</li>\n<li>分解待分类数据中的特征 (特征 1、特征 2、特征 3、特征 4…)</li>\n<li>计算各特征的各条件概率的乘积，如下所示：<br>\n判断为 A 类的概率：p (A | 特征 1) * p (A | 特征 2) * p (A | 特征 3) * p (A | 特征 4)…<br>\n 判断为 B 类的概率：p (B | 特征 1) * p (B | 特征 2) * p (B | 特征 3) * p (B | 特征 4)…<br>\n 判断为 C 类的概率：p (C | 特征 1) * p (C | 特征 2) * p (C | 特征 3) * p (C | 特征 4)…<br>\n…</li>\n<li>结果中的最大值就是该样本所属的类别</li>\n</ul>\n<h5><span id=\"3-拉普拉斯平滑\"> 3 拉普拉斯平滑</span></h5>\n<ul>\n<li>\n<p><strong>问题</strong>：从下面的例子的到娱乐概率为 0，这是不合理的，如果词频列表里面有很多出现次数为 0，很可能计算结果都为 0。</p>\n</li>\n<li>\n<p><strong>解决方法</strong>：拉普拉斯平滑</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155254197.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<blockquote>\n<p>a 为指定的系数一般为 1，m 为训练文档中统计出的特征词个数</p>\n</blockquote>\n</li>\n</ul>\n<h5><span id=\"4-案例实现\"> 4 案例实现</span></h5>\n<p><strong>商品评论情感分析</strong></p>\n<ul>\n<li>1）获取数据</li>\n<li>2）数据基本处理\n<ul>\n<li>2.1） 取出内容列，对数据进行分析</li>\n<li>2.2） 判定评判标准</li>\n<li>2.3） 选择停用词</li>\n<li>2.4） 把内容处理，转化成标准格式</li>\n<li>2.5） 统计词的个数</li>\n<li>2.6）准备训练集和测试集</li>\n</ul>\n</li>\n<li>3）模型训练</li>\n<li>4）模型评估</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> jieba</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.feature_extraction.text <span class=\"keyword\">import</span> CountVectorizer</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.naive_bayes <span class=\"keyword\">import</span> MultinomialNB</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载数据</span></span><br><span class=\"line\">data = pd.read_csv(<span class=\"string\">&quot;./data/书籍评价.csv&quot;</span>, encoding=<span class=\"string\">&quot;gbk&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据基本处理</span></span><br><span class=\"line\"><span class=\"comment\"># 2.1） 取出内容列，对数据进行分析</span></span><br><span class=\"line\">content = data[<span class=\"string\">&quot;内容&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2.2） 判定评判标准 -- 1好评;0差评</span></span><br><span class=\"line\">data.loc[data.loc[:, <span class=\"string\">&#x27;评价&#x27;</span>] == <span class=\"string\">&quot;好评&quot;</span>, <span class=\"string\">&quot;评论标号&quot;</span>] = <span class=\"number\">1</span>  <span class=\"comment\"># 把好评修改为1</span></span><br><span class=\"line\">data.loc[data.loc[:, <span class=\"string\">&#x27;评价&#x27;</span>] == <span class=\"string\">&#x27;差评&#x27;</span>, <span class=\"string\">&#x27;评论标号&#x27;</span>] = <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># data.head()</span></span><br><span class=\"line\">good_or_bad = data[<span class=\"string\">&#x27;评价&#x27;</span>].values  <span class=\"comment\"># 获取数据</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(good_or_bad)</span><br><span class=\"line\"><span class=\"comment\"># [&#x27;好评&#x27; &#x27;好评&#x27; &#x27;好评&#x27; &#x27;好评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;好评&#x27; &#x27;差评&#x27; &#x27;差评&#x27; &#x27;差评&#x27;]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2.3） 选择停用词</span></span><br><span class=\"line\"><span class=\"comment\"># 加载停用词</span></span><br><span class=\"line\">stopwords=[]</span><br><span class=\"line\"><span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;./data/stopwords.txt&#x27;</span>,<span class=\"string\">&#x27;r&#x27;</span>,encoding=<span class=\"string\">&#x27;utf-8&#x27;</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    lines=f.readlines()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(lines)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> tmp <span class=\"keyword\">in</span> lines:</span><br><span class=\"line\">        line=tmp.strip()</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(line)</span><br><span class=\"line\">        stopwords.append(line)</span><br><span class=\"line\"><span class=\"comment\"># stopwords  # 查看新产生列表</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#对停用词表进行去重</span></span><br><span class=\"line\">stopwords=<span class=\"built_in\">list</span>(<span class=\"built_in\">set</span>(stopwords))<span class=\"comment\">#去重  列表形式</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(stopwords)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2.4） 把“内容”处理，转化成标准格式</span></span><br><span class=\"line\">comment_list = []</span><br><span class=\"line\"><span class=\"keyword\">for</span> tmp <span class=\"keyword\">in</span> content:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(tmp)</span><br><span class=\"line\">    <span class=\"comment\"># 对文本数据进行切割</span></span><br><span class=\"line\">    <span class=\"comment\"># cut_all 参数默认为 False,所有使用 cut 方法时默认为精确模式</span></span><br><span class=\"line\">    seg_list = jieba.cut(tmp, cut_all=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(seg_list)  <span class=\"comment\"># &lt;generator object Tokenizer.cut at 0x0000000007CF7DB0&gt;</span></span><br><span class=\"line\">    seg_str = <span class=\"string\">&#x27;,&#x27;</span>.join(seg_list)  <span class=\"comment\"># 拼接字符串</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(seg_str)</span><br><span class=\"line\">    comment_list.append(seg_str)  <span class=\"comment\"># 目的是转化成列表形式</span></span><br><span class=\"line\"><span class=\"comment\"># print(comment_list)  # 查看comment_list列表。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2.5） 统计词的个数</span></span><br><span class=\"line\"><span class=\"comment\"># 进行统计词个数</span></span><br><span class=\"line\"><span class=\"comment\"># 实例化对象</span></span><br><span class=\"line\"><span class=\"comment\"># CountVectorizer 类会将文本中的词语转换为词频矩阵</span></span><br><span class=\"line\">con = CountVectorizer(stop_words=stopwords)</span><br><span class=\"line\"><span class=\"comment\"># 进行词数统计</span></span><br><span class=\"line\">X = con.fit_transform(comment_list)  <span class=\"comment\"># 它通过 fit_transform 函数计算各个词语出现的次数</span></span><br><span class=\"line\">name = con.get_feature_names()  <span class=\"comment\"># 通过 get_feature_names()可获取词袋中所有文本的关键字</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(X.toarray())  <span class=\"comment\"># 通过 toarray()可看到词频矩阵的结果</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(name)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2.6）准备训练集和测试集</span></span><br><span class=\"line\"><span class=\"comment\"># 准备训练集   这里将文本前10行当做训练集  后3行当做测试集</span></span><br><span class=\"line\">x_train = X.toarray()[:<span class=\"number\">10</span>, :]</span><br><span class=\"line\">y_train = good_or_bad[:<span class=\"number\">10</span>]</span><br><span class=\"line\"><span class=\"comment\"># 准备测试集</span></span><br><span class=\"line\">x_text = X.toarray()[<span class=\"number\">10</span>:, :]</span><br><span class=\"line\">y_text = good_or_bad[<span class=\"number\">10</span>:]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 模型训练</span></span><br><span class=\"line\"><span class=\"comment\"># 构建贝叶斯算法分类器</span></span><br><span class=\"line\">mb = MultinomialNB(alpha=<span class=\"number\">1</span>)  <span class=\"comment\"># alpha 为可选项，默认 1.0，添加拉普拉修/Lidstone 平滑参数</span></span><br><span class=\"line\"><span class=\"comment\"># 训练数据</span></span><br><span class=\"line\">mb.fit(x_train, y_train)</span><br><span class=\"line\"><span class=\"comment\"># 预测数据</span></span><br><span class=\"line\">y_predict = mb.predict(x_text)</span><br><span class=\"line\"><span class=\"comment\">#预测值与真实值展示</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;预测值：&#x27;</span>,y_predict)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;真实值：&#x27;</span>,y_text)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 模型评估</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(mb.score(x_text, y_text))</span><br></pre></td></tr></table></figure>\n<h4><span id=\"18-支持向量机\"> 1.8 支持向量机</span></h4>\n<h5><span id=\"1-基本元素\"> 1 基本元素</span></h5>\n<ul>\n<li>\n<p>【data】数据</p>\n</li>\n<li>\n<p>【classifier】分类</p>\n</li>\n<li>\n<p>【optimization】最优化</p>\n</li>\n<li>\n<p>【kernelling】核方法</p>\n</li>\n<li>\n<p>【hyperplane】超平面</p>\n</li>\n</ul>\n<h5><span id=\"2-基本思想\"> 2 基本思想</span></h5>\n<p>SVM (supported vector machine，支持向量机)，即寻找到一个超平面使样本分成两类，并且间隔最大。</p>\n<h5><span id=\"3-用途\"> 3 用途</span></h5>\n<ul>\n<li>\n<p>线性或非线性分类、回归，甚至是异常值检测</p>\n</li>\n<li>\n<p>特别适用于中小型复杂数据集的分类</p>\n</li>\n</ul>\n<h5><span id=\"4-硬间隔和软间隔\"> 4 硬间隔和软间隔</span></h5>\n<p><strong>硬间隔</strong>：严格地让所有实例都不在最大间隔之间，并且位于正确的一边。它只在数据是线性可分离的时候才有效；其次，它对异常值非常敏感。</p>\n<p><strong>软间隔</strong>：尽可能在保持最大间隔宽阔和限制间隔违例（即位于最大间隔之上，甚至在错误的一边的实例）之间找到良好的平衡。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155332648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h5><span id=\"5-支持向量机推导\"> 5 支持向量机推导</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155358502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<h5><span id=\"6-损失函数\"> 6 损失函数</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155425223.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<blockquote>\n<p>SVM Hinge 损失（折页损失函数、铰链损失函数）</p>\n</blockquote>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155444524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h5><span id=\"7-svm-回归\"> 7 SVM 回归</span></h5>\n<p>让尽可能多的实例位于预测线上，同时限制间隔违例（也就是不在预测线距上的实例）。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.svm.SVC(C=<span class=\"number\">1.0</span>, kernel=<span class=\"string\">&#x27;rbf&#x27;</span>, degree=<span class=\"number\">3</span>,coef0=<span class=\"number\">0.0</span>,random_state=<span class=\"literal\">None</span>)</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">C:</span></span><br><span class=\"line\"><span class=\"string\">\t惩罚系数，用来控制损失函数的惩罚系数，类似于线性回归中的正则化系数。</span></span><br><span class=\"line\"><span class=\"string\">  - C越大，相当于惩罚松弛变量，希望松弛变量接近0，即**对误分类的惩罚增大**，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱，容易导致过拟合。</span></span><br><span class=\"line\"><span class=\"string\">  - C值小，对误分类的惩罚减小，容错能力增强，泛化能力较强，但也可能欠拟合。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">- kernel:</span></span><br><span class=\"line\"><span class=\"string\">  算法中采用的核函数类型，核函数是用来将非线性问题转化为线性问题的一种方法。</span></span><br><span class=\"line\"><span class=\"string\">  - 参数选择有RBF, Linear, Poly, Sigmoid或者自定义一个核函数。</span></span><br><span class=\"line\"><span class=\"string\">    - 默认的是&quot;RBF&quot;，即径向基核，也就是高斯核函数；</span></span><br><span class=\"line\"><span class=\"string\">    - 而Linear指的是线性核函数，</span></span><br><span class=\"line\"><span class=\"string\">    - Poly指的是多项式核，</span></span><br><span class=\"line\"><span class=\"string\">    - Sigmoid指的是双曲正切函数tanh核；。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">- degree:</span></span><br><span class=\"line\"><span class=\"string\">  - 当指定kernel为&#x27;poly&#x27;时，表示选择的多项式的最高次数，默认为三次多项式；</span></span><br><span class=\"line\"><span class=\"string\">  - 若指定kernel不是&#x27;poly&#x27;，则忽略，即该参数只对&#x27;poly&#x27;有用。</span></span><br><span class=\"line\"><span class=\"string\">    - 多项式核函数是将低维的输入空间映射到高维的特征空间。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">- coef0:核函数常数值(y=kx+b中的b值)，</span></span><br><span class=\"line\"><span class=\"string\">  - 只有‘poly’和‘sigmoid’核函数有，默认值是0。</span></span><br><span class=\"line\"><span class=\"string\">  &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sklearn.svm.LinearSVC(penalty=<span class=\"string\">&#x27;l2&#x27;</span>, loss=<span class=\"string\">&#x27;squared_hinge&#x27;</span>, dual=<span class=\"literal\">True</span>, C=<span class=\"number\">1.0</span>)</span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">penalty:正则化参数，L1和L2两种参数可选，仅LinearSVC有。</span></span><br><span class=\"line\"><span class=\"string\">loss:损失函数，有hinge和squared_hinge两种可选，前者又称L1损失，后者称为L2损失，默认是squared_hinge，其中hinge是SVM的标准损失，</span></span><br><span class=\"line\"><span class=\"string\">\tsquared_hinge是hinge的平方</span></span><br><span class=\"line\"><span class=\"string\">dual:是否转化为对偶问题求解，默认是True。</span></span><br><span class=\"line\"><span class=\"string\">C:惩罚系数，用来控制损失函数的惩罚系数，类似于线性回归中的正则化系数。</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>\n<h5><span id=\"8-svm-优缺点\"> 8 SVM 优缺点</span></h5>\n<ul>\n<li>SVM 的优点：\n<ul>\n<li>在高维空间中非常高效；</li>\n<li>即使在数据维度比样本数量大的情况下仍然有效；</li>\n<li>在决策函数（称为支持向量）中使用训练集的子集，因此它也是高效利用内存的；</li>\n<li>通用性：不同的核函数与特定的决策函数一一对应；</li>\n</ul>\n</li>\n<li>SVM 的缺点：\n<ul>\n<li>如果特征数量比样本数量大得多，在选择核函数时要避免过拟合；</li>\n<li>对缺失数据敏感；</li>\n<li>对于核函数的高维映射解释力不强</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"19-em-算法\"> 1.9 EM 算法</span></h4>\n<h5><span id=\"1-基本思想\"> 1 基本思想</span></h5>\n<ul>\n<li>首先<strong>根据己经给出的观测数据，估计出模型参数的值</strong>；</li>\n<li>然后<strong>再依据上一步估计出的参数值估计缺失数据的值</strong>，再根据估计出的缺失数据加上之前己经观测到的数据<strong>重新再对参数值进行估计</strong>；</li>\n<li>然后反复迭代，直至最后收敛，迭代结束。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155512541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h5><span id=\"2-算法流程\"> 2 算法流程</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155537945.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h4><span id=\"110-hmm-模型\"> 1.10 HMM 模型</span></h4>\n<h5><span id=\"1-定义\"> 1 定义</span></h5>\n<p>隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。</p>\n<blockquote>\n<p>马尔科夫链即为<strong>状态空间中从一个状态到另一个状态转换的随机过程。</strong></p>\n<p>马尔科夫链的无记忆性：<strong>下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关</strong>。</p>\n</blockquote>\n<h5><span id=\"2-常见术语\"> 2 常见术语</span></h5>\n<ul>\n<li>可见状态链</li>\n<li>隐含状态链</li>\n<li>转换概率</li>\n<li>输出概率</li>\n</ul>\n<h5><span id=\"3-hmm-两个重要假设\"> 3 HMM 两个重要假设</span></h5>\n<p>1） 齐次马尔科夫链假设</p>\n<ul>\n<li>即任意时刻的隐藏状态只依赖于它前一个隐藏状态。</li>\n</ul>\n<p>2） 观测独立性假设</p>\n<ul>\n<li>即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。</li>\n</ul>\n<h5><span id=\"4-hmm-模型算法原理\"> 4 HMM 模型算法原理</span></h5>\n<p>一个 HMM 模型，可以由<strong>隐藏状态初始概率分布 Π , 状态转移概率矩阵 A 和观测状态概率矩阵 B 决定</strong>。Π，A 决定状态序列，B 决定观测序列。</p>\n<p>因此，HMM 模型可以由一个三元组 <code>λ</code>  表示如下：</p>\n<ul>\n<li><em>λ</em>=(<em>A</em>,<em>B</em>,Π)= (状态序列，观测序列，初始状态概率分布)</li>\n</ul>\n<h5><span id=\"5-hmm-模型三个基本问题\"> 5 HMM 模型三个基本问题</span></h5>\n<p>1）评估观察序列概率 —— <strong>前向后向的概率计算</strong></p>\n<ul>\n<li>即给定 <code>模型λ</code> =(A,B,Π) 和 <code>观测序列</code>  O={o_1,o_2,…o_T}，计算在模型 λ 下某一个 <code>观测序列O出现的概率</code>  P (O|λ)。</li>\n<li>这个问题的求解需要用到前向后向算法，是 HMM 模型三个问题中最简单的。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155603616.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<p>2）预测问题，也称为解码问题 ——<strong> 维特比（Viterbi）算法</strong></p>\n<ul>\n<li>即给定 <code>模型λ</code> =(A,B,Π) 和 <code>观测序列</code>  O={o_1,o_2,…o_T}，求给定观测序列条件下，最可能出现的对应的 <code>状态序列</code> 。</li>\n<li>这个问题的求解需要用到基于动态规划的维特比算法，是 HMM 模型三个问题中复杂度居中的算法。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155623262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 70%;\"></center>\n<p>3）模型参数学习问题 —— <strong>鲍姆 - 韦尔奇（Baum-Welch）算法</strong> (状态未知) ，这是一个学习问题</p>\n<ul>\n<li>即给定 <code>观测序列</code>  O={o_1,o_2,…o_T}，估计 <code>模型λ</code> =(A,B,Π) 的参数，使该模型下观测序列的条件概率 P (O∣λ) 最大。</li>\n<li>这个问题的求解需要用到基于 EM 算法的鲍姆 - 韦尔奇算法，是 HMM 模型三个问题中最复杂的。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155641884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<h5><span id=\"6-案例实现\"> 6 案例实现</span></h5>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> hmmlearn <span class=\"keyword\">import</span> hmm</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设定隐藏状态的集合</span></span><br><span class=\"line\">states = [<span class=\"string\">&quot;box 1&quot;</span>, <span class=\"string\">&quot;box 2&quot;</span>, <span class=\"string\">&quot;box3&quot;</span>]</span><br><span class=\"line\">n_states = <span class=\"built_in\">len</span>(states)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设定观察状态的集合</span></span><br><span class=\"line\">observations = [<span class=\"string\">&quot;red&quot;</span>, <span class=\"string\">&quot;white&quot;</span>]</span><br><span class=\"line\">n_observations = <span class=\"built_in\">len</span>(observations)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设定初始状态分布</span></span><br><span class=\"line\">start_probability = np.array([<span class=\"number\">0.2</span>, <span class=\"number\">0.4</span>, <span class=\"number\">0.4</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设定状态转移概率分布矩阵</span></span><br><span class=\"line\">transition_probability = np.array([</span><br><span class=\"line\">  [<span class=\"number\">0.5</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.3</span>],</span><br><span class=\"line\">  [<span class=\"number\">0.3</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.2</span>],</span><br><span class=\"line\">  [<span class=\"number\">0.2</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.5</span>]</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设定观测状态概率矩阵</span></span><br><span class=\"line\">emission_probability = np.array([</span><br><span class=\"line\">  [<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>],</span><br><span class=\"line\">  [<span class=\"number\">0.4</span>, <span class=\"number\">0.6</span>],</span><br><span class=\"line\">  [<span class=\"number\">0.7</span>, <span class=\"number\">0.3</span>]</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设定模型参数</span></span><br><span class=\"line\">model = hmm.MultinomialHMM(n_components=n_states)</span><br><span class=\"line\">model.startprob_=start_probability  <span class=\"comment\"># 初始状态分布</span></span><br><span class=\"line\">model.transmat_=transition_probability  <span class=\"comment\"># 状态转移概率分布矩阵</span></span><br><span class=\"line\">model.emissionprob_=emission_probability  <span class=\"comment\"># 观测状态概率矩阵</span></span><br><span class=\"line\"></span><br><span class=\"line\">seen = np.array([[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]]).T  <span class=\"comment\"># 设定观测序列</span></span><br><span class=\"line\">box = model.predict(seen)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;球的观测顺序为：\\n&quot;</span>, <span class=\"string\">&quot;, &quot;</span>.join(<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: observations[x], seen.flatten())))</span><br><span class=\"line\"><span class=\"comment\"># 注意：需要使用flatten方法，把seen从二维变成一维</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;最可能的隐藏状态序列为:\\n&quot;</span>， <span class=\"string\">&quot;, &quot;</span>.join(<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x: states[x], box)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(model.score(seen))</span><br><span class=\"line\"><span class=\"comment\"># 输出结果是：-2.03854530992</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对数处理结果，概率值</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\">math.exp(model.score(seen))</span><br><span class=\"line\"><span class=\"comment\"># ln0.13022≈−2.0385</span></span><br><span class=\"line\"><span class=\"comment\"># 输出结果是：0.13021800000000003</span></span><br></pre></td></tr></table></figure>\n<h4><span id=\"111-xgboost-算法\"> 1.11 xgboost 算法</span></h4>\n<h5><span id=\"1-定义\"> 1 定义</span></h5>\n<p>XGBoost（Extreme Gradient Boosting）全名叫极端梯度提升树，XGBoost 是集成学习方法的王牌，在 Kaggle 数据挖掘比赛中，大部分获胜者用了 XGBoost。</p>\n<h5><span id=\"2-最优模型构建方法\"> 2 最优模型构建方法</span></h5>\n<p>构建最优模型的一般方法是<strong>最小化训练数据的损失函数</strong></p>\n<ul>\n<li>\n<p><strong>经验风险最小化</strong></p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155702460.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<blockquote>\n<p>训练得到的模型复杂度较高。当训练数据较小时，模型很容易出现过拟合问题。</p>\n</blockquote>\n</li>\n<li>\n<p><strong>结构风险最小化</strong></p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155728354.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<blockquote>\n<p>结构风险最小化的模型往往对训练数据以及未知的测试数据都有较好的预测</p>\n</blockquote>\n</li>\n</ul>\n<h5><span id=\"3-目标函数\"> 3 目标函数</span></h5>\n<p>目标函数，即损失函数，通过最小化损失函数来构建最优模型。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155753744.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<blockquote>\n<p>其中 yi 是模型的实际输出结果，yi 是模型的输出结果；</p>\n<p>等式右边第一部分是模型的训练误差，第二部分是正则化项，这里的正则化项是 K 棵树的正则化项相加而来的。</p>\n</blockquote>\n<ul>\n<li>\n<p>XGBoost 使用 CART 树，则树的复杂度为</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519155818375.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<blockquote>\n<p>其中 T 为叶子节点的个数，||w|| 为叶子节点向量的模 。γ 表示节点切分的难度，λ 表示 L2 正则化系数。</p>\n</blockquote>\n</li>\n<li>\n<h6><span id=\"xgboost-的回归树构建方法\"> <strong>XGBoost 的回归树构建方法</strong></span></h6>\n<ul>\n<li>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/c02b45b8ca725d897dd16eca1373ff90.png\" alt=\"img\" style=\"zoom: 33%;#pic_center\"></center>\n</li>\n</ul>\n</li>\n<li>\n<h6><span id=\"xgboost-与-gdbt-的区别\"> <strong>XGBoost 与 GDBT 的区别</strong></span></h6>\n<ul>\n<li>区别一：\n<ul>\n<li>XGBoost 生成 CART 树考虑了树的复杂度，</li>\n<li>GDBT 未考虑，GDBT 在树的剪枝步骤中考虑了树的复杂度。</li>\n</ul>\n</li>\n<li>区别二：\n<ul>\n<li>XGBoost 是拟合上一轮损失函数的二阶导展开，GDBT 是拟合上一轮损失函数的一阶导展开，因此，XGBoost 的准确性更高，且满足相同的训练效果，需要的迭代次数更少。</li>\n</ul>\n</li>\n<li>区别三：\n<ul>\n<li>XGBoost 与 GDBT 都是逐次迭代来提高模型性能，但是 XGBoost 在选取最佳切分点时可以开启多线程进行，大大提高了运行速度。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<h6><span id=\"xgboost-中封装的参数\"> <strong>XGBoost 中封装的参数</strong></span></h6>\n<p>主要由三种类型构成：</p>\n<ul>\n<li>\n<p><strong>1 通用参数（general parameters）</strong>：主要是<strong>宏观函数控制；</strong></p>\n<ol>\n<li>\n<p><strong>booster</strong> [缺省值 = gbtree]</p>\n<p>决定使用哪个 booster，可以是 gbtree，gblinear 或者 dart。</p>\n<ul>\n<li>g<strong>btree 和 dart 使用基于树的模型 (dart 主要多了 Dropout)，而 gblinear 使用线性函数.</strong></li>\n</ul>\n</li>\n<li>\n<p><strong>silent</strong> [缺省值 = 0]</p>\n<ul>\n<li>设置为 0 打印运行信息；设置为 1 静默模式，不打印</li>\n</ul>\n</li>\n<li>\n<p><strong>nthread</strong> [缺省值 = 设置为最大可能的线程数]</p>\n<ul>\n<li>并行运行 xgboost 的线程数，输入的参数应该 &lt;= 系统的 CPU 核心数，若是没有设置算法会检测将其设置为 CPU 的全部核心数</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>\n<p><strong>2 Booster 参数（booster parameters）</strong>：取决于选择的 Booster 类型，<strong>用于控制每一步的 booster（tree, regressiong）</strong>；</p>\n<ol>\n<li><strong>eta</strong> [缺省值 = 0.3，别名：learning_rate]\n<ul>\n<li>更新中减少的步长来防止过拟合。</li>\n<li>在每次 boosting 之后，可以直接获得新的特征权值，这样可以使得 boosting 更加鲁棒。</li>\n<li>范围： [0,1]</li>\n</ul>\n</li>\n<li><strong>gamma</strong> [缺省值 = 0，别名: min_split_loss]（分裂最小 loss）\n<ul>\n<li>在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。</li>\n<li>Gamma 指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。</li>\n<li>范围: [0,∞]</li>\n</ul>\n</li>\n<li><strong>max_depth</strong> [缺省值 = 6]\n<ul>\n<li>这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth 越大，模型会学到更具体更局部的样本。设置为 0 代表没有限制</li>\n<li>范围: [0,∞]</li>\n</ul>\n</li>\n<li><strong>min_child_weight</strong> [缺省值 = 1]\n<ul>\n<li>决定最小叶子节点样本权重和。XGBoost 的这个参数是最小样本权重的和.</li>\n<li>当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用 CV 来调整。.</li>\n<li>范围: [0,∞]</li>\n</ul>\n</li>\n<li><strong>subsample</strong> [缺省值 = 1]\n<ul>\n<li>这个参数控制对于每棵树，随机采样的比例。</li>\n<li>减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。</li>\n<li>典型值：0.5-1，0.5 代表平均采样，防止过拟合.</li>\n<li>范围: (0,1]</li>\n</ul>\n</li>\n<li><strong>colsample_bytree</strong> [缺省值 = 1]\n<ul>\n<li>用来控制每棵随机采样的列数的占比 (每一列是一个特征)。</li>\n<li>典型值：0.5-1</li>\n<li>范围: (0,1]</li>\n</ul>\n</li>\n<li><strong>colsample_bylevel</strong> [缺省值 = 1]\n<ul>\n<li>用来控制树的每一级的每一次分裂，对列数的采样的占比。</li>\n<li>我个人一般不太用这个参数，因为 subsample 参数和 colsample_bytree 参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。</li>\n<li>范围: (0,1]</li>\n</ul>\n</li>\n<li><strong>lambda</strong> [缺省值 = 1，别名: reg_lambda]\n<ul>\n<li>权重的 L2 正则化项 (和 Ridge regression 类似)。</li>\n<li>这个参数是用来控制 XGBoost 的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数</li>\n<li>在减少过拟合上还是可以挖掘出更多用处的。.</li>\n</ul>\n</li>\n<li><strong>alpha</strong> [缺省值 = 0，别名: reg_alpha]\n<ul>\n<li>权重的 L1 正则化项。(和 Lasso regression 类似)。 可以应用在很高维度的情况下，使得算法的速度更快。</li>\n</ul>\n</li>\n<li><strong>scale_pos_weight</strong> [缺省值 = 1]\n<ul>\n<li>在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。通常可以将其设置为负</li>\n<li>样本的数目与正样本数目的比值。</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>\n<p><strong>3 学习目标参数（task parameters）</strong>：<strong>控制训练目标的表现</strong>。</p>\n<ol>\n<li>\n<p><strong>objective</strong> [缺省值 = reg:linear]</p>\n<ol>\n<li>“<strong>reg:linear</strong>” – 线性回归</li>\n<li><strong>“reg:logistic</strong>” – 逻辑回归</li>\n<li>“<strong>binary:logistic</strong>” – 二分类逻辑回归，输出为概率</li>\n<li>“<strong>multi:softmax</strong>” – 使用 softmax 的多分类器，返回预测的类别 (不是概率)。在这种情况下，你还需要多设一个参数：num_class (类别数目)</li>\n<li>“<strong>multi:softprob</strong>” – 和 multi:softmax 参数一样，但是返回的是每个数据属于各个类别的概率。</li>\n</ol>\n</li>\n<li>\n<p><strong>eval_metric</strong> [缺省值 = 通过目标函数选择]</p>\n<p>可供选择的如下所示：</p>\n<ol>\n<li>\n<p>“<strong>rmse</strong>”: 均方根误差</p>\n</li>\n<li>\n<p>“<strong>mae</strong>”: 平均绝对值误差</p>\n</li>\n<li>\n<p>“<strong>logloss</strong>”: 负对数似然函数值</p>\n</li>\n<li>\n<p>“</p>\n<dl>\n<dt>error”</dt>\n<dd>二分类错误率。</dd>\n</dl>\n<ul>\n<li>其值通过错误分类数目与全部分类数目比值得到。对于预测，预测值大于 0.5 被认为是正类，其它归为负类。</li>\n</ul>\n</li>\n<li>\n<p>“<strong>error@t</strong>”: 不同的划分阈值可以通过 ‘t’进行设置</p>\n</li>\n<li>\n<p>“<strong>merror</strong>”: 多分类错误率，计算公式为 (wrong cases)/(all cases)</p>\n</li>\n<li>\n<p>“<strong>mlogloss</strong>”: 多分类 log 损失</p>\n</li>\n<li>\n<p>“<strong>auc</strong>”: 曲线下的面积</p>\n</li>\n</ol>\n</li>\n<li>\n<p>seed [缺省值 = 0]</p>\n<ul>\n<li>随机数的种子</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<h5><span id=\"4-案例分析\"> 4 案例分析</span></h5>\n<p><strong>泰坦尼克号乘客存活分析</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.feature_extraction <span class=\"keyword\">import</span> DictVectorizer</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1、获取数据</span></span><br><span class=\"line\">titan = pd.read_csv(<span class=\"string\">&quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2.数据基本处理</span></span><br><span class=\"line\"><span class=\"comment\"># 2.1 确定特征值,目标值</span></span><br><span class=\"line\">x = titan[[<span class=\"string\">&quot;pclass&quot;</span>, <span class=\"string\">&quot;age&quot;</span>, <span class=\"string\">&quot;sex&quot;</span>]]</span><br><span class=\"line\">y = titan[<span class=\"string\">&quot;survived&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 缺失值需要处理，将特征当中有类别的这些特征进行字典特征抽取</span></span><br><span class=\"line\">x[<span class=\"string\">&#x27;age&#x27;</span>].fillna(x[<span class=\"string\">&#x27;age&#x27;</span>].mean(), inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class=\"number\">22</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对于x转换成字典数据x.to_dict(orient=&quot;records&quot;)</span></span><br><span class=\"line\"><span class=\"comment\"># [&#123;&quot;pclass&quot;: &quot;1st&quot;, &quot;age&quot;: 29.00, &quot;sex&quot;: &quot;female&quot;&#125;, &#123;&#125;]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3.特征工程(字典特征抽取)</span></span><br><span class=\"line\">transfer = DictVectorizer(sparse=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x_train = transfer.fit_transform(x_train.to_dict(orient=<span class=\"string\">&quot;records&quot;</span>))</span><br><span class=\"line\">x_test = transfer.fit_transform(x_test.to_dict(orient=<span class=\"string\">&quot;records&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4.xgboost模型训练和模型评估</span></span><br><span class=\"line\"><span class=\"comment\"># 模型初步训练</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> xgboost <span class=\"keyword\">import</span> XGBClassifier</span><br><span class=\"line\">xg = XGBClassifier()</span><br><span class=\"line\">xg.fit(x_train, y_train)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(xg.score(x_test, y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 针对max_depth进行模型调优</span></span><br><span class=\"line\">depth_range = <span class=\"built_in\">range</span>(<span class=\"number\">10</span>)</span><br><span class=\"line\">score = []</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> depth_range:</span><br><span class=\"line\">    xg = XGBClassifier(eta=<span class=\"number\">1</span>, gamma=<span class=\"number\">0</span>, max_depth=i)</span><br><span class=\"line\">    xg.fit(x_train, y_train)</span><br><span class=\"line\">    s = xg.score(x_test, y_test)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(s)</span><br><span class=\"line\">    score.append(s)</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># 结果可视化</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">plt.plot(depth_range, score)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h4><span id=\"112-lightgbm-算法\"> 1.12 lightGBM 算法</span></h4>\n<h5><span id=\"1-定义\"> 1 定义</span></h5>\n<p>LightGBM 提出的主要原因就是为了解决 GBDT 在海量数据遇到的问题，让 GBDT 可以更好更快地用于工业实践。</p>\n<blockquote>\n<p>GBDT 在每一次迭代的时候，都需要遍历整个训练数据多次。</p>\n<p>如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。</p>\n<p>尤其面对工业级海量的数据，普通的 GBDT 算法是不能满足其需求的。</p>\n</blockquote>\n<h5><span id=\"2-特点\"> 2 特点</span></h5>\n<p>在开源之后，就被别人冠以 “速度惊人”、“支持分布式”、“代码清晰易懂”、“占用内存小” 等属性。</p>\n<p>LightGBM 主打的高效并行训练让其性能超越现有其他 boosting 工具。在 Higgs 数据集上的试验表明，LightGBM 比 XGBoost 快将近 <code>10倍</code> ，内存占用率大约为 XGBoost 的 <code>1/6</code> 。</p>\n<p><strong>LightGBM 主要基于以下方面优化，提升整体特特性：</strong></p>\n<ol>\n<li>基于 Histogram（直方图）的决策树算法</li>\n<li>Lightgbm 的 Histogram（直方图）做差加速</li>\n<li>带深度限制的 Leaf-wise 的叶子生长策略</li>\n<li>直接支持类别特征</li>\n<li>直接支持高效并行</li>\n</ol>\n<h5><span id=\"3-优化特点详解\"> 3 优化特点详解</span></h5>\n<ol>\n<li>\n<p><strong>基于 Histogram（直方图）的决策树算法</strong></p>\n<p>直方图算法的基本思想是</p>\n<ul>\n<li>先把连续的浮点特征值离散化成 k 个整数，同时构造一个宽度为 k 的直方图。</li>\n<li>在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</li>\n</ul>\n<blockquote>\n<p><strong>内存消耗的降低，计算上的代价也大幅降低</strong></p>\n</blockquote>\n</li>\n<li>\n<p><strong>Lightgbm 的 Histogram（直方图）做差加速</strong></p>\n<p>一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。</p>\n<p>通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的 k 个桶。</p>\n<p>利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。</p>\n</li>\n<li>\n<p><strong>带深度限制的 Leaf-wise 的叶子生长策略</strong></p>\n<p><strong>Level-wise</strong> 便利一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。</p>\n<ul>\n<li>但实际上 Level-wise 是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</li>\n</ul>\n<p><strong>Leaf-wise</strong> 则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。</p>\n<ul>\n<li>因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。</li>\n<li>Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</li>\n</ul>\n</li>\n<li>\n<p><strong>直接支持类别特征</strong></p>\n<p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的 0/1 特征，降低了空间和时间的效率。</p>\n<p>而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM 优化了对类别特征的支持，可以直接输入类别特征，不需要额外的 0/1 展开。并在决策树算法上增加了类别特征的决策规则。</p>\n<p>在 Expo 数据集上的实验，相比 0/1 展开的方法，训练速度可以加速 8 倍，并且精度一致。目前来看，LightGBM 是第一个直接支持类别特征的 GBDT 工具。</p>\n</li>\n<li>\n<p><strong>直接支持高效并行</strong></p>\n<p>LightGBM 还具有支持高效并行的优点。LightGBM 原生支持并行学习，目前支持特征并行和数据并行的两种。</p>\n<ul>\n<li>特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。</li>\n<li>数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。</li>\n</ul>\n<p>LightGBM 针对这两种并行方法都做了优化:</p>\n<ul>\n<li>在<strong>特征并行</strong>算法中，通过在本地保存全部数据避免对数据切分结果的通信；</li>\n<li>在<strong>数据并行</strong>中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。</li>\n<li>** 基于投票的数据并行 (Voting Parallelization)** 则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。</li>\n</ul>\n</li>\n</ol>\n<h5><span id=\"4-api-相关参数介绍\"> 4 API 相关参数介绍</span></h5>\n<h6><span id=\"41-control-parameters\"> 4.1 Control Parameters</span></h6>\n<table>\n<thead>\n<tr>\n<th>Control Parameters</th>\n<th>含义</th>\n<th>用法</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>max_depth</td>\n<td>树的最大深度</td>\n<td>当模型过拟合时，可以考虑首先降低 max_depth</td>\n</tr>\n<tr>\n<td>min_data_in_leaf</td>\n<td>叶子可能具有的最小记录数</td>\n<td>默认 20，过拟合时用</td>\n</tr>\n<tr>\n<td>feature_fraction</td>\n<td>例如 为 0.8 时，意味着在每次迭代中随机选择 80％的参数来建树</td>\n<td>boosting 为 random forest 时用</td>\n</tr>\n<tr>\n<td>bagging_fraction</td>\n<td>每次迭代时用的数据比例</td>\n<td>用于加快训练速度和减小过拟合</td>\n</tr>\n<tr>\n<td>early_stopping_round</td>\n<td>如果一次验证数据的一个度量在最近的 early_stopping_round 回合中没有提高，模型将停止训练</td>\n<td>加速分析，减少过多迭代</td>\n</tr>\n<tr>\n<td>lambda</td>\n<td>指定正则化</td>\n<td>0～1</td>\n</tr>\n<tr>\n<td>min_gain_to_split</td>\n<td>描述分裂的最小 gain</td>\n<td>控制树的有用的分裂</td>\n</tr>\n<tr>\n<td>max_cat_group</td>\n<td>在 group 边界上找到分割点</td>\n<td>当类别数量很多时，找分割点很容易过拟合时</td>\n</tr>\n<tr>\n<td>n_estimators</td>\n<td>最大迭代次数</td>\n<td>最大迭代数不必设置过大，可以在进行一次迭代后，根据最佳迭代数设置</td>\n</tr>\n</tbody>\n</table>\n<h6><span id=\"42-core-parameters\"> 4.2 Core Parameters</span></h6>\n<table>\n<thead>\n<tr>\n<th>Core Parameters</th>\n<th>含义</th>\n<th>用法</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Task</td>\n<td>数据的用途</td>\n<td>选择 train 或者 predict</td>\n</tr>\n<tr>\n<td>application</td>\n<td>模型的用途</td>\n<td>选择 regression: 回归时， binary: 二分类时， multiclass: 多分类时</td>\n</tr>\n<tr>\n<td>boosting</td>\n<td>要用的算法</td>\n<td>gbdt， rf: random forest， dart: Dropouts meet Multiple Additive Regression Trees， goss: Gradient-based One-Side Sampling</td>\n</tr>\n<tr>\n<td>num_boost_round</td>\n<td>迭代次数</td>\n<td>通常 100+</td>\n</tr>\n<tr>\n<td>learning_rate</td>\n<td>学习率</td>\n<td>常用 0.1, 0.001, 0.003…</td>\n</tr>\n<tr>\n<td>num_leaves</td>\n<td>叶子数量</td>\n<td>默认 31</td>\n</tr>\n<tr>\n<td>device</td>\n<td></td>\n<td>cpu 或者 gpu</td>\n</tr>\n<tr>\n<td>metric</td>\n<td></td>\n<td>mae: mean absolute error ， mse: mean squared error ， binary_logloss: loss for binary classification ， multi_logloss: loss for multi classification</td>\n</tr>\n</tbody>\n</table>\n<h6><span id=\"43-io-parameter\"> 4.3 IO parameter</span></h6>\n<table>\n<thead>\n<tr>\n<th>IO parameter</th>\n<th>含义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>max_bin</td>\n<td>表示 feature 将存入的 bin 的最大数量</td>\n</tr>\n<tr>\n<td>categorical_feature</td>\n<td>如果 categorical_features = 0,1,2， 则列 0，1，2 是 categorical 变量</td>\n</tr>\n<tr>\n<td>ignore_column</td>\n<td>与 categorical_features 类似，只不过不是将特定的列视为 categorical，而是完全忽略</td>\n</tr>\n<tr>\n<td>save_binary</td>\n<td>这个参数为 true 时，则数据集被保存为二进制文件，下次读数据时速度会变快</td>\n</tr>\n</tbody>\n</table>\n<h6><span id=\"调参建议\"> 调参建议：</span></h6>\n<table>\n<thead>\n<tr>\n<th>IO parameter</th>\n<th style=\"text-align:left\">含义</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>num_leaves</code></td>\n<td style=\"text-align:left\">取值应 &lt;= 2^{(max_depth)} 2 (<em>m<strong>a</strong>x</em>_<em>d<strong>e</strong>p<strong>t</strong>h</em>)， 超过此值会导致过拟合</td>\n</tr>\n<tr>\n<td><code>min_data_in_leaf</code></td>\n<td style=\"text-align:left\">将它设置为较大的值可以避免生长太深的树，但可能会导致 underfitting，在大型数据集时就设置为数百或数千</td>\n</tr>\n<tr>\n<td><code>max_depth</code></td>\n<td style=\"text-align:left\">这个也是可以限制树的深度</td>\n</tr>\n</tbody>\n</table>\n<p>下表对应了 Faster Speed ，better accuracy ，over-fitting 三种目的时，可以调的参数</p>\n<table>\n<thead>\n<tr>\n<th>Faster Speed</th>\n<th style=\"text-align:left\">better accuracy</th>\n<th style=\"text-align:left\">over-fitting</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>将  <code>max_bin</code>  设置小一些</td>\n<td style=\"text-align:left\">用较大的  <code>max_bin</code></td>\n<td style=\"text-align:left\"><code>max_bin</code>  小一些</td>\n</tr>\n<tr>\n<td></td>\n<td style=\"text-align:left\"><code>num_leaves</code>  大一些</td>\n<td style=\"text-align:left\"><code>num_leaves</code>  小一些</td>\n</tr>\n<tr>\n<td>用  <code>feature_fraction</code>  来做  <code>sub-sampling</code></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\">用  <code>feature_fraction</code></td>\n</tr>\n<tr>\n<td>用  <code>bagging_fraction 和 bagging_freq</code></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\">设定  <code>bagging_fraction 和 bagging_freq</code></td>\n</tr>\n<tr>\n<td></td>\n<td style=\"text-align:left\">training data 多一些</td>\n<td style=\"text-align:left\">training data 多一些</td>\n</tr>\n<tr>\n<td>用  <code>save_binary</code>  来加速数据加载</td>\n<td style=\"text-align:left\">直接用 categorical feature</td>\n<td style=\"text-align:left\">用  <code>gmin_data_in_leaf 和 min_sum_hessian_in_leaf</code></td>\n</tr>\n<tr>\n<td>用 parallel learning</td>\n<td style=\"text-align:left\">用 dart</td>\n<td style=\"text-align:left\">用  <code>lambda_l1, lambda_l2 ，min_gain_to_split</code>  做正则化</td>\n</tr>\n<tr>\n<td></td>\n<td style=\"text-align:left\"><code>num_iterations</code>  大一些， <code>learning_rate</code>  小一些</td>\n<td style=\"text-align:left\">用  <code>max_depth</code>  控制树的深度</td>\n</tr>\n</tbody>\n</table>\n<h5><span id=\"5-案例分析\"> 5 案例分析</span></h5>\n<p><strong>鸢尾花数据集处理</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> GridSearchCV</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> mean_squared_error</span><br><span class=\"line\"><span class=\"keyword\">import</span> lightgbm <span class=\"keyword\">as</span> lgb</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载数据</span></span><br><span class=\"line\">iris = load_iris()</span><br><span class=\"line\">data = iris.data</span><br><span class=\"line\">target = iris.target</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=<span class=\"number\">0.2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 模型训练</span></span><br><span class=\"line\">gbm = lgb.LGBMRegressor(objective=<span class=\"string\">&#x27;regression&#x27;</span>, learning_rate=<span class=\"number\">0.05</span>, n_estimators=<span class=\"number\">20</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric=<span class=\"string\">&#x27;l1&#x27;</span>, early_stopping_rounds=<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(gbm.score(X_test, y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.810605595102488</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#  网格搜索，参数优化</span></span><br><span class=\"line\">estimator = lgb.LGBMRegressor(num_leaves=<span class=\"number\">31</span>)</span><br><span class=\"line\">param_grid = &#123;</span><br><span class=\"line\">    <span class=\"string\">&#x27;learning_rate&#x27;</span>: [<span class=\"number\">0.01</span>, <span class=\"number\">0.1</span>, <span class=\"number\">1</span>],</span><br><span class=\"line\">    <span class=\"string\">&#x27;n_estimators&#x27;</span>: [<span class=\"number\">20</span>, <span class=\"number\">40</span>]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">gbm = GridSearchCV(estimator, param_grid, cv=<span class=\"number\">4</span>)</span><br><span class=\"line\">gbm.fit(X_train, y_train)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Best parameters found by grid search are:&#x27;</span>, gbm.best_params_)</span><br><span class=\"line\"><span class=\"comment\"># Best parameters found by grid search are: &#123;&#x27;learning_rate&#x27;: 0.1, &#x27;n_estimators&#x27;: 40&#125;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 模型调优</span></span><br><span class=\"line\">gbm = lgb.LGBMRegressor(num_leaves=<span class=\"number\">31</span>, learning_rate=<span class=\"number\">0.1</span>, n_estimators=<span class=\"number\">40</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric=<span class=\"string\">&#x27;l1&#x27;</span>, early_stopping_rounds=<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(gbm.score(X_test, y_test))</span><br><span class=\"line\"><span class=\"comment\"># 0.9536626296481988</span></span><br></pre></td></tr></table></figure>\n<h3><span id=\"2-机器学习算法实现\"> 2 机器学习算法实现</span></h3>\n<h4><span id=\"1-获取数据集\"> 1. 获取数据集</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"number\">1.1</span> 方法一：</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\">iris = load_iris()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"number\">1.2</span> 方法二：</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">data = pd.read_csv(<span class=\"string\">&quot;C:\\\\data\\\\FBlocaltion\\\\train.csv&quot;</span>)</span><br></pre></td></tr></table></figure>\n<h4><span id=\"2-数据基本处理\"> 2. 数据基本处理</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># （备选）：缺失值处理</span></span><br><span class=\"line\">data = data.replace(to_replace=<span class=\"string\">&quot;?&quot;</span>, value=np.NaN)</span><br><span class=\"line\">data = data.dropna()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># x_train,x_test,y_train,y_test为训练集特征值、测试集特征值、训练集目标值、测试集目标值</span></span><br><span class=\"line\"><span class=\"comment\"># x：数据集的特征值   y：数据集的标签值   test_size：测试集的大小，一般为float   random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\">x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class=\"number\">0.2</span>, random_state=<span class=\"number\">22</span>)</span><br></pre></td></tr></table></figure>\n<h4><span id=\"3-特征工程标准化\"> 3. 特征工程：标准化</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程</span></span><br><span class=\"line\"><span class=\"comment\"># 方法一：（ 标准化）   &lt;在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。&gt;</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">transfer = StandardScaler()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法二：（归一化）    &lt;鲁棒性较差，只适合传统精确小数据场景。&gt;</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">transfer = MinMaxScaler()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法三：（特征工程（字典特征抽取））</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.feature_extraction <span class=\"keyword\">import</span> DictVectorizer</span><br><span class=\"line\">transfer = DictVectorizer(sparse=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"comment\"># 特征中出现类别符号，需要进行one-hot编码处理(DictVectorizer)，x.to_dict(orient=&quot;records&quot;) 需要将数组特征转换成字典数据</span></span><br><span class=\"line\">x_train = transfer.fit_transform(x_train.to_dict(orient=<span class=\"string\">&quot;records&quot;</span>))</span><br><span class=\"line\">x_test = transfer.fit_transform(x_test.to_dict(orient=<span class=\"string\">&quot;records&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化。</span></span><br><span class=\"line\">x_train = transfer.fit_transform(x_train)</span><br><span class=\"line\"><span class=\"comment\"># 对剩余的数据（testData）使用同样的均值、方差、最大最小值等指标进行转换transform(testData)，从而保证train、test处理方式相同。</span></span><br><span class=\"line\">x_test = transfer.transform(x_test)</span><br></pre></td></tr></table></figure>\n<h4><span id=\"4-机器学习-模型训练\"> 4. 机器学习 (模型训练)</span></h4>\n<ul>\n<li>\n<h5><span id=\"41-模型估计\"> 4.1 模型估计</span></h5>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 方法一：（K-近邻算法）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neighbors <span class=\"keyword\">import</span> KNeighborsClassifier</span><br><span class=\"line\"><span class=\"comment\"># n_neighbors：查询默认使用的邻居数（默认= 5）</span></span><br><span class=\"line\"><span class=\"comment\"># algorithm：&#123;‘auto’，‘ball_tree’，‘kd_tree’，‘brute’&#125;</span></span><br><span class=\"line\">estimator = KNeighborsClassifier(n_neighbors=<span class=\"number\">7</span>,algorithm=<span class=\"string\">&#x27;auto&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法二：（线性回归）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression</span><br><span class=\"line\">estimator = LinearRegression()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法三：（逻辑回归）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\">estimator = LogisticRegression()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法四：（决策树）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.tree <span class=\"keyword\">import</span> DecisionTreeClassifier, export_graphviz</span><br><span class=\"line\"><span class=\"comment\"># criterion：特征选择标准（&quot;gini&quot;或者&quot;entropy&quot;），前者代表基尼系数，后者代表信息增益。默认&quot;gini&quot;，即CART算法。</span></span><br><span class=\"line\"><span class=\"comment\"># min_samples_split：内部节点再划分所需最小样本数（默认：2）  </span></span><br><span class=\"line\"><span class=\"comment\"># min_samples_leaf：叶子节点最少样本数（默认：1）  </span></span><br><span class=\"line\"><span class=\"comment\"># max_depth：决策树最大深度（10-100）# random_state：随机数种子</span></span><br><span class=\"line\">estimator = DecisionTreeClassifier(criterion=<span class=\"string\">&quot;entropy&quot;</span>, max_depth=<span class=\"number\">5</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<h5><span id=\"42-模型调优\"> 4.2 模型调优</span></h5>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 方法一：（网格搜索）</span></span><br><span class=\"line\"><span class=\"comment\"># estimator：估计器对象   param_grid：估计器参数(dict)&#123;“n_neighbors”:[1,3,5]&#125;     cv：指定几折交叉验证</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> GridSearchCV</span><br><span class=\"line\">estimator = GridSearchCV(estimator, param_grid=&#123;<span class=\"string\">&quot;n_neighbors&quot;</span>: [<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>]&#125;, cv=<span class=\"number\">3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法二：（留出法）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\">train_X , test_X, train_Y ,test_Y = train_test_split(X, Y, test_size=<span class=\"number\">0.2</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法三：（留一法）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> LeaveOneOut</span><br><span class=\"line\">data = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\">loo = LeaveOneOut()</span><br><span class=\"line\"><span class=\"keyword\">for</span> train, test <span class=\"keyword\">in</span> loo.split(data):</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;%s %s&quot;</span> % (train, test))</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># 方法四：（K折交叉验证）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> KFold</span><br><span class=\"line\">folder = KFold(n_splits = <span class=\"number\">4</span>, random_state=<span class=\"number\">0</span>, shuffle = <span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法五：（分层K折交叉验证）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> StratifiedKFold</span><br><span class=\"line\">sfolder = StratifiedKFold(n_splits = <span class=\"number\">4</span>, random_state = <span class=\"number\">0</span>, shuffle = <span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法六：（自助法）</span></span><br></pre></td></tr></table></figure>\n</li>\n<li>\n<h5><span id=\"43-模型训练\"> 4.3 模型训练</span></h5>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">estimator.fit(x_train, y_train)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h4><span id=\"5-模型评估\"> 5. 模型评估</span></h4>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 方法一：比对真实值和预测值</span></span><br><span class=\"line\">y_predict = estimator.predict(x_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;&gt;&gt;&gt;预测结果为:\\n&quot;</span>, y_predict)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;&gt;&gt;&gt;比对真实值和预测值：\\n&quot;</span>, y_predict == y_test)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法二：回归系数（线性回归）</span></span><br><span class=\"line\"><span class=\"comment\"># 回归系数（regression coefficient）：在回归方程中表示自变量x 对因变量y 影响大小的参数。回归系数越大表示x 对y 影响越大，正回归系数表示y 随x 增大而增大，负回归系数表示y 随x增大而减小。</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;回归系数：\\n&#x27;</span>, estimator.coef_)</span><br><span class=\"line\">estimator.predict([[<span class=\"number\">100</span>, <span class=\"number\">80</span>]])  <span class=\"comment\"># 平时成绩100， 期末成绩80的概率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法二：直接计算准确率</span></span><br><span class=\"line\">score = estimator.score(x_test, y_test)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;&gt;&gt;&gt;准确率为：\\n&quot;</span>, score)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;&gt;&gt;&gt;预测最优得分：\\n&quot;</span>, estimator.best_score_)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;&gt;&gt;&gt;最优估计器:\\n&quot;</span>, estimator.best_estimator_)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;&gt;&gt;&gt;最优结果：\\n&quot;</span>, pd.DataFrame(estimator.cv_results_))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法三：分类评估报告生成</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> classification_report</span><br><span class=\"line\">ret = classification_report(y_test, y_predict, labels=(<span class=\"number\">2</span>, <span class=\"number\">4</span>), target_names=(<span class=\"string\">&quot;良性&quot;</span>, <span class=\"string\">&quot;恶性&quot;</span>)</span><br><span class=\"line\">                            </span><br><span class=\"line\"><span class=\"comment\"># 方法四：AUC指标评测</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.metrics <span class=\"keyword\">import</span> roc_auc_score</span><br><span class=\"line\"><span class=\"comment\"># 0.5~1之间，越接近于1约好</span></span><br><span class=\"line\">y_test = np.where(y_test &gt; <span class=\"number\">2.5</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\">roc_auc_score(y_test, y_predict)</span><br></pre></td></tr></table></figure>",
            "tags": [
                "人工智能",
                "算法"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2021/05/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/",
            "url": "https://leezhao415.github.io/2021/05/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/",
            "title": "机器学习算法导论",
            "date_published": "2021-05-19T08:35:27.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95\">机器学习算法</a>\n<ul>\n<li><a href=\"#1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E4%BE%9D%E6%8D%AE\">1 机器学习分类依据</a>\n<ul>\n<li><a href=\"#11-%E6%8C%89%E7%85%A7%E4%BB%BB%E5%8A%A1%E6%98%AF%E5%90%A6%E9%9C%80%E8%A6%81%E5%92%8C%E7%8E%AF%E5%A2%83%E8%BF%9B%E8%A1%8C%E4%BA%A4%E4%BA%92%E8%8E%B7%E5%8F%96%E7%BB%8F%E9%AA%8C\">1.1 按照任务是否需要和环境进行交互获取经验</a>\n<ul>\n<li><a href=\"#111-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0\">1.1.1 监督学习</a></li>\n<li><a href=\"#112-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0\">1.1.2 强化学习</a></li>\n</ul>\n</li>\n<li><a href=\"#12-%E6%8C%89%E7%85%A7%E7%AE%97%E6%B3%95%E5%8A%9F%E8%83%BD%E5%92%8C%E5%BD%A2%E5%BC%8F%E7%9A%84%E7%B1%BB%E4%BC%BC%E6%80%A7\">1.2 按照算法功能和形式的类似性</a>\n<ul>\n<li><a href=\"#121-%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95\">1.2.1 回归算法</a></li>\n<li><a href=\"#122-%E5%9F%BA%E4%BA%8E%E5%AE%9E%E4%BE%8B%E7%9A%84%E7%AE%97%E6%B3%95\">1.2.2 基于实例的算法</a></li>\n<li><a href=\"#123-%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95\">1.2.3 正则化方法</a></li>\n<li><a href=\"#124-%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0\">1.2.4 决策树学习</a></li>\n<li><a href=\"#125-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95\">1.2.5 贝叶斯方法</a></li>\n<li><a href=\"#126-%E5%9F%BA%E4%BA%8E%E6%A0%B8%E7%9A%84%E7%AE%97%E6%B3%95\">1.2.6 基于核的算法</a></li>\n<li><a href=\"#127-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95\">1.2.7 聚类算法</a></li>\n<li><a href=\"#128-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E5%AD%A6%E4%B9%A0\">1.2.8 关联规则学习</a></li>\n<li><a href=\"#129-%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\">1.2.9 人工神经网络</a></li>\n<li><a href=\"#1210-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\">1.2.10 深度学习</a></li>\n<li><a href=\"#1211-%E9%99%8D%E4%BD%8E%E7%BB%B4%E5%BA%A6%E7%AE%97%E6%B3%95\">1.2.11 降低维度算法</a></li>\n<li><a href=\"#1212-%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95\">1.2.12 集成算法</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0\">2 监督学习</a>\n<ul>\n<li><a href=\"#21-%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0\">2.1 有监督学习</a>\n<ul>\n<li><a href=\"#211-k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95\">2.1.1 K - 近邻算法</a></li>\n<li><a href=\"#212-%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95\">2.1.2 决策树和随机森林算法</a></li>\n<li><a href=\"#213-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF\">2.1.3 朴素贝叶斯</a></li>\n<li><a href=\"#214-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92\">2.1.4 线性回归</a></li>\n<li><a href=\"#215-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92\">2.1.5 逻辑回归</a></li>\n<li><a href=\"#216-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA\">2.1.6 支持向量机</a></li>\n<li><a href=\"#217-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0\">2.1.7 集成学习</a></li>\n<li><a href=\"#218-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\">2.1.8 神经网络</a></li>\n</ul>\n</li>\n<li><a href=\"#22-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0\">2.2 无监督学习</a>\n<ul>\n<li><a href=\"#221-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95\">2.2.1 聚类算法</a></li>\n<li><a href=\"#222-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90pca\">2.2.2 主成分分析（PCA）</a></li>\n<li><a href=\"#223-svd%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3\">2.2.3 SVD 矩阵分解</a></li>\n<li><a href=\"#224-%E7%8B%AC%E7%AB%8B%E6%88%90%E5%88%86%E5%88%86%E6%9E%90ica\">2.2.4 独立成分分析 (ICA)</a></li>\n<li><a href=\"#225-em%E7%AE%97%E6%B3%95\">2.2.5 EM 算法</a></li>\n</ul>\n</li>\n<li><a href=\"#23-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0\">2.3 半监督学习</a></li>\n</ul>\n</li>\n<li><a href=\"#3-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0\">3 强化学习</a>\n<ul>\n<li><a href=\"#31-%E6%A6%82%E5%BF%B5%E8%A7%A3%E9%87%8A\">3.1 概念解释</a></li>\n<li><a href=\"#32-%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95\">3.2 常见算法</a></li>\n<li><a href=\"#33-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF\">3.3 应用场景</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h2><span id=\"机器学习算法\"> 机器学习算法</span></h2>\n<h3><span id=\"1-机器学习分类依据\"> 1 机器学习分类依据</span></h3>\n<h4><span id=\"11-按照任务是否需要和环境进行交互获取经验\"> 1.1 按照任务是否需要和环境进行交互获取经验</span></h4>\n<h5><span id=\"111-监督学习\"> 1.1.1 监督学习</span></h5>\n<ul>\n<li>\n<p>1 按照训练数据是否存在标签</p>\n<ul>\n<li>\n<p><code>监督学习</code></p>\n<p>监督学习是从 <code>标记的训练数据</code> 来推断一个功能的机器学习任务。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例。一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签。</p>\n</li>\n<li>\n<p><code>无监督学习</code></p>\n<p>所有数据 <code>只有特征向量没有标签</code> ，但是可以发现这些数据呈现出聚群的结构，本质是一个相似的类型的会聚集在一起。把这些没有标签的数据分成一个一个组合，就是聚类（Clustering）</p>\n</li>\n<li>\n<p><code>半监督学习</code></p>\n<p>半监督学习在训练阶段结合了大量 <code>未标记的数据</code> 和 <code>少量标签数据</code> 。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确，而且训练成本更低。</p>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>2 按照标签是连续还是离散的</strong></p>\n<ul>\n<li>\n<p><code>分类问题</code></p>\n</li>\n<li>\n<p><code>回归问题</code></p>\n<p>探索自变量与因变量之间的关系的问题，回归算法试图采用对误差的衡量来探索变量之间的关系。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h5><span id=\"112-强化学习\"> 1.1.2 强化学习</span></h5>\n<p>智能系统 <code>从环境到行为映射</code> 的学习，以使 <code>奖励信号(强化信号)函数值最大</code> 。如果 Agent 的某个行为策略导致环境正的奖赏 (强化信号)，那么 Agent 以后产生这个行为策略的趋势便会加强</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519105825413.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li><strong>常见算法</strong>\n<ul>\n<li>Q-Learning</li>\n<li>时间差学习（Temporal difference learning）</li>\n</ul>\n</li>\n<li><strong>应用场景</strong>\n<ul>\n<li>动态系统</li>\n<li>机器人控制</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"12-按照算法功能和形式的类似性\"> 1.2 按照算法功能和形式的类似性</span></h4>\n<h5><span id=\"121-回归算法\"> 1.2.1 回归算法</span></h5>\n<p>回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。</p>\n<p>常见的算法：</p>\n<ul>\n<li>最小二乘法（Ordinary Least Square）</li>\n<li>逻辑回归（Logistic Regression）</li>\n<li>逐步式回归（Stepwise Regression）</li>\n<li>多元自适应回归样条（Multivariate Adaptive Regression Splines）</li>\n<li>本地散点平滑估计（Locally Estimated Scatterplot Smoothing）</li>\n</ul>\n<h5><span id=\"122-基于实例的算法\"> 1.2.2 基于实例的算法</span></h5>\n<p>基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为 “赢家通吃” 学习或者 “基于记忆的学习”。</p>\n<p>常见的算法:</p>\n<ul>\n<li>k-Nearest Neighbor(KNN)</li>\n<li>学习矢量量化（Learning Vector Quantization， LVQ）</li>\n<li>以及自组织映射算法（Self-Organizing Map ， SOM）</li>\n</ul>\n<h5><span id=\"123-正则化方法\"> 1.2.3 正则化方法</span></h5>\n<p>正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。</p>\n<p>常见的算法：</p>\n<ul>\n<li>\n<p>岭回归（Ridge Regression）</p>\n</li>\n<li>\n<p>LASSO 回归（Least Absolute Shrinkage and Selection Operator）</p>\n</li>\n<li>\n<p>弹性网络（Elastic Net）</p>\n</li>\n</ul>\n<h5><span id=\"124-决策树学习\"> 1.2.4 决策树学习</span></h5>\n<p>决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。</p>\n<p>常见的算法：</p>\n<ul>\n<li>\n<p>CART 树（Classification And Regression Tree）</p>\n</li>\n<li>\n<p>ID3 (Iterative Dichotomiser 3)</p>\n</li>\n<li>\n<p>C4.5</p>\n</li>\n<li>\n<p>Chi-squared Automatic Interaction Detection(CHAID)</p>\n</li>\n<li>\n<p>Decision Stump</p>\n</li>\n<li>\n<p>随机森林（Random Forest）</p>\n</li>\n<li>\n<p>多元自适应回归样条（MARS）</p>\n</li>\n<li>\n<p>梯度推进机（Gradient Boosting Machine， GBM）</p>\n</li>\n</ul>\n<h5><span id=\"125-贝叶斯方法\"> 1.2.5 贝叶斯方法</span></h5>\n<p>贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。</p>\n<p>常见的算法：</p>\n<ul>\n<li>朴素贝叶斯算法</li>\n<li>平均单依赖估计（Averaged One-Dependence Estimators， AODE）</li>\n<li>Bayesian Belief Network（BBN）</li>\n</ul>\n<h5><span id=\"126-基于核的算法\"> 1.2.6 基于核的算法</span></h5>\n<p>基于核的算法中最著名的莫过于支持向量机（SVM）了。 基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。</p>\n<p>常见的算法：</p>\n<ul>\n<li>\n<p>支持向量机（Support Vector Machine， SVM）</p>\n</li>\n<li>\n<p>径向基函数（Radial Basis Function ，RBF)</p>\n</li>\n<li>\n<p>线性判别分析（Linear Discriminate Analysis ，LDA)</p>\n</li>\n</ul>\n<h5><span id=\"127-聚类算法\"> 1.2.7 聚类算法</span></h5>\n<p>聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。</p>\n<p>常见的算法：</p>\n<ul>\n<li>\n<p>k-Means 算法</p>\n</li>\n<li>\n<p>期望最大化算法（Expectation Maximization， EM）</p>\n</li>\n</ul>\n<h5><span id=\"128-关联规则学习\"> 1.2.8 关联规则学习</span></h5>\n<p>关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。</p>\n<p>常见的算法：</p>\n<ul>\n<li>\n<p>Apriori 算法</p>\n</li>\n<li>\n<p>Eclat 算法</p>\n</li>\n</ul>\n<h5><span id=\"129-人工神经网络\"> 1.2.9 人工神经网络</span></h5>\n<p>人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。</p>\n<p>常见的算法：</p>\n<ul>\n<li>感知器神经网络（Perceptron Neural Network）</li>\n<li>反向传递（Back Propagation）</li>\n<li>Hopfield 网络</li>\n<li>自组织映射（Self-Organizing Map, SOM）</li>\n<li>学习矢量量化（Learning Vector Quantization， LVQ）</li>\n</ul>\n<h5><span id=\"1210-深度学习\"> 1.2.10 深度学习</span></h5>\n<p>深度学习算法是对人工神经网络的发展。 在近期赢得了很多关注， 特别是百度也开始发力深度学习后， 更是在国内引起了很多关注。   在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。</p>\n<p>常见的算法：</p>\n<ul>\n<li>受限波尔兹曼机（Restricted Boltzmann Machine， RBN）</li>\n<li>Deep Belief Networks（DBN）</li>\n<li>卷积网络（Convolutional Network）</li>\n<li>堆栈式自动编码器（Stacked Auto-encoders）</li>\n</ul>\n<h5><span id=\"1211-降低维度算法\"> 1.2.11 降低维度算法</span></h5>\n<p>像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。</p>\n<p>常见的算法：</p>\n<ul>\n<li>主成份分析（Principle Component Analysis， PCA）</li>\n<li>偏最小二乘回归（Partial Least Square Regression，PLS）</li>\n<li>Sammon 映射</li>\n<li>多维尺度（Multi-Dimensional Scaling, MDS）</li>\n<li>投影追踪（Projection Pursuit）</li>\n</ul>\n<h5><span id=\"1212-集成算法\"> 1.2.12 集成算法</span></h5>\n<p>集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。</p>\n<p>常见的算法：</p>\n<ul>\n<li>\n<p>Boosting</p>\n</li>\n<li>\n<p>Bootstrapped Aggregation（Bagging）</p>\n</li>\n<li>\n<p>AdaBoost，堆叠泛化（Stacked Generalization， Blending）</p>\n</li>\n<li>\n<p>梯度推进机（Gradient Boosting Machine, GBM）</p>\n</li>\n<li>\n<p>随机森林（Random Forest）</p>\n</li>\n</ul>\n<h3><span id=\"2-监督学习\"> 2 监督学习</span></h3>\n<h4><span id=\"21-有监督学习\"> 2.1 有监督学习</span></h4>\n<h5><span id=\"211-k-近邻算法\"> 2.1.1 K - 近邻算法</span></h5>\n<p>最近邻 (k-Nearest Neighbors， KNN) 算法是一种分类算法， 1968 年由 Cover 和 Hart 提出， 应用场景有字符识别、 文本分类、 图像识别等领域。<br>\n该算法的思想是： 一个样本与数据集中的 k 个样本最相似， 如果这 k 个样本中的大多数属于某一个类别， 则该样本也属于这个类别。</p>\n<p>KNN 算法流程：</p>\n<ul>\n<li>1）计算已知类别数据集中的点与当前点之间的距离</li>\n<li>2）按距离递增次序排序</li>\n<li>3）选取与当前点距离最小的 k 个点</li>\n<li>4）统计前 k 个点所在的类别出现的频率</li>\n<li>5）返回前 k 个点出现频率最高的类别作为当前点的预测分类</li>\n</ul>\n<h5><span id=\"212-决策树和随机森林算法\"> 2.1.2 决策树和随机森林算法</span></h5>\n<p><code>决策树(Decision Trees)</code>  是一种树形结构，为人们提供决策依据，决策树可以用来回答 yes 和 no 问题，它通过树形结构将各种情况组合都表示出来，每个分支表示一次选择（选择 yes 还是 no），直到所有选择都进行完毕，最终给出正确答案。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110013519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p><code>随机森林(Random Forests)</code></p>\n<ul>\n<li>专为决策树分类器设计的集成算法，是装袋法（Bagging）的一种拓展。</li>\n</ul>\n<p><code>GBDT (梯度提升决策树)</code></p>\n<ul>\n<li>\n<p>在不改变原来模型结构的基础上提升模型的拟合能力</p>\n</li>\n<li>\n<p>利用梯度下降，同损失函数的 <code>负梯度值</code> 作为 <code>残差值</code> 来拟合回归决策树。</p>\n</li>\n<li>\n<p>较为出色的是 XGBoost 树提升系统</p>\n</li>\n</ul>\n<h5><span id=\"213-朴素贝叶斯\"> 2.1.3 朴素贝叶斯</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110031792.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<p>P (A|B) 是后验概率， P (B|A) 是似然，P (A) 为先验概率，P (B) 为我们要预测的值。</p>\n<p>具体应用有：垃圾邮件检测、文章分类、情感分类、人脸识别等。</p>\n<h5><span id=\"214-线性回归\"> 2.1.4 线性回归</span></h5>\n<p>线性回归 (Linear Regression) 的基本思想：找到一条线使得平面内的所有点到这条线的欧式距离和最小。这条线就是我们要求取得线。线性指的是用一条线对数据进行拟合，距离代表的是数据误差，最小二乘法可以看做是误差最小化。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110051627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h5><span id=\"215-逻辑回归\"> 2.1.5 逻辑回归</span></h5>\n<p>逻辑回归 (Logistic Regression) 模型是一个二分类模型，它选取不同的特征与权重来对样本进行概率分类，用一个 log 函数计算样本属于某一类的概率。即一个样本会有一定的概率属于一个类，会有一定的概率属于另一类，概率大的类即为样本所属类。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110110777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>具体应用有：信用评级、营销活动成功概率、产品销售预测、某天是否将会地震发生。</p>\n<h5><span id=\"216-支持向量机\"> 2.1.6 支持向量机</span></h5>\n<p>支持向量机 (Support Vector Machines) 是一个二分类算法，它可以在 N 维空间找到一个 (N-1) 维的超平面，这个超平面可以将这些点分为两类。也就是说，平面内如果存在线性可分的两类点，SVM 可以找到一条最优的直线将这些点分开。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110128579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>具体应用有：广告展示、性别检测、大规模图像识别等。</p>\n<h5><span id=\"217-集成学习\"> 2.1.7 集成学习</span></h5>\n<p>集成学习就是将很多分类器集成在一起，每个分类器有不同的权重，将这些分类器的分类结果合并在一起，作为最终的分类结果。最初集成方法为贝叶斯决策，现在多采用 error-correcting output coding, bagging, and boosting 等方法进行集成。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110145802.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>那么为什集成分类器要比单个分类器效果好呢？</p>\n<p>1. <code>偏差均匀化</code> ：如果你将民主党与共和党的投票数算一下均值，肯定会得到你原先没有发现的结果，集成学习与这个也类似，它可以学到其它任何一种方式都学不到的东西。</p>\n<p>2. <code>减少方差</code> ：总体的结果要比单一模型的结果好，因为其从多个角度考虑问题。类似于股票市场，综合考虑多只股票肯定要比只考虑一只股票好，这就是为什么多数据比少数据效果好原因，因为其考虑的因素更多。</p>\n<p>3. <code>不容易过拟合</code> 。如果的一个模型不过拟合，那么综合考虑多种因素的多模型就更不容易过拟合了。</p>\n<h5><span id=\"218-神经网络\"> 2.1.8 神经网络</span></h5>\n<p>神经网络 (也称之为人工神经网络，ANN)(Neural networks) 算法是 80 年代机器学习界非常流行的算法，不过在 90 年代中途衰落。现在，携着 “深度学习” 之势，神经网络重装归来，重新成为最强大的机器学习算法之一。</p>\n<p>一个简单的神经网络的逻辑架构分成输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是” 神经网络”。</p>\n<h4><span id=\"22-无监督学习\"> 2.2 无监督学习</span></h4>\n<p>常见的无监督学习算法分为三类：</p>\n<ul>\n<li>\n<p>聚类（Clustering）</p>\n<ul>\n<li>k-Means</li>\n<li>Hierarchical Cluster Analysis (HCA)</li>\n<li>Expectation Maximization</li>\n</ul>\n</li>\n<li>\n<p>可视化与降维（Visualization and dimensionality reduction）</p>\n<ul>\n<li>Principal Component Analysis (PCA)</li>\n<li>Kernel PCA</li>\n<li>Locally-Linear Embedding (LLE)</li>\n<li>t-distributed Stochastic Neighbor Embedding (t-SNE)</li>\n</ul>\n</li>\n<li>\n<p>关联规则学习（Association rule learning）</p>\n<ul>\n<li>Apriori</li>\n<li>Eclat</li>\n</ul>\n</li>\n</ul>\n<h5><span id=\"221-聚类算法\"> 2.2.1 聚类算法</span></h5>\n<p>聚类算法就是将一堆数据进行处理，根据它们的相似性对数据进行聚类。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110216990.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>聚类算法有很多种，具体如下：中心聚类、关联聚类、密度聚类、概率聚类、降维、神经网络 / 深度学习。</p>\n<h5><span id=\"222-主成分分析pca\"> 2.2.2 主成分分析（PCA）</span></h5>\n<p>主成分分析是利用正交变换将一些列可能相关数据转换为线性无关数据，从而找到主成分。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110236813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>PCA 主要用于简单学习与可视化中数据压缩、简化。但是 PCA 有一定的局限性，它需要你拥有特定领域的相关知识。对噪音比较多的数据并不适用。</p>\n<h5><span id=\"223-svd-矩阵分解\"> 2.2.3 SVD 矩阵分解</span></h5>\n<p>SVD 矩阵是一个复杂的实复负数矩阵，给定一个 m 行、n 列的矩阵 M, 那么 M 矩阵可以分解为 M = UΣV。U 和 V 是酉矩阵，Σ 为对角阵。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/2021051911025468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>PCA 实际上就是一个简化版本的 SVD 分解。在计算机视觉领域，第一个脸部识别算法就是基于 PCA 与 SVD 的，用特征对脸部进行特征表示，然后降维、最后进行面部匹配。尽管现在面部识别方法复杂，但是基本原理还是类似的。</p>\n<h5><span id=\"224-独立成分分析-ica\"> 2.2.4 独立成分分析 (ICA)</span></h5>\n<p>ICA 是一门统计技术，用于发现存在于随机变量下的隐性因素。ICA 为给观测数据定义了一个生成模型。在这个模型中，其认为数据变量是由隐性变量，经一个混合系统线性混合而成，这个混合系统未知。并且假设潜在因素属于非高斯分布、并且相互独立，称之为可观测数据的独立成分。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110315572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h5><span id=\"225-em-算法\"> 2.2.5 EM 算法</span></h5>\n<p>EM 算法也称期望最大化（Expectation-Maximum, 简称 EM）算法。</p>\n<p>它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）等等。</p>\n<p>EM 算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，</p>\n<ul>\n<li>一个为期望步（E 步）</li>\n<li>一个为极大步（M 步）</li>\n</ul>\n<p>所以算法被称为 EM 算法（Expectation-Maximization Algorithm）。</p>\n<p>EM 算法受到缺失思想影响，最初是为了 <code>解决数据缺失情况下的参数估计问题</code> ，其算法基础和收敛有效性等问题在 Dempster、Laird 和 Rubin 三人于 1977 年所做的文章《Maximum likelihood from incomplete data via the EM algorithm》中给出了详细的阐述。其基本思想是：</p>\n<ul>\n<li>首先根据己经给出的观测数据，估计出模型参数的值；</li>\n<li>然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计；</li>\n<li>然后反复迭代，直至最后收敛，迭代结束。</li>\n</ul>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110340484.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h4><span id=\"23-半监督学习\"> 2.3 半监督学习</span></h4>\n<h3><span id=\"3-强化学习\"> 3 强化学习</span></h3>\n<p>强化学习又称再励学习、评价学习或增强学习。智能系统 <code>从环境到行为映射</code> 的学习，以使 <code>奖励信号(强化信号)函数值最大</code> 。如果 Agent 的某个行为策略导致环境正的奖赏 (强化信号)，那么 Agent 以后产生这个行为策略的趋势便会加强。强化学习四要素： <code>状态(state)</code> 、 <code>动作(action)</code> 、 <code>策略（policy）</code> 、 <code>奖励(reward)</code> 。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210519110511897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h4><span id=\"31-概念解释\"> 3.1 概念解释</span></h4>\n<table>\n<thead>\n<tr>\n<th>名词</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>智能体</td>\n<td>学习器与决策者的角色</td>\n</tr>\n<tr>\n<td>环境</td>\n<td>智能体之外一切组成的、与之交互的事物</td>\n</tr>\n<tr>\n<td>动作</td>\n<td>智能体的行为表征</td>\n</tr>\n<tr>\n<td>状态</td>\n<td>智能体从环境中获取的信息</td>\n</tr>\n<tr>\n<td>奖励</td>\n<td>环境对于动作的反馈</td>\n</tr>\n<tr>\n<td>策略</td>\n<td>智能体根据状态进行下一步动作的函数</td>\n</tr>\n<tr>\n<td>状态转移概率</td>\n<td>智能体做出动作后进入下一状态的概率</td>\n</tr>\n</tbody>\n</table>\n<h4><span id=\"32-常见算法\"> 3.2 常见算法</span></h4>\n<ul>\n<li>Q-Learning</li>\n<li>时间差学习（Temporal difference learning）</li>\n</ul>\n<h4><span id=\"33-应用场景\"> 3.3 应用场景</span></h4>\n<ul>\n<li>动态系统</li>\n<li>机器人控制</li>\n</ul>\n",
            "tags": [
                "人工智能",
                "算法"
            ]
        }
    ]
}