{
    "version": "https://jsonfeed.org/version/1",
    "title": "且听风吟，御剑于心！ • All posts by \"深度模型（目标检测）\" tag",
    "description": "",
    "home_page_url": "https://leezhao415.github.io",
    "items": [
        {
            "id": "https://leezhao415.github.io/2022/02/20/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94/",
            "url": "https://leezhao415.github.io/2022/02/20/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94/",
            "title": "【精华】目标检测模型对比",
            "date_published": "2022-02-20T15:08:01.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#yolo-fastestyoloxyolo-fastestv2nanodetnanodet-plus%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94\">YOLO fastest/YOLOX/YOLO fastestv2/Nanodet/Nanodet Plus 模型对比</a>\n<ul>\n<li><a href=\"#1%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84\">（1）网络结构</a></li>\n<li><a href=\"#2%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%B7%AE%E5%BC%82%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9D%97\">（2）模型结构差异（优化模块）</a></li>\n<li><a href=\"#3%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD\">（3）模型性能</a></li>\n<li><a href=\"#4%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90\">（4）关键概念解析</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h4><span id=\"yolo-fastestyoloxyolo-fastestv2nanodetnanodet-plus-模型对比\"> YOLO fastest/YOLOX/YOLO fastestv2/Nanodet/Nanodet Plus 模型对比</span></h4>\n<ul>\n<li><strong>YOLO fastest</strong>\n<ul>\n<li>Paper</li>\n<li><a href=\"https://github.com/dog-qiuqiu/Yolo-Fastest\">Github 库</a></li>\n</ul>\n</li>\n<li><strong>YOLOX-Nano</strong>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2107.08430\">Paper</a></li>\n<li>Github 库</li>\n</ul>\n</li>\n<li><strong>YOLO fastestv2</strong>\n<ul>\n<li>Paper</li>\n<li><a href=\"https://github.com/dog-qiuqiu/Yolo-FastestV2\">Github 库</a></li>\n</ul>\n</li>\n<li><strong>NanoDet</strong>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2006.04388.pdf\">Paper</a></li>\n<li><a href=\"https://github.com/RangiLyu/nanodet\">Github 库</a></li>\n</ul>\n</li>\n<li><strong>NanoDet Plus</strong>\n<ul>\n<li>Paper</li>\n<li><a href=\"https://github.com/RangiLyu/nanodet\">Github 库</a></li>\n</ul>\n</li>\n</ul>\n<h5><span id=\"1网络结构\"> （1）网络结构</span></h5>\n<h6><span id=\"1gt-yolo-fastest\"> 1&gt; YOLO fastest</span></h6>\n<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/7f27fb556eef47c9bd96c2122172eba4c96598de8c39461d8d0e605c6c982374\" alt=\"img\" style=\"zoom: 33%;\"></center>\n<h6><span id=\"2gt-yolox-nano\"> 2&gt; YOLOX-Nano</span></h6>\n<center><img src=\"https://img-blog.csdnimg.cn/20210720132240327.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FtdXNpMTk5NA==,size_16,color_FFFFFF,t_70\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<h6><span id=\"3gt-yolo-fastestv2\"> 3&gt; YOLO fastestv2</span></h6>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/559dbac4b6c75714bc82f5feca2c3342.png\" alt=\"检测头\" style=\"zoom:110%;\"></center>\n<h6><span id=\"4gt-nanodet\"> 4&gt; NanoDet</span></h6>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/9715e24503e3868f971fbfd722b29edf.png\" alt=\"img\" style=\"zoom: 120%;\"></center>\n<center><img src=\"https://pic2.zhimg.com/v2-b1116f4df5f4e4e79e294b3ccaf294ee_1440w.jpg?source=172ae18b\" alt=\"大白话 Generalized Focal Loss\" style=\"zoom:70%;\"></center>\n<h6><span id=\"5gt-nanodet-plus\"> 5&gt; NanoDet Plus</span></h6>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/ab1c5febbde9c97fce8d1ca414245c04.png\" alt=\"ab1c5febbde9c97fce8d1ca414245c04.png\" style=\"zoom:80%;\"></center>\n<h5><span id=\"2模型结构差异优化模块\"> （2）模型结构差异（优化模块）</span></h5>\n<h6><span id=\"1gt-yolo-fastest\"> 1&gt; YOLO fastest</span></h6>\n<ul>\n<li>\n<p><code>Backbone</code></p>\n<p>EfficientNet-lite</p>\n</li>\n<li>\n<p>注重单核的实时推理性能，在满足实时的条件下的低 CPU 占用，不单单能在手机移动端达到实时，还要在 RK3399，树莓派 4 以及多种 Cortex-A53 低成本低功耗设备上满足一定实时性，毕竟这些嵌入式的设备相比与移动端手机要弱很多，但是使用更加广泛，成本更加低廉。</p>\n</li>\n</ul>\n<h6><span id=\"2gt-yolox-nano\"> 2&gt; YOLOX-Nano</span></h6>\n<ul>\n<li><code>Backbone</code> ：YOLOv3（Darknet-53）</li>\n<li><code>Head</code> ：使用解耦合检测头</li>\n<li><code>标签匹配策略</code> ：SimOTA</li>\n</ul>\n<h6><span id=\"3gt-yolo-fastestv2\"> 3&gt; YOLO fastestv2</span></h6>\n<ul>\n<li>\n<p><code>Backbone</code></p>\n<p>Shufflenetv2（相比于 EfficientNet-lite，访存减少了，更加轻量）</p>\n</li>\n<li>\n<p>Anchor 匹配机制：参考 YOLOv5</p>\n</li>\n<li>\n<p><code>Head</code> ：参考 YOLOX，使用解耦合检测头。</p>\n<ul>\n<li>检测框的回归、前景背景的分类、检测类别的分类</li>\n<li>前景背景的分类以及检测类别的分类采用同一网络分支参数共享</li>\n<li>检测类别分类的 loss 由 sigmoid 替换为 softmax</li>\n<li>输出尺度由 3 个变为 2 个：（11x11、22x22、44x44）变为（11x11、22x22）</li>\n</ul>\n</li>\n</ul>\n<h6><span id=\"4gt-nanodet\"> 4&gt; NanoDet</span></h6>\n<ul>\n<li>\n<p><code>项目思路</code></p>\n<ul>\n<li>大模型发展历程：Two stage 到 One stage，Anchor-base 到 Anchor-free，Transformer</li>\n<li>移动端目标检测：YOLO 系列和 SSD 等 Anchor-base 模型</li>\n<li>NanoDet 项目：希望能够开源一个移动端实时的 Anchor-free 的检测模型。能够提供不亚于 YOLO 系列的性能，而且同样方便训练和移植。\n<ul>\n<li>思路一：将 FCOS 轻量化 &lt;原因：FCOS 的 centerness 分支在轻量化模型上很难收敛&gt;（效果不佳，不如 MobileNet+YOLOv3）</li>\n<li>思路二：<a href=\"https://zhuanlan.zhihu.com/p/147691786\">GFocalLoss</a> 完美去掉了 FCOS 系列的 centerness 分支，省去了这一分支上的大量卷积，减少了检测头的计算开销，非常适合移动端的轻量化部署。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><code>Backbone</code> ：</p>\n<ul>\n<li>\n<p>尝试了 Mbilenet 系列、GhostNet、Shufflenet、EfficientNet</p>\n</li>\n<li>\n<p>使用 <code>Shufflenet v2</code> ：权衡参数量、计算量以及权重大小，该模型在相似精度下体积最小，而且对移动端 CPU 推理比较友好。</p>\n<p>使用 Shufflenetv2 1.0x 作为 Backbone，去掉最后一层卷积，并且抽取 8、16、32 倍下采样的特征输入进 PAN 做多尺度的特征融合。</p>\n</li>\n</ul>\n</li>\n<li>\n<p><code>Neck</code> ：PAFPN</p>\n<ul>\n<li>BiFPN：EfficientDet  （性能强大，但堆叠的特征融合操作势必会带来运行速度的降低）</li>\n<li>PAN：YOLOv4/YOLOv5  （只有自下而上和自上而下的两条通路，非常简洁，是轻量化模型特征融合的不二选择）</li>\n<li>BalancedFPN</li>\n<li>PAFPN\n<ul>\n<li>完全去掉 PAN 中的所有卷积，只保留从骨干网络特征提取后的 1x1 卷积来进行特征通道维度的对齐，上采样和下采样均使用插值来完成。</li>\n<li>与 yolo 使用的 concatenate 操作不同，将多尺度的 Feature Map 直接相加，使得整个特征融合模块的计算量变得非常非常小。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><code>Head</code></p>\n<ul>\n<li>\n<p>使用 2 个深度可分离卷积模块同时预测分类和回归，并将卷积堆叠的数量从 4 个减少到 2 组，通道由 256 压缩到 96 维（大模型中使用 4 组 256channel 的 3x3 卷积预测分类和回归）</p>\n</li>\n<li>\n<p>检测头不共享权重：取消 FCOS 系列模型的共享权重策略，由于移动端模型推理由 CPU 进行计算，共享权重并不会对推理过程进行加速，而且在检测头非常轻量的情况下，共享权重使得其检测能力进一步下降，因此还是选择每一层特征使用一组卷积比较合适。</p>\n</li>\n<li>\n<p>用 BN 代替 GN (Group Normalization)：在推理时能够将其归一化的参数直接融合进卷积中，节省归一化时间。</p>\n</li>\n</ul>\n</li>\n<li>\n<p><code>标签匹配策略</code></p>\n<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/1912.02424\">ATSS</a>：根据 IOU 的均值和方差为每一层 feature map 动态选取匹配样本（本质上依然时基于先验信息（中心点和 Anchor）的静态匹配策略）</p>\n<p>在每个 FPN 层选取离 gt 框中心点最近的 k 个 anchor，之后对所有选取的 anchor 与 gt 计算 IOU，同时计算 IOU 均值和方差，最后保留 IOU 大于均值加方差的并且中心点在 gt 之内的 anchor 作为正样本。</p>\n</li>\n</ul>\n</li>\n<li>\n<p><code>训练策略</code></p>\n<ul>\n<li>SGD+momentum+MutiStepLr</li>\n</ul>\n</li>\n</ul>\n<h6><span id=\"5gt-nanodet-plus\"> 5&gt; NanoDet Plus</span></h6>\n<ul>\n<li>\n<p><code>Backbone</code></p>\n<ul>\n<li>FBNetv5/PicoDet：ESNet（使用 NAS 搜索，在约束了计算量参数量和精度的搜索空间内搜出强的 Backbone）</li>\n<li>NanoDet Plus：沿用 NanoDet 的 Backbone，后期可修改为 ESNet。（算力霸权下妥协）</li>\n</ul>\n</li>\n<li>\n<p><code>Neck</code></p>\n<ul>\n<li>YOLOX/PicoDet/YOLOv5：CSP-PAN</li>\n<li>NanoDet： <code>Ghost-PAN</code> （GhostNet 中的 GhostBlock（1x1 和 3x3 的 depthwise））(<strong>mAP 提升 2%</strong>)</li>\n</ul>\n</li>\n<li>\n<p><code>Head</code></p>\n<ul>\n<li>ThunderNet：轻量级模型中将深度可分离卷积的 depthwise 部分从 3x3 改成 5x5（增加较少参数量的同时提升检测器感受野并提升性能）</li>\n<li>PicoDet：在原本 NanoDet 的 3 层特征基础上增加一层下采样特征</li>\n<li>NanoDet Plus：沿用通用技巧，将检测头的 depthwise 卷积的卷积核大小改成 5x5，并在 NanoDet 的 3 层特征基础上增加一层下采样特征。（<strong>mAP 提升 0.7%</strong>）</li>\n</ul>\n</li>\n<li>\n<p><code>标签匹配策略</code> （使用 AGM（Assign Guidance Module）并配合动态的软标签分配策略 DSLA（ynamic Soft Label Assigner）来解决轻量级模型中的最优标签匹配问题）(<strong>mAP 提升 2.1%</strong>)</p>\n<p>使用 AGM 预测的分类概率和检测框会送入 DSLA 模块计算 Matching Cost。Cost 函数由三部分组成：classification cost，regression cost 以及 distance cost：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/a1f20aaf1f5618bf51359ec171a9b3ac.png\" alt=\"a1f20aaf1f5618bf51359ec171a9b3ac.png\" style=\"zoom: 75%;\"></center>\n<p>最终的代价函数就是这样：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/img_convert/8108380145fa6e6c367bcf5678a41508.png\" alt=\"8108380145fa6e6c367bcf5678a41508.png\" style=\"zoom:80%;\"></center>\n</li>\n<li>\n<p><code>训练策略</code></p>\n<ul>\n<li>优化器：SGD+momentum 改成 AdamW（对超参数更不敏感且收敛更快）</li>\n<li>学习率衰减策略：从 MultiStepLr 改成 CosineAnnealingLR，反向传播计算梯度时加了梯度裁剪。</li>\n<li>其他：增加模型平滑策略 EMA</li>\n</ul>\n</li>\n<li>\n<p><code>部署优化</code></p>\n<ul>\n<li>NanoDet：使用多尺度检测头，每层都有分类和回归两个输出，加上有三个尺度的特征图，共有 6 个输出。（对不熟悉模型结构的人不友好）</li>\n<li>NanoDet Plus：将模型输出合为一个，所有的输出 Tensor 都提前 reshape，然后 concatenate 到一起。（略微影响后处理速度，但模型友好）</li>\n</ul>\n</li>\n</ul>\n<h5><span id=\"3模型性能\"> （3）模型性能</span></h5>\n<h6><span id=\"yolo-fastest-官方库\"> YOLO fastest 官方库</span></h6>\n<table>\n<thead>\n<tr>\n<th>Network</th>\n<th>COCO mAP(0.5)</th>\n<th>Resolution</th>\n<th>Run Time(Ncnn 4xCore)</th>\n<th>Run Time(Ncnn 1xCore)</th>\n<th>FLOPS</th>\n<th>Params</th>\n<th>Weight size</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/dog-qiuqiu/Yolo-Fastest/tree/master/ModelZoo/yolo-fastest-1.1_coco\">Yolo-Fastest-1.1</a></td>\n<td>24.40 %</td>\n<td>320X320</td>\n<td>5.59 ms</td>\n<td>7.52 ms</td>\n<td>0.252BFlops</td>\n<td>0.35M</td>\n<td>1.4M</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/dog-qiuqiu/Yolo-Fastest/tree/master/ModelZoo/yolo-fastest-1.1_coco\">Yolo-Fastest-1.1-xl</a></td>\n<td>34.33 %</td>\n<td>320X320</td>\n<td>9.27ms</td>\n<td>15.72ms</td>\n<td>0.725BFlops</td>\n<td>0.925M</td>\n<td>3.7M</td>\n</tr>\n<tr>\n<td><a href=\"https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-tiny-prn.cfg\">Yolov3-Tiny-Prn</a></td>\n<td>33.1%</td>\n<td>416X416</td>\n<td>%ms</td>\n<td>%ms</td>\n<td>3.5BFlops</td>\n<td>4.7M</td>\n<td>18.8M</td>\n</tr>\n<tr>\n<td><a href=\"https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny.cfg\">Yolov4-Tiny</a></td>\n<td>40.2%</td>\n<td>416X416</td>\n<td>23.67ms</td>\n<td>40.14ms</td>\n<td>6.9 BFlops</td>\n<td>5.77M</td>\n<td>23.1M</td>\n</tr>\n</tbody>\n</table>\n<h6><span id=\"yolo-fastest-11-multi-platform-benchmark\"> Yolo-Fastest-1.1 Multi-platform benchmark</span></h6>\n<table>\n<thead>\n<tr>\n<th>Equipment</th>\n<th>Computing backend</th>\n<th>System</th>\n<th>Framework</th>\n<th>Run time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Mi 11</td>\n<td>Snapdragon 888</td>\n<td>Android(arm64)</td>\n<td>ncnn</td>\n<td>5.59ms</td>\n</tr>\n<tr>\n<td>Mate 30</td>\n<td>Kirin 990</td>\n<td>Android(arm64)</td>\n<td>ncnn</td>\n<td>6.12ms</td>\n</tr>\n<tr>\n<td>Meizu 16</td>\n<td>Snapdragon 845</td>\n<td>Android(arm64)</td>\n<td>ncnn</td>\n<td>7.72ms</td>\n</tr>\n<tr>\n<td>Development board</td>\n<td>Snapdragon 835(Monkey version)</td>\n<td>Android(arm64)</td>\n<td>ncnn</td>\n<td>20.52ms</td>\n</tr>\n<tr>\n<td>Development board</td>\n<td>RK3399</td>\n<td>Linux(arm64)</td>\n<td>ncnn</td>\n<td>35.04ms</td>\n</tr>\n<tr>\n<td>Raspberrypi 3B</td>\n<td>4xCortex-A53</td>\n<td>Linux(arm64)</td>\n<td>ncnn</td>\n<td>62.31ms</td>\n</tr>\n<tr>\n<td>Orangepi Zero Lts</td>\n<td>H2+ 4xCortex-A7</td>\n<td>Linux(armv7)</td>\n<td>ncnn</td>\n<td>550ms</td>\n</tr>\n<tr>\n<td>Nvidia</td>\n<td>Gtx 1050ti</td>\n<td>Ubuntu(x64)</td>\n<td>darknet</td>\n<td>4.73ms</td>\n</tr>\n<tr>\n<td>Intel</td>\n<td>i7-8700</td>\n<td>Ubuntu(x64)</td>\n<td>ncnn</td>\n<td>5.78ms</td>\n</tr>\n</tbody>\n</table>\n<p>Pascal VOC performance index comparison</p>\n<table>\n<thead>\n<tr>\n<th>Network</th>\n<th>Model Size</th>\n<th>mAP(VOC 2007)</th>\n<th>FLOPS</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Tiny YOLOv2</td>\n<td>60.5MB</td>\n<td>57.1%</td>\n<td>6.97BFlops</td>\n</tr>\n<tr>\n<td>Tiny YOLOv3</td>\n<td>33.4MB</td>\n<td>58.4%</td>\n<td>5.52BFlops</td>\n</tr>\n<tr>\n<td>YOLO Nano</td>\n<td>4.0MB</td>\n<td>69.1%</td>\n<td>4.51Bflops</td>\n</tr>\n<tr>\n<td>MobileNetv2-SSD-Lite</td>\n<td>13.8MB</td>\n<td>68.6%</td>\n<td>&amp;Bflops</td>\n</tr>\n<tr>\n<td>MobileNetV2-YOLOv3</td>\n<td>11.52MB</td>\n<td>70.20%</td>\n<td>2.02Bflos</td>\n</tr>\n<tr>\n<td>Pelee-SSD</td>\n<td>21.68MB</td>\n<td>70.09%</td>\n<td>2.40Bflos</td>\n</tr>\n<tr>\n<td><em><strong>Yolo Fastest</strong></em></td>\n<td>1.3MB</td>\n<td>61.02%</td>\n<td>0.23Bflops</td>\n</tr>\n<tr>\n<td><em><strong>Yolo Fastest-XL</strong></em></td>\n<td>3.5MB</td>\n<td>69.43%</td>\n<td>0.70Bflops</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/dog-qiuqiu/MobileNet-Yolo#mobilenetv2-yolov3-litenano-darknet\"><em><strong>MobileNetv2-YOLOv3-Lite</strong></em></a></td>\n<td>8.0MB</td>\n<td>73.26%</td>\n<td>1.80Bflops</td>\n</tr>\n</tbody>\n</table>\n<h6><span id=\"yolox-nano-官方库\"> YOLOX-Nano 官方库</span></h6>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>size</th>\n<th>mAPval 0.5:0.95</th>\n<th>Params (M)</th>\n<th>FLOPs (G)</th>\n<th>weights</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/Megvii-BaseDetection/YOLOX/blob/main/exps/default/nano.py\">YOLOX-Nano</a></td>\n<td>416</td>\n<td>25.8</td>\n<td>0.91</td>\n<td>1.08</td>\n<td><a href=\"https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_nano.pth\">github</a></td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/Megvii-BaseDetection/YOLOX/blob/main/exps/default/yolox_tiny.py\">YOLOX-Tiny</a></td>\n<td>416</td>\n<td>32.8</td>\n<td>5.06</td>\n<td>6.45</td>\n<td><a href=\"https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth\">github</a></td>\n</tr>\n</tbody>\n</table>\n<h6><span id=\"nanodet-官方库\"> NanoDet 官方库</span></h6>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Resolution</th>\n<th>mAPval 0.5:0.95</th>\n<th>CPU Latency (i7-8700)</th>\n<th>ARM Latency (4xA76)</th>\n<th>FLOPS</th>\n<th>Params</th>\n<th>Model Size</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NanoDet-m</td>\n<td>320*320</td>\n<td>20.6</td>\n<td><strong>4.98ms</strong></td>\n<td><strong>10.23ms</strong></td>\n<td><strong>0.72G</strong></td>\n<td><strong>0.95M</strong></td>\n<td><strong>1.8MB(FP16)</strong> | <strong>980KB(INT8)</strong></td>\n</tr>\n<tr>\n<td>NanoDet-m</td>\n<td>416*416</td>\n<td>21.7</td>\n<td></td>\n<td><strong>16.44ms</strong></td>\n<td><strong>1.2G</strong></td>\n<td><strong>0.95M</strong></td>\n<td><strong>1.8MB(FP16)</strong> | <strong>980KB(INT8)</strong></td>\n</tr>\n<tr>\n<td><strong>NanoDet-Plus-m</strong></td>\n<td>320*320</td>\n<td><strong>27.0</strong></td>\n<td><strong>5.25ms</strong></td>\n<td><strong>11.97ms</strong></td>\n<td><strong>0.9G</strong></td>\n<td><strong>1.17M</strong></td>\n<td><strong>2.3MB(FP16)</strong> | <strong>1.2MB(INT8)</strong></td>\n</tr>\n<tr>\n<td><strong>NanoDet-Plus-m</strong></td>\n<td>416*416</td>\n<td><strong>30.4</strong></td>\n<td><strong>8.32ms</strong></td>\n<td><strong>19.77ms</strong></td>\n<td><strong>1.52G</strong></td>\n<td><strong>1.17M</strong></td>\n<td><strong>2.3MB(FP16)</strong> | <strong>1.2MB(INT8)</strong></td>\n</tr>\n<tr>\n<td><strong>NanoDet-Plus-m-1.5x</strong></td>\n<td>320*320</td>\n<td><strong>29.9</strong></td>\n<td><strong>7.21ms</strong></td>\n<td><strong>15.90ms</strong></td>\n<td><strong>1.75G</strong></td>\n<td><strong>2.44M</strong></td>\n<td><strong>4.7MB(FP16)</strong> | <strong>2.3MB(INT8)</strong></td>\n</tr>\n<tr>\n<td><strong>NanoDet-Plus-m-1.5x</strong></td>\n<td>416*416</td>\n<td><strong>34.1</strong></td>\n<td><strong>11.50ms</strong></td>\n<td><strong>25.49ms</strong></td>\n<td><strong>2.97G</strong></td>\n<td><strong>2.44M</strong></td>\n<td><strong>4.7MB(FP16)</strong> | <strong>2.3MB(INT8)</strong></td>\n</tr>\n<tr>\n<td>YOLOv3-Tiny</td>\n<td>416*416</td>\n<td>16.6</td>\n<td>-</td>\n<td>37.6ms</td>\n<td>5.62G</td>\n<td>8.86M</td>\n<td>33.7MB</td>\n</tr>\n<tr>\n<td>YOLOv4-Tiny</td>\n<td>416*416</td>\n<td>21.7</td>\n<td>-</td>\n<td>32.81ms</td>\n<td>6.96G</td>\n<td>6.06M</td>\n<td>23.0MB</td>\n</tr>\n<tr>\n<td>YOLOX-Nano</td>\n<td>416*416</td>\n<td>25.8</td>\n<td>-</td>\n<td>23.08ms</td>\n<td>1.08G</td>\n<td>0.91M</td>\n<td>1.8MB(FP16)</td>\n</tr>\n<tr>\n<td>YOLOv5-n</td>\n<td>640*640</td>\n<td>28.4</td>\n<td>-</td>\n<td>44.39ms</td>\n<td>4.5G</td>\n<td>1.9M</td>\n<td>3.8MB(FP16)</td>\n</tr>\n<tr>\n<td>FBNetV5</td>\n<td>320*640</td>\n<td>30.4</td>\n<td>-</td>\n<td>-</td>\n<td>1.8G</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>MobileDet</td>\n<td>320*320</td>\n<td>25.6</td>\n<td>-</td>\n<td>-</td>\n<td>0.9G</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<h6><span id=\"yolo-fastestv2-官方库\"> YOLO fastestv2 官方库</span></h6>\n<table>\n<thead>\n<tr>\n<th>Network</th>\n<th>COCO mAP(0.5)</th>\n<th>Resolution</th>\n<th>Run Time(4xCore)</th>\n<th>Run Time(1xCore)</th>\n<th>FLOPs(G)</th>\n<th>Params(M)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/dog-qiuqiu/Yolo-FastestV2/tree/main/modelzoo\">Yolo-FastestV2</a></td>\n<td>24.10 %</td>\n<td>352X352</td>\n<td>3.29 ms</td>\n<td>5.37 ms</td>\n<td>0.212</td>\n<td>0.25M</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/dog-qiuqiu/Yolo-Fastest/tree/master/ModelZoo/yolo-fastest-1.1_coco\">Yolo-FastestV1.1</a></td>\n<td>24.40 %</td>\n<td>320X320</td>\n<td>4.23 ms</td>\n<td>7.54 ms</td>\n<td>0.252</td>\n<td>0.35M</td>\n</tr>\n<tr>\n<td><a href=\"https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny.cfg\">Yolov4-Tiny</a></td>\n<td>40.2%</td>\n<td>416X416</td>\n<td>26.00ms</td>\n<td>55.44ms</td>\n<td>6.9</td>\n<td>5.77M</td>\n</tr>\n</tbody>\n</table>\n<h6><span id=\"1gt-yolo-fastest\"> 1&gt; YOLO fastest</span></h6>\n<p><code>初衷就是打破算力的瓶颈，能在更多的低成本的边缘端设备实时运行目标检测算法。</code></p>\n<ul>\n<li>基于 NCNN 推理框架开启 BF16s，在树莓派 3b，4 核 A53 1.2Ghz，320x320 图像单次推理时间在 60ms。</li>\n<li>在性能更加强劲的树莓派 4b，单次推理 33ms，达到了 30fps 的全实时。</li>\n<li>而相比较下应用最广泛的轻量化目标检测算法 MobileNet-SSD 要在树莓派 3b 跑 200ms 左右，Yolo-Fastest 速度整整要快 3 倍 +，而且模型才只有 1.3MB，而 MobileNet-SSD 模型达到 23.2MB，Yolo-Fastest 整整比它小了 20 倍，当然这也是有代价的，在 Pascal voc 上的 mAP，MobileNet-SSD 是 72.7，Yolo-Fastest 是 61.2，带来了接近 10 个点的精度损失</li>\n</ul>\n<p><strong>总结</strong>：YOLO-Fastest 是个牺牲一定精度 （大约 5% 的 mAP）、大幅提升速度的目标检测模型。</p>\n<h6><span id=\"2gt-yolox-nano\"> 2&gt; YOLOX-Nano</span></h6>\n<ul>\n<li>对于 YOLO-Nano，所提方法仅需 0.91M 参数 + 1.08G FLOPs 取得了 25.3% AP 指标，以 1.8% 超越了 NanoDet；</li>\n<li>对于 YOLOv3，所提方法将指标提升到了 47.3%，以 3% 超越了当前最佳；</li>\n<li>具有与 YOLOv4-CSP、YOLOv5-L 相当的参数量，YOLOX-L 取得了 50.0% AP 指标同事具有 68.9fps 推理速度 (Tesla V100)，指标超过 YOLOv5-L 1.8%;</li>\n<li>值得一提的是，YOLOX-L 凭借单模型取得了 Streaming Perception (Workshop on Autonomous Driving at CVPR 2021) 竞赛冠军。</li>\n</ul>\n<h6><span id=\"3gt-yolo-fastestv2\"> 3&gt; YOLO fastestv2</span></h6>\n<ul>\n<li>用 0.3% 的精度损失换取 30% 推理速度的提升以及 25% 的参数量的减少</li>\n</ul>\n<h6><span id=\"4gt-nanodet\"> 4&gt; NanoDet</span></h6>\n<p>在经过对 one-stage 检测模型三大模块（Head、Neck、Backbone）都进行轻量化之后，得到了目前开源的 NanoDet-m 模型，在 320x320 输入分辨率的情况下，整个模型的 Flops 只有 0.72B，而 yolov4-tiny 则有 6.96B，小了将近十倍！模型的参数量也只有 0.95M，权重文件在使用 ncnn optimize 进行 16 位存储之后，只有 1.8mb，非常适合在移动端部署，能够有效减少 APP 体积，同时也对更低端的嵌入式设备更加友好。</p>\n<p>尽管模型非常的轻量，但是性能却依旧强劲。对于小模型，往往选择使用 AP50 这种比较宽容的评价指标进行对比，这里我选择用更严格一点的 COCO mAP (0.5:0.95) 作为评估指标，同时兼顾检测和定位的精度。在 COCO val 5000 张图片上测试，并没有使用 Testing-Time-Augmentation 的情况下，320 分辨率输入能够达到 20.6 的 mAP，比 tiny-yolov3 高 4 分，只比 yolov4-tiny 低 1 个百分点，而将输入分辨率与 yolo 保持一致，都使用 416 输入的情况下，得分持平。</p>\n<p>最后用 ncnn 部署到手机上之后跑了一下 benchmark，模型前向计算时间只要 10 毫秒左右，对比 yolov3 和 v4 tiny，均在 30 毫秒的量级。在安卓摄像头 demo app 上，算上图片预处理，检测框后处理以及绘制检测框的时间，也能轻松跑到 40+FPS~。</p>\n<h6><span id=\"5gt-nanodet-plus\"> 5&gt; NanoDet Plus</span></h6>\n<ul>\n<li>NanoDet Plus 与上一代<a href=\"https://blog.csdn.net/qq_29462849/article/details/122206615\"> NanoDet</a> 相比，在仅增加 1 毫秒多的延时的情况下，精度提升了 30%。</li>\n<li>改进了代码和架构，提出了一种非常简单的训练辅助模块，使模型变得更易训练，同时新版本也更易部署。</li>\n</ul>\n<h5><span id=\"4关键概念解析\"> （4）关键概念解析</span></h5>\n<h6><span id=\"1gt-基于-matching-cost-的动态匹配\"> 1&gt; 基于 Matching Cost 的动态匹配</span></h6>\n<p>简单来说，就是直接使用模型检测头的输出，与每一个 Ground Truth 计算一个匹配的代价，这个代价一般由分类 loss 和回归 loss 组成。Feature Map 上所有的点（N 个）的预测值与所有的 Ground Truth（M 个）计算得到的 NxM 的矩阵，就是所谓的 Cost Matrix，基于这个 Cost Matrix 进行二分图匹配也好还是传输优化也好再或者直接取 TopK 也好，就是一种动态匹配策略。这种策略与之前的基于 Anchor 算 IOU 的匹配最大的不同就是，它不再只依赖先验的静态的信息，而是使用当前的预测结果去动态寻找最优的匹配，只要模型预测的越准确，匹配算法求得的结果也会更优秀。</p>\n<h6><span id=\"2gt-标签匹配策略\"> 2&gt; 标签匹配策略</span></h6>\n<ul>\n<li>基于位置</li>\n<li>基于 Anchor IOU</li>\n<li>基于 Matching Cost（直接使用检测头的输出与每一个 Ground Truth 计算一个匹配的代价（分类 Loss 和回归 Loss））\n<ul>\n<li>基于全局的动态匹配策略\n<ul>\n<li><code>DETR</code> ：使用匈牙利匹配算法进行双边匹配</li>\n<li><code>OTA</code> ：使用 Sinkhorn 迭代求解匹配中的最优传输问题（位置约束：使用 5x5 的中心区域限制匹配自由度）</li>\n<li><code>YOLOX</code> ：使用 OTA 的近似算法 SimOTA（位置约束：使用 5x5 的中心区域限制匹配自由度）</li>\n<li><code>----</code> ：使用<a href=\"https://arxiv.org/abs/2108.10520\"> LAD（Label Assignment Distillation）</a>用教室网络的结果计算标签匹配来指导学生网络的训练</li>\n<li><code>IQDet</code> ：使用 QDE 模块对每个实例预测 PAA 中提出的高斯混合质量分布的三个参数来指导检测头的训练</li>\n<li><code>NanoDet Plus</code> ：使用 AGM（Assign Guidance Module）并配合动态的软标签分配策略 DSLA（ynamic Soft Label Assigner）来解决轻量级模型中的最优标签匹配问题</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h6><span id=\"3gt-label-assignment\"> 3&gt; </span></h6>\n<ul>\n<li>\n<p>主要是指检测算法在训练阶段，如何给特征图上的每个位置进行合适的学习目标的表示，以及如何进行正负样本的分配的。</p>\n<table>\n<thead>\n<tr>\n<th>算法类型</th>\n<th>先验</th>\n<th>学习目标的表示</th>\n<th>正负样本的分配</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>anchor box</td>\n<td>anchor box</td>\n<td>bounding box</td>\n<td>IoU</td>\n</tr>\n<tr>\n<td>anchor point</td>\n<td>center</td>\n<td>高斯等</td>\n<td>高斯热图等</td>\n</tr>\n<tr>\n<td>key point</td>\n<td>point</td>\n<td>representative points</td>\n<td>feature map bin 和 IoU 等</td>\n</tr>\n<tr>\n<td>set prediction</td>\n<td>embedding</td>\n<td>bounding box</td>\n<td>Hungarian 算法</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>\n<p><strong>（1）ATSS</strong></p>\n<p>论文标题：</p>\n<p>Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</p>\n<p><a href=\"https://arxiv.org/abs/1912.02424\">论文链接</a> | <a href=\"https://github.com/sfzhang15/ATSS\">代码链接</a></p>\n<p>这篇文章从 anchor-free 和 anchor-base 算法的本质区别出发，通过分析对比 anchor-base 经典算法 retinanet 和 anchor-free 经典算法 FCOS 来说明正负样本分配（label assignment）的重要性。</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9Qc2hvOWRtN29ESGRvYmljdXE3QWJRaWMwWjJ1NWlhWkdpYUxpYlJFN2N4Y0FpYXZpYkpDOVQwZ01OTmlhRFhCVWFiSkpDaDNWQmZiZHliTWJUMWhGMzZpYThUcHZpYlEvNjQw?x-oss-process=image/format,png\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>如上图所示，RetinaNet 使用 IoU 阈值来区分正负 anchor box，处于中间 anchor box 的全部忽略。FCOS 使用空间（spatial）和尺寸（scale）限制来区分正负 anchor point，正样本首先选择在 GT box 内的 anchor points，其次选择 GT 尺寸对应的层 anchor points，其余均为负样本。</p>\n<p>最后通过交叉实验，发现在相同正负样本定义下情况下，RetinaNet 和 FCOS 性能几乎一样，而且 spatila and scale constraint 的方式比 IOU 的效果好，如下表：</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9HVDFDVE9wVHdJSUJsRnNyOVZPN1ZTbFpOdlNEQjMyRFRKWmxrRzYxaWFZYUtCeEh0eUtVaGxyZ2hZTXR0UlBhSTJIMlNOZ05xa21DbXFDM1JYcnExeWcvNjQw?x-oss-process=image/format,png\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>因此 ATSS 提出了一种新的正负样本选取方式，这种方法几乎不会引入额外的超参数并且更加鲁棒。</p>\n<p>主要就是在每个 FPN 层选取离 gt 框中心点最近的 k 个 anchor，之后对所有选取的 anchor 与 gt 计算 IOU，同时计算 IOU 均值和方差，最后保留 IOU 大于均值加方差的并且中心点在 gt 之内的 anchor 作为正样本。</p>\n<p>根据下表可以发现，即使 anchor box 数量为 1 的 RetinaNet 和 FOCS 在都加上 ATSS 策略之后，效果都有明显的提升，这也证明了 ATSS 策略的有效性。</p>\n</li>\n</ul>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9HVDFDVE9wVHdJSUJsRnNyOVZPN1ZTbFpOdlNEQjMyRFRKWmxrRzYxaWFZYUtCeEh0eUtVaGxyZ2hZTXR0UlBhSTJIMlNOZ05xa21DbXFDM1JYcnExeWcvNjQw?x-oss-process=image/format,png\" alt=\"img\" style=\"zoom:80%;\"></center>\n<ul>\n<li>\n<p><strong>（2）SAPD</strong></p>\n<p>论文标题：</p>\n<p>Soft Anchor-Point Object Detection</p>\n<p><a href=\"https://arxiv.org/abs/1911.12448\">论文链接</a> | <a href=\"https://github.com/xuannianz/SAPD\">代码链接</a></p>\n<p>SAPD 就是对 anchor-free 检测器中的 anchor-point 检测器进行了训练策略的改进。SAPD 分析了两个问题：注意力偏差（attention bias）和特征选择（feature selection）。其中，特征选择的问题对金字塔特征层级做软选择，这里就不深入了。而为了解决注意力偏差（attention bias），SAPD 使用了一个新颖的训练策略：Soft-weighted anchor points。</p>\n<p>3.1 Attention bias 注意力偏差</p>\n<p>在自然图像中，可能会出现遮挡、背景混乱等干扰，SAPD 发现原始的 anchor-point 检测器在处理这些具有挑战性的场景时存在注意力偏差的问题，即具有清晰明亮 views 的目标会生成过高的得分区域，从而抑制了周围的其他目标的得分区域。</p>\n<p>这个问题是由于特征不对齐导致了靠近目标边界的位置会得到不必要的高分 所导致的。</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9ON0hTTU96RXJaaWIyckM1TVZIQ2FicWo0aWNzZDhWWHBIRWRGZ2ljTE1lTFljaWNidWliT0NtSjZJeFlmRHBWVzZya1RVQVlKRE9YUjkzRUpiaWIwaWNpY1AwaWFVZy82NDA?x-oss-process=image/format,png\" alt=\"img\" style=\"zoom:90%;\"></center>\n<p>3.2 Soft-weighted anchor points</p>\n<p>将目标实际位置与 anchor point（也就是 center）的距离作为一个 anchor 的惩罚权重，加入到损失函数的计算中（仅针对正样本，负样本不做改动）。公式如下：</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9qbHRrYzdCNmo5WW9QY3RYMWdYZHZuRHdsRTlocW5zTmlicFNQb2ljeXV4aWFndnREb0owaWJBYXdLSjkxd05pYkJZd3M2NGdWWElIRE42V0w1V0tWOUxjeU53LzY0MA?x-oss-process=image/format,png\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>其中，η 控制递减幅度，权重  范围为 0~1，公式保证了目标边界处的 points 权重为 0，目标中心处的 ponit 权重为 1。</p>\n<p>这种通过对 anchor points 做软加权，就是 label assign 的进行优化，减少对靠近边界包含大量背景信息的锚点的关注。</p>\n</li>\n<li>\n<p><strong>（3）AutoAssign</strong></p>\n</li>\n</ul>\n<p>论文标题：</p>\n<p>AutoAssign: Differentiable Label Assignment for Dense Object Detection</p>\n<p><a href=\"https://arxiv.org/abs/2007.03496\">论文链接</a></p>\n<p>AutoAssign 对 label assignment 进行非常全面的讨论。主要解决了在给定一个 bounding box （x, y, w, h） 后，根据框内的物体形状，动态分配正负样本的问题。如下图所示：</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9RaWNoRGlhYmRReXNWaWJlaWNYUGlhbzBzNWxyYUNCbFlHT1kyQ3NsR25xMUJSVjFEeDlFTFBheGVpYUx2T3JpY0QzeFIwbFFjcGlibDZPZVB3NEo0RXlaQkpQTFRBLzY0MA?x-oss-process=image/format,png\" alt=\"img\" style=\"zoom:90%;\"></center>\n<p>（1）RetinaNet 是根据 anchor box 和 ground truth 的 IOU 阈值定义正负样本，这样会每个样本都是打上非正即负以及 ignore 的标签，而且 anchor box 的  num，size，aspect ratios 等等都是超参数；</p>\n<p>（2）FCOS 通过 centerness、空间和尺度约束来分配正负样本，也引入了很多超参数；</p>\n<p>（3）AutoAssign 将 label assignment 看做一种连续问题，没有真正意义上的正负样本之分，每个特征图上的位置都有正样本属性和负样本属性，只是权重不同罢了；而且如上图最左变所示，动态分配正负样本更符合目标的形状，可以说有利用分割做检测的思想。</p>\n<p>下面是 AutoAssign 的正负样本分配的示意图：</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9Qc2hvOWRtN29ESGRvYmljdXE3QWJRaWMwWjJ1NWlhWkdpYUwzcU1xVzU5YUlyU2QwZnVFMW5ZdE4zZThnVThZRWdOZHNaeUJkUVBBNHhPQm1pYXRUWmN1ekh3LzY0MA?x-oss-process=image/format,png\" alt=\"img\" style=\"zoom:90%;\"></center>\n<p>可以看到，比一般的检测算法多了一个 Implict Objectness 分支，用于背景与前景的判断，已解决引入的大量背景位置的问题。</p>\n<p>（1）Center Weighting</p>\n<p>先使用高斯中心先验确定图像中一个目标正负样本的权重：</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9MVGJCQjVDcDVzdVdkVWE5OTFuUUwwMGljaWNFaWI0aG5GU1VXcVU4UnRnV3RpY29IN2w5dXE3WmlhaWNBZ1N5dmljU0RWYzFjaWJwZ0IzTzRXRUN6dWs1V3g3VXF3LzY0MA?x-oss-process=image/format,png\" alt=\"img\"></center>\n<p>（2）Confidence Weighting</p>\n<p>通过 ImpObj 分支来避免引入大量背景位置</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8zWldlZ1BDSFpjSXRTVkpNazJ3aWEzalhpY1FoM3RpYWN1ZXFjM2ljZjlpYVpTeWhvc0x4YlVBZEJXenpYNERpYzVhV1Axckh5VmlhUll1cmF6NUxPZWxkd3BHcmcvNjQw?x-oss-process=image/format,png\" alt=\"img\"></center>\n<p>与 FreeAnchor 相似，将分类和定位联合看成极大似然估计问题，学习出样本的置信度 Confidence Weighting，即下面的 C (Pi)：</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9PUGd3ZUdxaWNDdmdpYk4waWJYVmliZnE2S1ZQdWRieExGcllXRndpYWliaWFpYVZUbloyTUl4MXdHTU5NaWF3c0k2S2dHd2JvSW92NkV6d3psWHV5ZE5pYTRmMkVFV3cvNjQw?x-oss-process=image/format,png\" alt=\"img\"></center>\n<p>直观的理解 C (Pi) 就是，分类得分高、框预测的准的 location 拥有较大的 C (Pi) 值的概率就会高。</p>\n<p>（3）正负样本的权重（w+/w-）</p>\n<p>positive weights：通过 Center Weighting 和 Confidence Weighting 得到 Positive weights</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9aSnJ3VGNBZXp3QWhPMDg3MWtkOWljOW1pYktzbnJaUUE0d1gxVEFYVUVxNzIyYlFUVU5OTm5EZDFpYm40empxcmJXRjdFc3Z1MG03SkNsRzVuTnFyd2pZUS82NDA?x-oss-process=image/format,png\" alt=\"img\"></center>\n<p>negative weights：通过最大 IOU 得到 Negative weights</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy85dUROQUpEMzd5Q0F4VXpOcHRKMTdCVXVpY0hpYmZ1Q1c0SW5zMnVNbWFiNk9WWk9zdERpYjQzeWliYlFCRThvdGVGRUo1SXR3dzZZbjR0d3V6Z0hTcVI0RkEvNjQw?x-oss-process=image/format,png\" alt=\"img\"></center>\n<p>对于前景和背景的 weighting function，有一个共同的特点是 “单调递增”；这就保证了一个位置预测 pos /neg 的置信度越高，那么对应的权重就越大。</p>\n<p>（4）loss function</p>\n<p>有了对于正负样本的权重之后，对于一个 gt box，其 loss 如下：</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy80eGNFdktvRGg5aWJpY3RnUjVWdjNvS3NPTE1LUER4azRremJld0RqNTZ2YUNyUHlaMEtzaWJKWW5LdjFJaWF3MzR6UjA0dzlpYkEwM1RiTnk0dGZpY2JpYnMzaWFnLzY0MA?x-oss-process=image/format,png\" alt=\"img\" style=\"zoom:80%;\"></center>\n<p>Positive weights 和 Negative weights 在训练过程中动态调整达到平衡，像是在学该目标的形状。</p>\n<ul>\n<li>\n<p><strong>（4）DETR</strong></p>\n<p>论文标题：</p>\n<p>End-to-End Object Detection with Transformers</p>\n<p><a href=\"https://arxiv.org/abs/2005.12872\">论文链接</a> | <a href=\"https://github.com/facebookresearch/detr\">代码链接</a></p>\n<p>4.1 Object detection set prediction</p>\n<p>DETR 将目标检测任务视为一个图像到集合（image-to-set）的问题，即给定一张图像，模型的预测结果是一个包含了所有目标的无序集合。</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy8xd1duMk5tM0RVMG1wem5jaWEwclpQdUZxbTliU2JBaWJQZTFpYmlhUm92UUt6SXU1WlVvVHJnVGRNYnpuOVRWcE9VUm91V2ppY2sxNmtZRFZlTG5tNG5hS3BnLzY0MA?x-oss-process=image/format,png\" alt=\"img\"></center>\n<p>那么对于一个目标 ground truth，如何找到对应的 prediction 呢？Detr 用的是 Hungarian algorithm 实现预测值与真值实现最大的匹配，并且是一一对应。</p>\n<p>假设有 4 个 prediction（a,b,c,d），有 4 个 ground truth（p,q,r,s），每个 prediction 匹配 ground truth 的好坏都不同，那么便可构造一个代价矩阵（cost matrix，是 cost_bbox、cost_class 和 cost_giou 的加权和），通过求解最优的分配后，得到的每个 prediction 对应 ground truth 最佳分配的结果。</p>\n<p>4.2 object queries</p>\n<p>传统的 Anchor 是人工设计，铺在特征图上。最初人们给 Anchor 加上 scales 和 aspect ration，后来还有加上了 dense，再到后来，也出现了可学习的 Guided Anchoring，把 anchor 拆解为：位置预测和形状预测。</p>\n<p>这种方式的 anchor 有个缺陷是：在推理阶段会产生大量的框，需要 NMS 进行抑制，这说明人工设计的 anchor 是存在冗余的（多个 anchor 匹配到一个 gt 上）。</p>\n<p>而 DETR 的 object queries 就是一个 embedding 形式的 learned anchor，目的是让网络自己根据数据集自己学习 anchor。并且 DETR 的实验结果也证明 embedding 已经足够学习 anchor 了。</p>\n<p>Detr 也在 coco 2017 val 上对把每个 object query 预测的框做了可视化，如下，选取 N=100 中的 20 个 object query，可以看到不同的 query vector 具有不同的分布（有些注重左下角，有些注重中间…），可以想成：有 N 个不同的人用不同的角度进行观测。</p>\n<center><img src=\"https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9hSEtKUFdhNUhvUlo3eHBqbnhHaWIzcjJqclhQV3gwQkJvNDVhcDN5V25OQ2VOdUh0aG1pYzlYOVI5dmx3MUZpYmliWkFEV0cxbHVEV1JndzBET2hycDZaV3cvNjQw?x-oss-process=image/format,png\" alt=\"img\"></center>\n</li>\n</ul>\n<h6><span id=\"4gt-generalized-focal-loss\"> 4&gt; </span></h6>\n<p>论文地址：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2006.04388.pdf\">https://arxiv.org/pdf/2006.04388.pdf</a></p>\n<p>源码和预训练模型地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/implus/GFocal\">https://github.com/implus/GFocal</a></p>\n<p>MMDetection 官方收录地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmdetection/blob/master/configs/gfl/README.md\">https://github.com/open-mmlab/mmdetection/blob/master/configs/gfl/README.md</a></p>\n<p><strong>总结</strong>：基于任意 one-stage 检测器上，调整框本身与框质量估计的表示，同时用泛化版本的 GFocal Loss 训练该改进的表示，无 cost 涨点（一般 1 个点出头）AP。</p>\n",
            "tags": [
                "人工智能",
                "深度模型（目标检测）"
            ]
        }
    ]
}