{
    "version": "https://jsonfeed.org/version/1",
    "title": "且听风吟，御剑于心！ • All posts by \"nlp\" tag",
    "description": "",
    "home_page_url": "https://leezhao415.github.io",
    "items": [
        {
            "id": "https://leezhao415.github.io/2021/06/24/NLP%E4%B9%8BTransformer%E8%AF%A6%E8%A7%A3/",
            "url": "https://leezhao415.github.io/2021/06/24/NLP%E4%B9%8BTransformer%E8%AF%A6%E8%A7%A3/",
            "title": "NLP之Transformer详解",
            "date_published": "2021-06-24T15:23:24.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<h1><span id=\"transformer-详解\"> Transformer 详解</span></h1>\n<p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762\">Attention is all you need</a> 是一篇将 Attention 思想发挥到极致的论文，出自 Google。这篇论文中提出一个全新的模型，叫 Transformer，抛弃了以往深度学习任务里面使用到的 CNN 和 RNN (其实也不完全是，还是用到了一维卷积)。这个模型广泛应用于 NLP 领域，例如机器翻译，问答系统，文本摘要和语音识别等等方向。</p>\n<p>参考资料：</p>\n<ul>\n<li><a href=\"https://link.zhihu.com/?target=https%3A//juejin.im/post/5b9f1af0e51d450e425eb32d%23comment\">Transformer 模型的 PyTorch 实现</a></li>\n<li><a href=\"https://link.zhihu.com/?target=https%3A//kexue.fm/archives/4765\">《Attention is All You Need》浅读（简介 + 代码）</a></li>\n<li><a href=\"https://link.zhihu.com/?target=https%3A//cloud.tencent.com/developer/article/1143127\">深度学习中的注意力机制 - 云 + 社区 - 腾讯云</a></li>\n<li><a href=\"https://link.zhihu.com/?target=https%3A//lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\">Attention? Attention!</a></li>\n<li><a href=\"https://link.zhihu.com/?target=https%3A//medium.com/%40kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-ii-c85bf8fd145\">Building the Mighty Transformer for Sequence Tagging in PyTorch</a></li>\n</ul>\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#1-transformer-%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6\">1 Transformer 整体框架</a>\n<ul>\n<li><a href=\"#11-encoder\">1.1 Encoder</a></li>\n<li><a href=\"#12-decoder\">1.2 Decoder</a></li>\n<li><a href=\"#13-attention\">1.3 Attention</a></li>\n<li><a href=\"#14-self-attention\">1.4 Self-Attention</a></li>\n<li><a href=\"#15-context-attention\">1.5 Context-Attention</a></li>\n<li><a href=\"#16-scaled-dot-product-attention\">1.6 Scaled Dot-Product Attention</a></li>\n<li><a href=\"#17-scaled-dot-product-attention-%E5%AE%9E%E7%8E%B0\">1.7 Scaled Dot-Product Attention 实现</a></li>\n<li><a href=\"#18-multi-head-attention\">1.8 Multi-head attention</a></li>\n<li><a href=\"#19-multi-head-attention-%E5%AE%9E%E7%8E%B0\">1.9 Multi-head attention 实现</a></li>\n<li><a href=\"#110-layer-normalization\">1.10 Layer normalization</a></li>\n<li><a href=\"#111-mask\">1.11 Mask</a></li>\n<li><a href=\"#112-positional-embedding\">1.12 Positional Embedding</a></li>\n<li><a href=\"#113-position-wise-feed-forward-network\">1.13 Position-wise Feed-Forward network</a></li>\n</ul>\n</li>\n<li><a href=\"#2-transformer%E7%9A%84%E5%AE%9E%E7%8E%B0\">2 Transformer 的实现</a>\n<ul>\n<li><a href=\"#21-encoder-%E7%AB%AF\">2.1 Encoder 端</a></li>\n<li><a href=\"#22-decoder-%E7%AB%AF\">2.2 Decoder 端</a></li>\n<li><a href=\"#23-transformer-%E6%A8%A1%E5%9E%8B\">2.3 Transformer 模型</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h2><span id=\"1-transformer-整体框架\"> 1 Transformer 整体框架</span></h2>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624170833621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<p>和经典的 seq2seq 模型一样，Transformer 模型中也采用了 encoer-decoder 架构。上图的左半边用 <strong>NX</strong> 框出来的，就代表一层 encoder，其中论文里面的 encoder 一共有 6 层这样的结构。上图的右半边用 <strong>NX</strong> 框出来的，则代表一层 decoder，同样也有 6 层。</p>\n<p>定义输入序列首先经过 word embedding，再和 positional encoding 相加后，输入到 encoder 中。输出序列经过的处理和输入序列一样，然后输入到 decoder。</p>\n<p>最后，decoder 的输出经过一个线性层，再接 Softmax。</p>\n<p>于上便是 Transformer 的整体框架，下面先来介绍 encoder 和 decoder。</p>\n<h3><span id=\"11-encoder\"> 1.1 Encoder</span></h3>\n<p>encoder 由 6 层相同的层组成，每一层分别由两部分组成：</p>\n<ul>\n<li>第一部分是 multi-head self-attention</li>\n<li>第二部分是 position-wise feed-forward network，是一个全连接层</li>\n</ul>\n<p>两个部分，都有一个残差连接 (residual connection)，然后接着一个 Layer Normalization。</p>\n<h3><span id=\"12-decoder\"> 1.2 Decoder</span></h3>\n<p>和 encoder 类似，decoder 也是由 6 个相同的层组成，每一个层包括以下 3 个部分:</p>\n<ul>\n<li>第一个部分是 multi-head self-attention mechanism</li>\n<li>第二部分是 multi-head context-attention mechanism</li>\n<li>第三部分是一个 position-wise feed-forward network</li>\n</ul>\n<p>和 encoder 一样，上面三个部分的每一个部分，都有一个残差连接，后接一个 <strong>Layer Normalization</strong>。</p>\n<p>decoder 和 encoder 不同的地方在 multi-head context-attention mechanism</p>\n<h3><span id=\"13-attention\"> 1.3 Attention</span></h3>\n<p>我在以前的文章中讲过，Attention 如果用一句话来描述，那就是 encoder 层的输出经过加权平均后再输入到 decoder 层中。它主要应用在 seq2seq 模型中，这个加权可以用矩阵来表示，也叫 Attention 矩阵。它表示对于某个时刻的输出 y，它在输入 x 上各个部分的注意力。这个注意力就是我们刚才说到的加权。</p>\n<p>Attention 又分为很多种，其中两种比较典型的有加性 Attention 和乘性 Attention。加性 Attention 对于输入的隐状态 直接做 concat 操作，乘性 Attention 则是对输入和输出做 dot 操作。</p>\n<p>在 Google 这篇论文中，使用的 Attention 模型是乘性 Attention。</p>\n<p>我在之前讲 <a href=\"https://zhuanlan.zhihu.com/p/47580077\">ESIM</a> 模型的文章里面写过一个 soft-align-attention，大家可以参考体会一下。</p>\n<h3><span id=\"14-self-attention\"> 1.4 Self-Attention</span></h3>\n<p>上面我们说 attention 机制的时候，都会说到两个隐状态 hi 和 St，前者是输入序列第 i 个位置产生的隐状态，后者是输出序列在第 t 个位置产生的隐状态。所谓 self-attention 实际上就是，输出序列就是输入序列。因而自己计算自己的 attention 得分。</p>\n<h3><span id=\"15-context-attention\"> 1.5 Context-Attention</span></h3>\n<p>context-attention 是 encoder 和 decoder 之间的 attention，是两个不同序列之间的 attention，与来源于自身的 self-attention 相区别。</p>\n<p>不管是哪种 attention，我们在计算 attention 权重的时候，可以选择很多方式，常用的方法有</p>\n<ul>\n<li>additive attention</li>\n<li>local-base</li>\n<li>general</li>\n<li>dot-product</li>\n<li>scaled dot-product</li>\n</ul>\n<p>Transformer 模型采用的是最后一种：scaled dot-product attention。</p>\n<h3><span id=\"16-scaled-dot-product-attention\"> 1.6 Scaled Dot-Product Attention</span></h3>\n<p>那么什么是 scaled dot-product attention 呢？</p>\n<p>Google 在论文中对 Attention 机制这么来描述：</p>\n<blockquote>\n<p>An attention function can be described as a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility of the query with the corresponding key.</p>\n</blockquote>\n<p>通过 query 和 key 的相似性程度来确定 value 的权重分布。论文中的公式长下面这个样子：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171329256.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>看到 Q，K，V 会不会有点晕，没事，后面会解释。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20210624171405951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>scaled dot-product attention 的结构图如下所示。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171433764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 60%;\"></center>\n<p>现在来说下 K、Q、V 分别代表什么：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20210624171456964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>目前可能描述有有点抽象，不容易理解。结合一些应用来说，比如，如果是在自动问答任务中的话，Q 可以代表答案的词向量序列，取 K = V 为问题的词向量序列，那么输出就是所谓的 Aligned Question Embedding。</p>\n<p>Google 论文的主要贡献之一是它表明了内部注意力在机器翻译 (甚至是一般的 Seq2Seq 任务）的序列编码上是相当重要的，而之前关于 Seq2Seq 的研究基本都只是把注意力机制用在解码端。</p>\n<h3><span id=\"17-scaled-dot-product-attention-实现\"> 1.7 Scaled Dot-Product Attention 实现</span></h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ScaledDotProductAttention</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;Scaled dot-product attention mechanism.&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, attention_dropout=<span class=\"number\">0.0</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class=\"line\">        self.dropout = nn.Dropout(attention_dropout)</span><br><span class=\"line\">        self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, q, k, v, scale=<span class=\"literal\">None</span>, attn_mask=<span class=\"literal\">None</span></span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        前向传播.</span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">        \tq: Queries张量，形状为[B, L_q, D_q]</span></span><br><span class=\"line\"><span class=\"string\">        \tk: Keys张量，形状为[B, L_k, D_k]</span></span><br><span class=\"line\"><span class=\"string\">        \tv: Values张量，形状为[B, L_v, D_v]，一般来说就是k</span></span><br><span class=\"line\"><span class=\"string\">        \tscale: 缩放因子，一个浮点标量</span></span><br><span class=\"line\"><span class=\"string\">        \tattn_mask: Masking张量，形状为[B, L_q, L_k]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">        \t上下文张量和attention张量</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        attention = torch.bmm(q, k.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> scale:</span><br><span class=\"line\">            attention = attention * scale</span><br><span class=\"line\">        <span class=\"keyword\">if</span> attn_mask:</span><br><span class=\"line\">            <span class=\"comment\"># 给需要 mask 的地方设置一个负无穷</span></span><br><span class=\"line\">            attention = attention.masked_fill_(attn_mask, -np.inf)</span><br><span class=\"line\">\t<span class=\"comment\"># 计算softmax</span></span><br><span class=\"line\">        attention = self.softmax(attention)</span><br><span class=\"line\">\t<span class=\"comment\"># 添加dropout</span></span><br><span class=\"line\">        attention = self.dropout(attention)</span><br><span class=\"line\">\t<span class=\"comment\"># 和V做点积</span></span><br><span class=\"line\">        context = torch.bmm(attention, v)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> context, attention</span><br></pre></td></tr></table></figure>\n<h3><span id=\"18-multi-head-attention\"> 1.8 Multi-head attention</span></h3>\n<p>理解了 Scaled dot-product attention，Multi-head attention 也很容易理解啦。论文提到，他们发现将 Q、K、V 通过一个线性映射之后，分成 h 份，对每一份进行 scaled dot-product attention 效果更好。然后，把各个部分的结果合并起来，再次经过线性映射，得到最终的输出。这就是所谓的 multi-head attention。上面的超参数 h 就是 heads 的数量。论文默认是 8。</p>\n<p>multi-head attention 的结构图如下所示。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171522631.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:60%;\"></center>\n<p><img src=\"https://img-blog.csdnimg.cn/202106241715563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3><span id=\"19-multi-head-attention-实现\"> 1.9 Multi-head attention 实现</span></h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MultiHeadAttention</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, model_dim=<span class=\"number\">512</span>, num_heads=<span class=\"number\">8</span>, dropout=<span class=\"number\">0.0</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(MultiHeadAttention, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        self.dim_per_head = model_dim // num_heads</span><br><span class=\"line\">        self.num_heads = num_heads</span><br><span class=\"line\">        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)</span><br><span class=\"line\">        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)</span><br><span class=\"line\">        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)</span><br><span class=\"line\"></span><br><span class=\"line\">        self.dot_product_attention = ScaledDotProductAttention(dropout)</span><br><span class=\"line\">        self.linear_final = nn.Linear(model_dim, model_dim)</span><br><span class=\"line\">        self.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">\t</span><br><span class=\"line\">        <span class=\"comment\"># multi-head attention之后需要做layer norm</span></span><br><span class=\"line\">        self.layer_norm = nn.LayerNorm(model_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, key, value, query, attn_mask=<span class=\"literal\">None</span></span>):</span></span><br><span class=\"line\">\t<span class=\"comment\"># 残差连接</span></span><br><span class=\"line\">        residual = query</span><br><span class=\"line\">        dim_per_head = self.dim_per_head</span><br><span class=\"line\">        num_heads = self.num_heads</span><br><span class=\"line\">        batch_size = key.size(<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># linear projection</span></span><br><span class=\"line\">        key = self.linear_k(key)</span><br><span class=\"line\">        value = self.linear_v(value)</span><br><span class=\"line\">        query = self.linear_q(query)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># split by heads</span></span><br><span class=\"line\">        key = key.view(batch_size * num_heads, -<span class=\"number\">1</span>, dim_per_head)</span><br><span class=\"line\">        value = value.view(batch_size * num_heads, -<span class=\"number\">1</span>, dim_per_head)</span><br><span class=\"line\">        query = query.view(batch_size * num_heads, -<span class=\"number\">1</span>, dim_per_head)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> attn_mask:</span><br><span class=\"line\">            attn_mask = attn_mask.repeat(num_heads, <span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># scaled dot product attention</span></span><br><span class=\"line\">        scale = (key.size(-<span class=\"number\">1</span>)) ** -<span class=\"number\">0.5</span></span><br><span class=\"line\">        context, attention = self.dot_product_attention(</span><br><span class=\"line\">          query, key, value, scale, attn_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># concat heads</span></span><br><span class=\"line\">        context = context.view(batch_size, -<span class=\"number\">1</span>, dim_per_head * num_heads)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># final linear projection</span></span><br><span class=\"line\">        output = self.linear_final(context)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># dropout</span></span><br><span class=\"line\">        output = self.dropout(output)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># add residual and norm layer</span></span><br><span class=\"line\">        output = self.layer_norm(residual + output)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attention</span><br></pre></td></tr></table></figure>\n<p>上面代码中出现的 <a href=\"https://zhuanlan.zhihu.com/p/47846504\">Residual connection</a> 我在之前一篇文章中讲过，这里不再赘述，只解释 Layer normalization。</p>\n<h3><span id=\"110-layer-normalization\"> 1.10 Layer normalization</span></h3>\n<blockquote>\n<p>Normalization 有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为 0 方差为 1 的数据。我们在把数据送入激活函数之前进行 normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。</p>\n</blockquote>\n<p>说到 normalization，那就肯定得提到 Batch Normalization。</p>\n<p>BN 的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。</p>\n<p>BN 的具体做法就是对每一小批数据，在批这个方向上做归一化。如下图所示：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171627429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<p>可以看到，右半边求均值是<strong>沿着数据 batch N 的方向进行的</strong>！</p>\n<p>Batch normalization 的计算公式如下：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171645479.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>那么什么是 Layer normalization 呢？它也是归一化数据的一种方式，不过 LN 是<strong>在每一个样本上计算均值和方差，而不是 BN 那种在批方向计算均值和方差</strong>！</p>\n<p>下面是 LN 的示意图：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171702600.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<p>和上面的 BN 示意图一比较就可以看出二者的区别啦！</p>\n<p>下面看一下 LN 的公式：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171721496.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<h3><span id=\"111-mask\"> 1.11 Mask</span></h3>\n<p>mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。</p>\n<p>其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。</p>\n<p><strong>Padding Mask</strong></p>\n<p>什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。因为这些填充的位置，其实是没什么意义的，所以我们的 attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p>\n<p>具体的做法是，把这些位置的值加上一个非常大的负数 (负无穷)，这样的话，经过 softmax，这些位置的概率就会接近 0！</p>\n<p>而我们的 padding mask 实际上是一个张量，每个值都是一个 Boolean，值为 false 的地方就是我们要进行处理的地方。</p>\n<p>实现：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">padding_mask</span>(<span class=\"params\">seq_k, seq_q</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># seq_k 和 seq_q 的形状都是 [B,L]</span></span><br><span class=\"line\">    len_q = seq_q.size(<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># `PAD` is 0</span></span><br><span class=\"line\">    pad_mask = seq_k.eq(<span class=\"number\">0</span>)</span><br><span class=\"line\">    pad_mask = pad_mask.unsqueeze(<span class=\"number\">1</span>).expand(-<span class=\"number\">1</span>, len_q, -<span class=\"number\">1</span>)  <span class=\"comment\"># shape [B, L_q, L_k]</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> pad_mask</span><br></pre></td></tr></table></figure>\n<p><strong>Sequence mask</strong></p>\n<p>文章前面也提到，sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p>\n<p>那么具体怎么做呢？也很简单：<strong>产生一个上三角矩阵，上三角的值全为 1，下三角的值权威 0，对角线也是 0</strong>。把这个矩阵作用在每一个序列上，就可以达到我们的目的啦。</p>\n<p>具体的代码实现如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def sequence_mask(seq):</span><br><span class=\"line\">    batch_size, seq_len &#x3D; seq.size()</span><br><span class=\"line\">    mask &#x3D; torch.triu(torch.ones((seq_len, seq_len), dtype&#x3D;torch.uint8),</span><br><span class=\"line\">                    diagonal&#x3D;1)</span><br><span class=\"line\">    mask &#x3D; mask.unsqueeze(0).expand(batch_size, -1, -1)  # [B, L, L]</span><br><span class=\"line\">    return mask</span><br></pre></td></tr></table></figure>\n<p>效果如下，</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171739606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<ul>\n<li>对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要 padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个 mask 相加作为 attn_mask。</li>\n<li>其他情况，attn_mask 一律等于 padding mask。</li>\n</ul>\n<h3><span id=\"112-positional-embedding\"> 1.12 Positional Embedding</span></h3>\n<p>现在的 Transformer 架构还没有提取序列顺序的信息，这个信息对于序列而言非常重要，如果缺失了这个信息，可能我们的结果就是：所有词语都对了，但是无法组成有意义的语句。</p>\n<p>为了解决这个问题。论文使用了 Positional Embedding：对序列中的词语出现的位置进行编码。</p>\n<p>在实现的时候使用正余弦函数。公式如下：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171828791.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>其中，pos 是指词语在序列中的位置。可以看出，在<strong>偶数位置，使用正弦编码，在奇数位置，使用余弦编码</strong>。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/20210624171811744.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>上面的位置编码是<strong>绝对位置编码</strong>。但是词语的<strong>相对位置</strong>也非常重要。这就是论文为什么要使用三角函数的原因！</p>\n<p>正弦函数能够表达相对位置信息，主要数学依据是以下两个公式：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171859955.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:55%;\"></center>\n<p><img src=\"https://img-blog.csdnimg.cn/2021062417191513.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>具体实现如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PositionalEncoding</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, d_model, max_seq_len</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;初始化。</span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">            d_model: 一个标量。模型的维度，论文默认是512</span></span><br><span class=\"line\"><span class=\"string\">            max_seq_len: 一个标量。文本序列的最大长度</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(PositionalEncoding, self).__init__()</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 根据论文给的公式，构造出PE矩阵</span></span><br><span class=\"line\">        position_encoding = np.array([</span><br><span class=\"line\">          [pos / np.power(<span class=\"number\">10000</span>, <span class=\"number\">2.0</span> * (j // <span class=\"number\">2</span>) / d_model) <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(d_model)]</span><br><span class=\"line\">          <span class=\"keyword\">for</span> pos <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(max_seq_len)])</span><br><span class=\"line\">        <span class=\"comment\"># 偶数列使用sin，奇数列使用cos</span></span><br><span class=\"line\">        position_encoding[:, <span class=\"number\">0</span>::<span class=\"number\">2</span>] = np.sin(position_encoding[:, <span class=\"number\">0</span>::<span class=\"number\">2</span>])</span><br><span class=\"line\">        position_encoding[:, <span class=\"number\">1</span>::<span class=\"number\">2</span>] = np.cos(position_encoding[:, <span class=\"number\">1</span>::<span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding</span></span><br><span class=\"line\">        <span class=\"comment\"># 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似</span></span><br><span class=\"line\">        <span class=\"comment\"># 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐，</span></span><br><span class=\"line\">        <span class=\"comment\"># 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码</span></span><br><span class=\"line\">        pad_row = torch.zeros([<span class=\"number\">1</span>, d_model])</span><br><span class=\"line\">        position_encoding = torch.cat((pad_row, position_encoding))</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码，</span></span><br><span class=\"line\">        <span class=\"comment\"># Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似</span></span><br><span class=\"line\">        self.position_encoding = nn.Embedding(max_seq_len + <span class=\"number\">1</span>, d_model)</span><br><span class=\"line\">        self.position_encoding.weight = nn.Parameter(position_encoding,</span><br><span class=\"line\">                                                     requires_grad=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, input_len</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;神经网络的前向传播。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Args:</span></span><br><span class=\"line\"><span class=\"string\">          input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">        Returns:</span></span><br><span class=\"line\"><span class=\"string\">          返回这一批序列的位置编码，进行了对齐。</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 找出这一批序列的最大长度</span></span><br><span class=\"line\">        max_len = torch.<span class=\"built_in\">max</span>(input_len)</span><br><span class=\"line\">        tensor = torch.cuda.LongTensor <span class=\"keyword\">if</span> input_len.is_cuda <span class=\"keyword\">else</span> torch.LongTensor</span><br><span class=\"line\">        <span class=\"comment\"># 对每一个序列的位置进行对齐，在原序列位置的后面补上0</span></span><br><span class=\"line\">        <span class=\"comment\"># 这里range从1开始也是因为要避开PAD(0)的位置</span></span><br><span class=\"line\">        input_pos = tensor(</span><br><span class=\"line\">          [<span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"built_in\">len</span> + <span class=\"number\">1</span>)) + [<span class=\"number\">0</span>] * (max_len - <span class=\"built_in\">len</span>) <span class=\"keyword\">for</span> <span class=\"built_in\">len</span> <span class=\"keyword\">in</span> input_len])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.position_encoding(input_pos)</span><br></pre></td></tr></table></figure>\n<h3><span id=\"113-position-wise-feed-forward-network\"> 1.13 Position-wise Feed-Forward network</span></h3>\n<p>这是一个全连接网络，包含两个线性变换和一个非线性函数 (实际上就是 ReLU)。公式如下</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624171933883.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:50%;\"></center>\n<p>这个线性变换在不同的位置都表现地一样，并且在不同的层之间使用不同的参数。</p>\n<p><strong>这里实现上用到了两个一维卷积。</strong></p>\n<p>实现如下:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PositionalWiseFeedForward</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, model_dim=<span class=\"number\">512</span>, ffn_dim=<span class=\"number\">2048</span>, dropout=<span class=\"number\">0.0</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(PositionalWiseFeedForward, self).__init__()</span><br><span class=\"line\">        self.w1 = nn.Conv1d(model_dim, ffn_dim, <span class=\"number\">1</span>)</span><br><span class=\"line\">        self.w2 = nn.Conv1d(ffn_dim, model_dim, <span class=\"number\">1</span>)</span><br><span class=\"line\">        self.dropout = nn.Dropout(dropout)</span><br><span class=\"line\">        self.layer_norm = nn.LayerNorm(model_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        output = x.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</span><br><span class=\"line\">        output = self.w2(F.relu(self.w1(output)))</span><br><span class=\"line\">        output = self.dropout(output.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># add residual and norm layer</span></span><br><span class=\"line\">        output = self.layer_norm(x + output)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></figure>\n<h2><span id=\"2-transformer-的实现\"> 2 Transformer 的实现</span></h2>\n<p>现在可以开始完成 Transformer 模型的构建了，encoder 端和 decoder 端分别都有 6 层，实现如下，首先是</p>\n<h3><span id=\"21-encoder-端\"> 2.1 Encoder 端</span></h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">EncoderLayer</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">\t<span class=\"string\">&quot;&quot;&quot;Encoder的一层。&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, model_dim=<span class=\"number\">512</span>, num_heads=<span class=\"number\">8</span>, ffn_dim=<span class=\"number\">2048</span>, dropout=<span class=\"number\">0.0</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(EncoderLayer, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        self.attention = MultiHeadAttention(model_dim, num_heads, dropout)</span><br><span class=\"line\">        self.feed_forward = PositionalWiseFeedForward(model_dim, ffn_dim, dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, inputs, attn_mask=<span class=\"literal\">None</span></span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># self attention</span></span><br><span class=\"line\">        context, attention = self.attention(inputs, inputs, inputs, padding_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># feed forward network</span></span><br><span class=\"line\">        output = self.feed_forward(context)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attention</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Encoder</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">\t<span class=\"string\">&quot;&quot;&quot;多层EncoderLayer组成Encoder。&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               vocab_size,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               max_seq_len,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               num_layers=<span class=\"number\">6</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               model_dim=<span class=\"number\">512</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               num_heads=<span class=\"number\">8</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               ffn_dim=<span class=\"number\">2048</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               dropout=<span class=\"number\">0.0</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Encoder, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        self.encoder_layers = nn.ModuleList(</span><br><span class=\"line\">          [EncoderLayer(model_dim, num_heads, ffn_dim, dropout) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span></span><br><span class=\"line\">           <span class=\"built_in\">range</span>(num_layers)])</span><br><span class=\"line\"></span><br><span class=\"line\">        self.seq_embedding = nn.Embedding(vocab_size + <span class=\"number\">1</span>, model_dim, padding_idx=<span class=\"number\">0</span>)</span><br><span class=\"line\">        self.pos_embedding = PositionalEncoding(model_dim, max_seq_len)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, inputs, inputs_len</span>):</span></span><br><span class=\"line\">        output = self.seq_embedding(inputs)</span><br><span class=\"line\">        output += self.pos_embedding(inputs_len)</span><br><span class=\"line\"></span><br><span class=\"line\">        self_attention_mask = padding_mask(inputs, inputs)</span><br><span class=\"line\"></span><br><span class=\"line\">        attentions = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> encoder <span class=\"keyword\">in</span> self.encoder_layers:</span><br><span class=\"line\">            output, attention = encoder(output, self_attention_mask)</span><br><span class=\"line\">            attentions.append(attention)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, attentions</span><br></pre></td></tr></table></figure>\n<h3><span id=\"22-decoder-端\"> 2.2 Decoder 端</span></h3>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DecoderLayer</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, model_dim, num_heads=<span class=\"number\">8</span>, ffn_dim=<span class=\"number\">2048</span>, dropout=<span class=\"number\">0.0</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(DecoderLayer, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        self.attention = MultiHeadAttention(model_dim, num_heads, dropout)</span><br><span class=\"line\">        self.feed_forward = PositionalWiseFeedForward(model_dim, ffn_dim, dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">              dec_inputs,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">              enc_outputs,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">              self_attn_mask=<span class=\"literal\">None</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">              context_attn_mask=<span class=\"literal\">None</span></span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># self attention, all inputs are decoder inputs</span></span><br><span class=\"line\">        dec_output, self_attention = self.attention(</span><br><span class=\"line\">          dec_inputs, dec_inputs, dec_inputs, self_attn_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># context attention</span></span><br><span class=\"line\">        <span class=\"comment\"># query is decoder&#x27;s outputs, key and value are encoder&#x27;s inputs</span></span><br><span class=\"line\">        dec_output, context_attention = self.attention(</span><br><span class=\"line\">          enc_outputs, enc_outputs, dec_output, context_attn_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># decoder&#x27;s output, or context</span></span><br><span class=\"line\">        dec_output = self.feed_forward(dec_output)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> dec_output, self_attention, context_attention</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Decoder</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               vocab_size,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               max_seq_len,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               num_layers=<span class=\"number\">6</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               model_dim=<span class=\"number\">512</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               num_heads=<span class=\"number\">8</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               ffn_dim=<span class=\"number\">2048</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">               dropout=<span class=\"number\">0.0</span></span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Decoder, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">        self.num_layers = num_layers</span><br><span class=\"line\"></span><br><span class=\"line\">        self.decoder_layers = nn.ModuleList(</span><br><span class=\"line\">          [DecoderLayer(model_dim, num_heads, ffn_dim, dropout) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span></span><br><span class=\"line\">           <span class=\"built_in\">range</span>(num_layers)])</span><br><span class=\"line\"></span><br><span class=\"line\">        self.seq_embedding = nn.Embedding(vocab_size + <span class=\"number\">1</span>, model_dim, padding_idx=<span class=\"number\">0</span>)</span><br><span class=\"line\">        self.pos_embedding = PositionalEncoding(model_dim, max_seq_len)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, inputs, inputs_len, enc_output, context_attn_mask=<span class=\"literal\">None</span></span>):</span></span><br><span class=\"line\">        output = self.seq_embedding(inputs)</span><br><span class=\"line\">        output += self.pos_embedding(inputs_len)</span><br><span class=\"line\"></span><br><span class=\"line\">        self_attention_padding_mask = padding_mask(inputs, inputs)</span><br><span class=\"line\">        seq_mask = sequence_mask(inputs)</span><br><span class=\"line\">        self_attn_mask = torch.gt((self_attention_padding_mask + seq_mask), <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        self_attentions = []</span><br><span class=\"line\">        context_attentions = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> decoder <span class=\"keyword\">in</span> self.decoder_layers:</span><br><span class=\"line\">            output, self_attn, context_attn = decoder(</span><br><span class=\"line\">            output, enc_output, self_attn_mask, context_attn_mask)</span><br><span class=\"line\">            self_attentions.append(self_attn)</span><br><span class=\"line\">            context_attentions.append(context_attn)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, self_attentions, context_attentions</span><br></pre></td></tr></table></figure>\n<p>组合一下</p>\n<h3><span id=\"23-transformer-模型\"> 2.3 Transformer 模型</span></h3>\n<p>class Transformer(nn.Module):</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           src_vocab_size,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           src_max_len,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           tgt_vocab_size,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           tgt_max_len,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           num_layers=<span class=\"number\">6</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           model_dim=<span class=\"number\">512</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           num_heads=<span class=\"number\">8</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           ffn_dim=<span class=\"number\">2048</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">           dropout=<span class=\"number\">0.2</span></span>):</span></span><br><span class=\"line\">    <span class=\"built_in\">super</span>(Transformer, self).__init__()</span><br><span class=\"line\"></span><br><span class=\"line\">    self.encoder = Encoder(src_vocab_size, src_max_len, num_layers, model_dim,</span><br><span class=\"line\">                           num_heads, ffn_dim, dropout)</span><br><span class=\"line\">    self.decoder = Decoder(tgt_vocab_size, tgt_max_len, num_layers, model_dim,</span><br><span class=\"line\">                           num_heads, ffn_dim, dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    self.linear = nn.Linear(model_dim, tgt_vocab_size, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, src_seq, src_len, tgt_seq, tgt_len</span>):</span></span><br><span class=\"line\">    context_attn_mask = padding_mask(tgt_seq, src_seq)</span><br><span class=\"line\"></span><br><span class=\"line\">    output, enc_self_attn = self.encoder(src_seq, src_len)</span><br><span class=\"line\"></span><br><span class=\"line\">    output, dec_self_attn, ctx_attn = self.decoder(</span><br><span class=\"line\">      tgt_seq, tgt_len, output, context_attn_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">    output = self.linear(output)</span><br><span class=\"line\">    output = self.softmax(output)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> output, enc_self_attn, dec_self_attn, ctx_attn</span><br><span class=\"line\">                           num_heads, ffn_dim, dropout)</span><br><span class=\"line\"></span><br><span class=\"line\">    self.linear = nn.Linear(model_dim, tgt_vocab_size, bias=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    self.softmax = nn.Softmax(dim=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, src_seq, src_len, tgt_seq, tgt_len</span>):</span></span><br><span class=\"line\">    context_attn_mask = padding_mask(tgt_seq, src_seq)</span><br><span class=\"line\"></span><br><span class=\"line\">    output, enc_self_attn = self.encoder(src_seq, src_len)</span><br><span class=\"line\"></span><br><span class=\"line\">    output, dec_self_attn, ctx_attn = self.decoder(</span><br><span class=\"line\">      tgt_seq, tgt_len, output, context_attn_mask)</span><br><span class=\"line\"></span><br><span class=\"line\">    output = self.linear(output)</span><br><span class=\"line\">    output = self.softmax(output)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> output, enc_self_attn, dec_self_attn, ctx_attn</span><br></pre></td></tr></table></figure>",
            "tags": [
                "人工智能",
                "NLP"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2021/06/24/NLP%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%BB%8Etransformer%E5%88%B0albert/",
            "url": "https://leezhao415.github.io/2021/06/24/NLP%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%BB%8Etransformer%E5%88%B0albert/",
            "title": "NLP模型：从transformer到albert",
            "date_published": "2021-06-24T15:21:14.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<h3><span id=\"nlp-模型从-transformer-到-albert\"> NLP 模型：从 transformer 到 albert</span></h3>\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#1-transformer\">1 Transformer</a>\n<ul>\n<li><a href=\"#11-transformer%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84\">1.1 transformer 整体架构</a></li>\n<li><a href=\"#12-transformer%E7%BB%93%E6%9E%84%E5%8E%9F%E7%90%86\">1.2 transformer 结构原理</a></li>\n<li><a href=\"#13-transformer%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82\">1.3 transformer 的技术细节</a></li>\n<li><a href=\"#14-transformer%E7%9A%84%E6%80%BB%E7%BB%93\">1.4 transformer 的总结</a></li>\n</ul>\n</li>\n<li><a href=\"#2-bert\">2 bert</a>\n<ul>\n<li><a href=\"#21-bert%E7%9A%84%E8%83%8C%E6%99%AF\">2.1 bert 的背景</a></li>\n<li><a href=\"#22-bert%E7%9A%84%E6%B5%81%E7%A8%8B\">2.2 bert 的流程</a></li>\n<li><a href=\"#23-bert%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82\">2.3 bert 的技术细节</a></li>\n<li><a href=\"#24-bert%E7%9A%84%E6%80%BB%E7%BB%93\">2.4 bert 的总结</a></li>\n</ul>\n</li>\n<li><a href=\"#3-xlnet\">3 xlnet</a>\n<ul>\n<li><a href=\"#31-xlnet%E7%9A%84%E8%83%8C%E6%99%AF\">3.1 xlnet 的背景</a></li>\n<li><a href=\"#32-xlnet%E7%9A%84%E6%B5%81%E7%A8%8B\">3.2 xlnet 的流程</a></li>\n<li><a href=\"#33-xlnet%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82\">3.3 xlnet 的技术细节</a></li>\n<li><a href=\"#34-xlnet%E7%9A%84%E6%80%BB%E7%BB%93\">3.4 xlnet 的总结</a></li>\n</ul>\n</li>\n<li><a href=\"#4-albert\">4 albert</a>\n<ul>\n<li><a href=\"#41-albert%E7%9A%84%E8%83%8C%E6%99%AF\">4.1 albert 的背景</a></li>\n<li><a href=\"#42-albert%E7%9A%84%E6%B5%81%E7%A8%8B\">4.2 albert 的流程</a></li>\n<li><a href=\"#43-albert%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82\">4.3 albert 的技术细节</a></li>\n<li><a href=\"#44-albert%E7%9A%84%E6%80%BB%E7%BB%93\">4.4 albert 的总结</a></li>\n</ul>\n</li>\n<li><a href=\"#5-%E5%85%B6%E4%BB%96%E8%AE%BA%E6%96%87\">5. 其他论文</a>\n<ul>\n<li><a href=\"#51-gpt\">5.1 gpt</a></li>\n<li><a href=\"#52-structbert\">5.2 structbert</a></li>\n<li><a href=\"#53-roberta\">5.3 roberta</a></li>\n</ul>\n</li>\n<li><a href=\"#6-%E6%80%BB%E7%BB%93\">6. 总结</a></li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h4><span id=\"1-transformer\"> 1 Transformer</span></h4>\n<h5><span id=\"11-transformer-整体架构\"> 1.1 transformer 整体架构</span></h5>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624165944329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 40%;\"></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624170040624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:60%;\"></center>\n<h5><span id=\"12-transformer-结构原理\"> 1.2 transformer 结构原理</span></h5>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;<span class=\"number\">1</span>&gt; Inputs是经过padding的输入数据，大小是[batch size, <span class=\"built_in\">max</span> seq length]。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">2</span>&gt; 初始化embedding matrix，通过embedding lookup将Inputs映射成token embedding，大小是[batch size, <span class=\"built_in\">max</span> seq length, embedding size]，然后乘以embedding size的开方。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">3</span>&gt; 通过sin和cos函数创建positional encoding，表示一个token的绝对位置信息，并加入到token embedding中，然后dropout。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">4</span>&gt; multi-head attention</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">4.1</span>&gt; 输入token embedding，通过Dense生成Q，K，V，大小是[batch size, <span class=\"built_in\">max</span> seq length, embedding size]，然后按第<span class=\"number\">2</span>维split成num heads份并按第<span class=\"number\">0</span>维concat，生成新的Q，K，V，大小是[num heads*batch size, <span class=\"built_in\">max</span> seq length, embedding size/num heads]，完成multi-head的操作。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">4.2</span>&gt; 将K的第<span class=\"number\">1</span>维和第<span class=\"number\">2</span>维进行转置，然后Q和转置后的K的进行点积，结果的大小是[num heads*batch size, <span class=\"built_in\">max</span> seq length, <span class=\"built_in\">max</span> seq length]。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">4.3</span>&gt; 将&lt;<span class=\"number\">4.2</span>&gt;的结果除以hidden size的开方(在transformer中，hidden size=embedding size)，完成scale的操作。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">4.4</span>&gt; 将&lt;<span class=\"number\">4.3</span>&gt;中padding的点积结果置成一个很小的数(-<span class=\"number\">2</span>**<span class=\"number\">32</span>+<span class=\"number\">1</span>)，完成mask操作，后续softmax对padding的结果就可以忽略不计了。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">4.5</span>&gt; 将经过mask的结果进行softmax操作。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">4.6</span>&gt; 将softmax的结果和V进行点积，得到attention的结果，大小是[num heads*batch size, <span class=\"built_in\">max</span> seq length, hidden size/num heads]。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">4.7</span>&gt; 将attention的结果按第<span class=\"number\">0</span>维split成num heads份并按第<span class=\"number\">2</span>维concat，生成multi-head attention的结果，大小是[batch size, <span class=\"built_in\">max</span> seq length, hidden size]。Figure <span class=\"number\">2</span>上concat之后还有一个linear的操作，但是代码里并没有。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">5</span>&gt; 将token embedding和multi-head attention的结果相加，并进行Layer Normalization。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">6</span>&gt; 将&lt;<span class=\"number\">5</span>&gt;的结果经过<span class=\"number\">2</span>层Dense，其中第<span class=\"number\">1</span>层的activation=relu，第<span class=\"number\">2</span>层activation=<span class=\"literal\">None</span>。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">7</span>&gt; 功能和&lt;<span class=\"number\">5</span>&gt;一样。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">8</span>&gt; Outputs是经过padding的输出数据，与Inputs不同的是，Outputs的需要在序列前面加上一个起始符号“&lt;s&gt;”，用来表示序列生成的开始，而Inputs不需要。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">9</span>&gt; 功能和&lt;<span class=\"number\">2</span>&gt;一样。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">10</span>&gt; 功能和&lt;<span class=\"number\">3</span>&gt;一样。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">11</span>&gt; 功能和&lt;<span class=\"number\">4</span>&gt;类似，唯一不同的一点在于mask，&lt;<span class=\"number\">11</span>&gt;中的mask不仅将padding的点积结果置成一个很小的数，而且将当前token与之后的token的点积结果也置成一个很小的数。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">12</span>&gt; 功能和&lt;<span class=\"number\">5</span>&gt;一样。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">13</span>&gt; 功能和&lt;<span class=\"number\">4</span>&gt;类似，唯一不同的一点在于Q，K，V的输入，&lt;<span class=\"number\">13</span>&gt;的Q的输入来自于Outputs 的token embedding，&lt;<span class=\"number\">13</span>&gt;的K，V来自于&lt;<span class=\"number\">7</span>&gt;的结果。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">14</span>&gt; 功能和&lt;<span class=\"number\">5</span>&gt;一样。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">15</span>&gt; 功能和&lt;<span class=\"number\">6</span>&gt;一样。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">16</span>&gt; 功能和&lt;<span class=\"number\">7</span>&gt;一样，结果的大小是[batch size, <span class=\"built_in\">max</span> seq length, hidden size]。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">17</span>&gt; 将&lt;<span class=\"number\">16</span>&gt;的结果的后<span class=\"number\">2</span>维和embedding matrix的转置进行点积，生成的结果的大小是[batch size, <span class=\"built_in\">max</span> seq length, vocab size]。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">18</span>&gt; 将&lt;<span class=\"number\">17</span>&gt;的结果进行softmax操作，生成的结果就表示当前时刻预测的下一个token在vocab上的概率分布。</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;<span class=\"number\">19</span>&gt; 计算&lt;<span class=\"number\">18</span>&gt;得到的下一个token在vocab上的概率分布和真实的下一个token的one-hot形式的cross entropy，然后<span class=\"built_in\">sum</span>非padding的token的cross entropy当作loss，利用adam进行训练。</span><br></pre></td></tr></table></figure>\n<h5><span id=\"13-transformer-的技术细节\"> 1.3 transformer 的技术细节</span></h5>\n<p>transformer 中的 self-attention 是从普通的点积 attention 中演化出来的</p>\n<h6><span id=\"131-为什么-lt2gt-要乘以-embedding-size-的开方\"> 1.3.1 为什么 &lt;2&gt; 要乘以 embedding size 的开方？</span></h6>\n<p>论文并没有讲为什么这么做，我看了代码，猜测是因为 embedding matrix 的初始化方式是 xavier init，这种方式的方差是 1/embedding size，因此乘以 embedding size 的开方使得 embedding matrix 的方差是 1，在这个 scale 下可能更有利于 embedding matrix 的收敛。</p>\n<h6><span id=\"132-为什么-inputs-embedding-要加入-positional-encoding\"> 1.3.2 为什么 inputs embedding 要加入 positional encoding？</span></h6>\n<p>因为 self-attention 是位置无关的，无论句子的顺序是什么样的，通过 self-attention 计算的 token 的 hidden embedding 都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个 token 的位置信息，transformer 使用了固定的 positional encoding 来表示 token 在句子中的绝对位置信息。positional encoding 的公式如下：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624170127842.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 50%;\"></center>\n<h6><span id=\"133-为什么-lt42gt-的结果要-scale\"> 1.3.3 为什么 &lt;4.2&gt; 的结果要 scale？</span></h6>\n<p>以数组为例，2 个长度是 len，均值是 0，方差是 1 的数组点积会生成长度是 len，均值是 0，方差是 len 的数组。而方差变大会导致 softmax 的输入推向正无穷或负无穷，这时的梯度会无限趋近于 0，不利于训练的收敛。因此除以 len 的开方，可以是数组的方差重新回归到 1，有利于训练的收敛。</p>\n<h6><span id=\"134-为什么-lt5gt-要将-multi-head-attention-的输入和输出相加\"> 1.3.4 为什么 &lt;5&gt; 要将 multi-head attention 的输入和输出相加？</span></h6>\n<p>类似于 resnet 中的残差学习单元，有 ensemble 的思想在里面，解决网络退化问题。</p>\n<h6><span id=\"135-为什么-attention-需要-multi-head一个大-head-行不行\"> 1.3.5 为什么 attention 需要 multi-head，一个大 head 行不行？</span></h6>\n<p>multi-head 相当于把一个大空间划分成多个互斥的小空间，然后在小空间内分别计算 attention，虽然单个小空间的 attention 计算结果没有大空间计算得精确，但是多个小空间并行然后 concat 有助于网络捕捉到更丰富的信息，类比 cnn 网络中的 channel。</p>\n<h6><span id=\"136-为什么-multi-head-attention-后面要加一个-ffn\"> 1.3.6 为什么 multi-head attention 后面要加一个 ffn？</span></h6>\n<p>类比 cnn 网络中，cnn block 和 fc 交替连接，效果更好。相比于单独的 multi-head attention，在后面加一个 ffn，可以提高整个 block 的非线性变换的能力。</p>\n<h6><span id=\"137-为什么-lt11gt-要-mask-当前时刻的-token-与后续-token-的点积结果\"> 1.3.7 为什么 &lt;11&gt; 要 mask 当前时刻的 token 与后续 token 的点积结果？</span></h6>\n<p>自然语言生成 (例如机器翻译，文本摘要) 是 auto-regressive 的，在推理的时候只能依据之前的 token 生成当前时刻的 token，正因为生成当前时刻的 token 的时候并不知道后续的 token 长什么样，所以为了保持训练和推理的一致性，训练的时候也不能利用后续的 token 来生成当前时刻的 token。这种方式也符合人类在自然语言生成中的思维方式。</p>\n<h5><span id=\"14-transformer-的总结\"> 1.4 transformer 的总结</span></h5>\n<p>transformer 刚发表的时候，我刚好在百度 nlp 部实习，当时觉得 transformer 噱头更多一些，在小模型上 self-attention 并不比 rnn，lstm 好。直到大力出奇迹的 bert 出现，深深地打了我的脸，当模型变得越来越大，样本数越来越多的时候，self-attention 无论是并行化带来的训练提速，还是在长距离上的建模，都是要比传统的 rnn，lstm 好很多。transformer 现在已经各种具有代表性的 nlp 预训练模型的基础，bert 系列使用了 transformer 的 encoder，gpt 系列 transformer 的 decoder。在推荐领域，transformer 的 multi-head attention 也应用得很广泛。</p>\n<h4><span id=\"2-bert\"> 2 bert</span></h4>\n<h5><span id=\"21-bert-的背景\"> 2.1 bert 的背景</span></h5>\n<p>在 bert 之前，将预训练的 embedding 应用到下游任务的方式大致可以分为 2 种，一种是 feature-based，例如 ELMo 这种将经过预训练的 embedding 作为特征引入到下游任务的网络中；一种是 fine-tuning，例如 GPT 这种将下游任务接到预训练模型上，然后一起训练。然而这 2 种方式都会面临同一个问题，就是无法直接学习到上下文信息，像 ELMo 只是分别学习上文和下文信息，然后 concat 起来表示上下文信息，抑或是 GPT 只能学习上文信息。因此，作者提出一种基于 transformer encoder 的预训练模型，可以直接学习到上下文信息，叫做 bert。bert 使用了 12 个 transformer encoder block，在 13G 的数据上进行了预训练，可谓是 nlp 领域大力出奇迹的代表。</p>\n<h5><span id=\"22-bert-的流程\"> 2.2 bert 的流程</span></h5>\n<p>bert 是在 transformer encoder 的基础之上进行改进的，因此在整个流程上与 transformer encoder 没有大的差别，只是在 embedding，multi-head attention，loss 上有所差别。</p>\n<h6><span id=\"221-bert-和-transformer-在-embedding-上的差异\"> 2.2.1 bert 和 transformer 在 embedding 上的差异</span></h6>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624170158406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 75%;\"></center>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624170221363.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<p>bert 预训练 and fine-tune</p>\n<p>bert 和 transformer 在 embedding 上的差异主要有 3 点：</p>\n<p>&lt;1&gt; transformer 的 embedding 由 2 部分构成，一个是 token embedding，通过 embedding matrix lookup 到 token_ids 上生成表示 token 的向量；一个是 position embedding，是通过 sin 和 cos 函数创建的定值向量。而 bert 的 embedding 由 3 部分构成，第一个同样是 token embedding，通过 embedding matrix lookup 到 token_ids 上生成表示 token 的向量；第二个是 segment embedding，用来表达当前 token 是来自于第一个 segment，还是第二个 segment，因此 segment vocab size 是 2；第三个是 position embedding，与 transformer 不同的是，bert 创建了一个 position embedding matrix，通过 position embedding matrix lookup 到 token_ids 的位置上生成表示 token 位置的位置向量。</p>\n<p>&lt;2&gt; transformer 在 embedding 之后跟了一个 dropout，但是 bert 在 embedding 之后先跟了一个 layer normalization，再跟了一个 dropout。</p>\n<p>&lt;3&gt; bert 在 token 序列之前加了一个特定的 token“[cls]”，这个 token 对应的向量后续会用在分类任务上；如果是句子对的任务，那么两个句子间使用特定的 token“[seq]” 来分割。</p>\n<h6><span id=\"222-bert-和-transformer-在-multi-head-attention-上的差异\"> 2.2.2 bert 和 transformer 在 multi-head attention 上的差异</span></h6>\n<p>bert 和 transformer 在 multi-head attention 上的差异主要有 1 点：</p>\n<p>&lt;1&gt; transformer 在 &lt; 4.7 &gt; 之后没有 linear 的操作 (也可能是因为我看的 transformer 代码不是官方 transformer 的缘故)，而 bert 在 transformer 的 &lt; 4.7 &gt; 之后有一个 linear 的操作。</p>\n<h6><span id=\"223-bert-和-transformer-在-loss-上的差异\"> 2.2.3 bert 和 transformer 在 loss 上的差异</span></h6>\n<p>bert 和 transformer 在 loss 上的差异主要有 2 点：</p>\n<p>&lt;1&gt; transformer 的 loss 是在 decoder 阶段计算的，loss 的计算方式是 transformer 的 &lt; 19&gt;。bert 预训练的 loss 由 2 部分构成，一部分是 NSP 的 loss，就是 token“[cls]” 经过 1 层 Dense，然后接一个二分类的 loss，其中 0 表示 segment B 是 segment A 的下一句，1 表示 segment A 和 segment B 来自 2 篇不同的文本；另一部分是 MLM 的 loss，segment 中每个 token 都有 15% 的概率被 mask，而被 mask 的 token 有 80% 的概率用 “<mask>” 表示，有 10% 的概率随机替换成某一个 token，有 10% 的概率保留原来的 token，被 mask 的 token 经过 encoder 后乘以 embedding matrix 的转置会生成在 vocab 上的分布，然后计算分布和真实的 token 的 one-hot 形式的 cross entropy，最后 sum 起来当作 loss。这两部分 loss 相加起来当作 total loss，利用 adam 进行训练。bert fine-tune 的 loss 会根据任务性质来设计，例如分类任务中就是 token“[cls]” 经过 1 层 Dense，然后接了一个二分类的 loss；例如问题回答任务中会在 paragraph 上的 token 中预测一个起始位置，一个终止位置，然后以起始位置和终止位置的预测分布和真实分布为基础设计 loss；例如序列标注，预测每一个 token 的词性，然后以每一个 token 在词性的预测分布和真实分布为基础设计 loss。</mask></p>\n<p>&lt;2&gt; bert 在 encoder 之后，在计算 NSP 和 MLM 的 loss 之前，分别对 NSP 和 MLM 的输入加了一个 Dense 操作，这部分参数只对预训练有用，对 fine-tune 没用。而 transformer 在 decoder 之后就直接计算 loss 了，中间没有 Dense 操作。</p>\n<h5><span id=\"23-bert-的技术细节\"> 2.3 bert 的技术细节</span></h5>\n<h6><span id=\"231-为什么-bert-需要额外的-segment-embedding\"> 2.3.1 为什么 bert 需要额外的 segment embedding?</span></h6>\n<p>因为 bert 预训练的其中一个任务是判断 segment A 和 segment B 之间的关系，这就需要 embedding 中能包含当前 token 属于哪个 segment 的信息，然而无论是 token embedding，还是 position embedding 都无法表示出这种信息，因此额外创建一个 segment embedding matrix 用来表示当前 token 属于哪个 segment 的信息，segment vocab size 就是 2，其中 index=0 表示 token 属于 segment A，index=1 表示 token 属于 segment B。</p>\n<h6><span id=\"232-为什么-transformer-的-embedding-后面接了一个-dropout而-bert-是先接了一个-layer-normalization再接-dropout\"> 2.3.2 为什么 transformer 的 embedding 后面接了一个 dropout，而 bert 是先接了一个 layer normalization，再接 dropout?</span></h6>\n<p>LN 是为了解决梯度消失的问题，dropout 是为了解决过拟合的问题。在 embedding 后面加 LN 有利于 embedding matrix 的收敛。</p>\n<h6><span id=\"233-为什么-token-被-mask-的概率是-15为什么被-mask-后还要分-3-种情况\"> 2.3.3 为什么 token 被 mask 的概率是 15%？为什么被 mask 后，还要分 3 种情况？</span></h6>\n<p>15% 的概率是通过实验得到的最好的概率，xlnet 也是在这个概率附近，说明在这个概率下，既能有充分的 mask 样本可以学习，又不至于让 segment 的信息损失太多，以至于影响 mask 样本上下文信息的表达。然而因为在下游任务中不会出现 token“<mask>”，所以预训练和 fine-tune 出现了不一致，为了减弱不一致性给模型带来的影响，被 mask 的 token 有 80% 的概率用 “<mask>” 表示，有 10% 的概率随机替换成某一个 token，有 10% 的概率保留原来的 token，这 3 个百分比也是多次实验得到的最佳组合，在这 3 个百分比的情况下，下游任务的 fine-tune 可以达到最佳的实验结果。</mask></mask></p>\n<h5><span id=\"24-bert-的总结\"> 2.4 bert 的总结</span></h5>\n<p>相比于那些说自己很好，但是在实际场景中然并软的论文，bert 是真正地影响了学术界和工业界。无论是 GLUE，还是 SQUAD，现在榜单上的高分方法都是在 bert 的基础之上进行了改进。在我的工作中，用 bert 落地的业务效果也比我预想的要好一些。bert 在 nlp 领域的地位可以类比 cv 领域的 inception 或者 resnet，cv 领域的算法效果在几年前就已经超过了人类的标注准确率，而 nlp 领域直到 bert 的出现才做到这一点。不过 bert 也并不是万能的，bert 的框架决定了这个模型适合解决自然语言理解的问题，因为没有解码的过程，所以 bert 不适合解决自然语言生成的问题。因此如何将 bert 改造成适用于解决机器翻译，文本摘要问题的框架，是今后值得研究的一个点。</p>\n<h4><span id=\"3-xlnet\"> 3 xlnet</span></h4>\n<h5><span id=\"31-xlnet-的背景\"> 3.1 xlnet 的背景</span></h5>\n<p>目前语言预训练模型的模式主要有 2 种，第一种是像 gpt 这种的 auto-regressive 模型，每个时刻都依据之前所有时刻的 token 来预测下一个 token，auto-regressive 的 loss 的定义如下：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624170312408.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<p>auto-regressive 的 loss</p>\n<p>第二种是像 bert 这种的 auto-encoder 模型，随机 mask 掉句子中若干个 token，然后依据上下文预测被 mask 掉的 token，auto-encoder 的 loss 的定义如下：</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624170332480.png#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom:67%;\"></center>\n<p>auto-encoder 的 loss</p>\n<p>auto-regressive 模型在训练的过程中只能用到上文的信息，但是不会出现训练和推理的 gap；auto-encoder 模型在训练的过程中能利用到上下文信息，但是会出现训练和推理的 gap，训练过程中的<mask>在推理的时候并不会出现。因此，作者就提出一种基于 transformer-xl 的融合了 auto-regressive 模型和 auto-encoder 模型优势的 auto-regressive 模型。</mask></p>\n<h5><span id=\"32-xlnet-的流程\"> 3.2 xlnet 的流程</span></h5>\n<h6><span id=\"321-因子分解序\"> 3.2.1 因子分解序</span></h6>\n<p>一个句子的因子分解序就是这个句子的 token 的一种随机排列。为了能融合 auto-regressive 模型和 auto-encoder 模型的优势，xlnet 使用因子分解序将上下文信息引入 auto-regressive 的 loss 中。例如句子 1-&gt;2-&gt;3-&gt;4-&gt;5，在 auto-regressive 的 loss 中，预测 token 2 可以利用 token 1 的信息，但是不能利用 token 2/3/4/5 的信息；在引入了因子分解序之后，假设使用了 1-&gt;4-&gt;2-&gt;3-&gt;5 的因子分解序，那么预测 token 2 可以利用 token 1/4 的信息，但是不能利用 token 3/5 的信息。在使用因子分解序之后，并不会影响句子的输入顺序，只是在 transformer-xl 的 multi-head attention 中计算每一个 token 的 attention 结果时会有所改变，原先的方式是 mask 掉当前 token 以及句子中的后续 token，而现在是 mask 掉当前 token 以及因子分解序中的后续 token。这种方式可以在计算当前 token 的 attention 结果时利用到当前 token 的上下文信息，例如上面这个因子分解序，计算 token 2 的 attention 结果时就是用到了 token 1/4 的信息，在原始句子中，token 1 在 token 2 之前，token 4 在 token 2 之后。</p>\n<p>因子分解序的实现方式是在计算 multi-head attention 的时候进行了 proper mask。例如 1-&gt;4-&gt;2-&gt;3-&gt;5 的因子分解序，在输入 token 2 时，由于在因子分解序中 token 2 排在 token 1/4 的后面，所以在计算 token 2 的 attention 结果时将 token 2/3/5 进行了 mask，只计算 token 2 和 token 1/4 的点积结果，然后 softmax 以及加权求和当作 attention 的结果。</p>\n<h6><span id=\"322-双流自注意力机制\"> 3.2.2 双流自注意力机制</span></h6>\n<p>xlnet 使用了 transformer-xl 的框架，并在 transformer 的基础之上使用了双流自注意力机制。</p>\n<center><img src=\"https://img-blog.csdnimg.cn/20210624170351960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzIyODg3,size_16,color_FFFFFF,t_70#pic_center\" alt=\"在这里插入图片描述\" style=\"zoom: 67%;\"></center>\n<p>双流自注意力机制</p>\n<p>相比于普通的 transformer，xlnet 多加了一个 multi-head attention+ffn 的计算。双流自注意力机制分为查询流 g 和内容流 h 2 个流。h 就是和 transformer 一样的 multi-head attention，计算第 t 个时刻的 attention 的结果时用到了因子分解序中前 t 个位置的位置信息和 token 信息，而 g 在 transformer 的 multi-head attention 的基础之上做了修改，计算第 t 个时刻的 attention 的结果时只用到了因子分解序中前 t 个位置的位置信息和前 t-1 个位置的 token 信息。在预训练的过程当中，为了降低优化的难度，xlnet 只会计算因子分解序最后的 1/6 或者 1/7 的 token 的 g，然后把 g 融合到 auto-regressive 的 loss 当中进行训练，顺带着训练 h。在预训练结束之后，放弃 g，使用 h 做下游任务的 fine-tune，fine-tune 的过程就和普通的 transfomer 的 fine-tune 一模一样了。</p>\n<h5><span id=\"33-xlnet-的技术细节\"> 3.3 xlnet 的技术细节</span></h5>\n<h6><span id=\"331-因子分解序的优势\"> 3.3.1 因子分解序的优势</span></h6>\n<p>因子分解序创新地将上下文信息融入到 auto-regressive 的 loss 中，理论上，只要模型的预训练将一个句子的所有因子分解序都训练一遍，那么模型就能准确地 get 到句子中每一个 token 和上下文之间的联系。然而实际情况下，一个句子的因子分解序的数量是随着句子长度指数增长的，因此在实际训练中只是用到了句子的某个因子分解序或者某几个因子分解序而已。即便如此，相比于只能 get 到上文信息的 auto-regressive，加了因子分解序之后可以同时 get 到上下文信息，能够提高模型的推理能力。</p>\n<h6><span id=\"332-为什么自注意力要用双流\"> 3.3.2 为什么自注意力要用双流？</span></h6>\n<p>因为普通的 transformer 无法融合因子分解序和 auto-regressive 的 loss，例如 2 个不同的因子分解序 1-&gt;3-&gt;2-&gt;4-&gt;5 和 1-&gt;3-&gt;2-&gt;5-&gt;4，第 1 个句子的 4 和第 2 个句子的 5 在 auto-regressive 的 loss 下的 attention 结果是一样的，因此第 1 个句子的 4 和第 2 个句子的 5 在 vocab 上的预测概率分布也是一样的，这就不符合常理了。造成这种现象的原因在于，auto-regressive 的 loss 是利用前 t-1 个 token 的 token 信息和位置信息预测第 t 个 token，然而因子分解序的第 t 个 token 在原始句子中的位置是不确定的，因此需要额外的信息表示因子分解序中需要预测的 token 在原始句子中的位置。为了达到目的，xlnet 使用双流的 multi-head attention+ffn，查询流 g 利用因子分解序中前 t 个位置的位置信息和前 t-1 个位置的 token 信息计算第 t 个位置的输出信息，而内容流 h 利用因子分解序中前 t 个位置的位置信息和 token 信息计算第 t 个位置的输出信息。在预训练的过程中，使用 g 计算 auto-regressive 的 loss，然后最小化的 loss 的值，顺带着训练 h。预训练完成之后，放弃 g，使用 h 无缝切换到普通 transformer 的 fine-tune。</p>\n<h5><span id=\"34-xlnet-的总结\"> 3.4 xlnet 的总结</span></h5>\n<p>由于我也是只看过论文，并没有在实际工作中用过 xlnet，因此我也只能讲讲 xlnet 的理论。在 bert 之后，有很多论文都对 bert 进行了改进，但是创新点都很有限，xlnet 是在我看过的论文中唯一一篇在 transformer 的框架之下将上下文信息和 auto-regressive 的 loss 融合在一起的论文。但是 xlnet 是否真的比 bert 优秀，这还是一个疑问，xlnet 使用了 126G 的数据进行预训练，相比于 bert 的 13G 数据大了一个数量级，在 xlnet 发布之后不久，bert 的改进版 roberta 使用了 160G 的数据进行预训练，又打败了 xlnet。</p>\n<h4><span id=\"4-albert\"> 4 albert</span></h4>\n<h5><span id=\"41-albert-的背景\"> 4.1 albert 的背景</span></h5>\n<p>增大预训练模型的大小通常能够提高预训练模型的推理能力，但是当预训练模型增大到一定程度之后，会碰到 GPU/TPU memory 的限制。因此，作者在 bert 中加入了 2 项减少参数的技术，能够缩小 bert 的大小，并且修改了 bert NSP 的 loss，在和 bert 有相同参数量的前提之下，有更强的推理能力。</p>\n<h5><span id=\"42-albert-的流程\"> 4.2 albert 的流程</span></h5>\n<h6><span id=\"421-词向量矩阵的分解\"> 4.2.1 词向量矩阵的分解</span></h6>\n<p>在 bert 以及诸多 bert 的改进版中，embedding size 都是等于 hidden size 的，这不一定是最优的。因为 bert 的 token embedding 是上下文无关的，而经过 multi-head attention+ffn 后的 hidden embedding 是上下文相关的，bert 预训练的目的是提供更准确的 hidden embedding，而不是 token embedding，因此 token embedding 没有必要和 hidden embedding 一样大。albert 将 token embedding 进行了分解，首先降低 embedding size 的大小，然后用一个 Dense 操作将低维的 token embedding 映射回 hidden size 的大小。bert 的 embedding size=hidden size，因此词向量的参数量是 vocab size * hidden size，进行分解后的参数量是 vocab size * embedding size + embedding size * hidden size，只要 embedding size &lt;&lt; hidden size，就能起到减少参数的效果。</p>\n<h6><span id=\"422-参数共享\"> 4.2.2 参数共享</span></h6>\n<p>bert 的 12 层 transformer encoder block 是串行在一起的，每个 block 虽然长得一模一样，但是参数是不共享的。albert 将 transformer encoder block 进行了参数共享，这样可以极大地减少整个模型的参数量。</p>\n<h6><span id=\"423-sentence-order-predictionsop\"> 4.2.3 sentence order prediction(SOP)</span></h6>\n<p>在 auto-encoder 的 loss 之外，bert 使用了 NSP 的 loss，用来提高 bert 在句对关系推理任务上的推理能力。而 albert 放弃了 NSP 的 loss，使用了 SOP 的 loss。NSP 的 loss 是判断 segment A 和 segment B 之间的关系，其中 0 表示 segment B 是 segment A 的下一句，1 表示 segment A 和 segment B 来自 2 篇不同的文本。SOP 的 loss 是判断 segment A 和 segment B 的的顺序关系，0 表示 segment B 是 segment A 的下一句，1 表示 segment A 是 segment B 的下一句。</p>\n<h5><span id=\"43-albert-的技术细节\"> 4.3 albert 的技术细节</span></h5>\n<h6><span id=\"431-参数减少技术\"> 4.3.1 参数减少技术</span></h6>\n<p>albert 使用了 2 项参数减少的技术，但是 2 项技术对于参数减少的贡献是不一样的，第 1 项是词向量矩阵的分解，当 embedding size 从 768 降到 64 时，可以节省 21M 的参数量，但是模型的推理能力也会随之下降。第 2 项是 multi-head attention+ffn 的参数共享，在 embedding size=128 时，可以节省 77M 的参数量，模型的推理能力同样会随之下降。虽然参数减少会导致了模型推理能力的下降，但是可以通过增大模型使得参数量变回和 bert 一个量级，这时模型的推理能力就超过了 bert。</p>\n<p>现在学术界发论文有 2 种常见的套路，第 1 种是往死里加参数加数据量，然后提高模型的推理能力；第 2 种是减参数，然后使模型的推理能力不怎么降。albert 使用的参数减少技术看似是第 2 种，实则是第 1 种。当 bert 从 large 变到 xlarge 时，虽然模型变大到了 1270M，但是模型出现了退化现象，推理能力下跌了一大截，说明在 bert 的框架下，large 已经是模型推理能力的极限了。albert 使用了参数减少技术，相比于 bert 的 large 是 334M，albert 的 large 只有 18M，虽然推理能力比 bert 差，但是参数减少后的 albert 还有成长空间，将 albert 从 large 变到 xlarge，甚至是 xxlarge 时，模型的推理能力又得到了提高，并且超过了 bert 最好的模型。</p>\n<h6><span id=\"432-loss\"> 4.3.2 loss</span></h6>\n<p>在 albert 之前，很多 bert 的改进版都对 NSP 的 loss 提出了质疑。structbert 在 NSP 的 loss 上进行了修改，有 1/3 的概率是 segment B 是 segment A 的下一句，有 1/3 的概率是 segment A 是 segment B 的下一句，有 1/3 的概率是 segment A 和 segment B 来自 2 篇不同的文本。roberta 则是直接放弃了 NSP 的 loss，修改了样本的构造方式，将输入 2 个 segment 修改为从一个文本中连续 sample 句子直到塞满 512 的长度。当到达文本的末尾且未塞满 512 的长度时，先增加一个 “[sep]”，再从另一个文本接着 sample，直到塞满 512 的长度。</p>\n<p>albert 在 structbert 的基础之上又抛弃了 segment A 和 segment B 来自 2 篇不同的文本的做法，只剩下 1/2 的概率是 segment B 是 segment A 的下一句，1/2 的概率是 segment A 是 segment B 的下一句。论文中给出了这么做的解释，NSP 的 loss 包含了 2 部分功能：topic prediction 和 coherence prediction，其中 topic prediction 要比 coherence prediction 更容易学习，而 MLM 的 loss 也包含了 topic prediction 的功能，因此 bert 难以学到 coherence prediction 的能力。albert 的 SOP loss 抛弃了 segment A 和 segment B 来自 2 篇不同的文本的做法，让 loss 更关注于 coherence prediction，这样就能提高模型在句对关系推理上的能力。</p>\n<h5><span id=\"44-albert-的总结\"> 4.4 albert 的总结</span></h5>\n<p>albert 虽然减少参数量，但是并不会减少推理时间，推理的过程只不过是从串行计算 12 个 transformer encoder block 变成了循环计算 transformer encoder block 12 次。albert 最大的贡献在于使模型具备了比原始的 bert 更强的成长性，在模型变向更大的时候，推理能力还能够得到提高。</p>\n<h4><span id=\"5-其他论文\"> 5. 其他论文</span></h4>\n<h5><span id=\"51-gpt\"> 5.1 gpt</span></h5>\n<p>gpt 在 bert 之前就发表了，使用了 transformer decoder 作为预训练的框架。在看到了 decoder 只能 get 上文信息，不能 get 下文信息的缺点之后，bert 改用了 transformer encoder 作为预训练的框架，能够同时 get 上下文信息，获得了巨大的成功。</p>\n<h5><span id=\"52-structbert\"> 5.2 structbert</span></h5>\n<p>structbert 的创新点主要在 loss 上，除了 MLM 的 loss 外，还有一个重构 token 顺序的 loss 和一个判断 2 个 segment 关系的 loss。重构 token 顺序的 loss 是以一定的概率挑选 segment 中的 token 三元组，然后随机打乱顺序，最后经过 encoder 之后能够纠正被打乱顺序的 token 三元组的顺序。判断 2 个 segment 关系的 loss 是 1/3 的概率是 segment B 是 segment A 的下一句，有 1/3 的概率是 segment A 是 segment B 的下一句，有 1/3 的概率是 segment A 和 segment B 来自 2 篇不同的文本，通过 “[cls]” 预测样本属于这 3 种的某一种。</p>\n<h5><span id=\"53-roberta\"> 5.3 roberta</span></h5>\n<p>在 xlnet 使用 126G 的数据登顶 GLUE 之后不久，roberta 使用 160G 的数据又打败了 xlnet。roberta 的创新点主要有 4 点：第 1 点是动态 mask，之前 bert 使用的是静态 mask，就是数据预处理的时候完成 mask 操作，之后训练的时候同一个样本都是相同的 mask 结果，动态 mask 就是在训练的时候每输入一个样本都要重新 mask，动态 mask 相比静态 mask 有更多不同 mask 结果的数据用于训练，效果很好。第 2 点是样本的构造方式，roberta 放弃了 NSP 的 loss，修改了样本的构造方式，将输入 2 个 segment 修改为从一个文本中连续 sample 句子直到塞满 512 的长度。当到达文本的末尾且未塞满 512 的长度时，先增加一个 “[sep]”，再从另一个文本接着 sample，直到塞满 512 的长度。第 3 点是增大了 batch size，在训练相同数据量的前提之下，增大 batch size 能够提高模型的推理能力。第 4 点是使用了 subword 的分词方法，类比于中文的字，相比于 full word 的分词方法，subword 的分词方法使得词表的大小从 30k 变成了 50k，虽然实验效果上 subword 的分词方法比 full word 差，但是作者坚信 subword 具备了理论优越性，今后肯定会比 full word 好 (手动黑脸)。</p>\n<h4><span id=\"6-总结\"> 6. 总结</span></h4>\n<p>nlp 和 cv 的不同点在于 nlp 是认识学习，而 cv 是感知学习，nlp 在 cv 的基础之上多了一个符号映射的过程，正因如此，nlp 领域发展得比 cv 慢很多，cv 领域有很多比较成功的创业公司，有很多能够达到商用程度的子领域，而 nlp 领域就比较少。不过 nlp 领域在 17 年的 transformer 发布之后开始进入快速迭代的时期，bert 的发表使得 nlp 领域的 benchmark 提高了一大截，产生了不少可以达到商用程度的子领域。到了 19 年，nlp 领域的发展可以说是越来越快了，我在国庆的时候开始执笔写这个技术分享，当时 albert 刚发表 1 个星期，等我写完这个技术分享已经到 11 月了，前几天谷歌又发表了一篇 T5，又把 albert 打败了。T5 的论文据说有 50 页，是 nlp 预训练模型的一个综述，值得花时间一看。</p>\n",
            "tags": [
                "人工智能",
                "NLP"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2021/06/24/NLP%E4%B9%8B%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/",
            "url": "https://leezhao415.github.io/2021/06/24/NLP%E4%B9%8B%E5%B8%B8%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/",
            "title": "NLP之常用预训练模型详解",
            "date_published": "2021-06-24T15:20:07.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#nlp%E4%B8%AD%E6%B5%81%E8%A1%8C%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B\">NLP 中流行的预训练模型</a>\n<ul>\n<li><a href=\"#1-bert%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93\">1 BERT 及其变体</a></li>\n<li><a href=\"#2-gpt\">2 GPT</a></li>\n<li><a href=\"#3-gpt-2%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93\">3 GPT-2 及其变体</a></li>\n<li><a href=\"#4-transformer-xl\">4 Transformer-XL</a></li>\n<li><a href=\"#5-xlnet%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93\">5 XLNet 及其变体</a></li>\n<li><a href=\"#6-xlm\">6 XLM</a></li>\n<li><a href=\"#7-roberta%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93\">7 RoBERTa 及其变体</a></li>\n<li><a href=\"#8-distilbert%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93\">8 DistilBERT 及其变体</a></li>\n<li><a href=\"#9-albert\">9 ALBERT</a></li>\n<li><a href=\"#10-t5%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93\">10 T5 及其变体</a></li>\n<li><a href=\"#11-xlm-roberta%E5%8F%8A%E5%85%B6%E5%8F%98%E4%BD%93\">11 XLM-RoBERTa 及其变体</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h3><span id=\"nlp-中流行的预训练模型\"> NLP 中流行的预训练模型</span></h3>\n<ul>\n<li>BERT</li>\n<li>GPT</li>\n<li>GPT-2</li>\n<li>Transformer-XL</li>\n<li>XLNet</li>\n<li>XLM</li>\n<li>RoBERTa</li>\n<li>DistilBERT</li>\n<li>ALBERT</li>\n<li>T5</li>\n<li>XLM-RoBERTa</li>\n</ul>\n<hr>\n<h4><span id=\"1-bert-及其变体\"> 1 BERT 及其变体</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">bert-base-uncased</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">110M</td>\n<td style=\"text-align:center\">小写英文文本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bert-large-uncased</td>\n<td style=\"text-align:center\">24</td>\n<td style=\"text-align:center\">1024</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">340M</td>\n<td style=\"text-align:center\">小写英文文本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bert-base-cased</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">110M</td>\n<td style=\"text-align:center\">不区分大小写的英文文本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bert-large-cased</td>\n<td style=\"text-align:center\">24</td>\n<td style=\"text-align:center\">1024</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">340M</td>\n<td style=\"text-align:center\">不区分大小写的英文文本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bert-base-multilingual-uncased</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">110M</td>\n<td style=\"text-align:center\">小写的 102 种语言文本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bert-large-multilingual-uncased</td>\n<td style=\"text-align:center\">24</td>\n<td style=\"text-align:center\">1024</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">340M</td>\n<td style=\"text-align:center\">小写的 102 种语言文本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">bert-base-chinese</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">110M</td>\n<td style=\"text-align:center\">简体和繁体中文文本</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>bert-base-uncased</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在小写的英文文本上进行训练而得到.</li>\n<li><code>bert-large-uncased</code> : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 340M 参数量，在小写的英文文本上进行训练而得到.</li>\n<li><code>bert-base-cased</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在不区分大小写的英文文本上进行训练而得到.</li>\n<li><code>bert-large-cased</code> : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 340M 参数量，在不区分大小写的英文文本上进行训练而得到.</li>\n<li><code>bert-base-multilingual-uncased</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在小写的 102 种语言文本上进行训练而得到.</li>\n<li><code>bert-large-multilingual-uncased</code> : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 340M 参数量，在小写的 102 种语言文本上进行训练而得到.</li>\n<li><code>bert-base-chinese</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在简体和繁体中文文本上进行训练而得到.</li>\n</ul>\n<hr>\n<h4><span id=\"2-gpt\"> 2 GPT</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">openai-gpt</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">110M</td>\n<td style=\"text-align:center\">英文语料</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>openai-gpt</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，由 OpenAI 在英文语料上进行训练而得到.</li>\n</ul>\n<hr>\n<h4><span id=\"3-gpt-2-及其变体\"> 3 GPT-2 及其变体</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">gpt2</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">117M</td>\n<td style=\"text-align:center\">GPT-2 英文语料</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">gpt2-xl</td>\n<td style=\"text-align:center\">48</td>\n<td style=\"text-align:center\">1600</td>\n<td style=\"text-align:center\">25</td>\n<td style=\"text-align:center\">1558M</td>\n<td style=\"text-align:center\">GPT-2 英文语料</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>gpt2</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 117M 参数量，在 OpenAI GPT-2 英文语料上进行训练而得到.</li>\n<li><code>gpt2-xl</code> : 编码器具有 48 个隐层，输出 1600 维张量，25 个自注意力头，共 1558M 参数量，在大型的 OpenAI GPT-2 英文语料上进行训练而得到.</li>\n</ul>\n<hr>\n<h4><span id=\"4-transformer-xl\"> 4 Transformer-XL</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">transfo-xl-wt103</td>\n<td style=\"text-align:center\">18</td>\n<td style=\"text-align:center\">1024</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">257M</td>\n<td style=\"text-align:center\">wikitext-103 英文语料</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>transfo-xl-wt103</code> : 编码器具有 18 个隐层，输出 1024 维张量，16 个自注意力头，共 257M 参数量，在 wikitext-103 英文语料进行训练而得到.</li>\n</ul>\n<hr>\n<h4><span id=\"5-xlnet-及其变体\"> 5 XLNet 及其变体</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">xlnet-base-cased</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">110M</td>\n<td style=\"text-align:center\">英文语料</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">xlnet-large-cased</td>\n<td style=\"text-align:center\">24</td>\n<td style=\"text-align:center\">1024</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">240M</td>\n<td style=\"text-align:center\">英文语料</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>xlnet-base-cased</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 110M 参数量，在英文语料上进行训练而得到.</li>\n<li><code>xlnet-large-cased</code> : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 240 参数量，在英文语料上进行训练而得到.</li>\n</ul>\n<hr>\n<h4><span id=\"6-xlm\"> 6 XLM</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">xlm-mlm-en-2048</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">2048</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">/</td>\n<td style=\"text-align:center\">英文语料</td>\n</tr>\n</tbody>\n</table>\n<p><code>xlm-mlm-en-2048</code> : 编码器具有 12 个隐层，输出 2048 维张量，16 个自注意力头，在英文文本上进行训练而得到.</p>\n<hr>\n<h4><span id=\"7-roberta-及其变体\"> 7 RoBERTa 及其变体</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">roberta-base</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">125M</td>\n<td style=\"text-align:center\">英文文本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">roberta-large</td>\n<td style=\"text-align:center\">24</td>\n<td style=\"text-align:center\">1024</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">355M</td>\n<td style=\"text-align:center\">英文文本</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>roberta-base</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 125M 参数量，在英文文本上进行训练而得到.</li>\n<li><code>roberta-large</code> : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 355M 参数量，在英文文本上进行训练而得到.</li>\n</ul>\n<hr>\n<h4><span id=\"8-distilbert-及其变体\"> 8 DistilBERT 及其变体</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">distilbert-base-uncased6</td>\n<td style=\"text-align:center\">6</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">66M</td>\n<td style=\"text-align:center\">/</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">distilbert-base-multilingual-cased</td>\n<td style=\"text-align:center\">6</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">66M</td>\n<td style=\"text-align:center\">/</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>distilbert-base-uncased</code> : 基于 bert-base-uncased 的蒸馏 (压缩) 模型，编码器具有 6 个隐层，输出 768 维张量，12 个自注意力头，共 66M 参数量.</li>\n<li><code>distilbert-base-multilingual-cased</code> : 基于 bert-base-multilingual-uncased 的蒸馏 (压缩) 模型，编码器具有 6 个隐层，输出 768 维张量，12 个自注意力头，共 66M 参数量.</li>\n</ul>\n<hr>\n<h4><span id=\"9-albert\"> 9 ALBERT</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">albert-base-v1</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">125M</td>\n<td style=\"text-align:center\">英文文本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">albert-base-v2</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">125M</td>\n<td style=\"text-align:center\">英文文本</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>albert-base-v1</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 125M 参数量，在英文文本上进行训练而得到.</li>\n<li><code>albert-base-v2</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 125M 参数量，在英文文本上进行训练而得到，相比 v1 使用了更多的数据量，花费更长的训练时间.</li>\n</ul>\n<hr>\n<h4><span id=\"10-t5-及其变体\"> 10 T5 及其变体</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">t5-small</td>\n<td style=\"text-align:center\">6</td>\n<td style=\"text-align:center\">512</td>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">60M</td>\n<td style=\"text-align:center\">C4 语料</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">t5-base</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">220M</td>\n<td style=\"text-align:center\">C4 语料</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">t5-large</td>\n<td style=\"text-align:center\">24</td>\n<td style=\"text-align:center\">1024</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">770M</td>\n<td style=\"text-align:center\">C4 语料</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>t5-small</code> : 编码器具有 6 个隐层，输出 512 维张量，8 个自注意力头，共 60M 参数量，在 C4 语料上进行训练而得到.</li>\n<li><code>t5-base</code> : 编码器具有 12 个隐层，输出 768 维张量，12 个自注意力头，共 220M 参数量，在 C4 语料上进行训练而得到.</li>\n<li><code>t5-large</code> : 编码器具有 24 个隐层，输出 1024 维张量，16 个自注意力头，共 770M 参数量，在 C4 语料上进行训练而得到.</li>\n</ul>\n<hr>\n<h4><span id=\"11-xlm-roberta-及其变体\"> 11 XLM-RoBERTa 及其变体</span></h4>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">模型名称</th>\n<th style=\"text-align:center\">隐层数</th>\n<th style=\"text-align:center\">张量维度</th>\n<th style=\"text-align:center\">自注意力头数</th>\n<th style=\"text-align:center\">参数量</th>\n<th style=\"text-align:center\">训练语料</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">xlm-roberta-base</td>\n<td style=\"text-align:center\">12</td>\n<td style=\"text-align:center\">768</td>\n<td style=\"text-align:center\">8</td>\n<td style=\"text-align:center\">125M</td>\n<td style=\"text-align:center\">2.5TB 的 100 种语言文本</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">xlm-roberta-large</td>\n<td style=\"text-align:center\">24</td>\n<td style=\"text-align:center\">1027</td>\n<td style=\"text-align:center\">16</td>\n<td style=\"text-align:center\">355M</td>\n<td style=\"text-align:center\">2.5TB 的 100 种语言文本</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li><code>xlm-roberta-base</code> : 编码器具有 12 个隐层，输出 768 维张量，8 个自注意力头，共 125M 参数量，在 2.5TB 的 100 种语言文本上进行训练而得到.</li>\n<li><code>xlm-roberta-large</code> : 编码器具有 24 个隐层，输出 1027 维张量，16 个自注意力头，共 355M 参数量，在 2.5TB 的 100 种语言文本上进行训练而得到.</li>\n</ul>\n<hr>\n<p>预训练模型说明:</p>\n<ul>\n<li>所有上述预训练模型及其变体都是以 transformer 为基础，只是在模型结构如神经元连接方式，编码器隐层数，多头注意力的头数等发生改变，这些改变方式的大部分依据都是由在标准数据集上的表现而定，因此，对于我们使用者而言，不需要从理论上深度探究这些预训练模型的结构设计的优劣，只需要在自己处理的目标数据上，尽量遍历所有可用的模型对比得到最优效果即可.</li>\n</ul>\n",
            "tags": [
                "人工智能",
                "NLP"
            ]
        },
        {
            "id": "https://leezhao415.github.io/2021/06/24/NLP%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%A6%E8%A7%A3/",
            "url": "https://leezhao415.github.io/2021/06/24/NLP%E4%B9%8B%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%A6%E8%A7%A3/",
            "title": "NLP之常用数据集详解",
            "date_published": "2021-06-24T15:18:54.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<p>GLUE 数据集合的介绍:</p>\n<ul>\n<li>自然语言处理（NLP）主要自然语言理解（NLU）和自然语言生成（NLG）。GLUE（General Language Understanding Evaluation）由纽约大学，华盛顿大学，Google 联合推出，涵盖不同 NLP 任务类型，截止至 2020 年 1 月其中包括 11 个子任务数据集，成为衡量 NLP 研究发展的衡量标准.</li>\n<li>GLUE 九项任务涉及到自然语言推断、文本蕴含、情感分析、语义相似等多个任务。像 BERT、XLNet、RoBERTa、ERINE、T5 等知名模型都会在此基准上进行测试。</li>\n</ul>\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#glue%E6%95%B0%E6%8D%AE%E9%9B%86%E5%90%88%E5%8C%85%E5%90%AB%E4%BB%A5%E4%B8%8B%E6%95%B0%E6%8D%AE%E9%9B%86\">GLUE 数据集合包含以下数据集</a></li>\n<li><a href=\"#glue%E6%95%B0%E6%8D%AE%E9%9B%86%E5%90%88\">GLUE 数据集合</a>\n<ul>\n<li><a href=\"#1-cola%E6%95%B0%E6%8D%AE%E9%9B%86\">1 CoLA 数据集</a></li>\n<li><a href=\"#2-sst-2%E6%95%B0%E6%8D%AE%E9%9B%86\">2 SST-2 数据集</a></li>\n<li><a href=\"#3-mrpc%E6%95%B0%E6%8D%AE%E9%9B%86\">3 MRPC 数据集</a></li>\n<li><a href=\"#4-sts-b%E6%95%B0%E6%8D%AE%E9%9B%86\">4 STS-B 数据集</a></li>\n<li><a href=\"#5-qqp%E6%95%B0%E6%8D%AE%E9%9B%86\">5 QQP 数据集</a></li>\n<li><a href=\"#6-mnlisnli%E6%95%B0%E6%8D%AE%E9%9B%86\">6 (MNLI/SNLI) 数据集</a></li>\n<li><a href=\"#7-qnlirtewnli%E6%95%B0%E6%8D%AE%E9%9B%86\">7 (QNLI/RTE/WNLI) 数据集</a></li>\n</ul>\n</li>\n</ul>\n<!-- tocstop -->\n<hr>\n<h3><span id=\"glue-数据集合包含以下数据集\"> GLUE 数据集合包含以下数据集</span></h3>\n<ul>\n<li>CoLA 数据集</li>\n<li>SST-2 数据集</li>\n<li>MRPC 数据集</li>\n<li>STS-B 数据集</li>\n<li>QQP 数据集</li>\n<li>MNLI 数据集</li>\n<li>SNLI 数据集</li>\n<li>QNLI 数据集</li>\n<li>RTE 数据集</li>\n<li>WNLI 数据集</li>\n<li>diagnostics 数据集 (官方未完善)</li>\n</ul>\n<hr>\n<ul>\n<li>GLUE 数据集合的下载方式:</li>\n</ul>\n<p>下载脚本代码:<a href=\"https://gluebenchmark.com/\">https://gluebenchmark.com/</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27; Script for downloading all GLUE data.&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"><span class=\"keyword\">import</span> shutil</span><br><span class=\"line\"><span class=\"keyword\">import</span> argparse</span><br><span class=\"line\"><span class=\"keyword\">import</span> tempfile</span><br><span class=\"line\"><span class=\"keyword\">import</span> urllib.request</span><br><span class=\"line\"><span class=\"keyword\">import</span> zipfile</span><br><span class=\"line\"></span><br><span class=\"line\">TASKS = [<span class=\"string\">&quot;CoLA&quot;</span>, <span class=\"string\">&quot;SST&quot;</span>, <span class=\"string\">&quot;MRPC&quot;</span>, <span class=\"string\">&quot;QQP&quot;</span>, <span class=\"string\">&quot;STS&quot;</span>, <span class=\"string\">&quot;MNLI&quot;</span>, <span class=\"string\">&quot;SNLI&quot;</span>, <span class=\"string\">&quot;QNLI&quot;</span>, <span class=\"string\">&quot;RTE&quot;</span>, <span class=\"string\">&quot;WNLI&quot;</span>, <span class=\"string\">&quot;diagnostic&quot;</span>]</span><br><span class=\"line\">TASK2PATH = &#123;<span class=\"string\">&quot;CoLA&quot;</span>:<span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&amp;token=46d5e637-3411-4188-bc44-5809b5bfb5f4&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;SST&quot;</span>:<span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&amp;token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;MRPC&quot;</span>:<span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&amp;token=ec5c0836-31d5-48f4-b431-7480817f1adc&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;QQP&quot;</span>:<span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&amp;token=700c6acf-160d-4d89-81d1-de4191d02cb5&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;STS&quot;</span>:<span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&amp;token=bddb94a7-8706-4e0d-a694-1109e12273b5&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;MNLI&quot;</span>:<span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&amp;token=50329ea1-e339-40e2-809c-10c40afff3ce&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;SNLI&quot;</span>:<span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&amp;token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;QNLI&quot;</span>: <span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&amp;token=6fdcf570-0fc5-4631-8456-9505272d1601&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;RTE&quot;</span>:<span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&amp;token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;WNLI&quot;</span>:<span class=\"string\">&#x27;https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&amp;token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf&#x27;</span>,</span><br><span class=\"line\">             <span class=\"string\">&quot;diagnostic&quot;</span>:<span class=\"string\">&#x27;https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&amp;Expires=2498860800&amp;Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">MRPC_TRAIN = <span class=\"string\">&#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt&#x27;</span></span><br><span class=\"line\">MRPC_TEST = <span class=\"string\">&#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">download_and_extract</span>(<span class=\"params\">task, data_dir</span>):</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Downloading and extracting %s...&quot;</span> % task)</span><br><span class=\"line\">    data_file = <span class=\"string\">&quot;%s.zip&quot;</span> % task</span><br><span class=\"line\">    urllib.request.urlretrieve(TASK2PATH[task], data_file)</span><br><span class=\"line\">    <span class=\"keyword\">with</span> zipfile.ZipFile(data_file) <span class=\"keyword\">as</span> zip_ref:</span><br><span class=\"line\">        zip_ref.extractall(data_dir)</span><br><span class=\"line\">    os.remove(data_file)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\tCompleted!&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">format_mrpc</span>(<span class=\"params\">data_dir, path_to_data</span>):</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Processing MRPC...&quot;</span>)</span><br><span class=\"line\">    mrpc_dir = os.path.join(data_dir, <span class=\"string\">&quot;MRPC&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.isdir(mrpc_dir):</span><br><span class=\"line\">        os.mkdir(mrpc_dir)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> path_to_data:</span><br><span class=\"line\">        mrpc_train_file = os.path.join(path_to_data, <span class=\"string\">&quot;msr_paraphrase_train.txt&quot;</span>)</span><br><span class=\"line\">        mrpc_test_file = os.path.join(path_to_data, <span class=\"string\">&quot;msr_paraphrase_test.txt&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Local MRPC data not specified, downloading data from %s&quot;</span> % MRPC_TRAIN)</span><br><span class=\"line\">        mrpc_train_file = os.path.join(mrpc_dir, <span class=\"string\">&quot;msr_paraphrase_train.txt&quot;</span>)</span><br><span class=\"line\">        mrpc_test_file = os.path.join(mrpc_dir, <span class=\"string\">&quot;msr_paraphrase_test.txt&quot;</span>)</span><br><span class=\"line\">        urllib.request.urlretrieve(MRPC_TRAIN, mrpc_train_file)</span><br><span class=\"line\">        urllib.request.urlretrieve(MRPC_TEST, mrpc_test_file)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> os.path.isfile(mrpc_train_file), <span class=\"string\">&quot;Train data not found at %s&quot;</span> % mrpc_train_file</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> os.path.isfile(mrpc_test_file), <span class=\"string\">&quot;Test data not found at %s&quot;</span> % mrpc_test_file</span><br><span class=\"line\">    urllib.request.urlretrieve(TASK2PATH[<span class=\"string\">&quot;MRPC&quot;</span>], os.path.join(mrpc_dir, <span class=\"string\">&quot;dev_ids.tsv&quot;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    dev_ids = []</span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(os.path.join(mrpc_dir, <span class=\"string\">&quot;dev_ids.tsv&quot;</span>), encoding=<span class=\"string\">&quot;utf8&quot;</span>) <span class=\"keyword\">as</span> ids_fh:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> ids_fh:</span><br><span class=\"line\">            dev_ids.append(row.strip().split(<span class=\"string\">&#x27;\\t&#x27;</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(mrpc_train_file, encoding=<span class=\"string\">&quot;utf8&quot;</span>) <span class=\"keyword\">as</span> data_fh, \\</span><br><span class=\"line\">         <span class=\"built_in\">open</span>(os.path.join(mrpc_dir, <span class=\"string\">&quot;train.tsv&quot;</span>), <span class=\"string\">&#x27;w&#x27;</span>, encoding=<span class=\"string\">&quot;utf8&quot;</span>) <span class=\"keyword\">as</span> train_fh, \\</span><br><span class=\"line\">         <span class=\"built_in\">open</span>(os.path.join(mrpc_dir, <span class=\"string\">&quot;dev.tsv&quot;</span>), <span class=\"string\">&#x27;w&#x27;</span>, encoding=<span class=\"string\">&quot;utf8&quot;</span>) <span class=\"keyword\">as</span> dev_fh:</span><br><span class=\"line\">        header = data_fh.readline()</span><br><span class=\"line\">        train_fh.write(header)</span><br><span class=\"line\">        dev_fh.write(header)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> row <span class=\"keyword\">in</span> data_fh:</span><br><span class=\"line\">            label, id1, id2, s1, s2 = row.strip().split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> [id1, id2] <span class=\"keyword\">in</span> dev_ids:</span><br><span class=\"line\">                dev_fh.write(<span class=\"string\">&quot;%s\\t%s\\t%s\\t%s\\t%s\\n&quot;</span> % (label, id1, id2, s1, s2))</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                train_fh.write(<span class=\"string\">&quot;%s\\t%s\\t%s\\t%s\\t%s\\n&quot;</span> % (label, id1, id2, s1, s2))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> <span class=\"built_in\">open</span>(mrpc_test_file, encoding=<span class=\"string\">&quot;utf8&quot;</span>) <span class=\"keyword\">as</span> data_fh, \\</span><br><span class=\"line\">            <span class=\"built_in\">open</span>(os.path.join(mrpc_dir, <span class=\"string\">&quot;test.tsv&quot;</span>), <span class=\"string\">&#x27;w&#x27;</span>, encoding=<span class=\"string\">&quot;utf8&quot;</span>) <span class=\"keyword\">as</span> test_fh:</span><br><span class=\"line\">        header = data_fh.readline()</span><br><span class=\"line\">        test_fh.write(<span class=\"string\">&quot;index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> idx, row <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(data_fh):</span><br><span class=\"line\">            label, id1, id2, s1, s2 = row.strip().split(<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">            test_fh.write(<span class=\"string\">&quot;%d\\t%s\\t%s\\t%s\\t%s\\n&quot;</span> % (idx, id1, id2, s1, s2))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\tCompleted!&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">download_diagnostic</span>(<span class=\"params\">data_dir</span>):</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;Downloading and extracting diagnostic...&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.isdir(os.path.join(data_dir, <span class=\"string\">&quot;diagnostic&quot;</span>)):</span><br><span class=\"line\">        os.mkdir(os.path.join(data_dir, <span class=\"string\">&quot;diagnostic&quot;</span>))</span><br><span class=\"line\">    data_file = os.path.join(data_dir, <span class=\"string\">&quot;diagnostic&quot;</span>, <span class=\"string\">&quot;diagnostic.tsv&quot;</span>)</span><br><span class=\"line\">    urllib.request.urlretrieve(TASK2PATH[<span class=\"string\">&quot;diagnostic&quot;</span>], data_file)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\tCompleted!&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_tasks</span>(<span class=\"params\">task_names</span>):</span></span><br><span class=\"line\">    task_names = task_names.split(<span class=\"string\">&#x27;,&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"string\">&quot;all&quot;</span> <span class=\"keyword\">in</span> task_names:</span><br><span class=\"line\">        tasks = TASKS</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        tasks = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> task_name <span class=\"keyword\">in</span> task_names:</span><br><span class=\"line\">            <span class=\"keyword\">assert</span> task_name <span class=\"keyword\">in</span> TASKS, <span class=\"string\">&quot;Task %s not found!&quot;</span> % task_name</span><br><span class=\"line\">            tasks.append(task_name)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> tasks</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span>(<span class=\"params\">arguments</span>):</span></span><br><span class=\"line\">    parser = argparse.ArgumentParser()</span><br><span class=\"line\">    parser.add_argument(<span class=\"string\">&#x27;--data_dir&#x27;</span>, <span class=\"built_in\">help</span>=<span class=\"string\">&#x27;directory to save data to&#x27;</span>, <span class=\"built_in\">type</span>=<span class=\"built_in\">str</span>, default=<span class=\"string\">&#x27;glue_data&#x27;</span>)</span><br><span class=\"line\">    parser.add_argument(<span class=\"string\">&#x27;--tasks&#x27;</span>, <span class=\"built_in\">help</span>=<span class=\"string\">&#x27;tasks to download data for as a comma separated string&#x27;</span>,</span><br><span class=\"line\">                        <span class=\"built_in\">type</span>=<span class=\"built_in\">str</span>, default=<span class=\"string\">&#x27;all&#x27;</span>)</span><br><span class=\"line\">    parser.add_argument(<span class=\"string\">&#x27;--path_to_mrpc&#x27;</span>, <span class=\"built_in\">help</span>=<span class=\"string\">&#x27;path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt&#x27;</span>,</span><br><span class=\"line\">                        <span class=\"built_in\">type</span>=<span class=\"built_in\">str</span>, default=<span class=\"string\">&#x27;&#x27;</span>)</span><br><span class=\"line\">    args = parser.parse_args(arguments)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> os.path.isdir(args.data_dir):</span><br><span class=\"line\">        os.mkdir(args.data_dir)</span><br><span class=\"line\">    tasks = get_tasks(args.tasks)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> task <span class=\"keyword\">in</span> tasks:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> task == <span class=\"string\">&#x27;MRPC&#x27;</span>:</span><br><span class=\"line\">            format_mrpc(args.data_dir, args.path_to_mrpc)</span><br><span class=\"line\">        <span class=\"keyword\">elif</span> task == <span class=\"string\">&#x27;diagnostic&#x27;</span>:</span><br><span class=\"line\">            download_diagnostic(args.data_dir)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            download_and_extract(task, args.data_dir)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&#x27;__main__&#x27;</span>:</span><br><span class=\"line\">    sys.exit(main(sys.argv[<span class=\"number\">1</span>:]))</span><br></pre></td></tr></table></figure>\n<hr>\n<p>运行脚本下载所有数据集:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 假设你已经将以上代码copy到download_glue_data.py文件中</span></span><br><span class=\"line\"><span class=\"comment\"># 运行这个python脚本, 你将同目录下得到一个glue文件夹</span></span><br><span class=\"line\">python download_glue_data.py</span><br></pre></td></tr></table></figure>\n<hr>\n<p>输出效果:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting CoLA...</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting SST...</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Processing MRPC...</span><br><span class=\"line\">Local MRPC data <span class=\"keyword\">not</span> specified, downloading data <span class=\"keyword\">from</span> https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting QQP...</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting STS...</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting MNLI...</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting SNLI...</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting QNLI...</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting RTE...</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting WNLI...</span><br><span class=\"line\">    Completed!</span><br><span class=\"line\">Downloading <span class=\"keyword\">and</span> extracting diagnostic...</span><br><span class=\"line\">    Completed!</span><br></pre></td></tr></table></figure>\n<hr>\n<h3><span id=\"glue-数据集合\"> GLUE 数据集合</span></h3>\n<h4><span id=\"1-cola-数据集\"> 1 CoLA 数据集</span></h4>\n<p>CoLA (The Corpus of Linguistic Acceptability，语言可接受性语料库)，单句子分类任务，语料来自语言理论的书籍和期刊，每个句子被标注为是否合乎语法的单词序列。本任务是一个二分类任务，标签共两个，分别是 0 和 1，其中 0 表示不合乎语法，1 表示合乎语法。</p>\n<p>样本个数：训练集 8, 551 个，开发集 1, 043 个，测试集 1, 063 个。</p>\n<p>任务：可接受程度，合乎语法与不合乎语法二分类。</p>\n<p><strong>文件样式</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- CoLA/</span><br><span class=\"line\">    - dev.tsv  </span><br><span class=\"line\">    - original/</span><br><span class=\"line\">    - test.tsv  </span><br><span class=\"line\">    - train.tsv</span><br></pre></td></tr></table></figure>\n<hr>\n<p>文件样式说明:</p>\n<ul>\n<li>在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据.</li>\n</ul>\n<hr>\n<p>train.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\">gj04    1       She coughed herself awake as the leaf landed on her nose.</span><br><span class=\"line\">gj04    1       The worm wriggled onto the carpet.</span><br><span class=\"line\">gj04    1       The chocolate melted onto the carpet.</span><br><span class=\"line\">gj04    0   *   The ball wriggled itself loose.</span><br><span class=\"line\">gj04    1       Bill wriggled himself loose.</span><br><span class=\"line\">bc01    1       The sinking of the ship to collect the insurance was very devious.</span><br><span class=\"line\">bc01    1       The ship<span class=\"string\">&#x27;s sinking was very devious.</span></span><br><span class=\"line\"><span class=\"string\">bc01    0   *   The ship&#x27;</span>s sinking to collect the insurance was very devious.</span><br><span class=\"line\">bc01    1       The testing of such drugs on oneself is too risky.</span><br><span class=\"line\">bc01    0   *   This drug<span class=\"string\">&#x27;s testing on oneself is too risky.</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br></pre></td></tr></table></figure>\n<hr>\n<p>train.tsv 数据样式说明:</p>\n<ul>\n<li>train.tsv 中的数据内容共分为 4 列，第一列数据，如 gj04, bc01 等代表每条文本数据的来源即出版物代号；第二列数据，0 或 1, 代表每条文本数据的语法是否正确，0 代表不正确，1 代表正确；第三列数据，'<em>’, 是作者最初的正负样本标记，与第二列意义相同，'</em>' 表示不正确；第四列即是被标注的语法使用是否正确的文本句子.</li>\n</ul>\n<hr>\n<p>test.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   sentence</span><br><span class=\"line\">0   Bill whistled past the house.</span><br><span class=\"line\">1   The car honked its way down the road.</span><br><span class=\"line\">2   Bill pushed Harry off the sofa.</span><br><span class=\"line\">3   the kittens yawned awake and played.</span><br><span class=\"line\">4   I demand that the more John eats, the more he pay.</span><br><span class=\"line\">5   If John eats more, keep your mouth shut tighter, OK?</span><br><span class=\"line\">6   His expectations are always lower than mine are.</span><br><span class=\"line\">7   The sooner you call, the more carefully I will word the letter.</span><br><span class=\"line\">8   The more timid he feels, the more people he interviews without asking questions of.</span><br><span class=\"line\">9   Once Janet left, Fred became a lot crazier.</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<hr>\n<p>test.tsv 数据样式说明:</p>\n<ul>\n<li>test.tsv 中的数据内容共分为 2 列，第一列数据代表每条文本数据的索引；第二列数据代表用于测试的句子.</li>\n</ul>\n<hr>\n<p>CoLA 数据集的任务类型:</p>\n<ul>\n<li>二分类任务</li>\n<li>评估指标为: MCC (马修斯相关系数，在正负样本分布十分不均衡的情况下使用的二分类评估指标)</li>\n</ul>\n<hr>\n<h4><span id=\"2-sst-2-数据集\"> 2 SST-2 数据集</span></h4>\n<p>SST-2 (The Stanford Sentiment Treebank，斯坦福情感树库)，单句子分类任务，包含电影评论中的句子和它们情感的人类注释。这项任务是给定句子的情感，类别分为两类正面情感（positive，样本标签对应为 1）和负面情感（negative，样本标签对应为 0），并且只用句子级别的标签。也就是，本任务也是一个二分类任务，针对句子级别，分为正面和负面情感。</p>\n<p>样本个数：训练集 67, 350 个，开发集 873 个，测试集 1, 821 个。</p>\n<p>任务：情感分类，正面情感和负面情感二分类。</p>\n<p>评价准则：accuracy。</p>\n<p><strong>文件样式</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- SST-2/</span><br><span class=\"line\">        - dev.tsv</span><br><span class=\"line\">        - original/</span><br><span class=\"line\">        - test.tsv</span><br><span class=\"line\">        - train.tsv</span><br></pre></td></tr></table></figure>\n<hr>\n<p>文件样式说明:</p>\n<ul>\n<li>在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据.</li>\n</ul>\n<hr>\n<p>train.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sentence    label</span><br><span class=\"line\">hide new secretions from the parental units     0</span><br><span class=\"line\">contains no wit , only labored gags     0</span><br><span class=\"line\">that loves its characters and communicates something rather beautiful about human nature    1</span><br><span class=\"line\">remains utterly satisfied to remain the same throughout     0</span><br><span class=\"line\">on the worst revenge-of-the-nerds clichés the filmmakers could dredge up    0</span><br><span class=\"line\">that <span class=\"string\">&#x27;s far too tragic to merit such superficial treatment  0</span></span><br><span class=\"line\"><span class=\"string\">demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .    1</span></span><br><span class=\"line\"><span class=\"string\">of saucy    1</span></span><br><span class=\"line\"><span class=\"string\">a depressed fifteen-year-old &#x27;</span>s suicidal poetry     0</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<hr>\n<p>train.tsv 数据样式说明:</p>\n<ul>\n<li>train.tsv 中的数据内容共分为 2 列，第一列数据代表具有感情色彩的评论文本；第二列数据，0 或 1, 代表每条文本数据是积极或者消极的评论，0 代表消极，1 代表积极.</li>\n</ul>\n<hr>\n<p>test.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   sentence</span><br><span class=\"line\">0   uneasy mishmash of styles and genres .</span><br><span class=\"line\">1   this film <span class=\"string\">&#x27;s relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .</span></span><br><span class=\"line\"><span class=\"string\">2   by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .</span></span><br><span class=\"line\"><span class=\"string\">3   director rob marshall went out gunning to make a great one .</span></span><br><span class=\"line\"><span class=\"string\">4   lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new .</span></span><br><span class=\"line\"><span class=\"string\">5   a well-made and often lovely depiction of the mysteries of friendship .</span></span><br><span class=\"line\"><span class=\"string\">6   none of this violates the letter of behan &#x27;</span>s book , but missing is its spirit , its ribald , full-throated humor .</span><br><span class=\"line\">7   although it bangs a very cliched drum at <span class=\"built_in\">times</span> , this crowd-pleaser <span class=\"string\">&#x27;s fresh dialogue , energetic music , and good-natured spunk are often infectious .</span></span><br><span class=\"line\"><span class=\"string\">8   it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another .</span></span><br><span class=\"line\"><span class=\"string\">9   this is junk food cinema at its greasiest .</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br></pre></td></tr></table></figure>\n<hr>\n<p>test.tsv 数据样式说明: * test.tsv 中的数据内容共分为 2 列，第一列数据代表每条文本数据的索引；第二列数据代表用于测试的句子.</p>\n<hr>\n<p>SST-2 数据集的任务类型:</p>\n<ul>\n<li>二分类任务</li>\n<li>评估指标为: ACC</li>\n</ul>\n<hr>\n<h4><span id=\"3-mrpc-数据集\"> 3 MRPC 数据集</span></h4>\n<p>MRPC (The Microsoft Research Paraphrase Corpus，微软研究院释义语料库)，相似性和释义任务，是从在线新闻源中自动抽取句子对语料库，并人工注释句子对中的句子是否在语义上等效。类别并不平衡，其中 68% 的正样本，所以遵循常规的做法，报告准确率（accuracy）和 F1 值。</p>\n<p>样本个数：训练集 3, 668 个，开发集 408 个，测试集 1, 725 个。</p>\n<p>任务：是否释义二分类，是释义，不是释义两类。</p>\n<p>评价准则：准确率（accuracy）和 F1 值。</p>\n<p><strong>文件样式</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- MRPC/</span><br><span class=\"line\">        - dev.tsv</span><br><span class=\"line\">        - test.tsv</span><br><span class=\"line\">        - train.tsv</span><br><span class=\"line\">    - dev_ids.tsv</span><br><span class=\"line\">    - msr_paraphrase_test.txt</span><br><span class=\"line\">    - msr_paraphrase_train.txt</span><br></pre></td></tr></table></figure>\n<hr>\n<p>文件样式说明:</p>\n<ul>\n<li>在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据.</li>\n</ul>\n<hr>\n<p>train.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Quality <span class=\"comment\">#1 ID   #2 ID   #1 String   #2 String</span></span><br><span class=\"line\">1   702876  702977  Amrozi accused his brother , whom he called <span class=\"string\">&quot; the witness &quot;</span> , of deliberately distorting his evidence . Referring to him as only <span class=\"string\">&quot; the witness &quot;</span> , Amrozi accused his brother of deliberately distorting his evidence .</span><br><span class=\"line\">0   2108705 2108831 Yucaipa owned Dominick <span class=\"string\">&#x27;s before selling the chain to Safeway in 1998 for $ 2.5 billion .   Yucaipa bought Dominick &#x27;</span>s <span class=\"keyword\">in</span> 1995 <span class=\"keyword\">for</span> $ 693 million and sold it to Safeway <span class=\"keyword\">for</span> $ 1.8 billion <span class=\"keyword\">in</span> 1998 .</span><br><span class=\"line\">1   1330381 1330521 They had published an advertisement on the Internet on June 10 , offering the cargo <span class=\"keyword\">for</span> sale , he added .   On June 10 , the ship <span class=\"string\">&#x27;s owners had published an advertisement on the Internet , offering the explosives for sale .</span></span><br><span class=\"line\"><span class=\"string\">0   3344667 3344648 Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 . Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .</span></span><br><span class=\"line\"><span class=\"string\">1   1236820 1236712 The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .   PG &amp; E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .</span></span><br><span class=\"line\"><span class=\"string\">1   738533  737951  Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier .   With the scandal hanging over Stewart &#x27;</span>s company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .</span><br><span class=\"line\">0   264589  264502  The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .    The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .</span><br><span class=\"line\">1   579975  579810  The DVD-CCA <span class=\"keyword\">then</span> appealed to the state Supreme Court .  The DVD CCA appealed that decision to the U.S. Supreme Court .</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<hr>\n<p>train.tsv 数据样式说明:</p>\n<ul>\n<li>train.tsv 中的数据内容共分为 5 列，第一列数据，0 或 1, 代表每对句子是否具有相同的含义，0 代表含义不相同，1 代表含义相同。第二列和第三列分别代表每对句子的 id, 第四列和第五列分别具有相同 / 不同含义的句子对.</li>\n</ul>\n<hr>\n<p>test.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   <span class=\"comment\">#1 ID   #2 ID   #1 String   #2 String</span></span><br><span class=\"line\">0   1089874 1089925 PCCW <span class=\"string\">&#x27;s chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So . Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .</span></span><br><span class=\"line\"><span class=\"string\">1   3019446 3019327 The world &#x27;</span>s two largest automakers said their U.S. sales declined more than predicted last month as a late summer sales frenzy caused more of an industry backlash than expected . Domestic sales at both GM and No. 2 Ford Motor Co. declined more than predicted as a late summer sales frenzy prompted a larger-than-expected industry backlash .</span><br><span class=\"line\">2   1945605 1945824 According to the federal Centers <span class=\"keyword\">for</span> Disease Control and Prevention ( news - web sites ) , there were 19 reported cases of measles <span class=\"keyword\">in</span> the United States <span class=\"keyword\">in</span> 2002 .   The Centers <span class=\"keyword\">for</span> Disease Control and Prevention said there were 19 reported cases of measles <span class=\"keyword\">in</span> the United States <span class=\"keyword\">in</span> 2002 .</span><br><span class=\"line\">3   1430402 1430329 A tropical storm rapidly developed <span class=\"keyword\">in</span> the Gulf of Mexico Sunday and was expected to hit somewhere along the Texas or Louisiana coasts by Monday night . A tropical storm rapidly developed <span class=\"keyword\">in</span> the Gulf of Mexico on Sunday and could have hurricane-force winds when it hits land somewhere along the Louisiana coast Monday night .</span><br><span class=\"line\">4   3354381 3354396 The company didn <span class=\"string\">&#x27;t detail the costs of the replacement and repairs .   But company officials expect the costs of the replacement work to run into the millions of dollars .</span></span><br><span class=\"line\"><span class=\"string\">5   1390995 1391183 The settling companies would also assign their possible claims against the underwriters to the investor plaintiffs , he added . Under the agreement , the settling companies will also assign their potential claims against the underwriters to the investors , he added .</span></span><br><span class=\"line\"><span class=\"string\">6   2201401 2201285 Air Commodore Quaife said the Hornets remained on three-minute alert throughout the operation . Air Commodore John Quaife said the security operation was unprecedented .</span></span><br><span class=\"line\"><span class=\"string\">7   2453843 2453998 A Washington County man may have the countys first human case of West Nile virus , the health department said Friday .  The countys first and only human case of West Nile this year was confirmed by health officials on Sept . 8 .</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br></pre></td></tr></table></figure>\n<hr>\n<p>test.tsv 数据样式说明: * test.tsv 中的数据内容共分为 5 列，第一列数据代表每条文本数据的索引；其余列的含义与 train.tsv 中相同.</p>\n<hr>\n<p>MRPC 数据集的任务类型:</p>\n<ul>\n<li>句子对二分类任务</li>\n<li>评估指标为: ACC 和 F1</li>\n</ul>\n<hr>\n<h4><span id=\"4-sts-b-数据集\"> 4 STS-B 数据集</span></h4>\n<p>STSB (The Semantic Textual Similarity Benchmark，语义文本相似性基准测试)，相似性和释义任务，是从新闻标题、视频标题、图像标题以及自然语言推断数据中提取的句子对的集合，每对都是由人类注释的，其相似性评分为 0-5 (大于等于 0 且小于等于 5 的浮点数，原始 paper 里写的是 1-5，可能是作者失误）。任务就是预测这些相似性得分，本质上是一个回归问题，但是依然可以用分类的方法，可以归类为句子对的文本五分类任务。</p>\n<p>样本个数：训练集 5, 749 个，开发集 1, 379 个，测试集 1, 377 个。</p>\n<p>任务：回归任务，预测为 1-5 之间的相似性得分的浮点数。但是依然可以使用分类的方法，作为五分类。</p>\n<p>评价准则：Pearson and Spearman correlation coefficients。</p>\n<p><strong>文件样式</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- STS-B/</span><br><span class=\"line\">        - dev.tsv</span><br><span class=\"line\">        - test.tsv</span><br><span class=\"line\">        - train.tsv</span><br><span class=\"line\">    - LICENSE.txt</span><br><span class=\"line\">    - readme.txt</span><br><span class=\"line\">    - original/</span><br></pre></td></tr></table></figure>\n<hr>\n<p>文件样式说明:</p>\n<ul>\n<li>在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据.</li>\n</ul>\n<hr>\n<p>train.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   genre   filename    year    old_index   source1 source2 sentence1   sentence2   score</span><br><span class=\"line\">0   main-captions   MSRvid  2012test    0001    none    none    A plane is taking off.  An air plane is taking off. 5.000</span><br><span class=\"line\">1   main-captions   MSRvid  2012test    0004    none    none    A man is playing a large flute. A man is playing a flute.   3.800</span><br><span class=\"line\">2   main-captions   MSRvid  2012test    0005    none    none    A man is spreading shreded cheese on a pizza.   A man is spreading shredded cheese on an uncooked pizza.    3.800</span><br><span class=\"line\">3   main-captions   MSRvid  2012test    0006    none    none    Three men are playing chess.Two men are playing chess.  2.600</span><br><span class=\"line\">4   main-captions   MSRvid  2012test    0009    none    none    A man is playing the cello.A man seated is playing the cello.   4.250</span><br><span class=\"line\">5   main-captions   MSRvid  2012test    0011    none    none    Some men are fighting.  Two men are fighting.   4.250</span><br><span class=\"line\">6   main-captions   MSRvid  2012test    0012    none    none    A man is smoking.   A man is skating.   0.500</span><br><span class=\"line\">7   main-captions   MSRvid  2012test    0013    none    none    The man is playing the piano.   The man is playing the guitar.  1.600</span><br><span class=\"line\">8   main-captions   MSRvid  2012test    0014    none    none    A man is playing on a guitar and singing.   A woman is playing an acoustic guitar and singing.  2.200</span><br><span class=\"line\">9   main-captions   MSRvid  2012test    0016    none    none    A person is throwing a cat on to the ceiling.   A person throws a cat on the ceiling.   5.000</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<hr>\n<p>train.tsv 数据样式说明:</p>\n<ul>\n<li>train.tsv 中的数据内容共分为 10 列，第一列数据是数据索引；第二列代表每对句子的来源，如 main-captions 表示来自字幕；第三列代表来源的具体保存文件名，第四列代表出现时间 (年); 第五列代表原始数据的索引；第六列和第七列分别代表句子对原始来源；第八列和第九列代表相似程度不同的句子对；第十列代表句子对的相似程度由低到高，值域范围是 [0, 5].</li>\n</ul>\n<hr>\n<p>test.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   genre   filename    year    old_index   source1 source2 sentence1   sentence2</span><br><span class=\"line\">0   main-captions   MSRvid  2012test    0024    none    none    A girl is styling her hair. A girl is brushing her hair.</span><br><span class=\"line\">1   main-captions   MSRvid  2012test    0033    none    none    A group of men play soccer on the beach.    A group of boys are playing soccer on the beach.</span><br><span class=\"line\">2   main-captions   MSRvid  2012test    0045    none    none    One woman is measuring another woman<span class=\"string\">&#x27;s ankle.   A woman measures another woman&#x27;</span>s ankle.</span><br><span class=\"line\">3   main-captions   MSRvid  2012test    0063    none    none    A man is cutting up a cucumber. A man is slicing a cucumber.</span><br><span class=\"line\">4   main-captions   MSRvid  2012test    0066    none    none    A man is playing a harp.    A man is playing a keyboard.</span><br><span class=\"line\">5   main-captions   MSRvid  2012test    0074    none    none    A woman is cutting onions.  A woman is cutting tofu.</span><br><span class=\"line\">6   main-captions   MSRvid  2012test    0076    none    none    A man is riding an electric bicycle.    A man is riding a bicycle.</span><br><span class=\"line\">7   main-captions   MSRvid  2012test    0082    none    none    A man is playing the drums. A man is playing the guitar.</span><br><span class=\"line\">8   main-captions   MSRvid  2012test    0092    none    none    A man is playing guitar.    A lady is playing the guitar.</span><br><span class=\"line\">9   main-captions   MSRvid  2012test    0095    none    none    A man is playing a guitar.  A man is playing a trumpet.</span><br><span class=\"line\">10  main-captions   MSRvid  2012test    0096    none    none    A man is playing a guitar.  A man is playing a trumpet.</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<hr>\n<p>test.tsv 数据样式说明:</p>\n<ul>\n<li>test.tsv 中的数据内容共分为 9 列，含义与 train.tsv 前 9 列相同.</li>\n</ul>\n<hr>\n<p>STS-B 数据集的任务类型:</p>\n<ul>\n<li>句子对多分类任务 / 句子对回归任务</li>\n<li>评估指标为: Pearson-Spearman Corr</li>\n</ul>\n<hr>\n<h4><span id=\"5-qqp-数据集\"> 5 QQP 数据集</span></h4>\n<p>QQP (The Quora Question Pairs, Quora 问题对数集)，相似性和释义任务，是社区问答网站 Quora 中问题对的集合。任务是确定一对问题在语义上是否等效。与 MRPC 一样，QQP 也是正负样本不均衡的，不同是的 QQP 负样本占 63%，正样本是 37%，所以我们也是报告准确率和 F1 值。我们使用标准测试集，为此我们从作者那里获得了专用标签。我们观察到测试集与训练集分布不同。</p>\n<p>样本个数：训练集 363, 870 个，开发集 40, 431 个，测试集 390, 965 个。</p>\n<p>任务：判定句子对是否等效，等效、不等效两种情况，二分类任务。</p>\n<p>评价准则：准确率（accuracy）和 F1 值。</p>\n<p><strong>文件样式</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- QQP/</span><br><span class=\"line\">        - dev.tsv</span><br><span class=\"line\">        - original/</span><br><span class=\"line\">        - test.tsv</span><br><span class=\"line\">        - train.tsv</span><br></pre></td></tr></table></figure>\n<hr>\n<p>文件样式说明:</p>\n<ul>\n<li>在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据.</li>\n</ul>\n<hr>\n<p>train.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">id  qid1    qid2    question1   question2   is_duplicate</span><br><span class=\"line\">133273  213221  213222  How is the life of a math student? Could you describe your own experiences?Which level of prepration is enough <span class=\"keyword\">for</span> the exam jlpt5?  0</span><br><span class=\"line\">402555  536040  536041  How <span class=\"keyword\">do</span> I control my horny emotions? How <span class=\"keyword\">do</span> you control your horniness?  1</span><br><span class=\"line\">360472  364011  490273  What causes stool color to change to yellow?    What can cause stool to come out as little balls?   0</span><br><span class=\"line\">150662  155721  7256    What can one <span class=\"keyword\">do</span> after MBBS? What <span class=\"keyword\">do</span> i <span class=\"keyword\">do</span> after my MBBS ?    1</span><br><span class=\"line\">183004  279958  279959  Where can I find a power outlet <span class=\"keyword\">for</span> my laptop at Melbourne Airport? Would a second airport <span class=\"keyword\">in</span> Sydney, Australia be needed <span class=\"keyword\">if</span> a high-speed rail link was created between Melbourne and Sydney?   0</span><br><span class=\"line\">119056  193387  193388  How not to feel guilty since I am Muslim and I<span class=\"string\">&#x27;m conscious we won&#x27;</span>t have sex together?  I don<span class=\"string\">&#x27;t beleive I am bulimic, but I force throw up atleast once a day after I eat something and feel guilty. Should I tell somebody, and if so who? 0</span></span><br><span class=\"line\"><span class=\"string\">356863  422862  96457   How is air traffic controlled?  How do you become an air traffic controller?0</span></span><br><span class=\"line\"><span class=\"string\">106969  147570  787 What is the best self help book you have read? Why? How did it change your life?    What are the top self help books I should read? 1</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br></pre></td></tr></table></figure>\n<hr>\n<p>train.tsv 数据样式说明:</p>\n<ul>\n<li>train.tsv 中的数据内容共分为 6 列，第一列代表文本数据索引；第二列和第三列数据分别代表问题 1 和问题 2 的 id; 第四列和第五列代表需要进行’是否重复’判定的句子对；第六列代表上述问题是 / 不是重复性问题的标签，0 代表不重复，1 代表重复.</li>\n</ul>\n<hr>\n<p>test.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">id  question1   question2</span><br><span class=\"line\">0   Would the idea of Trump and Putin <span class=\"keyword\">in</span> bed together scare you, given the geopolitical implications?   Do you think that <span class=\"keyword\">if</span> Donald Trump were elected President, he would be able to restore relations with Putin and Russia as he said he could, based on the rocky relationship Putin had with Obama and Bush?</span><br><span class=\"line\">1   What are the top ten Consumer-to-Consumer E-commerce online?    What are the top ten Consumer-to-Business E-commerce online?</span><br><span class=\"line\">2   Why don<span class=\"string\">&#x27;t people simply &#x27;</span>Google<span class=\"string\">&#x27; instead of asking questions on Quora?  Why do people ask Quora questions instead of just searching google?</span></span><br><span class=\"line\"><span class=\"string\">3   Is it safe to invest in social trade biz?   Is social trade geniune?</span></span><br><span class=\"line\"><span class=\"string\">4   If the universe is expanding then does matter also expand?  If universe and space is expanding? Does that mean anything that occupies space is also expanding?</span></span><br><span class=\"line\"><span class=\"string\">5   What is the plural of hypothesis?   What is the plural of thesis?</span></span><br><span class=\"line\"><span class=\"string\">6   What is the application form you need for launching a company?  What is the application form you need for launching a company in Austria?</span></span><br><span class=\"line\"><span class=\"string\">7   What is Big Theta? When should I use Big Theta as opposed to big O? Is O(Log n) close to O(n) or O(1)?</span></span><br><span class=\"line\"><span class=\"string\">8   What are the health implications of accidentally eating a small quantity of aluminium foil?What are the implications of not eating vegetables?</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br></pre></td></tr></table></figure>\n<hr>\n<p>test.tsv 数据样式说明:</p>\n<ul>\n<li>test.tsv 中的数据内容共分为 3 列，第一列数据代表每条文本数据的索引；第二列和第三列数据代表用于测试的问题句子对.</li>\n</ul>\n<hr>\n<p>QQP 数据集的任务类型:</p>\n<ul>\n<li>句子对二分类任务</li>\n<li>评估指标为: ACC/F1</li>\n</ul>\n<hr>\n<h4><span id=\"6-mnlisnli-数据集\"> 6 (MNLI/SNLI) 数据集</span></h4>\n<p>MNLI (The Multi-Genre Natural Language Inference Corpus, 多类型自然语言推理数据库)，自然语言推断任务，是通过众包方式对句子对进行文本蕴含标注的集合。给定前提（premise）语句和假设（hypothesis）语句，任务是预测前提语句是否包含假设（蕴含，entailment），与假设矛盾（矛盾，contradiction）或者两者都不（中立，neutral）。前提语句是从数十种不同来源收集的，包括转录的语音，小说和政府报告。</p>\n<p>样本个数：训练集 392, 702 个，开发集 dev-matched 9, 815 个，开发集 dev-mismatched9, 832 个，测试集 test-matched 9, 796 个，测试集 test-dismatched9, 847 个。因为 MNLI 是集合了许多不同领域风格的文本，所以又分为了 matched 和 mismatched 两个版本的数据集，matched 指的是训练集和测试集的数据来源一致，mismached 指的是训练集和测试集来源不一致。</p>\n<p>任务：句子对，一个前提，一个是假设。前提和假设的关系有三种情况：蕴含（entailment），矛盾（contradiction），中立（neutral）。句子对三分类问题。</p>\n<p>评价准则：matched accuracy/mismatched accuracy。</p>\n<p><strong>文件样式</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- (MNLI/SNLI)/</span><br><span class=\"line\">    - dev_matched.tsv</span><br><span class=\"line\">    - dev_mismatched.tsv</span><br><span class=\"line\">    - original/</span><br><span class=\"line\">    - test_matched.tsv</span><br><span class=\"line\">    - test_mismatched.tsv</span><br><span class=\"line\">    - train.tsv</span><br></pre></td></tr></table></figure>\n<hr>\n<p>文件样式说明:</p>\n<ul>\n<li>在使用中常用到的文件是 train.tsv, dev_matched.tsv, dev_mismatched.tsv, test_matched.tsv, test_mismatched.tsv 分别代表训练集，与训练集一同采集的验证集，与训练集不是一同采集验证集，与训练集一同采集的测试集，与训练集不是一同采集测试集。其中 train.tsv 与 dev_matched.tsv 和 dev_mismatched.tsv 数据样式相同，都是带有标签的数据，其中 test_matched.tsv 与 test_mismatched.tsv 数据样式相同，都是不带有标签的数据.</li>\n</ul>\n<hr>\n<p>train.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   promptID    pairID  genre   sentence1_binary_parse  sentence2_binary_parse  sentence1_parse sentence2_parse sentence1   sentence2   label1  gold_label</span><br><span class=\"line\">0   31193   31193n  government  ( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )  ( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )   (ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))  (ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))   Conceptually cream skimming has two basic dimensions - product and geography.   Product and geography are what make cream skimming work.    neutral neutral</span><br><span class=\"line\">1   101457  101457e telephone   ( you ( ( know ( during ( ( ( the season ) and ) ( i guess ) ) ) ) ( at ( at ( ( your level ) ( uh ( you ( ( ( lose them ) ( to ( the ( next level ) ) ) ) ( <span class=\"keyword\">if</span> ( ( <span class=\"keyword\">if</span> ( they ( decide ( to ( recall ( the ( the ( parent team ) ) ) ) ) ) ) ) ( ( the Braves ) ( decide ( to ( call ( to ( ( recall ( a guy ) ) ( from ( ( triple A ) ( ( ( <span class=\"keyword\">then</span> ( ( a ( double ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) and ) ( ( a ( single ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ( You ( ( ( ( lose ( the things ) ) ( to ( the ( following level ) ) ) ) ( <span class=\"keyword\">if</span> ( ( the people ) recall ) ) ) . ) )   (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN during) (NP (NP (DT the) (NN season)) (CC and) (NP (FW i) (FW guess)))) (PP (IN at) (IN at) (NP (NP (PRP$ your) (NN level)) (SBAR (S (INTJ (UH uh)) (NP (PRP you)) (VP (VBP lose) (NP (PRP them)) (PP (TO to) (NP (DT the) (JJ next) (NN level))) (SBAR (IN <span class=\"keyword\">if</span>) (S (SBAR (IN <span class=\"keyword\">if</span>) (S (NP (PRP they)) (VP (VBP decide) (S (VP (TO to) (VP (VB recall) (NP (DT the) (DT the) (NN parent) (NN team)))))))) (NP (DT the) (NNPS Braves)) (VP (VBP decide) (S (VP (TO to) (VP (VB call) (S (VP (TO to) (VP (VB recall) (NP (DT a) (NN guy)) (PP (IN from) (NP (NP (RB triple) (DT A)) (SBAR (S (S (ADVP (RB <span class=\"keyword\">then</span>)) (NP (DT a) (JJ double) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him))))))) (CC and) (S (NP (DT a) (JJ single) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him)))))))))))))))))))))))))))) (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT the) (NNS things)) (PP (TO to) (NP (DT the) (JJ following) (NN level))) (SBAR (IN <span class=\"keyword\">if</span>) (S (NP (DT the) (NNS people)) (VP (VBP recall))))) (. .))) you know during the season and i guess at at your level uh you lose them to the next level <span class=\"keyword\">if</span> <span class=\"keyword\">if</span> they decide to recall the the parent team the Braves decide to call to recall a guy from triple A <span class=\"keyword\">then</span> a double A guy goes up to replace him and a single A guy goes up to replace him You lose the things to the following level <span class=\"keyword\">if</span> the people recall.    entailment  entailment</span><br><span class=\"line\">2   134793  134793e fiction ( ( One ( of ( our number ) ) ) ( ( will ( ( ( carry out ) ( your instructions ) ) minutely ) ) . ) )   ( ( ( A member ) ( of ( my team ) ) ) ( ( will ( ( execute ( your orders ) ) ( with ( immense precision ) ) ) ) . ) )   (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PRP$ our) (NN number)))) (VP (MD will) (VP (VB carry) (PRT (RP out)) (NP (PRP$ your) (NNS instructions)) (ADVP (RB minutely)))) (. .))) (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN of) (NP (PRP$ my) (NN team)))) (VP (MD will) (VP (VB execute) (NP (PRP$ your) (NNS orders)) (PP (IN with) (NP (JJ immense) (NN precision))))) (. .)))  One of our number will carry out your instructions minutely.    A member of my team will execute your orders with immense precision.    entailment  entailment</span><br><span class=\"line\">3   37397   37397e  fiction ( ( How ( ( ( <span class=\"keyword\">do</span> you ) know ) ? ) ) ( ( All this ) ( ( ( is ( their information ) ) again ) . ) ) ) ( ( This information ) ( ( belongs ( to them ) ) . ) )  (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP <span class=\"keyword\">do</span>) (NP (PRP you)) (VP (VB know))) (. ?)) (NP (PDT All) (DT this)) (VP (VBZ is) (NP (PRP$ their) (NN information)) (ADVP (RB again))) (. .)))   (ROOT (S (NP (DT This) (NN information)) (VP (VBZ belongs) (PP (TO to) (NP (PRP them)))) (. .)))    How <span class=\"keyword\">do</span> you know? All this is their information again.   This information belongs to them.   entailment  entailment</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<hr>\n<p>train.tsv 数据样式说明:</p>\n<ul>\n<li>train.tsv 中的数据内容共分为 12 列，第一列代表文本数据索引；第二列和第三列数据分别代表句子对的不同类型 id; 第四列代表句子对的来源；第五列和第六列代表具有句法结构分析的句子对表示；第七列和第八列代表具有句法结构和词性标注的句子对表示，第九列和第十列代表原始的句子对，第十一和第十二列代表不同标准的标注方法产生的标签，在这里，他们始终相同，一共有三种类型的标签，neutral 代表两个句子既不矛盾也不蕴含，entailment 代表两个句子具有蕴含关系，contradiction 代表两个句子观点矛盾.</li>\n</ul>\n<hr>\n<p>test_matched.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   promptID    pairID  genre   sentence1_binary_parse  sentence2_binary_parse  sentence1_parse sentence2_parse sentence1   sentence2</span><br><span class=\"line\">0   31493   31493   travel  ( ( ( ( ( ( ( ( Hierbas , ) ( ans seco ) ) , ) ( ans dulce ) ) , ) and ) frigola ) ( ( ( are just ) ( ( a ( few names ) ) ( worth ( ( keeping ( a look-out ) ) <span class=\"keyword\">for</span> ) ) ) ) . ) )    ( Hierbas ( ( is ( ( a name ) ( worth ( ( looking out ) <span class=\"keyword\">for</span> ) ) ) ) . ) )   (ROOT (S (NP (NP (NNS Hierbas)) (, ,) (NP (NN ans) (NN seco)) (, ,) (NP (NN ans) (NN dulce)) (, ,) (CC and) (NP (NN frigola))) (VP (VBP are) (ADVP (RB just)) (NP (NP (DT a) (JJ few) (NNS names)) (PP (JJ worth) (S (VP (VBG keeping) (NP (DT a) (NN look-out)) (PP (IN <span class=\"keyword\">for</span>))))))) (. .))) (ROOT (S (NP (NNS Hierbas)) (VP (VBZ is) (NP (NP (DT a) (NN name)) (PP (JJ worth) (S (VP (VBG looking) (PRT (RP out)) (PP (IN <span class=\"keyword\">for</span>))))))) (. .)))    Hierbas, ans seco, ans dulce, and frigola are just a few names worth keeping a look-out <span class=\"keyword\">for</span>.    Hierbas is a name worth looking out <span class=\"keyword\">for</span>.</span><br><span class=\"line\">1   92164   92164   government  ( ( ( The extent ) ( of ( the ( behavioral effects ) ) ) ) ( ( would ( ( depend ( <span class=\"keyword\">in</span> ( part ( on ( ( the structure ) ( of ( ( ( the ( individual ( account program ) ) ) and ) ( any limits ) ) ) ) ) ) ) ) ( on ( accessing ( the funds ) ) ) ) ) . ) )    ( ( Many people ) ( ( would ( be ( very ( unhappy ( to ( ( loose control ) ( over ( their ( own money ) ) ) ) ) ) ) ) ) . ) )   (ROOT (S (NP (NP (DT The) (NN extent)) (PP (IN of) (NP (DT the) (JJ behavioral) (NNS effects)))) (VP (MD would) (VP (VB depend) (PP (IN <span class=\"keyword\">in</span>) (NP (NP (NN part)) (PP (IN on) (NP (NP (DT the) (NN structure)) (PP (IN of) (NP (NP (DT the) (JJ individual) (NN account) (NN program)) (CC and) (NP (DT any) (NNS limits)))))))) (PP (IN on) (S (VP (VBG accessing) (NP (DT the) (NNS funds))))))) (. .))) (ROOT (S (NP (JJ Many) (NNS people)) (VP (MD would) (VP (VB be) (ADJP (RB very) (JJ unhappy) (PP (TO to) (NP (NP (JJ loose) (NN control)) (PP (IN over) (NP (PRP$ their) (JJ own) (NN money)))))))) (. .))) The extent of the behavioral effects would depend <span class=\"keyword\">in</span> part on the structure of the individual account program and any limits on accessing the funds. Many people would be very unhappy to loose control over their own money.</span><br><span class=\"line\">2   9662    9662    government  ( ( ( Timely access ) ( to information ) ) ( ( is ( <span class=\"keyword\">in</span> ( ( the ( best interests ) ) ( of ( ( ( both GAO ) and ) ( the agencies ) ) ) ) ) ) . ) )    ( It ( ( ( is ( <span class=\"keyword\">in</span> ( ( everyone <span class=\"string\">&#x27;s ) ( best interest ) ) ) ) ( to ( ( have access ) ( to ( information ( in ( a ( timely manner ) ) ) ) ) ) ) ) . ) )   (ROOT (S (NP (NP (JJ Timely) (NN access)) (PP (TO to) (NP (NN information)))) (VP (VBZ is) (PP (IN in) (NP (NP (DT the) (JJS best) (NNS interests)) (PP (IN of) (NP (NP (DT both) (NNP GAO)) (CC and) (NP (DT the) (NNS agencies))))))) (. .))) (ROOT (S (NP (PRP It)) (VP (VBZ is) (PP (IN in) (NP (NP (NN everyone) (POS &#x27;</span>s)) (JJS best) (NN interest))) (S (VP (TO to) (VP (VB have) (NP (NN access)) (PP (TO to) (NP (NP (NN information)) (PP (IN <span class=\"keyword\">in</span>) (NP (DT a) (JJ timely) (NN manner))))))))) (. .)))   Timely access to information is <span class=\"keyword\">in</span> the best interests of both GAO and the agencies. It is <span class=\"keyword\">in</span> everyone<span class=\"string\">&#x27;s best interest to have access to information in a timely manner.</span></span><br><span class=\"line\"><span class=\"string\">3   5991    5991    travel  ( ( Based ( in ( ( the ( Auvergnat ( spa town ) ) ) ( of Vichy ) ) ) ) ( , ( ( the ( French government ) ) ( often ( ( ( ( proved ( more zealous ) ) ( than ( its masters ) ) ) ( in ( ( ( suppressing ( civil liberties ) ) and ) ( ( drawing up ) ( anti-Jewish legislation ) ) ) ) ) . ) ) ) ) ) ( ( The ( French government ) ) ( ( passed ( ( anti-Jewish laws ) ( aimed ( at ( helping ( the Nazi ) ) ) ) ) ) . ) )   (ROOT (S (PP (VBN Based) (PP (IN in) (NP (NP (DT the) (NNP Auvergnat) (NN spa) (NN town)) (PP (IN of) (NP (NNP Vichy)))))) (, ,) (NP (DT the) (JJ French) (NN government)) (ADVP (RB often)) (VP (VBD proved) (NP (JJR more) (NNS zealous)) (PP (IN than) (NP (PRP$ its) (NNS masters))) (PP (IN in) (S (VP (VP (VBG suppressing) (NP (JJ civil) (NNS liberties))) (CC and) (VP (VBG drawing) (PRT (RP up)) (NP (JJ anti-Jewish) (NN legislation))))))) (. .))) (ROOT (S (NP (DT The) (JJ French) (NN government)) (VP (VBD passed) (NP (NP (JJ anti-Jewish) (NNS laws)) (VP (VBN aimed) (PP (IN at) (S (VP (VBG helping) (NP (DT the) (JJ Nazi)))))))) (. .))) Based in the Auvergnat spa town of Vichy, the French government often proved more zealous than its masters in suppressing civil liberties and drawing up anti-Jewish legislation.   The French government passed anti-Jewish laws aimed at helping the Nazi.</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br></pre></td></tr></table></figure>\n<hr>\n<p>test_matched.tsv 数据样式说明:</p>\n<ul>\n<li>test_matched.tsv 中的数据内容共分为 10 列，与 train.tsv 的前 10 列含义相同.</li>\n</ul>\n<hr>\n<p>(MNLI/SNLI) 数据集的任务类型:</p>\n<ul>\n<li>句子对多分类任务</li>\n<li>评估指标为: ACC</li>\n</ul>\n<hr>\n<h4><span id=\"7-qnlirtewnli-数据集\"> 7 (QNLI/RTE/WNLI) 数据集</span></h4>\n<p>QNLI (Qusetion-answering NLI，问答自然语言推断)，自然语言推断任务。QNLI 是从另一个数据集 The Stanford Question Answering Dataset (斯坦福问答数据集，SQuAD 1.0)[<a href=\"https://zhuanlan.zhihu.com/p/135283598#ref_3\">3]</a> 转换而来的。SQuAD 1.0 是有一个问题 - 段落对组成的问答数据集，其中段落来自维基百科，段落中的一个句子包含问题的答案。这里可以看到有个要素，来自维基百科的段落，问题，段落中的一个句子包含问题的答案。通过将问题和上下文（即维基百科段落）中的每一句话进行组合，并过滤掉词汇重叠比较低的句子对就得到了 QNLI 中的句子对。相比原始 SQuAD 任务，消除了模型选择准确答案的要求；也消除了简化的假设，即答案适中在输入中并且词汇重叠是可靠的提示。</p>\n<p>样本个数：训练集 104, 743 个，开发集 5, 463 个，测试集 5, 461 个。</p>\n<p>任务：判断问题（question）和句子（sentence，维基百科段落中的一句）是否蕴含，蕴含和不蕴含，二分类。</p>\n<p>评价准则：准确率（accuracy）。</p>\n<p>RTE (The Recognizing Textual Entailment datasets，识别文本蕴含数据集)，自然语言推断任务，它是将一系列的年度文本蕴含挑战赛的数据集进行整合合并而来的，包含 RTE1 [<a href=\"https://zhuanlan.zhihu.com/p/135283598#ref_4\">4]</a>，RTE2，RTE3[<a href=\"https://zhuanlan.zhihu.com/p/135283598#ref_5\">5]</a>，RTE5 等，这些数据样本都从新闻和维基百科构建而来。将这些所有数据转换为二分类，对于三分类的数据，为了保持一致性，将中立（neutral）和矛盾（contradiction）转换为不蕴含（not entailment）。</p>\n<p>样本个数：训练集 2, 491 个，开发集 277 个，测试集 3, 000 个。</p>\n<p>任务：判断句子对是否蕴含，句子 1 和句子 2 是否互为蕴含，二分类任务。</p>\n<p>评价准则：准确率（accuracy）。</p>\n<p>WNLI (Winograd NLI，Winograd 自然语言推断)，自然语言推断任务，数据集来自于竞赛数据的转换。Winograd Schema Challenge [<a href=\"https://zhuanlan.zhihu.com/p/135283598#ref_6\">6]</a>，该竞赛是一项阅读理解任务，其中系统必须读一个带有代词的句子，并从列表中找到代词的指代对象。这些样本都是都是手动创建的，以挫败简单的统计方法：每个样本都取决于句子中单个单词或短语提供的上下文信息。为了将问题转换成句子对分类，方法是通过用每个可能的列表中的每个可能的指代去替换原始句子中的代词。任务是预测两个句子对是否有关（蕴含、不蕴含）。训练集两个类别是均衡的，测试集是不均衡的，65% 是不蕴含。</p>\n<p>样本个数：训练集 635 个，开发集 71 个，测试集 146 个。</p>\n<p>任务：判断句子对是否相关，蕴含和不蕴含，二分类任务。</p>\n<p>评价准则：准确率（accuracy）。</p>\n<p><strong>文件样式</strong></p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">* QNLI, RTE, WNLI三个数据集的样式基本相同.</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- (QNLI/RTE/WNLI)/</span><br><span class=\"line\">        - dev.tsv</span><br><span class=\"line\">        - test.tsv</span><br><span class=\"line\">        - train.tsv</span><br></pre></td></tr></table></figure>\n<hr>\n<p>文件样式说明:</p>\n<ul>\n<li>在使用中常用到的文件是 train.tsv, dev.tsv, test.tsv, 分别代表训练集，验证集和测试集。其中 train.tsv 与 dev.tsv 数据样式相同，都是带有标签的数据，其中 test.tsv 是不带有标签的数据.</li>\n</ul>\n<hr>\n<p>QNLI 中的 train.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   question    sentence    label</span><br><span class=\"line\">0   When did the third Digimon series begin?    Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who <span class=\"keyword\">do</span> not reincarnate after their deaths and more complex character development <span class=\"keyword\">in</span> the original Japanese. not_entailment</span><br><span class=\"line\">1   Which missile batteries often have individual launchers several kilometres from one another?    When MANPADS is operated by specialists, batteries may have several dozen teams deploying separately <span class=\"keyword\">in</span> small sections; self-propelled air defence guns may deploy <span class=\"keyword\">in</span> pairs.    not_entailment</span><br><span class=\"line\">2   What two things does Popper argue Tarski<span class=\"string\">&#x27;s theory involves in an evaluation of truth?   He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.   entailment</span></span><br><span class=\"line\"><span class=\"string\">3   What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians?    On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854.  entailment</span></span><br><span class=\"line\"><span class=\"string\">4   What famous palace is located in London?    London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret&#x27;</span>s Church; and the historic settlement of Greenwich (<span class=\"keyword\">in</span> <span class=\"built_in\">which</span> the Royal Observatory, Greenwich marks the Prime Meridian, 0° longitude, and GMT).  not_entailment</span><br><span class=\"line\">5   When is the term <span class=\"string\">&#x27;German dialects&#x27;</span> used <span class=\"keyword\">in</span> regard to the German language?   When talking about the German language, the term German dialects is only used <span class=\"keyword\">for</span> the traditional regional varieties.   entailment</span><br><span class=\"line\">6   What was the name of the island the English traded to the Dutch <span class=\"keyword\">in</span> <span class=\"built_in\">return</span> <span class=\"keyword\">for</span> New Amsterdam?    At the end of the Second Anglo-Dutch War, the English gained New Amsterdam (New York) <span class=\"keyword\">in</span> North America <span class=\"keyword\">in</span> exchange <span class=\"keyword\">for</span> Dutch control of Run, an Indonesian island.  entailment</span><br><span class=\"line\">7   How were the Portuguese expelled from Myanmar?  From the 1720s onward, the kingdom was beset with repeated Meithei raids into Upper Myanmar and a nagging rebellion <span class=\"keyword\">in</span> Lan Na.  not_entailment</span><br><span class=\"line\">8   What does the word <span class=\"string\">&#x27;customer&#x27;</span> properly apply to?    The bill also required rotation of principal maintenance inspectors and stipulated that the word <span class=\"string\">&quot;customer&quot;</span> properly applies to the flying public, not those entities regulated by the FAA. entailment</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<hr>\n<p>RTE 中的 train.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   sentence1   sentence2   label</span><br><span class=\"line\">0   No Weapons of Mass Destruction Found <span class=\"keyword\">in</span> Iraq Yet.   Weapons of Mass Destruction Found <span class=\"keyword\">in</span> Iraq.  not_entailment</span><br><span class=\"line\">1   A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered <span class=\"keyword\">in</span> downtown Chicago to mark the installation of new Pope Benedict XVI.Pope Benedict XVI is the new leader of the Roman Catholic Church. entailment</span><br><span class=\"line\">2   Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug <span class=\"keyword\">for</span> more breast cancer patients.  Herceptin can be used to treat breast cancer.   entailment</span><br><span class=\"line\">3   Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute <span class=\"keyword\">in</span> Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment.   The previous name of Ho Chi Minh City was Saigon.entailment</span><br><span class=\"line\">4   A man is due <span class=\"keyword\">in</span> court later charged with the murder 26 years ago of a teenager whose <span class=\"keyword\">case</span> was the first to be featured on BBC One<span class=\"string\">&#x27;s Crimewatch. Colette Aram, 16, was walking to her boyfriend&#x27;</span>s house <span class=\"keyword\">in</span> Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found <span class=\"keyword\">in</span> a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later.  Paul Stewart Hutchinson is accused of having stabbed a girl.    not_entailment</span><br><span class=\"line\">5   Britain said, Friday, that it has barred cleric, Omar Bakri, from returning to the country from Lebanon, <span class=\"built_in\">where</span> he was released by police after being detained <span class=\"keyword\">for</span> 24 hours. Bakri was briefly detained, but was released.   entailment</span><br><span class=\"line\">6   Nearly 4 million children who have at least one parent who entered the U.S. illegally were born <span class=\"keyword\">in</span> the United States and are U.S. citizens as a result, according to the study conducted by the Pew Hispanic Center. That<span class=\"string\">&#x27;s about three quarters of the estimated 5.5 million children of illegal immigrants inside the United States, according to the study. About 1.8 million children of undocumented immigrants live in poverty, the study found.  Three quarters of U.S. illegal immigrants have children.    not_entailment</span></span><br><span class=\"line\"><span class=\"string\">7   Like the United States, U.N. officials are also dismayed that Aristide killed a conference called by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the feuding parties together.  Aristide had Prime Minister Robert Malval  murdered in Port-au-Prince.  not_entailment</span></span><br><span class=\"line\"><span class=\"string\">8   WASHINGTON --  A newly declassified narrative of the Bush administration&#x27;</span>s advice to the CIA on harsh interrogations shows that the small group of Justice Department lawyers who wrote memos authorizing controversial interrogation techniques were operating not on their own but with direction from top administration officials, including then-Vice President Dick Cheney and national security adviser Condoleezza Rice. At the same time, the narrative suggests that then-Defense Secretary Donald H. Rumsfeld and then-Secretary of State Colin Powell were largely left out of the decision-making process. Dick Cheney was the Vice President of Bush. entailment</span><br></pre></td></tr></table></figure>\n<hr>\n<p>WNLI 中的 train.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   sentence1   sentence2   label</span><br><span class=\"line\">0   I stuck a pin through a carrot. When I pulled the pin out, it had a hole.   The carrot had a hole.  1</span><br><span class=\"line\">1   John couldn<span class=\"string\">&#x27;t see the stage with Billy in front of him because he is so short.  John is so short.   1</span></span><br><span class=\"line\"><span class=\"string\">2   The police arrested all of the gang members. They were trying to stop the drug trade in the neighborhood.   The police were trying to stop the drug trade in the neighborhood.  1</span></span><br><span class=\"line\"><span class=\"string\">3   Steve follows Fred&#x27;</span>s example <span class=\"keyword\">in</span> everything. He influences him hugely.   Steve influences him hugely.    0</span><br><span class=\"line\">4   When Tatyana reached the cabin, her mother was sleeping. She was careful not to disturb her, undressing and climbing back into her berth.   mother was careful not to disturb her, undressing and climbing back into her berth. 0</span><br><span class=\"line\">5   George got free tickets to the play, but he gave them to Eric, because he was particularly eager to see it. George was particularly eager to see it.    0</span><br><span class=\"line\">6   John was jogging through the park when he saw a man juggling watermelons. He was very impressive.   John was very impressive.   0</span><br><span class=\"line\">7   I couldn<span class=\"string\">&#x27;t put the pot on the shelf because it was too tall.    The pot was too tall.   1</span></span><br><span class=\"line\"><span class=\"string\">8   We had hoped to place copies of our newsletter on all the chairs in the auditorium, but there were simply not enough of them.   There were simply not enough copies of the newsletter.  1</span></span><br></pre></td></tr></table></figure>\n<hr>\n<p>(QNLI/RTE/WNLI) 中的 train.tsv 数据样式说明:</p>\n<ul>\n<li>train.tsv 中的数据内容共分为 4 列，第一列代表文本数据索引；第二列和第三列数据代表需要进行’是否蕴含’判定的句子对；第四列数据代表两个句子是否具有蕴含关系，0/not_entailment 代表不是蕴含关系，1/entailment 代表蕴含关系.</li>\n</ul>\n<hr>\n<p>QNLI 中的 test.tsv 数据样式:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   question    sentence</span><br><span class=\"line\">0   What organization is devoted to Jihad against Israel?   For some decades prior to the First Palestine Intifada <span class=\"keyword\">in</span> 1987, the Muslim Brotherhood <span class=\"keyword\">in</span> Palestine took a <span class=\"string\">&quot;quiescent&quot;</span> stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel<span class=\"string\">&#x27;s &quot;indulgence&quot; to build up a network of mosques and charitable organizations.</span></span><br><span class=\"line\"><span class=\"string\">1   In what century was the Yarrow-Schlick-Tweedy balancing system used?    In the late 19th century, the Yarrow-Schlick-Tweedy balancing &#x27;</span>system<span class=\"string\">&#x27; was used on some marine triple expansion engines.</span></span><br><span class=\"line\"><span class=\"string\">2   The largest brand of what store in the UK is located in Kingston Park?  Close to Newcastle, the largest indoor shopping centre in Europe, the MetroCentre, is located in Gateshead.</span></span><br><span class=\"line\"><span class=\"string\">3   What does the IPCC rely on for research?    In principle, this means that any significant new evidence or events that change our understanding of climate science between this deadline and publication of an IPCC report cannot be included.</span></span><br><span class=\"line\"><span class=\"string\">4   What is the principle about relating spin and space variables?  Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive.</span></span><br><span class=\"line\"><span class=\"string\">5   Which network broadcasted Super Bowl 50 in the U.S.?    CBS broadcast Super Bowl 50 in the U.S., and charged an average of $5 million for a 30-second commercial during the game.</span></span><br><span class=\"line\"><span class=\"string\">6   What did the museum acquire from the Royal College of Science?  To link this to the rest of the museum, a new entrance building was constructed on the site of the former boiler house, the intended site of the Spiral, between 1978 and 1982.</span></span><br><span class=\"line\"><span class=\"string\">7   What is the name of the old north branch of the Rhine?  From Wijk bij Duurstede, the old north branch of the Rhine is called Kromme Rijn (&quot;Bent Rhine&quot;) past Utrecht, first Leidse Rijn (&quot;Rhine of Leiden&quot;) and then, Oude Rijn (&quot;Old Rhine&quot;).</span></span><br><span class=\"line\"><span class=\"string\">8   What was one of Luther&#x27;</span>s most personal writings?    It remains <span class=\"keyword\">in</span> use today, along with Luther<span class=\"string\">&#x27;s hymns and his translation of the Bible.</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br></pre></td></tr></table></figure>\n<hr>\n<ul>\n<li>(RTE/WNLI) 中的 test.tsv 数据样式:</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index   sentence1   sentence2</span><br><span class=\"line\">0   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came <span class=\"keyword\">in</span> sight.    Horses ran away when Maude and Dora came <span class=\"keyword\">in</span> sight.</span><br><span class=\"line\">1   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came <span class=\"keyword\">in</span> sight.    Horses ran away when the trains came <span class=\"keyword\">in</span> sight.</span><br><span class=\"line\">2   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came <span class=\"keyword\">in</span> sight.    Horses ran away when the puffs came <span class=\"keyword\">in</span> sight.</span><br><span class=\"line\">3   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came <span class=\"keyword\">in</span> sight.    Horses ran away when the roars came <span class=\"keyword\">in</span> sight.</span><br><span class=\"line\">4   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came <span class=\"keyword\">in</span> sight.    Horses ran away when the whistles came <span class=\"keyword\">in</span> sight.</span><br><span class=\"line\">5   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they came <span class=\"keyword\">in</span> sight.    Horses ran away when the horses came <span class=\"keyword\">in</span> sight.</span><br><span class=\"line\">6   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming.   Maude and Dora saw a train coming.</span><br><span class=\"line\">7   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming.   The trains saw a train coming.</span><br><span class=\"line\">8   Maude and Dora had seen the trains rushing across the prairie, with long, rolling puffs of black smoke streaming back from the engine. Their roars and their wild, clear whistles could be heard from far away. Horses ran away when they saw a train coming.   The puffs saw a train coming.</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>(QNLI/RTE/WNLI) 中的 test.tsv 数据样式说明:</p>\n<ul>\n<li>test.tsv 中的数据内容共分为 3 列，第一列数据代表每条文本数据的索引；第二列和第三列数据代表需要进行’是否蕴含’判定的句子对.</li>\n</ul>\n<hr>\n<p>(QNLI/RTE/WNLI) 数据集的任务类型:</p>\n<ul>\n<li>句子对二分类任务</li>\n<li>评估指标为: ACC</li>\n</ul>\n<hr>\n",
            "tags": [
                "人工智能",
                "NLP"
            ]
        }
    ]
}