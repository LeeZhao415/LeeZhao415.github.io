{
    "version": "https://jsonfeed.org/version/1",
    "title": "且听风吟，御剑于心！ • All posts by \"模型性能指标\" tag",
    "description": "",
    "home_page_url": "https://leezhao415.github.io",
    "items": [
        {
            "id": "https://leezhao415.github.io/2022/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E9%87%8F-FLOPs%E8%AE%A1%E7%AE%97/",
            "url": "https://leezhao415.github.io/2022/02/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E9%87%8F-FLOPs%E8%AE%A1%E7%AE%97/",
            "title": "深度学习模型参数量_FLOPs计算",
            "date_published": "2022-02-20T15:06:30.000Z",
            "content_html": "<meta name=\"referrer\" content=\"no-referrer\">\n<hr>\n<p><strong>文章目录</strong></p>\n<!-- toc -->\n<ul>\n<li><a href=\"#1%E5%AF%B9cnn%E8%80%8C%E8%A8%80%E6%AF%8F%E4%B8%AA%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F%E8%AE%A1%E7%AE%97%E5%A6%82%E4%B8%8B\">1. 对 CNN 而言，每个卷积层的参数量计算如下：</a></li>\n<li><a href=\"#2%E5%AF%B9cnn%E8%80%8C%E8%A8%80%E6%AF%8F%E4%B8%AA%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E8%BF%90%E7%AE%97%E9%87%8F%E8%AE%A1%E7%AE%97%E5%A6%82%E4%B8%8B\">2. 对 CNN 而言，每个卷积层的运算量计算如下：</a></li>\n<li><a href=\"#3%E5%AF%B9%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E8%80%8C%E8%A8%80%E5%85%B6%E5%8F%82%E6%95%B0%E9%87%8F%E9%9D%9E%E5%B8%B8%E5%AE%B9%E6%98%93%E8%AE%A1%E7%AE%97\">3. 对全连接层而言，其参数量非常容易计算：</a></li>\n<li><a href=\"#4%E5%AF%B9%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E8%80%8C%E8%A8%80%E5%85%B6%E8%BF%90%E7%AE%97%E9%87%8F%E8%AE%A1%E7%AE%97%E5%A6%82%E4%B8%8B\">4. 对全连接层而言，其运算量计算如下：</a></li>\n</ul>\n<!-- tocstop -->\n<hr>\n<p>本文是对卷积神经网络模型参数量和浮点运算量的计算推导公式和方法，使用 API 自动计算这些数据请移步另一篇博客：<a href=\"https://www.jianshu.com/p/ca6da98b2ecd\">自动计算模型参数量、FLOPs、乘加数以及所需内存等数据</a></p>\n<h4><span id=\"1-对-cnn-而言每个卷积层的参数量计算如下\"> 1. 对 CNN 而言，每个卷积层的参数量计算如下：</span></h4>\n<center><img src=\"https://math.jianshu.com/math?formula=params%20%3D%20C_o%20%C3%97%20(k_w%20%C3%97%20k_h%20%C3%97%20C_i%20%2B1)\" alt=\"params = C_o × (k_w × k_h × C_i +1)\"></center>\n<p>其中<img src=\"https://math.jianshu.com/math?formula=C_o\" alt=\"C_o\"> 表示输出通道数，<img src=\"https://math.jianshu.com/math?formula=C_i\" alt=\"C_i\"> 表示输入通道数，<img src=\"https://math.jianshu.com/math?formula=k_w\" alt=\"k_w\"> 表示卷积核宽，<img src=\"https://math.jianshu.com/math?formula=k_h\" alt=\"k_h\"> 表示卷积核高。<br>\n括号内的<img src=\"https://math.jianshu.com/math?formula=w%20%C3%97%20h%20%C3%97%20C_i\" alt=\"w × h × C_i\"> 表示一个卷积核的权重数量，+1 表示 bias，括号表示一个卷积核的参数量，<img src=\"https://math.jianshu.com/math?formula=C_o%20%C3%97\" alt=\"C_o ×\"> 表示该层有<img src=\"https://math.jianshu.com/math?formula=C_o\" alt=\"C_o\"> 个卷积核。</p>\n<p>若卷积核是方形的，即<img src=\"https://math.jianshu.com/math?formula=k_w%20%3D%20k_h%20%3D%20k\" alt=\"k_w = k_h = k\">，则上式变为：</p>\n <center><img src=\"https://math.jianshu.com/math?formula=params%20%3D%20C_o%20%C3%97%20(k%5E2%20%C3%97%20C_i%20%2B1)\" alt=\"params = C_o × (k^2 × C_i +1)\"></center>\n<p>需要注意的是，使用 Batch Normalization 时不需要 bias，此时计算式中的 + 1 项去除。</p>\n<h4><span id=\"2-对-cnn-而言每个卷积层的运算量计算如下\"> 2. 对 CNN 而言，每个卷积层的运算量计算如下：</span></h4>\n<center><img src=\"https://math.jianshu.com/math?formula=FLOPs%20%3D%20%5B(C_i%20%C3%97%20k_w%20%C3%97%20k_h)%20%2B%20(C_i%20%C3%97%20k_w%20%C3%97%20k_h%20-%201)%20%2B%201%5D%20%C3%97%20C_o%20%C3%97%20W%20%C3%97%20H\" alt=\"FLOPs = [(C_i × k_w × k_h) + (C_i × k_w × k_h - 1) + 1] × C_o × W × H\"></center>\n<p>FLOPs 是英文 floating point operations 的缩写，表示<strong>浮点运算量</strong>，中括号内的值表示卷积操作计算出 feature map 中一个点所需要的运算量（乘法和加法），<img src=\"https://math.jianshu.com/math?formula=C_i%20%C3%97%20k_w%20%C3%97%20k_h\" alt=\"C_i × k_w × k_h\"> 表示一次卷积操作中的乘法运算量，<img src=\"https://math.jianshu.com/math?formula=C_i%20%C3%97%20k_w%20%C3%97%20k_h%20-%201\" alt=\"C_i × k_w × k_h - 1\"> 表示一次卷积操作中的加法运算量，+ 1 表示 bias，W 和 H 分别表示 feature map 的长和宽，<img src=\"https://math.jianshu.com/math?formula=%C3%97%20C_o%20%C3%97%20W%20%C3%97%20H\" alt=\"× C_o × W × H\"> 表示 feature map 的所有元素数。<br>\n若是方形卷积核，即<img src=\"https://math.jianshu.com/math?formula=k_w%20%3D%20k_h%20%3D%20k\" alt=\"k_w = k_h = k\">，则有：</p>\n <center><img src=\"https://math.jianshu.com/math?formula=FLOPs%20%3D%202%20%C3%97%20C_i%20%C3%97%20k%5E2%20%C3%97%20C_o%20%C3%97%20W%20%C3%97%20H\" alt=\"FLOPs = 2 × C_i × k^2 × C_o × W × H\"></center>\n<p>上面是乘运算和加运算的总和，将一次乘运算或加运算都视作一次浮点运算。<br>\n在计算机视觉论文中，常常将一个‘乘 - 加’组合视为一次浮点运算，英文表述为’Multi-Add’，运算量正好是上面的算法减半，此时的运算量为：</p>\n <center><img src=\"https://math.jianshu.com/math?formula=FLOPs%20%3D%20C_i%20%C3%97%20k%5E2%20%C3%97%20C_o%20%C3%97%20W%20%C3%97%20H\" alt=\"FLOPs = C_i × k^2 × C_o × W × H\"></center>\n<h4><span id=\"3-对全连接层而言其参数量非常容易计算\"> 3. 对全连接层而言，其参数量非常容易计算：</span></h4>\n<center><img src=\"https://math.jianshu.com/math?formula=params%20%3D%20(I%20%2B%201)%20%C3%97%20O%20%3D%20I%C3%97O%20%2B%20O\" alt=\"params = (I + 1) × O = I×O + O\"></center>\n<p>值得注意的是，最初由 feature map flatten 而来的向量视为第一层全连接层，即此处的<img src=\"https://math.jianshu.com/math?formula=I\" alt=\"I\">。<br>\n可以这样理解上式：每一个输出神经元连接着所有输入神经元，所以有<img src=\"https://math.jianshu.com/math?formula=I\" alt=\"I\"> 个权重，每个输出神经元还要加一个 bias。<br>\n也可以这样理解：每一层神经元 (O 这一层) 的权重数为<img src=\"https://math.jianshu.com/math?formula=I%C3%97O\" alt=\"I×O\">，bias 数量为<em> O</em>。</p>\n<h4><span id=\"4-对全连接层而言其运算量计算如下\"> 4. 对全连接层而言，其运算量计算如下：</span></h4>\n<center><img src=\"https://math.jianshu.com/math?formula=FLOPs%20%3D%20%5BI%20%2B%20(I-1)%20%2B1%5D%C3%97O%20%3D%20(2%20%C3%97%20I)%20%C3%97%20O\" alt=\"FLOPs = [I + (I-1) +1]×O = (2 × I) × O\"></center>\n<p>其中 <img src=\"https://math.jianshu.com/math?formula=I%20%3D%20input%5C%20nerons%2C%20O%20%3D%20output%5C%20nerons\" alt=\"I = input\\ nerons, O = output\\ nerons\"><br>\n 中括号的值表示计算出一个神经元所需的运算量，第一个<img src=\"https://math.jianshu.com/math?formula=I\" alt=\"I\"> 表示乘法运算量，<img src=\"https://math.jianshu.com/math?formula=I-1\" alt=\"I-1\"> 表示加法运算量，+1 表示 bias，<img src=\"https://math.jianshu.com/math?formula=%C3%97O\" alt=\"×O\"> 表示计算 O 个神经元的值。</p>\n",
            "tags": [
                "人工智能",
                "模型性能指标"
            ]
        }
    ]
}