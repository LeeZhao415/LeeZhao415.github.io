<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://leezhao415.github.io</id>
    <title>且听风吟，御剑于心！ • Posts by &#34;vslam&#34; tag</title>
    <link href="https://leezhao415.github.io" />
    <updated>2022-03-10T13:35:51.000Z</updated>
    <category term="人工智能/CV" />
    <category term="Transformer/DETR(CV)" />
    <category term="人工智能" />
    <category term="数据集" />
    <category term="大数据框架" />
    <category term="编程工具" />
    <category term="NLP" />
    <category term="模型部署" />
    <category term="数据结构与算法" />
    <category term="Python数据分析" />
    <category term="网络通信" />
    <category term="YOLOX" />
    <category term="CV算法" />
    <category term="VSLAM" />
    <category term="NCNN部署" />
    <category term="YOLOX目标检测" />
    <category term="多模态" />
    <category term="目标跟踪" />
    <category term="目标检测（人脸检测）" />
    <category term="深度学习" />
    <category term="CV未来" />
    <category term="NLP-BERT" />
    <category term="且读文摘" />
    <category term="自然语言处理NLP" />
    <category term="IOU" />
    <category term="OpenCV之DNN模块" />
    <category term="深度模型" />
    <category term="NLP-模型优化" />
    <category term="激活函数" />
    <category term="梯度更新" />
    <category term="人脸识别" />
    <category term="概述" />
    <category term="名人名言" />
    <category term="寒窑赋" />
    <category term="度量学习" />
    <category term="NLP/评估指标" />
    <category term="机器学习/损失函数" />
    <category term="智能家居" />
    <category term="机器学习" />
    <category term="CV/目标检测工具箱" />
    <category term="科研项目成果" />
    <category term="模型性能指标" />
    <category term="计算机顶会" />
    <category term="表面缺陷检测" />
    <category term="计算机视觉CV" />
    <category term="网络编程" />
    <category term="NLP/数据增强工具" />
    <category term="计算机视觉" />
    <category term="模型优化" />
    <category term="三维建模" />
    <category term="计算机视觉库" />
    <category term="深度学习环境配置" />
    <category term="多任务学习模型" />
    <category term="知识蒸馏" />
    <category term="数据库原理" />
    <category term="算法" />
    <category term="操作系统" />
    <category term="深度模型（目标检测）" />
    <category term="视频理解" />
    <category term="ReID" />
    <category term="MOT" />
    <category term="AIGC前沿" />
    <category term="NLP-发展史" />
    <category term="编程语言" />
    <category term="CV数据集" />
    <category term="Linux" />
    <category term="PaddlePaddle" />
    <entry>
        <id>https://leezhao415.github.io/2022/03/10/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91VSLAM%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/</id>
        <title>【精华】VSLAM技术综述</title>
        <link rel="alternate" href="https://leezhao415.github.io/2022/03/10/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91VSLAM%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/"/>
        <content type="html">&lt;meta name=&#34;referrer&#34; content=&#34;no-referrer&#34;&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;文章目录&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#1-%E4%BB%80%E4%B9%88%E6%98%AF%E8%A7%86%E8%A7%89slam&#34;&gt;1 什么是视觉 SLAM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-vslam%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB&#34;&gt;2 VSLAM 算法分类&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#1%E9%97%B4%E6%8E%A5%E6%B3%95%E5%8F%8A%E5%85%B6%E5%85%B8%E5%9E%8B%E7%B3%BB%E7%BB%9F&#34;&gt;（1）间接法及其典型系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2%E7%9B%B4%E6%8E%A5%E6%B3%95%E5%8F%8A%E5%85%B6%E5%85%B8%E5%9E%8B%E7%B3%BB%E7%BB%9F&#34;&gt;（2）直接法及其典型系统&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-%E6%80%BB%E7%BB%93&#34;&gt;3 总结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
&lt;hr&gt;
&lt;p&gt;随着 ORB-SLAM 的崛起，视觉 SLAM 技术掀起了一波热潮，今天小白带着小伙伴一起学习视觉 SLAM 经典的解决方案。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;1-什么是视觉-slam&#34;&gt; 1 什么是视觉 SLAM&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;SLAM 是 “Simultaneous Localization And Mapping” 的缩写，可译为同步定位与建图。概率 SLAM 问题 (the probabilistic SLAM problem) 起源于 1986 年的 IEEE Robotics and Automation Conference 大会上，研究人员希望能将估计理论方法 (estimation-theoretic methods) 应用在构图和定位问题中。 SLAM 最早被应用在机器人领域，其目标是在没有任何先验知识的情况下，根据传感器数据实时构建周围环境地图，同时根据这个地图推测自身的定位。&lt;/p&gt;
&lt;p&gt;假设机器人携带传感器 (相机) 在未知环境中运动，为方便起见，把一段连续时间的运动变成离散时刻 t=1，…k，而在这些时刻，用 x 表示机器人的自身位置，则各时刻的位置就记为 x1，x2…xk，它构成了机器人的轨迹。地图方面，假设地图由许多个路标组成，而每个时刻，传感器会测量到一部分路标点，得到它们的观测数据。设路标点共有 N 个，用 y1，y2…yn 表示。通过运动测量 u 和传感器读数 z 来求解定位问题 (估计 x) 和建图问题 (估计 y)。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s8.51cto.com/images/blog/202107/29/9e930d6b379bc81df2cfefc0cdb0103f.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;只利用相机作为外部感知传感器的 SLAM 称为视觉 SLAM (vSLAM)。相机具有视觉信息丰富、硬件成本低等优点，经典的 vSLAM 系统一般包含前端视觉里程计、 后端优化、 闭环检测和构图四个主要部分。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s7.51cto.com/images/blog/202107/29/2be47cb0ee887e3e6b63f1711e20a794.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_02&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;● 视觉里程计 (Visual Odometry)：仅有视觉输入的姿态估计；&lt;/p&gt;
&lt;p&gt;● 后端优化 (Optimization): 后端接受不同时刻视觉里程计测量的相机位姿，以及闭环检测的信息，对它们进行优化，得到全局一致的轨迹和地图 [5]；&lt;/p&gt;
&lt;p&gt;● 闭环检测 (Loop Closing): 指机器人在地图构建过程中，通过视觉等传感器信息检测是否发生了轨迹闭环，即判断自身是否进入历史同一地点 [6];&lt;/p&gt;
&lt;p&gt;● 建图 (Mapping): 根据估计的轨迹，建立与任务要求对应的地图 [5]。&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;2-vslam-算法分类&#34;&gt; 2 VSLAM 算法分类&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;根据生成方法的不同，SLAM 可以分成两大类：间接方法和直接方法&lt;/p&gt;
&lt;h5&gt;&lt;span id=&#34;1间接法及其典型系统&#34;&gt; （1）间接法及其典型系统&lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;间接法首先对测量数据进行预处理来产生中间层，通过稀疏的特征点提取和匹配来实现的，也可以采用稠密规则的光流，或者提取直线或曲线特征来实现。然后计算出地图点坐标或光流向量等几何量，因此间接法优化的是几何误差：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s3.51cto.com/images/blog/202107/29/f47ca6d0787ab03dbad2e921887ce4b1.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_03&#34; style=&#34;zoom: 25%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;其中 ui 为 Ik-1 中任意像素点，它投影到空间点的坐标为 Pi， u′i 是 Pi 投影到 Ik 上的坐标。之后利用中间层的数值来估计周围环境的三维模型和相机运动。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s5.51cto.com/images/blog/202107/29/93b9a6766f7185f25040f322af7f91bd.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_04&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;h6&gt;&lt;span id=&#34;1gt-monoslam&#34;&gt; 1&amp;gt; MonoSLAM&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;MonoSLAM 是第一个实时的单目视觉 SLAM 系统 [7]。 MonoSLAM 以 EKF (扩展卡尔曼滤波) 为后端，追踪前端稀疏的特征点，以相机的当前状态和所有路标点为状态量，更新其均值和协方差。在 EKF 中，每个特征点的位置服从高斯分布，可以用一个椭球表示它的均值和不确定性，它们在某个方向上越长，说明在该方向上越不稳定。 该方法的缺点：场景窄、路标数有限、稀疏特征点易丢失等。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s2.51cto.com/images/blog/202107/29/28dc022b704dc9604084b9ed8b90140c.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_05&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;h6&gt;&lt;span id=&#34;2gt-ptam&#34;&gt; 2&amp;gt; PTAM&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;PTAM [8] 提出并实现了跟踪和建图的并行化，首次区分出前后端 (跟踪需要实时响应图像数据，地图优化放在后端进行)，后续许多视觉 SLAM 系统设计也采取了类似的方法。PTAM 是第一个使用非线性优化作为后端的方案，而不是滤波器的后端方案。提出了关键帧 (keyframes) 机制，即不用精细处理每一幅图像，而是把几个关键图像串起来优化其轨迹和地图。该方法的缺点是：场景小、跟踪容易丢失。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s5.51cto.com/images/blog/202107/29/056374056f2d69d2120a93785c588f17.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_06&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;h6&gt;&lt;span id=&#34;3gt-orb-slam-orb_slam2&#34;&gt; 3&amp;gt; ORB-SLAM (ORB_SLAM2)&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;ORB-SLAM [9] 围绕 ORB 特征计算，包括视觉里程计与回环检测的 ORB 字典。ORB 特征计算效率比 SIFT 或 SURF 高，又具有良好的旋转和缩放不变性。ORB-SLAM 创新地使用了三个线程完成 SLAM，三个线程是：实时跟踪特征点的 Tracking 线程，局部 Bundle Adjustment 的优化线程和全局 Pose Graph 的回环检测与优化线程。该方法的缺点：每幅图像都计算一遍 ORB 特征非常耗时，三线程结构给 CPU 带来了较重负担。稀疏特征点地图只能满足定位需求，无法提供导航、避障等功能。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s7.51cto.com/images/blog/202107/29/4d63756f6f63ded35f24d4eed7b2183f.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_07&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;ORB-SLAM2 [10] 基于单目的 ORB-SLAM 做了如下贡献：第一个用于单目、双目和 RGB-D 的开源 SLAM 系统，包括闭环，重定位和地图重用；RGB-D 结果显示，通过使用 bundle adjustment，比基于迭代最近点 (ICP) 或者光度和深度误差最小化的最先进方法获得更高的精度；通过使用近距离和远距离的立体点和单目观察结果，立体效果比最先进的直接立体 SLAM 更准确；轻量级的本地化模式，当建图不可用时，可以有效地重新使用地图。&lt;/p&gt;
&lt;h5&gt;&lt;span id=&#34;2直接法及其典型系统&#34;&gt; （2）直接法及其典型系统&lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;直接法跳过预处理步骤直接使用实际传感器测量值，例如在特定时间内从某个方向接收的光，如下图所示。在被动视觉的情况下，由于相机提供光度测量，因此直接法优化的是光度误差：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s4.51cto.com/images/blog/202107/29/58e9d9ae882690ff6e489aa6f0aaf6fb.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_08&#34; style=&#34;zoom: 33%;&#34;&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&#34;https://s3.51cto.com/images/blog/202107/29/0f55571be0181780fca4d2d81e3645ca.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_09&#34; style=&#34;zoom:33%;&#34;&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&#34;https://s3.51cto.com/images/blog/202107/29/a33e09938b52d1dcc4ef81735cf7712e.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_10&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;h6&gt;&lt;span id=&#34;1gt-dtam&#34;&gt; 1&amp;gt; DTAM&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;DTAM [11] 是单目 VSLAM 系统，是一种直接稠密的方法，通过最小化全局空间规范能量函数来计算关键帧构建稠密深度图，而相机的位姿则使用深度地图通过直接图像匹配来计算得到。对特征缺失、 图像模糊有很好的鲁棒性。该方法的缺点是：计算量非常大，需要 GPU 并行计算。 DTAM 假设光度恒定，对全局照明处理不够鲁棒。&lt;/p&gt;
&lt;h6&gt;&lt;span id=&#34;2gt-lsd-slam&#34;&gt; 2&amp;gt; LSD-SLAM&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;LSD-SLAM [12] 建了一个大尺度直接单目 SLAM 的框架，提出了一种用来直接估计关键帧之间相似变换、尺度感知的图像匹配算法，在 CPU 上实现了半稠密场景的重建。该方法的缺点：对相机内参敏感和曝光敏感，相机快速运动时容易丢失，依然需要特征点进行回环检测。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s3.51cto.com/images/blog/202107/29/113b0b61808e7a64bcc034c0f6989f6a.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_11&#34;&gt;&lt;/center&gt;
&lt;h6&gt;&lt;span id=&#34;3gt-svo&#34;&gt; 3&amp;gt; SVO&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;SVO 是一种半直接法的视觉里程计，它是特征点和直接法的混合使用：跟踪了一些角点，然后像直接法那样，根据关键点周围信息估计相机运动及位置。由于不需要计算大量描述子，因此速度极快，在消费级笔记本电脑上可以达到每秒 300 帧，在无人机上可以达到每秒 55 帧。该方法的缺点是：舍弃了后端优化和回环检测，位姿估计存在累积误差，丢失后重定位困难。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s3.51cto.com/images/blog/202107/29/7fc1778d4c435fd0c7c3f04558a220cc.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_12&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;h6&gt;&lt;span id=&#34;4gt-dso&#34;&gt; 4&amp;gt; DSO&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;DSO 是基于高度精确的稀疏直接结构和运动公式的视觉里程计的方法。不考虑几何先验信息，能够直接优化光度误差。并且考虑了光度标定模型，其优化范围不是所有帧，而是由最近帧及其前几帧形成的滑动窗口，并且保持这个窗口有 7 个关键帧。DSO 中除了完善直接法位姿估计的误差模型外，还加入了仿射亮度变换、 光度标定、 深度优化等。该方法没有回环检测。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s3.51cto.com/images/blog/202107/29/ce84242bda9b58b50cb19936fd8ef114.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_13&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;h6&gt;&lt;span id=&#34;5gt-基于深度学习的-slam&#34;&gt; 5&amp;gt; 基于深度学习的 SLAM&lt;/span&gt;&lt;/h6&gt;
&lt;p&gt;传统的视觉 SLAM 在环境的适应性方面依然存在瓶颈，深度学习有望在这方面发挥较大的作用。目前，深度学习已经在语义地图、重定位、回环检测、特征点提取与匹配以及端到端的视觉里程计等问题上有了相关工作，下面列举一些典型成果：&lt;/p&gt;
&lt;p&gt;●CNN-SLAM [17] 在 LSD-SLAM [12] 基础上将深度估计以及图像匹配改为基于卷积神经网络的方法，并且可以融合语义信息，得到了较鲁棒的效果；&lt;/p&gt;
&lt;p&gt;● 剑桥大学开发的 PoseNet [18]，是在 GoogleNet [19] 的基础上将 6 自由度位姿作为回归问题进行的网络改进，可以利用单张图片得到对应的相机位姿；&lt;/p&gt;
&lt;p&gt;● 《视觉 SLAM 十四讲》[5] 一书的作者高翔，利用深度神经网络而不是常见的视觉特征来学习原始数据的特征，实现了基于深度网络的回环检测 [20]；&lt;/p&gt;
&lt;p&gt;● LIFT [21] 利用深度神经网络学习图像中的特征点，相比于 SIFT [22] 匹配度更高，其流程图如下图所示：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s7.51cto.com/images/blog/202107/29/6c2f019a8a2140424d1a5c6d7e5db643.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_14&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;LIFT (Learned Invariant Feature Transform) 由三个部分组成：Detector，Orientation Estimator 和 Descriptor。每一个部分都基于 CNN 实现，作者用 Spatial Transformers 将它们联系起来，并用 soft argmax 函数替代了传统的非局部最大值抑制，保证了端到端的可微性。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s2.51cto.com/images/blog/202107/29/834dcb13745181287c227fb3ba0d7f84.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_15&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;孪生训练架构（Siamese Network）包含四个分支，其中 P1 和 P2 对应同一个点的不同视角，作为训练 Descriptor 的正样本，P3 代表不同的 3D 点，作为 Descriptor 的负样本；P4 不包含特征点，仅作为训练 Detector 的负样本。由大 P，Detector，softargmax 和 Spatial Transformer layer Crop 共同得到的小 p 反馈到 Orientation Estimator，Orientation Estimator 和 Spatial Transformer layer Rot 提供 pθ 给 Descriptor，得到最终的描述向量 d。作者给出了 LIFT 与 SIFT 特征匹配的效果对比。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s9.51cto.com/images/blog/202107/29/d682f3582473880344c8cf2b5fbb00f5.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_16&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;特征匹配对比图中左列为 SIFT 匹配结果，右列为 LIFT。绿色线条代表正确匹配，红色圆圈代表描述子区域，可以看到，LIFT 得到了比 SIFT 更稠密的匹配效果。&lt;/p&gt;
&lt;p&gt;UnDeepVO [23] 能够通过使用深度神经网络估计单目相机的 6 自由度位姿及其视野内的深度，整体系统框架概图见下图。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s3.51cto.com/images/blog/202107/29/a1c47e2036fc75798931a4863f80fdc0.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_17&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;p&gt;UnDeepVO 有两个显著的特点：一个是采用了无监督深度学习机制，另一个是能够恢复绝对尺度。UnDeepVO 在训练过程中使用双目图像恢复尺度，但是在测试过程中只使用连续的单目图像。该文的主要贡献包括以下几点：&lt;/p&gt;
&lt;p&gt;1. 通过空间和时间几何约束，用无监督的方式恢复了单目视觉里程计的绝对尺度；&lt;/p&gt;
&lt;p&gt;2. 利用训练过程中的双目图像对，不仅估计了姿态还估计了稠密的带有绝对尺度的深度图；&lt;/p&gt;
&lt;p&gt;3. 在 KITTI 数据集上评价了该系统， UnDeepVO 对于单目相机有良好的位姿估计结果。&lt;/p&gt;
&lt;p&gt;UnDeepVO 由位姿估计和深度估计构成，两个估计系统均把单目连续图像作为输入，分别以带有尺度的 6 自由度位姿和深度作为输出。对于位姿估计器，它是基于 VGG 的 CNN 架构。 它把两个连续的单目图像作为输入，并预测它们之间的 6 自由度变换。 由于旋转（由欧拉角表示）具有高非线性，因此与平移相比通常难以训练。 在有监督训练中，一种常用的方法是将旋转损失作为一种归一化方式给予更大的权重。 为了使用无监督学习更好地训练旋转，作者在最后一个卷积层之后用两组独立的全连接层解耦平移和旋转。 这使得作者能够引入一个权重来标准化旋转和平移的预测，以获得更好的性能。对于深度估计器，它基于 encoder-decoder 结构来生成稠密的深度图。 与其他深度估计方法不同的是，该方法从网络中产生视差图像（深度的倒数），UnDeepVO 的深度估计器可以直接预测深度图，以这种方式训练整个系统更容易收敛。系统结构图如下图所示。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;https://s6.51cto.com/images/blog/202107/29/0ac23c7e266764116eba015435b7382a.jpeg?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=&#34; alt=&#34;vSLAM技术综述_vSLAM_18&#34; style=&#34;zoom:80%;&#34;&gt;&lt;/center&gt;
&lt;h4&gt;&lt;span id=&#34;3-总结&#34;&gt; 3 总结&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;本文介绍了基于传统算法和深度学习的代表性 SLAM 方法。当前的 SLAM 算法在复杂的机器人运动和环境中很容易失效 (例如：机器人的快速运动， 高度动态性的环境)，通常不能面对严格的性能要求，例如，用于快速闭环控制的高速率估计。大多数的 SLAM 没有自由主动地收集数据，行动方案不够高效，并且，目前 vSLAM 方案中所采用的图像特征的语义级别太低，造成特征的可区别性太弱。因此，今后的视觉 SLAM 将向着主动 SLAM、语义 SLAM 以及与其它传感器（例如 IMU）融合的方向发展。&lt;/p&gt;
&lt;p&gt;参考文献&lt;/p&gt;
&lt;p&gt;[1] Durrant-Whyte, H, and Bailey, Tim. ”Simultaneous Localization and Mapping: Part I.” IEEE Robotics &amp;amp; Amp Amp Automation Magazine 13.2(2006):99 - 110.&lt;/p&gt;
&lt;p&gt;[2] Fuentes-Pacheco, Jorge, J. Ruiz-Ascencio, and J. M. Rendón-Mancha. ”Visual simultaneous localization and mapping: a survey.” Artifcial Intelligence Review 43.1(2015):55-81.&lt;/p&gt;
&lt;p&gt;[3] 陈常，朱华，由韶泽。基于视觉的同时定位与地图构建的研究进展 [J/OL]. 计算机应用研究，2018,(03):1-9 (2017-08-18).&lt;/p&gt;
&lt;p&gt;[4] Nister, D, O. Naroditsky, and J. Bergen. ”Visual odometry.” Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on IEEE, 2004:I-652-I-659 Vol.1.&lt;/p&gt;
&lt;p&gt;[5] 高翔。视觉 SLAM 十四讲 [M]. 北京：电子工业出版社，2017.&lt;/p&gt;
&lt;p&gt;[6] 赵洋等. ” 基于深度学习的视觉 SLAM 综述.” 机器人 39.6 (2017):889-896.&lt;/p&gt;
&lt;p&gt;[7] Davison, Andrew J., et al. ”MonoSLAM: Real-time single camera SLAM.” IEEE transactions on pattern analysis and machine intelligence 29.6 (2007): 1052-1067.&lt;/p&gt;
&lt;p&gt;[8] Klein, Georg, and David Murray. ”Parallel tracking and mapping for small AR workspaces.” Mixed and Augmented Reality, 2007. ISMAR 2007. 6th IEEE and ACM International Symposium on. IEEE, 2007.&lt;/p&gt;
&lt;p&gt;[9] Mur-Artal, Raúl, J. M. M. Montiel, and J. D. Tardós. ”ORB-SLAM: A Versatile and Accurate Monocular SLAM System.” IEEE Transactions on Robotics 31.5(2015):1147-1163.&lt;/p&gt;
&lt;p&gt;[10] Mur-Artal, Raul, and Juan D. Tardós. ”Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras.” IEEE Transactions on Robotics 33.5 (2017): 1255-1262.&lt;/p&gt;
&lt;p&gt;[11] Newcombe, Richard A, S. J. Lovegrove, and A. J. Davison. ”DTAM: Dense tracking and mapping in real-time.” International Conference on Computer Vision IEEE Computer Society, 2011:2320-2327.&lt;/p&gt;
&lt;p&gt;[12] Engel, Jakob, T. Schöps, and D. Cremers. ”LSD-SLAM: Large-Scale Direct Monocular SLAM.” 8690(2014):834-849.&lt;/p&gt;
&lt;p&gt;[13] Forster, Christian, M. Pizzoli, and D. Scaramuzza. ”SVO: Fast semi-direct monocular visual odometry.” IEEE International Conference on Robotics and Automation IEEE, 2014:15-22.&lt;/p&gt;
&lt;p&gt;[14] Engel, Jakob, V. Koltun, and D. Cremers. ”Direct Sparse Odometry.” IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence PP.99(2016):1-1.&lt;/p&gt;
&lt;p&gt;[15] Cadena, Cesar, et al. ”Past, Present, and Future of Simultaneous Localization and Mapping:Toward the Robust-Perception Age.” IEEE Transactions on Robotics 32.6(2016):1309-1332.&lt;/p&gt;
&lt;p&gt;[16] 吕霖华。基于视觉的即时定位与地图重建 (V-SLAM) 综述 [J]. 中国战略新兴产业，2017 (4).&lt;/p&gt;
&lt;p&gt;[17] Tateno K, Tombari F, Laina I, et al. CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017, 2.&lt;/p&gt;
&lt;p&gt;[18] Kendall A, Grimes M, Cipolla R. Posenet: A convolutional network for real-time 6-dof camera relocalization[C]//Proceedings of the IEEE international conference on computer vision. 2015: 2938-2946.&lt;/p&gt;
&lt;p&gt;[19] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.&lt;/p&gt;
&lt;p&gt;[20] Gao X, Zhang T. Loop closure detection for visual slam systems using deep neural networks[C]//Control Conference (CCC), 2015 34th Chinese. IEEE, 2015: 5851-5856.&lt;/p&gt;
&lt;p&gt;[21] Yi K M, Trulls E, Lepetit V, et al. Lift: Learned invariant feature transform[C]//European Conference on Computer Vision. Springer, Cham, 2016: 467-483.&lt;/p&gt;
&lt;p&gt;[22] Lowe D G. Distinctive image features from scale-invariant keypoints[J]. International journal of computer vision, 2004, 60(2): 91-110.&lt;/p&gt;
&lt;p&gt;[23] Li R, Wang S, Long Z, et al. Undeepvo: Monocular visual odometry through unsupervised deep learning[C]//2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018: 7286-7291.&lt;/p&gt;
</content>
        <category term="人工智能" />
        <category term="VSLAM" />
        <updated>2022-03-10T13:35:51.000Z</updated>
    </entry>
    <entry>
        <id>https://leezhao415.github.io/2022/03/10/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91VSLAM%E5%8F%8A3D%E8%A7%86%E8%A7%89%E8%AF%A6%E8%A7%A3/</id>
        <title>【精华】VSLAM及3D视觉详解</title>
        <link rel="alternate" href="https://leezhao415.github.io/2022/03/10/%E3%80%90%E7%B2%BE%E5%8D%8E%E3%80%91VSLAM%E5%8F%8A3D%E8%A7%86%E8%A7%89%E8%AF%A6%E8%A7%A3/"/>
        <content type="html">&lt;meta name=&#34;referrer&#34; content=&#34;no-referrer&#34;&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;文章目录&lt;/strong&gt;&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%B8%80-vslam%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93&#34;&gt;一 VSLAM 课程总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%BA%8C-3d%E8%A7%86%E8%A7%89%E7%B2%BE%E5%8D%8E%E6%80%BB%E7%BB%93&#34;&gt;二 3D 视觉精华总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#%E4%B8%89-3d%E8%A7%86%E8%A7%89%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A&#34;&gt;三 3D 视觉从入门到精通&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- tocstop --&gt;
&lt;hr&gt;
&lt;h4&gt;&lt;span id=&#34;一-vslam-课程总结&#34;&gt; 一 VSLAM 课程总结&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/0-mb6M6HvKU3Uau0a4taTw&#34;&gt;国内首个面向自动驾驶领域的多传感器融合标定系统学习课程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/L3YkFH7JSAhM4rDam7mdWg&#34;&gt;视觉 - 惯性 SLAM 入门与实践教程（基于 VINS-Fusion）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/9gtdrloSWR2kJlYk5CnWIQ&#34;&gt;彻底搞透视觉三维重建：原理剖析、代码讲解、及优化改进&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/POWVOCbTxAEp7z2FZ3mdVQ&#34;&gt;彻底搞懂基于 LOAM 框架的 3D 激光 SLAM: 源码剖析到算法优化 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/uJ34xjWjAFCzDDduoo0ixA&#34;&gt;三维点云从入门到精通系统视频学习课程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/Un0-2L-FU2QTmW0zez8RTg&#34;&gt;彻底剖析室内、室外激光 SLAM 关键算法原理、代码和实战 (cartographer+LOAM+LIO-SAM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/WLyuE3ByntGA-Y0w8GZ29A&#34;&gt;国内首个基于结构光投影三维重建系列视频课程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://app0s6nfqrg6303.h5.xiaoeknow.com/v1/course/column/p_6092922ee4b09134c98de375?type=3&#34;&gt;相机标定从入门到精通：基本原理与实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://app0s6nfqrg6303.h5.xiaoeknow.com/v1/course/column/p_60a87bcee4b0c7264217f91b?type=3&#34;&gt;机械臂位姿估计与抓取（基本原理 + 代码实战）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/dgc25DwLqJJlnq_d29qeZg&#34;&gt;从零搭建一套结构光 3D 重建系统理论 + 源码 + 实践]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/dNMm3XnIVuyo2ANS9eVQ1A&#34;&gt;彻底剖析激光 - 视觉 - IMU-GPS 融合 SLAM 算法：理论推导、代码讲解和实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/xCtq4nI9Jd5HyVIW1bCKTg&#34;&gt;彻底搞懂视觉 - 惯性 SLAM：基于 VINS-Fusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/9gtdrloSWR2kJlYk5CnWIQ&#34;&gt;视觉三维重建的关键技术及实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/LVx1SkxYXWQvv7UeFToXwQ&#34;&gt;ROS2 从入门到精通：理论与实战&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/tXdADZnApwHpb63NFEHYqg&#34;&gt;四旋翼飞行器：算法与实战&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;span id=&#34;二-3d-视觉精华总结&#34;&gt; 二 3D 视觉精华总结&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;1、&lt;a href=&#34;https://github.com/qxiaofan/awsome-3D-Computer-Vision-Resources&#34;&gt;总结 | VSLAM、点云后处理、相机标定、深度学习等边角知识&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2、&lt;a href=&#34;https://github.com/qxiaofan/awesome_3d_restruction&#34;&gt;三维重建&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3、&lt;a href=&#34;https://github.com/qxiaofan/awesome_3d_vision_company_summary&#34;&gt;3D 视觉相关公司汇总&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;5、&lt;a href=&#34;https://github.com/qxiaofan/awesome_PointCloud_process&#34;&gt;点云后处理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;6、&lt;a href=&#34;https://github.com/qxiaofan/awesome_3d_conference_journals&#34;&gt;3D 视觉会议期刊&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;7、&lt;a href=&#34;https://github.com/qxiaofan/awesome-Elas-demo&#34;&gt;Elas 立体匹配算法&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;8、&lt;a href=&#34;https://github.com/qxiaofan/awesome-fisheye-pinhole-camera-calibration&#34;&gt;相机标定 demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;9、&lt;a href=&#34;https://github.com/qxiaofan/awesome-computer-vision-papers-daily&#34;&gt;3D 视觉优质文章汇总&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;10、&lt;a href=&#34;https://github.com/qxiaofan/awesome-3D-Computer-Vision-From-0-To-1&#34;&gt;3D 视觉入门到精通精华汇总&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;11、&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzU1MjY4MTA1MQ==&amp;amp;mid=100060043&amp;amp;idx=2&amp;amp;sn=2a02843a04d195151cb40a76c445d9fa&amp;amp;chksm=7bfc00bf4c8b89a90ceca84a2d373feaef19b70c456e0a69e3b3b84cb42667809adb8fbfb0b3&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0310yHya33Mg3TwjV9pcaLxG&amp;amp;sharer_sharetime=1615342114815&amp;amp;sharer_shareid=08a5efa40af25b6a57bd07cf52cdcd42&amp;amp;exportkey=Aw%2BXvOW4NlMCf3hnC45NN4o%3D&amp;amp;pass_ticket=Agt2XaSAqcO2duzuIhjnDnI%2Brv%2FxL7dvwHyuwQ2A0Ut4leBcK6M0EA3ApzhYe9XH&amp;amp;wx_header=0#rd&#34;&gt;计算机视觉系统学习必读书籍&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span id=&#34;三-3d-视觉从入门到精通&#34;&gt; 三 3D 视觉从入门到精通&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;开源代码（涉及立体匹配、结构光、缺陷检测、点云、三维重建、位姿估计、深度不全、）&lt;/li&gt;
&lt;li&gt;vip 系列视频课程 + 课件（三维重建、自动驾驶、结构光、ORB-SLAM3、点云、机械臂抓取等系列视频课程）&lt;/li&gt;
&lt;li&gt;3D 视觉优质 github 资源汇总&lt;/li&gt;
&lt;li&gt;3D 视觉学习路线&lt;/li&gt;
&lt;li&gt;3D 视觉顶会 | 期刊&lt;/li&gt;
&lt;li&gt;往期帖子与问答汇总（涉及超过近 3000 + 干货帖子与精彩问答）&lt;/li&gt;
&lt;li&gt;汇总 | 项目对接与招聘信息（包括 SLAM、点云、结构光、相机标定、三维重建等项目对接与求职信息）&lt;/li&gt;
&lt;li&gt;公众号原创文章汇总&lt;/li&gt;
&lt;li&gt;工具与课件（涉及编程与 GPU 优化及项目中的编译优化方法）&lt;/li&gt;
&lt;li&gt;论文阅读（主要为星主、合伙人及各位嘉宾分享的行业优质论文）&lt;/li&gt;
&lt;/ul&gt;
</content>
        <category term="人工智能" />
        <category term="VSLAM" />
        <updated>2022-03-10T13:35:34.000Z</updated>
    </entry>
</feed>
